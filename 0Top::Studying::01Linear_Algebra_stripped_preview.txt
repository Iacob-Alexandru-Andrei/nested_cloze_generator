=== Stripped Clozes Preview (No Changes in Anki) ===

  

    <ol><li>Given a {{c3::lin indp}} list we can {{c1::extend}} it to a basis</li><li>Given a {{c5::{{c4::spaning}}}} list we can {{c5::{{c2::reduce}}}} it to a basis</li></ol>

============================================================

  

    What is a linear map?<br><br>Definition: linear map<br>A linear map from V to W is a function T: V -&gt; W with the following<br>properties:<br>• {{c5::additivity}}: {{c1::T(u1 + u2)}} = {{c2::Tu1 + Tu2}} for all u1, u2 € V;<br>• {{c5::homogeneity}}: {{c3::T(b*u)}} = {{c4::b*(Tu)}} for all b € F and all u € V.

============================================================

  

    What are the <b>three</b> algebraic properties of the products of linear maps?<br><ul><li>{{c5::{{c2::Associativity: {{c1::\(&nbsp; (T_1T_2)T_3\) =&nbsp;\(T_1(T_2T_3)\)}}::\(&nbsp; (T_1T_2)T_3\) =&nbsp;\(T_1(T_2T_3)\)}}}}</li><li>{{c4::{{c4::Identity:&nbsp;{{c3::\(TI = IT = T\)}}::\(TI = IT = T\)}}}}</li><li>Distributivity:&nbsp;{{c4::\(( S_1 + S_2) T = S_1 T + S_2 T\)}} and&nbsp;\( S(T_1 + T_2) = ST_1 + ST_2\)</li></ul>

============================================================

  

    A vector space is a ({{c4::group}} theory):<br><ul><li>{{c1::Additive}}<br></li><li>{{c5::{{c2::Abelian}}}}</li><li>{{c5::{{c3::Group}}}}</li></ul>

============================================================

  

    <ul><li>The product of a {{c3::{{c3::{{c1::matrix and a column vector}}::matrix and a column vector}}}} is a {{c3::{{c3::{{c2::linear combination of the matrix columns}}::linear combination of the matrix columns}}}}</li><li>The product of a {{c3::{{c1::row vector and a matrix}}}} is a {{c3::{{c2::linear combination of the matrix rows}}}}</li></ul>

============================================================

  

    <br>A linear map \( T \in L(V, W) \) is called {{c4::invertible}} if there exists {{c5::a linear<br>map \( S \in L(W, V) \)}}&nbsp;such that {{c5::{{c3::{{c1::ST equals the identity map on V}}::ST equals the identity map on V}} and {{c5::{{c2::TS<br>equals the identity map on W.}}}}}}

============================================================

  

    Definition: {{c5::{{c1::inverse}}}}<br>A linear map S € L(W, V) satisfying {{c4::{{c3::ST = I}}}} and {{c4::{{c3::TS = I}}}} is called an<br>{{c1::inverse}} of T (note that {{c4::{{c3:: the first / is {{c2::the identity map on V}}::the identity map on V}} and the<br>second I is the {{c4::{{c2::identity map on W}}}})}}.

============================================================

  

    A linear map is {{c5::{{c4::invertible}}}} if and only if it is {{c4::{{c3::{{c1::injective}}::injective}} and {{c4::{{c2::surjective}}}}}}

============================================================

  

    &nbsp;Two vector spaces are called {{c3::{{c1::isomorphic}}}} if there is an<br>{{c3::{{c2::isomorphism}}}} from {{c3::{{c3::one vector space onto the other one.}}}}

============================================================

  

    <div>{{c5::{{c1::L(V,W)}}}} and&nbsp;{{c4::{{c2::\(F^{m,n}\)}}}} are {{c4::{{c3::isomorphic}}}}</div>

============================================================

  

    • A linear map from {{c2::a vector space}} to {{c2::itself}} is called an {{c1::operator}}.<br>• The notation {{c5::{{c3::L (V)}}}} denotes {{c5::{{c4::the set of all {{c2::operators}}::operators}} on V}}. In other words, {{c5::{{c3::L(V)}}}} = {{c5::{{c4::L(V, V)}}}}.

============================================================

  

    For {{c5::{{c2::operators}}}}, {{c4::{{c3::{{c1::injectivity}}::injectivity}}}} is equivalent to {{c4::{{c4::{{c1::surjectivity}}::surjectivity}}}} in {{c5::{{c2::finite}}}} dimensions

============================================================

  

    Suppose V is finite-dimensional and T € L(V). Then the following are {{c5::{{c1::equivalent}}}}:<br>• T is {{c4::{{c2::invertible}}}};<br>• T is {{c4::{{c3::injective}}}};<br>• T is {{c4::{{c4::surjective}}}}.

============================================================

  

    L(V,W) =&nbsp;{{c5::{{c4::{{c1::\(V^\prime&nbsp; \)}}::\(V^\prime&nbsp; \)}} {{c4::{{c2::\(\otimes\)&nbsp;}}}}}}{{c4::{{c3::\(W\)}}}}

============================================================

  

    L(V,W) is a vector space because:<br><ul><li>{{c5::{{c3::{{c1::O(v)}}::O(v)}}}} = {{c5::{{c3::{{c2::0}}::0}}}}</li><li>{{c4::{{c4::{{c1::(T+S)(v)}}::(T+S)(v)}}}} = {{c4::{{c4::{{c2::T(v) + S(v)}}::T(v) + S(v)}}}}</li><li>{{c4::{{c1::\(T(\lambda v) \)}}}} =&nbsp;{{c4::{{c2::\(\lambda T(v)\)}}}}<br></li></ul>

============================================================

  

    Linear maps can be {{c5::{{c1::composed}}}} when {{c4::{{c3::their domains and codomains match up}}}} because {{c4::{{c2::they are functions}}}}

============================================================

  

    For non-operators, L(V,W):<br><ul><li>{{c1::\(I_{lhs}\)&nbsp;}}{{c2::\(\neq\)}}&nbsp;{{c5::{{c3::\(I_{rhs}\)}}}}<br></li><li>\(I_{lhs}\)&nbsp;\(\in\)&nbsp;\(W\) : \( (I_{lhs} \cdot f )(v) \) = {{c5::{{c4::\(I_{lhs} (fv) = I_{lhs} (w) = w\)}}}}<br></li></ul>

============================================================

  

    Composition {{c5::{{c4::{{c1::distributes}}::distributes}} over {{c4::{{c2::linear map addition}}}}}}<br><ul><li>{{c4::{{c3::\(E \cdot (T+S)\)}}}} =&nbsp;{{c5::{{c4::\(E \cdot T + E \cdot S\)}}}}<br></li><li>{{c4::{{c3::\((T+S) \cdot E\)}}}} =&nbsp;{{c5::{{c4::\(T \cdot E + S \cdot E\)}}}}<br></li></ul>

============================================================

  

    Proof null space must be zero for a linear map to be injective:<br><ul><li>If v in {{c5::{{c5::{{c1::\(v \in null T\)}}::\(v \in null T\)}} {{c4::{{c4::{{c2::T(v) = 0 = f(0)}}::T(v) = 0 = f(0)}} {{c4::{{c3::implies injectivity because 0 must equal v}}}}}}}}</li></ul>

============================================================

  

    If&nbsp;\(null T = 0\)&nbsp; then T must be inj beause if:<br><ul><li>{{c5::{{c3::f(u) = f(v)}}}} implies {{c4::{{c2::0 = f(u) - f(v) = f(u-v) = 0}}}}&nbsp; \(null (T) = 0\) implies&nbsp;{{c4::{{c1::\(u= v = 0\)}}}}</li></ul>

============================================================

  

    Proof&nbsp; range(T) is a subspace of W:<br><ul><li>{{c3::{{c1::\(0 \in range(T) \)}}::\(0 \in range(T) \)}} because&nbsp;{{c2::\(T(0) = 0\)}}<br></li><li>{{c5::{{c4::\(Tv_1 + Tv_2 \) =&nbsp; \( T(v_1+v_2) \) = \( w_1 + w_2\) = \(w_3\)}}}}<br></li><li>{{c5::{{c5::aT(v) = T(av) = aw}}}}</li></ul>

============================================================

  

    The {{c1::Rank-nullity}} Theorem:<br><ul><li>{{c5::dim(null(T)) + dim(range(T))}} = {{c5::dim(V)}}</li><li>If V is {{c3::finite-dimensional}} and T {{c4::\(\in L(V,W)\)}}<br></li><li>range(T) is a {{c2::finite-dimensional subspace of W}}</li></ul>

============================================================

  

    Proof for the rank-nulity/fundamental theorem step:<br><ul><li>{{c1::Choose basis of null(T):&nbsp;\(\vec{u}\)}}</li><li>{{c2::Extend basis to a basis of V :&nbsp;\(\vec{u}, \vec{r}\)}}</li><li>{{c5::\(\forall v \in V\)}}&nbsp;{{c5::{{c4::\(T(v) = T(\vec{b} \cdot \vec{u} + \vec{c} \cdot \vec{r}) \)}}}} =&nbsp;{{c5::{{c4::{{c3::\(0 + \vec{c} \cdot T(\vec{r})\)}}}}}}</li><li>Which implies&nbsp;\(T(\vec{r})\) is a spanning list of the range</li></ul>

============================================================

  

    There are two strategies to show linear independence:<br><ul><li>Use {{c5::{{c1::linear dependence lemma}}}} as a {{c4::{{c2::counterpositive}}}}</li><li>Show that&nbsp;{{c4::{{c3::\( \vec{c} \cdot \vec{v} = 0 \implies \vec{c} = 0\)}}}}</li></ul>

============================================================

  

    For fin dim vector spaces:<br><ul><li>if {{c5::{{c2::dim V &gt; dim W}}}} then no linear map from V to W can be {{c4::{{c1::injective}}}}</li><li>if {{c4::{{c4::dim V &lt; dim W}}}} then no linear map from V to W can be {{c4::{{c3::surjective}}}}</li></ul>

============================================================

  

    Proof that for fin dim vector spaces, if dim V &lt; dim W then no linear map is {{c1::surjective}}:<br><ul><li>dim V = dim null + dim range</li><li>dim range = dim V - dim null</li><li>dim range &lt;=&nbsp; dim W - dim null</li><li>{{c5::dim range &lt; dim W - dim null by assumption}}</li><li>which implies {{c5::{{c4::{{c2::range(T) does not equal W}}::range(T) does not equal W}} and {{c5::{{c3::T is thus not {{c1::surjective}}::surjective}}}}}}</li></ul>

============================================================

  

    Proof that if T is surjective then dim W &lt;= dimV for fin dim vector spces:<br><ul><li>surj(T) implies {{c5::range(T) = W}}</li><li>{{c5::{{c4::dim(range(T)) = dim(V) - dim(null(T))}}}}</li><li>{{c5::{{c4::{{c3::{{c2::dim(W) = dim(V) - dim(null(T))}}}}}}}}</li><li>{{c5::{{c4::{{c3::{{c1::dim(W) &lt;= dim(V)}}}}}}}}</li></ul>

============================================================

  

    <ul><li>If a list of vectors is</li><ul><li>{{c5::{{c3::{{c1::Linearly independent&nbsp;}}::Linearly independent&nbsp;}}}}</li><li>But {{c5::{{c3::{{c2::not spanning&nbsp;}}::not spanning&nbsp;}}}}</li></ul><li>Then every vector:</li><ul><li>Has {{c4::{{c4::a unique representation}}}}</li><li>But {{c4::not every vector can be represented}}</li></ul></ul>

============================================================

  

    The reason M(T+S)&nbsp; = M(T) + M(S) holds for bases \(\vec{v}\) and \(\vec{w}\):<br><ul><li>\((T+S)(v_j)\) =&nbsp;\(\vec{c} \cdot \vec{w} + \vec{b} \cdot \vec{w} \) ={{c5::{{c4::&nbsp;\( (\vec{c} + \vec{b}) \cdot \vec{w}\)}}}}</li><li>This implies&nbsp;{{c5::{{c3::{{c1::\(a_{i,j} \)}} = {{c2::\( c_i,j + b_{i,j} \)}}}}}}</li></ul>

============================================================

  

    Suppose&nbsp;\(V,W\) are fin.dim vector spaces over the same field&nbsp;\(F\) and they have bases&nbsp;\(\vec{v}, \vec{w}\)<br><ul><li>M is a {{c5::{{c1::linear transformation}}}} and an {{c4::{{c2::isomorphism}}}} between&nbsp;{{c4::{{c3::\(L(V,W)\) and&nbsp;\(F^{m,n}\)}}}}</li></ul>

============================================================

  

    &nbsp;The fundamental reasons for why vectors are generally represented as column vectors is because:<br><ul><li>Each column vector can be interpreted as a {{c5::{{c1::linear map from F to V}}}}<ul><li>Thus v<b>&nbsp;</b>\(\in\)&nbsp; {{c4::{{c2::\(L(F,V)\)}}}}</li></ul></li><li>In the sense that the row indicies are {{c4::{{c3::the basis vectors of V}}}} and the single column index is {{c4::{{c4::the basis of F}}}}</li></ul><br>

============================================================

  

    <ul><li>Because {{c5::{{c1::column vectors}}::column vectors}} can be fundamentally identified {{c3::{{c2::as linear maps from the field to V, i.e&nbsp;\(v \in L(F,V) \)}}::as linear maps from the field to V, i.e&nbsp;\(v \in L(F,V) \)}}</li><li>Then {{c5::{{c1::row vectors}}}} must be {{c4::{{c2::their transpose identified as linear maps from V to the field, i.e&nbsp;\(v^{T} \in L(V,F)\)}}}}</li></ul>

============================================================

  

    <ul><li>The {{c1::dot product}} is nothing more&nbsp;</li><li>Than {{c2::compositions}} of {{c3::vectors v}} {{c4::intepreted as linear maps&nbsp;\(v \in L(F,V)\)}} with {{c5::{{c3::their duals}}::their duals}} interpreted as linear maps&nbsp;\(v^{T} \in L(V,F)\)</li></ul>

============================================================

  

    Proof that an invertible linear map T can only have one inverse:<br><br>Assume it could have two inverses S, S'<br><ul><li>{{c5::{{c4::\(ST = I \)}}}}<br></li><li>{{c4::{{c3::(ST)S' =&nbsp; S(TS')}}}}</li><li>{{c4::{{c2::I S' = S I}}}}</li><li>{{c4::{{c1::S' = S&nbsp;}}}}</li></ul>

============================================================

  

    Proof that if a linear map is invertible it must be injective:<br><ul><li>Suppose T is inveritble</li><li>Proof it must be injective:</li><ul><li>\(u,v \in V\) such that&nbsp;\(T(u) = T(v)\)<br></li><li>{{c5::\(u\) =&nbsp;\(T^{-1} T(u)\)}}<br></li><li>{{c4::\(v\) =&nbsp;\(T^{-1} T(v)\)}}</li><li>{{c5::{{c3::Since&nbsp;{{c2::\(T(u) = T(v)\)}}::\(T(u) = T(v)\)}} this implies&nbsp;{{c5::{{c1::\(u = v\)}}}} since {{c5::{{c1::the inverse gets back the original elements}}}}}}</li></ul></ul>

============================================================

  

    Proof that if a linear map is invertible it must be surjective:<br><ul><li>Suppose T is inveritble</li><li>Proof it must be surjective:</li><ul><li>Suppose&nbsp;\(w \in W\)</li><li>{{c5::\(T^{-1}(w) \)&nbsp;\(\in \)&nbsp;&nbsp;\(V\)}}<br></li><li>{{c4::\(T(T^{-1}(w)) = w\)}}<br></li><li>This automatically implies that {{c5::{{c3::{{c1::every element in the codomain}}::every element in the codomain}} has {{c5::{{c2::an associated element in the domain}}}}}}</li></ul></ul>

============================================================

  

    Proof that if a map S is an inverse of T which is injective and surjective, S is also linear:<br><ul><li>T(S(w1) + S(w2)) =&nbsp;</li><ul><li>= {{c5::TS(w1) + TS(w2)}}</li><li>= {{c5::{{c4::w1 + w2&nbsp;}}}}</li><li>Which implies that {{c5::{{c4::{{c3::Sw1+Sw2 is the {{c2::preimage}} of w1 + w2}}}}}}</li><li>Meaning {{c5::{{c4::{{c3::{{c1::S(w1+w2) = Sw1 + Sw2}}}}}}}}</li></ul><li>T(a Sw) =&nbsp;</li><ul><li>= a T(S(w))&nbsp;</li><li>= a w</li><li>This means S(aw) = a Sw&nbsp;</li></ul></ul>

============================================================

  

    Product of vector spaces \(V_1,...,V_m\) over F:<br><ul><li>{{c3::{{c1::\(V_1 \times \ldots \times V_m\)}} =&nbsp;{{c2::\(\{ (u_1,\ldots,u_m): u_1 \in V_1,\ldots u_m \in V_m&nbsp; \}\)}}}}<br></li><li>Addition is:&nbsp; {{c4::{{c3::{{c2::\(\vec{u} + \vec{w} \) = \( (u_1+w+1,\ldots u_m+w_m)\)}}}}}}</li><li>Scalar multiplication is: {{c5::{{c3::{{c2::&nbsp;\(\lambda \vec{u}\) =&nbsp;\((\lambda u_1,\ldots,\lambda u_m )\)}}}}}}</li></ul>

============================================================

  

    Suppose&nbsp;\(U_1,\ldots,U_m\) are subspaces of V , define a linear map&nbsp;\(\Gamma\) : \( U_1 \times \ldots U_m \to U_1+\ldots U_m\) by:<br><ul><li>{{c3::{{c1::\(\Gamma(\vec{u}) \)}}}} = {{c5::{{c3::{{c2::\( \sum u_i\)}}::\( \sum u_i\)}}}}<br></li></ul><div>Then&nbsp;\(U_1+\ldots+U_m\) is {{c4::{{c4::a direct sum}}}} iff&nbsp;{{c4::\(\Gamma\) is injective}}</div>

============================================================

  

    While one can take a {{c5::{{c1::product}}}} of {{c4::{{c4::{{c2::arbitrary vector spaces}}::arbitrary vector spaces}}}}, a {{c5::{{c1::sum}}}} only makes sense for {{c4::{{c3::{{c2::subspaces of the same vector space}}::subspaces of the same vector space}}}}

============================================================

  

    Suppose V is finite dimensional and&nbsp;\(U_1 \ldots U_m\) are subspaces of V. Then&nbsp;\(U_1 + \ldots U_m\) is a {{c3::{{c1::direct sum}}}} iff<br><ul><li>{{c3::{{c2::dim(\(U_1+\ldots+U_m\))}}}} =&nbsp;{{c3::{{c3::\(dim(U_1) + \ldots + dim(U_m)\)}}}}</li></ul>

============================================================

  

    • An {{c1::affine subset of V}}: is a {{c2::subset of V of the form v + U for some v € V and some subspace U of V.}}<br>• For {{c3::v € V and U a subspace of v}}, {{c4::the affine subset v + U}} is said to be {{c5::parallel to U}}.

============================================================

  

    Two {{c3::{{c1::affine subsets parallel to U}}}} are {{c3::{{c2::equal}}}} or {{c3::{{c3::disjoint}}}}

============================================================

  

    Suppose U is a subspace of V and<b> v,w are in V</b>. Then the following are equivalent:<br><ol><li>{{c4::{{c1::v - w&nbsp;}}::v - w&nbsp;}} {{c2::\(\in\)}} {{c5::{{c3::U}}}}</li><li>{{c5::{{c5::v+U}}}}&nbsp; = w + U</li><li>(v+U)&nbsp;\(\cap\) (w+U) \(\neq\)&nbsp;\(\emptyset\)</li></ol>

============================================================

  

    <div>V/U is a {{c3::{{c1::vector space}}}} with {{c3::{{c2::addition}}}} and {{c3::{{c3::scalar multiplication&nbsp;}}}}</div>

============================================================

  

    Definition: addition and scalar multiplication on V/U<br>Suppose U is a subspace of V. Then addition and scalar multiplication are defined on V /U by:<br><ul><li>{{c3::{{c1::(v + U) + (w + U)}}::(v + U) + (w + U)}} = {{c4::{{c2::(v + w) + U}}::(v + w) + U}}<br></li><li>{{c5::{{c5::{{c1::\( \lambda(v + U )\)}}::\( \lambda(v + U )\)}}}} =&nbsp;{{c5::{{c2::\((\lambda v) + U\)}}}}</li></ul>

============================================================

  

    Suppose U is a subspace of V, the {{c1::quotient map}}&nbsp;\(\pi\) is the linear map {{c2::\(\pi \in V \to V/U\)}}&nbsp;defined by<br><ul><li>{{c5::{{c4::{{c2::\(\pi(v)\)}}::\(\pi(v)\)}} = {{c5::{{c3::v+U}}}}}}</li><li>for&nbsp;\(v \in V\)</li></ul>

============================================================

  

    Dimension of a {{c3::{{c1::quotient}}}} space is the difference of dimensions between the {{c3::{{c2::encompassing}}}} space and {{c3::{{c3::subspace}}}}

============================================================

  

    Suppose V is {{c1::fin}} dim and U is a {{c2::subspace}} of V, then:<br><ul><li>{{c5::{{c3::dim V/U}}}} = {{c5::{{c4::dim V - dim U}}}}</li></ul>

============================================================

  

    Proof for the dimension of a quotient space dim(V/U) = dim (V) - dim(U):<br><ul><li>Define&nbsp;\(\pi(v) \) =&nbsp;{{c5::{{c1::\(v+U\)}}}}</li><li>{{c4::{{c2::dim V}}}} = {{c4::{{c3::dim null&nbsp;\(\pi\) + dim range&nbsp;\(\pi\)}}}}</li><ul><li>= {{c4::{{c4::dim U + dim V/U}}}}</li></ul></ul>

============================================================

  

    The {{c2::null space}} of the {{c3:: quotient}} map {{c4::\(\pi\)}}: {{c5::\(V\)}} \(\to\) \(V/U\) is&nbsp;{{c1::\(U\)}}

============================================================

  

    Suppose&nbsp;\(T \in L(V,W)\). Define:<br><ul><li>\(\tilde{T}\)&nbsp;\(:\)&nbsp;{{c5::{{c3::{{c1::\(V/null(T)\)}}}}}}&nbsp;\(\to\)&nbsp;{{c5::{{c3::{{c2::\(W\)}}}}}}<br></li><li>{{c4::{{c4::\(\tilde{T}(v+null T)\)}}}} =&nbsp;{{c4::\(T(v)\)}}<br></li></ul>

============================================================

  

    Properties of the induced map&nbsp;\(\tilde{T}\)<br>Suppose&nbsp;\(T \in L(V,W)\) then:<br><ol><li>\(\tilde{T}\) is a {{c1::linear map from&nbsp;\(V/null(T)\) to W}}<br></li><li>\(\tilde{T}\) is {{c2::injective}}<br></li><li>{{c5::{{c3::\(range(\tilde{T})\)}} =&nbsp; {{c5::{{c4::range(T)}}}}::range?}}<br></li><li>V/(null T) is isomorphic to range T</li></ol>

============================================================

  

    A good way to think of the {{c1::quotient}} space is that it {{c5::{{c2::identifies everything}}::identifies everything}} in {{c5::{{c3::the divided subspace}}}} with {{c5::{{c4::0}}}}

============================================================

  

    <ul><li>A {{c1::linear functional}} on V is {{c3::{{c2::a linear map from V to F.&nbsp;}}::a linear map from V to F.&nbsp;}}</li><li>In other words, a {{c5::{{c2::linear functional}}}} is an element of {{c5::{{c4::{{c2::\(L(V,F)\)}}::\(L(V,F)\)}}}}</li></ul>

============================================================

  

    <ul><li>The {{c1::dual}} space of V, denoted {{c2::V'}}, is the {{c3::vectors space all linear functionals on V}}.&nbsp;</li><li>In other words, {{c5::{{c2::V'}}::V'}} =&nbsp; {{c4::{{c3::L(V,F)}}::L(V,F)}}</li></ul>

============================================================

  

    <div>Suppose V is {{c3::finite}} dimensional. Then:</div><div><ul><li>The {{c1::dual space}} {{c2::V'}} is also {{c4::{{c3::finite}}::finite}} dimensional</li><li>And {{c5::dim V'}} = dim V</li></ul></div>

============================================================

  

    Proof that dim V' = dim V<br><ul><li>dim V' =&nbsp;</li><ul><li>= {{c5::{{c4::dim L(V,F)&nbsp;&nbsp;}}}}</li><li>= {{c4::{{c3::dim(V) dim(F)}}}}</li><li>= {{c4::{{c2::dim(V) x 1&nbsp;}}}}</li><li>= {{c4::{{c1::dim(V)}}}}</li></ul></ul>

============================================================

  

    Each {{c5::{{c1::basis}}}} of V is associated with {{c4::{{c3::a {{c1::basis}}::basis}} of the {{c4::{{c2::dual space V'}}}}}}

============================================================

  

    If&nbsp;\(\vec{v}\) is a basis of V, then the dual basis of&nbsp;\(\vec{v}\) is:<br><ul><li>The list of elements&nbsp;\(\phi_1,\ldots,\phi_n\) of V'</li><li>Where each&nbsp;\(\phi_j\) is the linear functions on V such that:</li><ul><li>{{c5::\(\phi_j(v_k)\)}} =&nbsp;<br></li><ul><li>{{c4::{{c1::1}}}} if&nbsp;&nbsp;{{c5::{{c2::\(k = j\)}}}}</li><li>{{c5::{{c3::{{c1::0}}}} if&nbsp;{{c5::{{c2::\(k \neq j\)}}}}}}</li></ul></ul></ul>

============================================================

  

    The {{c5::{{c4::duals basis}}}} is a {{c4::{{c3::{{c2::basis}}::basis}} of {{c4::{{c1::the dual space}}}}}}

============================================================

  

    Suppose&nbsp;{{c5::{{c1::\(dim V &lt; \infty\)}}}}. Then the {{c4::{{c2::dual basis of a basis of V}}}} is a {{c4::{{c3::basis of V'}}}}

============================================================

  

    If&nbsp;\(T \in\)&nbsp;{{c2::&nbsp;\(L(V,W)\)}}, the the {{c1::dual map}} of T is:<br><ul><li>The linear map{{c3::&nbsp;\(T'\)}}&nbsp;\(\in\)&nbsp;{{c4::{{c2::\(L(W',V')\)}}::\(L(W',V')\)}}</li><li>Defined by&nbsp;&nbsp;\(T'(\phi)\) ={{c5::&nbsp;\(\phi \)}}&nbsp;\(\circ\)&nbsp;\(T\) for&nbsp;\(\phi\)&nbsp;\(\in \){{c4::{{c2::&nbsp;\(W'\)}}::&nbsp;\(W'\)}}</li></ul>

============================================================

  

    <ul><li>Define D : P(R) to P(R) by Dp = p'</li><li>D' maps from the dual space of P(R) to the dual space of P(R)</li><li>\(D'(\phi)\) =&nbsp;{{c3::\(\phi \circ D\)}}<br></li><li>{{c3::{{c3::\(\phi \circ D (p)\)}}}} =&nbsp;{{c3::{{c2::\(\phi(p')\)}}}}<br></li><li>E.g for \(\phi(p) = p(3) \) then {{c3::{{c1::\(\phi(p')\) = p'(3)}}}}</li></ul>

============================================================

  

    Algebraic properties of linear maps:<br><ul><li>{{c3::{{c3::(S+T)'}}}} =&nbsp; {{c3::{{c1::S' + T'}}}} for {{c3::{{c2::all S,T in L(V,W)}}}}</li><li>\((\lambda T)'\) =&nbsp;{{c3::\(\lambda T'\)}} for all&nbsp;\(\lambda \in F\) and all T in L(V,W)<br></li><li>(ST)' = T' S' for all T in L(U,V) and S in L(V,W)</li></ul>

============================================================

  

    For&nbsp;\(U \subset V\), the anihilator of U, denoted&nbsp;{{c5::{{c3::\(U^0\)}}}} is defined by:<br><ul><li>{{c5::{{c3::\(U^0\)}}}} =&nbsp;{{c5::{{c4::{{c2::\(\{&nbsp; &nbsp;\phi \in V': \phi(u) = 0&nbsp; \)}} {{c1::\( \forall u \in U \}\)}}}}}}<br></li></ul>

============================================================

  

    V^0 = {0}&nbsp; because the {{c3::{{c1::linear functional}} {{c3::{{c2::which takes all vectors to 0}}}} is {{c3::{{c3::by definition the 0 map}}}}}}

============================================================

  

    Suppose V is {{c1::finite-}}dimensional and U is a {{c2::susbspace}} of V. Then:<br><ul><li>{{c5::{{c4::dim U + dim \(U^0\)}}}} = {{c5::{{c3::dim V}}}}</li></ul>

============================================================

  

    Suppose V and W are finite-dimensional and&nbsp;\(T \in L(V,W)\) then:<br><ul><li>{{c5::{{c1::null}}::null}} T' =&nbsp;{{c3::{{c2::&nbsp;\( (\mathrm{range} \, T)^0\)}}}}</li><li>{{c5::{{c1::range}}}} T' =&nbsp;{{c4::{{c2::\((\mathrm{null} \, T)^0\)}}}}</li></ul>

============================================================

  

    Suppose T is {{c1::surjective}}, then:<br><ul><li>{{c2::range(T)}} = {{c2::W}}</li><li>{{c3::null(T')}} =</li><ul><li>= {{c4::(range T)^0&nbsp;}}</li><li>= {{c5::W^0}}</li><li>= {0} </li></ul><li>Meaning T' is injective</li></ul>

============================================================

  

    Proof that dim range T' = dim range T<br><ul><li>dim range T' = dim W' - dim null T'</li><li>dim range T' = {{c5::dim W' - dim range(T)^0}}</li><li>dim range T' = {{c5::{{c4::{{c3::dim L(W,F) - (dim W - dim range T)}}}}}}</li><li>dim range T' = {{c5::{{c4::{{c3::{{c2::dim W - (dim W - dim range T)}}}}}}}}</li><li>dim range T' = {{c5::{{c4::{{c3::{{c2::{{c1::dim range T}}}}}}}}}}</li></ul>

============================================================

  

    The {{c1::transpose}} of a matrix A, denoted A^t, is the matrix obtained from A by {{c2::interchanging the rows and columns}}. More specifically, if A<br>is an {{c3::m-by-n}} matrix, then A^t is the {{c3::n-by-m}} matrix whose entries are<br>given by the equation<br><ul><li>{{c4::\((A^t)_{k,j} }} = {{c5::A_{j,k}\) }}<br></li></ul>

============================================================

  

    If A is an {{c5::{{c3::m-by-n::size?}}::size?}} matrix and C is an {{c5::{{c3::n-by-p}}}} matrix then:<br><ul><li>{{c4::{{c2::(AC)^t}}}} = {{c4::{{c1::C^t A^t}}}}</li></ul>

============================================================

  

    <br>Suppose A is an m-by-n matrix with entries in F.<br><ul><li>The {{c5::{{c1::row rank}}::row rank}} of A is {{c4::{{c2::the dimension of the span of the rows of A}}::the dimension of the span of the rows of A}} in {{c4::{{c2::\(F^{1,n}\).}}}}</li><li>The {{c5::{{c1::column rank}}}} of A is {{c5::{{c3::{{c2::the dimension of the span of the columns of A}}::the dimension of the span of the columns of A}}}} in {{c3::{{c2::\(F^{m,1}\)}}}}</li></ul>

============================================================

  

    Suppose V and W are finite dimensiona and&nbsp;\(T \in L(V,W)\).&nbsp;<br><ul><li>Then {{c3::{{c2::dim {{c1::range T}}::range T}}}} equals the {{c3::{{c3::column}} rank}} of M(T)</li></ul>

============================================================

  

    Proof the column rank equal row rank:<br><ul><li>Define T:&nbsp;{{c4::{{c1::\(F^{n,1}\)&nbsp;}}\(\to\){{c1::&nbsp;\(F^{m,1}\)}}}} by {{c5::{{c5::{{c2::Tx}}::Tx}}}} = {{c5::{{c5::{{c2::Ax}}::Ax}}}}.</li><ul><li>Thus M(T) = {{c5::{{c3::{{c2::A&nbsp;}}::A&nbsp;}}}}</li></ul><li>column rank of A =&nbsp;</li><ul><li>= column rank of M(T)</li><li>= dim range T</li><li>= dim range T'</li><li>= column rank of M(T')</li><li>= column rank of A^t</li><li>= row rank of A</li></ul></ul>

============================================================

  

    Suppose an operator f is surjective, proof it must be injective and invertible:<br><ul><li>range(f) = V</li><li>dim V = dim range f</li><li>By rank-nulity theorem:</li><ul><li>dim null(f) = dim(V) - dim range(f)</li><li>dim null(f) = dim(V) - dim(V)</li><li>dim null(f) = 0</li><li>Thus f is {{c5::injective}}&nbsp;</li></ul><li>Since it is {{c4::both {{c3::{{c2::injective}}}} and {{c3::{{c2::surjective}}}} it is {{c3::{{c1::invertible}}}}}}</li></ul>

============================================================

  

    Proof that two vector spaces are isomorphic if they have the same dimension:<br><ul><li>Suppose {{c1::there exists an isomorphism f: V to W}}</li><li>{{c2::dim V =&nbsp; dim null(f) + dim range(f)}}</li><li>Since {{c3::an {{c1::isomorphism}}::isomorphism}} is injective and surjective:</li><ul><li>{{c4::dim null(f) = 0}}</li><li>{{c5::dim range f = dim W}}</li></ul><li>Thus dim V = 0 + dim W = dim W</li></ul><ul></ul>

============================================================

  

    Proof that if dimensions are equal two fin.dim vector spaces must be isomorphic:<br><ul><li>Take {{c5::{{c1::bases \(\vec{v}, \vec{w}\) for V,W}}}}</li><li>Define {{c4::{{c4::{{c2::f: V&nbsp;\(\to\) W}}::f: V&nbsp;\(\to\) W}}::f: V&nbsp;\(\to\) W::f: V&nbsp;\(\to\) W}}&nbsp; as:</li><ul><li>{{c4::{{c4::{{c3::f(v_i) = w_i}}::f(v_i) = w_i}}}}</li></ul><li>Whose matrix is the {{c4::identity}} matrix</li><li>And which is both injective and surjective</li><li>Thus the two vector spaces are isomorphic</li></ul>

============================================================

  

    Proposition:<br><br>Let&nbsp; V,W be fin dim vector spaces with bases&nbsp;\(\vec{v},\vec{w} \) then:<br><ul><li>{{c5::{{c4::\(M_{\vec{v},\vec{w} }\)}}}} {{c5::{{c3::{{c2::&nbsp;\(L(V,W)\)}}}}&nbsp;}}\(\to\)&nbsp;{{c5::{{c3::{{c1::\(F^{m,n}\)}}}}}}</li><li>Is an isomorphism between the vector space of linear maps and the vector space of matrices</li></ul>

============================================================

  

    Proving that the map M which takes linear maps to matrices is indeed an isomorphism:<br><ul><li>Proving injectivity:</li><ul><li>Suppose that f in L(V,W)</li><li>Such that M(f) = 0</li><li>For {{c5::any vector v in the basis of V}}</li><li>{{c4::M(fv)}} =&nbsp;</li><ul><li>= {{c3::M(f) M(v)&nbsp;}}</li><li>= {{c2::0}}</li></ul><li>Since {{c1::the only matrix which sends all vectors to 0 is the 0 matrix}}, M is injective</li></ul></ul>

============================================================

  

    Proving that the map M which takes linear maps to matrices is indeed an isomorphism:<br><ul><li>Proving Surjectivity, assuming bases \(\vec{v},\vec{w}\) for vector spaces V,W:&nbsp;</li><li>Let A be {{c5::{{c1::a matrix in F^{m,n}}}}}</li><li>Define a {{c4::{{c2::linear map f in L(V,W)}}}}</li><ul><li>Such that {{c5::{{c3::f(v_k) =&nbsp;\( \vec{a}_{,k} \cdot \vec{w}\)}}}}</li><li>Then {{c4::{{c4::M(f) = A}}}}</li><li>Thus M is surjective</li></ul></ul>

============================================================

  

    <ul><li>L(V,W) is {{c5::{{c3::{{c1::isomorphic}}::isomorphic}} to {{c4::{{c2::F^{m,n}}}}}}}</li><li>Thus dim L(V,W) = {{c4::{{c4::dim V * dim W = m * n}}}}</li></ul>

============================================================

  

    How the dual basis can be constructed:<br><ul><li>V has basis&nbsp;\(\vec{v}\)</li><li>Any vector can be written as</li><ul><li>v =&nbsp;{{c5::{{c1::\(\vec{v} \cdot \vec{x}\)}}}}</li></ul><li>`The dual basis&nbsp;\(\vec{\phi}\)<br></li><li>Can be defined as</li><ul><li>{{c4::{{c4::{{c3::\(\phi_i(v)\)}}::\(\phi_i(v)\)}}}} ={{c4::{{c4::&nbsp;{{c2::\(x_i\)}}::\(x_i\)}}}}<br></li><li>Which is equivalent to the {{c4::standard indicator}} defintion</li><li>It is also called a coordinate function</li></ul></ul>

============================================================

  

    Because V is isomorphic to its dual:<br><ul><li>For u,v in V</li><li>We can take the f_u elements of V' which corresponds to the {{c5::{{c4::dual of u}}}}</li><li>Then {{c4::{{c3::applying f_u to v gives us {{c2::a scalar}}::a scalar}}}}</li><li>Meaning that we have in-fact defined the {{c4::{{c1::dot product}}}}</li></ul>

============================================================

  

    <ul><li>While V is isomorphic to its dual V', this isomorphisn is {{c5::{{c1::not natural&nbsp;}}}}&nbsp;</li><li>Since we can {{c4::{{c4::{{c3::only associate elements of V to those of V'}}::only associate elements of V to those of V'}} after {{c4::{{c2::choosing a basis}}}}}}</li></ul>

============================================================

  

    <div>The dual dual<span style="font-family: -apple-system; font-size: 16px;">&nbsp;V'' = {{c5::{{c1::L(V',F)}}}} meaning it {{c4::{{c4::takes {{c2::linear functionals}}::linear functionals}} and returns {{c4::{{c3::scalaras}}}}}}</span></div>

============================================================

  

    A cannonical isomorphism between V and its dual dual V'' can be defined as:<br><ul><li>\(\phi \in V'\)<br></li><li>\(\alpha \in V''\) meaning&nbsp;\(\alpha \in L(V',F) \)<br></li><li>Create an isomorphism \(f : V \to V''\) by:</li><ul><li>f(v) =&nbsp;{{c5::{{c3::&nbsp;\(h \in V''\)&nbsp;}}}}</li><li>Where&nbsp;{{c5::{{c4::{{c2::\(h(\phi)\)}}}}}} =&nbsp;{{c5::{{c4::{{c1::\(\phi(v)\)}}}}}}</li></ul></ul>

============================================================

  

    17 Is the operation of addition on the subspaces of \(V\) associative? In other words, if \(U_{1}, U_{2}, U_{3}\) are subspaces of \(V\), is<br><br>\[<br>\left(U_{1}+U_{2}\right)+U_{3}=U_{1}+\left(U_{2}+U_{3}\right) ?<br>\]<br><br><ul><li>{{c5::{{c3::{{c1::Yes}}::Yes}}, vector space addition is {{c4::{{c1::associative}}}} since {{c4::{{c2::vector addition is associative}}}}}}<br></li></ul>

============================================================

  

    You can add up:<br><ul><li>Any {{c3::{{c2::multiples of elements}}}}&nbsp;</li><li>In {{c3::{{c3::spanning lists}}}} and {{c3::independent lists&nbsp;}}</li><li>While {{c3::{{c1::maintaing both properties}}}}</li></ul>

============================================================

  

    If \(v_{1}, v_{2}, \ldots, v_{m}\) is a linearly independent list of vectors in \(V\) and \(\lambda \in \mathbf{F}\) with {{c5::{{c1::\(\lambda \neq 0\)}}}}, then {{c4::{{c2::\(\lambda v_{1}, \lambda v_{2}, \ldots, \lambda v_{m}\)}}}} is {{c4::{{c3::linearly independent}}}}.

============================================================

  

    <ol><li>Suppose \(U\) and \(W\) are subspaces of \(V\) such that {{c3::\(V=U \oplus W\)}}.&nbsp;</li><li>Suppose also that \(u_{1}, \ldots, u_{m}\) is {{c4::a basis of \(U\)}} and \(w_{1}, \ldots, w_{n}\) is {{c5::a basis of \(W\)}}. </li><li>Prove that</li></ol><br>{{c1::\[<br>u_{1}, \ldots, u_{m}, w_{1}, \ldots, w_{n}<br>\]}}<br><br>is {{c2::a basis of \(V\)}}.<br>

============================================================

  

    2Show that the subspaces of \(\mathbf{R}^{2}\) are precisely:<br><ol><li>{{c5::{{c1::&nbsp;\(\{ 0 \} \)}}}},&nbsp;</li><li>{{c4::{{c2::\(\mathbf{R}^{2} \)}}}},&nbsp;</li><li>And {{c4::{{c3::all lines in \(\mathbf{R}^{2} \) through the origin}}}}.</li></ol>

============================================================

  

    Show that the subspaces of \(\mathbf{R}^{3}\) are precisely:<br><ol><li>{{c5::{{c1::&nbsp;\(\{0\}, \mathbf{R}^{3}\)}}}},&nbsp;</li><li>And {{c4::{{c2::all lines in \(\mathbf{R}^{3}\) through the origin,}}}}&nbsp;</li><li>And {{c4::{{c3::all planes in \(\mathbf{R}^{3}\) through the origin}}}}.</li></ol>

============================================================

  

    Suppose \(U\) and \(W\) are both five-dimensional subspaces of \(\mathbf{R}^{9}\). <br><ol><li>Prove that {{c5::{{c1::\(U \cap W\)}}}}&nbsp; {{c4::{{c2::\( \neq \)}}}} {{c4::{{c3::\( \{0\}\)}}}}.</li></ol>

============================================================

  

    Suppose \(U_{1}, \ldots, U_{m}\) are finite-dimensional subspaces of \(V\). Prove that \(U_{1}+\cdots+U_{m}\) is finite-dimensional and<br><br><ol><li>{{c5::{{c1::\(\operatorname{dim}\left(U_{1}+\cdots+U_{m}\right) \)}}}} {{c4::{{c2::\(\leq\)}}}} {{c4::{{c3::\( \operatorname{dim} U_{1}+\cdots+\operatorname{dim} U_{m} .\)}}}}</li></ol>

============================================================

  

    15 Suppose \(V\) is finite-dimensional, with \(\operatorname{dim} V=n \geq 1\). <br><ol><li>Prove that there exist {{c5::{{c1::1-dimensional}}}} subspaces \(U_{1}, \ldots, U_{n}\) of \(V\) such that</li><li>{{c4::{{c2::\(V\)}}}} \(=\) {{c4::{{c3::\(U_{1}&nbsp;\oplus \) \( \cdots \oplus U_{n}\)}}}}</li></ol>

============================================================

  

    16 Suppose \(U_{1}, \ldots, U_{m}\) are finite-dimensional subspaces of \(V\) such that \(U_{1}+\cdots+U_{m}\) is a direct sum. <br><ol><li>Prove that \(U_{1} \oplus \cdots \oplus U_{m}\) is {{c5::{{c1::finite}}}} dimensional and</li></ol><br>{{c4::{{c2::\(<br>\operatorname{dim} U_{1} \oplus \cdots \oplus U_{m}\)}}}}&nbsp;= {{c4::{{c3::\(\operatorname{dim} U_{1}+\cdots+\operatorname{dim} U_{m} .<br>\)}}}}<br>

============================================================

  

    Suppose \(T \in \mathcal{L}\left(\mathbf{F}^{n}, \mathbf{F}^{m}\right)\). <br><ul><li>Show that {{c5::{{c1::there exist scalars \(A_{j, k} \in \mathbf{F}\) for \(j=1, \ldots, m\) and \(k=1, \ldots, n\)}}}} such that&nbsp;</li><li>{{c4::{{c2::\(T\left(x_{1}, \ldots, x_{n}\right) \)}}}} = {{c4::{{c3::\(\left(A_{1,1} x_{1}+\cdots+A_{1, n} x_{n}, \ldots, A_{m, 1} x_{1}+\cdots+A_{m, n} x_{n}\right)\)&nbsp;}}}}</li><li>for every \(\left(x_{1}, \ldots, x_{n}\right) \in \mathbf{F}^{n}\).</li></ul>

============================================================

  

    Suppose \(T \in \mathcal{L}(V, W)\) and \(v_{1}, \ldots, v_{m}\) is a list of vectors in \(V\):<br><ul><li>&nbsp;Such that {{c5::{{c2::\(T v_{1}, \ldots, T v_{m}\)}}}} is a {{c4::{{c1::linearly independent}}}} list in \(W\).&nbsp;</li><li>Prove that {{c4::{{c3::\(v_{1}, \ldots, v_{m}\)}}}} is {{c4::{{c1::linearly independent.}}}}</li></ul>

============================================================

  

    <ul><li>Show that {{c1::every}} linear map from {{c2::a 1-dimensional vector space}} to {{c2::itself}} is {{c5::{{c3::multiplication by some scalar}}}}.&nbsp;</li><li>More precisely, prove that if {{c2::\(\operatorname{dim} V=1\)}} and {{c2::\(T \in \mathcal{L}(V, V)\)}}, then {{c5::{{c4::there exists \(\lambda \in \mathbf{F}\) such that \(T v=\lambda v\) for all \(v \in V\).}}}}</li></ul>

============================================================

  

    11 <br><br>Suppose \(V\) is finite-dimensional. <br><ul><li>Prove that every linear map on {{c1::a subspace of \(V\)}} can {{c2::be extended to a linear map on \(V\)}}.&nbsp;</li><li>In other words, show that if {{c5::{{c3::\(U\) is a subspace of \(V\)}}}} and {{c5::{{c3::\(S \in \mathcal{L}(U, W)\)}}}}, then there exists {{c5::{{c4::\(T \in \mathcal{L}(V, W)\)}}}} such that {{c5::{{c4::\(T u=S u\) for all \(u \in U\)}}}}.</li></ul>

============================================================

  

    Suppose \(V\) is {{c5::{{c1::finite}}}}-dimensional with \(\operatorname{dim} V&gt;0\), and suppose \(W\) is {{c4::{{c2::infinite}}}}-dimensional. <br><ul><li>Prove that \(\mathcal{L}(V, W)\) is {{c4::{{c3::infinite}}}}-dimensional.</li></ul>

============================================================

  

    13 <br><ul><li>Suppose \(v_{1}, \ldots, v_{m}\) is a {{c1::linearly dependent}} list of vectors in \(V\). </li><li>Suppose also that {{c2::\(W \neq\{0\}\)}}. </li><li>Prove that there exist {{c5::{{c3::\(w_{1}, \ldots, w_{m} \in W\)}}}} such that {{c5::{{c4::no \(T \in \mathcal{L}(V, W)\) satisfies \(T v_{k}=w_{k}\) for each \(k=1, \ldots, m\)}}}}.</li></ul>

============================================================

  

    <ul><li>Suppose \(T \in \mathcal{L}(V, W)\) is {{c1::injective}} and \(v_{1}, \ldots, v_{n}\) is {{c2::linearly independent}} in \(V\).&nbsp;</li><li>Prove that {{c5::{{c3::\(T v_{1}, \ldots, T v_{n}\)}}}} is {{c5::{{c4::linearly independent}}}} in \(W\).</li></ul>

============================================================

  

    10 <br><br><ul><li>Suppose {{c3::\(v_{1}, \ldots, v_{n}\)}} {{c1::spans}} {{c5::{{c2::\(V\)}}}} and \(T \in \mathcal{L}(V, W)\).&nbsp;</li><li>Prove that the list {{c5::{{c4::\(T v_{1}, \ldots, T v_{n}\)}}}} {{c1::spans}} {{c5::{{c2::range \(T\)}}}}.</li></ul>

============================================================

  

    <ul><li>Suppose \(S_{1}, \ldots, S_{n}\) are {{c5::{{c1::injective}}}} linear maps such that {{c4::{{c2::\(S_{1} S_{2} \cdots S_{n}\)}}}} makes sense.&nbsp;</li><li>Prove that {{c2::\(S_{1} S_{2} \cdots S_{n}\)}} is {{c4::{{c3::injective}}}}.</li></ul>

============================================================

  

    Suppose \(V\) and \(W\) are finite-dimensional and that \(U\) is a subspace of \(V\). <br><ul><li>Prove that there exists \(T \in \mathcal{L}(V, W)\) such that {{c5::{{c1::null \(T=U\)}}}} if and only if {{c4::{{c3::\(\operatorname{dim} U \)}}}} {{c4::{{c2::\( \geq \operatorname{dim} V-\operatorname{dim} W\)}}}}.</li></ul>

============================================================

  

    20 <br><br>Suppose \(W\) is finite-dimensional and \(T \in \mathcal{L}(V, W)\). <br><ul><li>Prove that \(T\) is {{c5::{{c1::injective}}}}&nbsp;</li><li>If and only if there exists&nbsp;


\( S&nbsp;\in &nbsp;\) {{c4::{{c2::\(\mathcal{L}(W, V)\)}}}} such that {{c4::{{c3::\(S T\) is the identity map on \(V\).}}}}</li></ul>

============================================================

  

    21 <br><br>Suppose \(V\) is finite-dimensional and \(T \in \mathcal{L}(V, W)\). <br><ul><li>Prove that \(T\) is {{c5::{{c1::surjective}}}}&nbsp;</li><li>If and only if there exists {{c4::{{c2::\(S \in \mathcal{L}(W, V)\)}}}} such that {{c4::{{c3::\(T S\) is the identity map on \(W\).}}}}</li></ul>

============================================================

  

    22 <br><br>Suppose \(U\) and \(V\) are finite-dimensional vector spaces and \(S \in \mathcal{L}(V, W)\) and \(T \in \mathcal{L}(U, V)\). <br><br><ol><li>Prove that&nbsp;</li><li>dim {{c1::\( \text { null } S T\)}}&nbsp; {{c2::\(\leq\)}}&nbsp; {{c3::\(\operatorname{dim} \operatorname{null} S\)}} {{c4::+}} {{c5::\(\operatorname{dim} \operatorname{null} T .\)}}</li></ol>

============================================================

  

    23 <br><br>Suppose \(U\) and \(V\) are finite-dimensional vector spaces and \(S \in \mathcal{L}(V, W)\) and \(T \in \mathcal{L}(U, V)\). <br><ul><li>Prove that</li><li>{{c1::\(\text { dim range } S T\)}} {{c2::\( \leq \)}} {{c3::\( \min \)}} {{c4::\( \{\text { dim range } S \)}} {{c5::\(\text {, dim range } T\} \text {. }\)}}</li></ul>

============================================================

  

    24 <br><br>Suppose \(W\) is finite-dimensional and \(T_{1}, T_{2} \in \mathcal{L}(V, W)\). <br><ul><li>Prove that {{c5::{{c1::null \(T_{1} \subset\) null \(T_{2}\)}}}} if and only if there exists {{c4::{{c2::\(S \in \mathcal{L}(W, W)\)}}}} such that {{c4::{{c3::\(T_{2}=S T_{1}\)}}}}.</li></ul>

============================================================

  

    25 <br>Suppose \(V\) is finite-dimensional and \(T_{1}, T_{2} \in \mathcal{L}(V, W)\). <br><ul><li>Prove that {{c5::{{c1::range \(T_{1} \subset\) range \(T_{2}\)}}}}&nbsp;</li><li>If and only if there exists {{c4::{{c2::\(S \in \mathcal{L}(V, V)\)}}}} such that {{c4::{{c3::\(T_{1}=T_{2} S\).}}}}</li></ul>

============================================================

  

    28 <br><br>Suppose \(T \in \mathcal{L}(V, W)\), and \(w_{1}, \ldots, w_{m}\) is a basis of <b>range \(T\). </b><br><ul><li>Prove that there exist \(\varphi_{1}, \ldots, \varphi_{m}\) \(\in \) {{c1::\(\mathcal{L}(V, \mathbf{F})\)&nbsp;}}</li><li>Such that {{c2::\(T v \)}} = {{c5::{{c3::\(\varphi_{1}(v) w_{1}+\cdots+\varphi_{m}(v) w_{m}\)&nbsp;}}}}</li><li>for {{c5::{{c4::every \(v \in V\)}}}}.</li></ul>

============================================================

  

    29 <br><br>Suppose \(\varphi \in \mathcal{L}(V, \mathbf{F})\). <br><br><ul><li>Suppose \(u \in V\) is not in {{c1::null \(\varphi\)}}.</li><li>Prove that</li><li>\(V \) = {{c2::\( \operatorname{null} \varphi\)}}&nbsp; {{c5::{{c3::\(\oplus\)}}}} {{c5::{{c4::\(\{a u: a \in \mathbf{F}\} .\)}}}}</li></ul>

============================================================

  

    30<br><br><ul><li>Suppose \(\varphi_{1}\) and \(\varphi_{2}\) are linear maps from \(V\) to \(\mathbf{F}\) that have the same {{c5::{{c1::null space}}}}.&nbsp;</li><li>Show that there exists {{c4::{{c2::a constant \(c \in \mathbf{F}\)}}}} such that {{c4::{{c3::\(\varphi_{1}=c \varphi_{2}\)}}}}.</li></ul>

============================================================

  

    1 <br><br>Suppose \(V\) and \(W\) are finite-dimensional and \(T \in \mathcal{L}(V, W)\). <br><br><ul><li>Show that with respect to {{c5::{{c1::each choice of bases of \(V\) and \(W\)}}}},&nbsp;</li><li>The matrix of \(T\) has at least {{c4::{{c2::dim range \(T\)}}}} {{c4::{{c3::nonzero entries}}}}.</li></ul>

============================================================

  

    3 <br><br>Suppose \(V\) and \(W\) are finite-dimensional and \(T \in \mathcal{L}(V, W)\). <br><br><ul><li>Prove that there exist a basis of \(V\) and a basis of \(W\)&nbsp;</li><li>Such that all entries of \(\mathcal{M}(T)\) are {{c1::0}} except that the entries in {{c2::row \(j\), column \(j\)}}, equal {{c5::{{c3::1}}}} for {{c5::{{c4::\(1 \leq j \leq \operatorname{dim} \operatorname{range} T\)}}}}.</li></ul>

============================================================

  

    4 <br>Suppose \(v_{1}, \ldots, v_{m}\) is a basis of \(V\) and \(W\) is finite-dimensional. Suppose \(T \in \mathcal{L}(V, W)\). <br><br><ul><li>Prove that there exists a basis \(w_{1}, \ldots, w_{n}\) of \(W\)&nbsp;</li><li>Such that {{c1::all}} the entries in {{c1::the first column of \(\mathcal{M}(T)\)}}&nbsp; are {{c2::0}} except for {{c5::{{c3::possibly a 1}}}} in {{c5::{{c4::the first row, first column}}}}.</li></ul>

============================================================

  

    5 <br><ul><li>Suppose \(w_{1}, \ldots, w_{n}\) is a basis of \(W\) and \(V\) is finite-dimensional.&nbsp;</li><li>Suppose \(T \in \mathcal{L}(V, W)\).&nbsp;</li><li>Prove that there exists a basis \(v_{1}, \ldots, v_{m}\) of \(V\)&nbsp;</li><li>Such that {{c1::all}} the entries in {{c2::the first row of \(\mathcal{M}(T)\)}}&nbsp; are {{c3::0}} except for {{c4::possibly a 1}} in the {{c5::first row, first column}}.</li></ul>

============================================================

  

    Suppose \(V\) and \(W\) are finite-dimensional and \(T \in \mathcal{L}(V, W)\). <br><ul><li>Prove that dim {{c1::range \(T\)}} = {{c2::\(1\)&nbsp;}}</li><li>If and only if there exist {{c3::a basis of \(V\)}} and {{c4::a basis of \(W\)}} such that {{c5::all entries of \(\mathcal{M}(T)\)}} equal 1 .</li></ul>

============================================================

  

    &nbsp;10 Suppose \(A\) is an \(m\)-by- \(n\) matrix and \(C\) is an \(n\)-by- \(p\) matrix. <br><ul><li>Prove that</li><li>{{c1::\((A C)_{j, \cdot} \)}} = {{c2::\(A_{j, \cdot} C\)}} for \(1 \leq j \leq m\).&nbsp;</li><li>In other words, show that {{c5::{{c3::row \(j\) of \(A C\)}}}} equals {{c5::{{c4::(row \(j\) of \(A\) ) times \(C\)}}}}.</li></ul>

============================================================

  

    11 <br>Suppose \(a=\left(\begin{array}{lll}a_{1} &amp; \cdots &amp; a_{n}\end{array}\right)\) is a 1-by- \(n\) matrix and \(C\) is an \(n\)-by- \(p\) matrix. <br><ul><li>Prove that</li><li>{{c1::\(a C\)}}= {{c2::\(a_{1} C_{1, \cdot}+\cdots+a_{n} C_{n, \cdots}\)}}</li><li>In other words, show that {{c1::\(a C\)}} is a {{c5::{{c3::linear combination of the rows of \(C\)}}}}, with {{c5::{{c4::the scalars that multiply the rows coming from \(a\)}}}}.</li></ul>

============================================================

  

    15 <br>Suppose \(A\) is an \(n\)-by- \(n\) matrix and \(1 \leq j, k \leq n\). <br><ul><li>Show that the entry in row \(j\), column \(k\), of \(A^{3}\) (which is defined to mean \(A A A\) ) is</li><li>{{c1::\(\sum_{p=1}^{n} \)}} {{c2::\( \sum_{r=1}^{n} \)}}&nbsp; {{c3::\( A_{j, p}\)}} {{c4::\( A_{p, r}\)}} {{c5::\( A_{r, k} .\)}}</li></ul><br>

============================================================

  

    1 <br><br>Suppose \(T \in \mathcal{L}(U, V)\) and \(S \in \mathcal{L}(V, W)\) are both {{c1::invertible}} linear maps. <br><ul><li>Prove that {{c2::\(S T \in \mathcal{L}(U, W)\)}} is {{c1::invertible}}&nbsp;</li><li>And that {{c3::\((S T)^{-1}\)}} {{c4::=}} {{c5::\(T^{-1} S^{-1}\)}}.</li></ul>

============================================================

  

    2 <br><br>Suppose \(V\) is finite-dimensional and \(\operatorname{dim} V\) {{c1::\(&gt;1\)}}. <br><ul><li>Prove that the set of {{c2::noninvertible}} {{c3::operators on \(V\)}} is {{c4::not}} {{c5::a subspace of \(\mathcal{L}(V)\)}}.</li></ul>

============================================================

  

    3 <br><br>Suppose \(V\) is finite-dimensional, \(U\) is a subspace of \(V\), and \(S \in \mathcal{L}(U, V)\). <br><br><ul><li>Prove there exists an {{c1::invertible}} {{c2::operator}} \(T \in \mathcal{L}(V)\)&nbsp;</li><li>Such that {{c3::\(T u\)}} = {{c3::\(S u\)}} for {{c4::every \(u \in U\)}} if and only if \(S\) is {{c5::injective}}.</li></ul>

============================================================

  

    4 Suppose \(W\) is finite-dimensional and \(T_{1}, T_{2} \in \mathcal{L}(V, W)\). <br><br><ul><li>Prove that {{c1::null \(T_{1}\)}} {{c5::=}} {{c2::null \(T_{2}\)&nbsp;}}</li><li>If and only if there exists an {{c3::invertible operator}} \(S \in \mathcal{L}(W)\) such that {{c4::\(T_{1}=S T_{2}\)}}.</li></ul>

============================================================

  

    5 <br><br>Suppose \(V\) is finite-dimensional and \(T_{1}, T_{2} \in \mathcal{L}(V, W)\). <br><br><ul><li>Prove that {{c1::range \(T_{1}\)}} {{c5::=}} {{c2::range \(T_{2}\)&nbsp;}}</li><li>If and only if there exists an {{c3::invertible operator}} \(S \in \mathcal{L}(V)\) such that {{c4::\(T_{1}=T_{2} S\)}}.</li></ul>

============================================================

  

    6 <br><br>Suppose \(V\) and \(W\) are finite-dimensional and \(T_{1}, T_{2} \in \mathcal{L}(V, W)\). <br><ul><li>Prove that there exist {{c1::invertible operators}} \(R \in \mathcal{L}(V)\) and \(S \in \mathcal{L}(W)\)&nbsp;</li><li>Such that {{c2::\(T_{1}\)}} = {{c3::\(S T_{2} R\)}} if and only if {{c4::\(\operatorname{dim}\) null \(T_{1}\)}} ={{c5::\(\operatorname{dim}\) null \(T_{2}\)}}.</li></ul>

============================================================

  

    8 <br><br>Suppose \(V\) is finite-dimensional and \(T: V \rightarrow W\) is a surjective linear map of \(V\) onto \(W\). <br><ul><li>Prove that there is a subspace \(U\) of \(V\)&nbsp;</li><li>Such that {{c5::{{c1::\(\left.T\right|_{U}\)}}}} is an {{c4::{{c2::isomorphism}}}} {{c4::{{c3::of \(U\) onto \(W\)}}}}.&nbsp;</li></ul>

============================================================

  

    9 <br><br>Suppose \(V\) is finite-dimensional and \(S, T \in \)&nbsp; {{c1::\( \mathcal{L}(V)\)}}. <br><br><ul><li>Prove that {{c3::\(S T\)}} is {{c2::invertible}} if and only if {{c4::both \(S\) and \(T\)}} {{c5::are invertible}}.</li></ul>

============================================================

  

    10 <br>Suppose \(V\) is finite-dimensional and \(S, T \in \mathcal{L}(V)\). <br><ul><li>Prove that {{c2::\(S T\)}} = {{c1::\(I\)}} if and only if {{c5::{{c3::\(T S\)}}}} = {{c5::{{c4::\(I\)}}}}.</li></ul>

============================================================

  

    11 <br><br>Suppose \(V\) is finite-dimensional and \(S, T, U \in \mathcal{L}(V)\) and {{c1::\(S T U\)}} {{c2::= \(I\)}}. <br><ul><li>Show that \(T\) is {{c3::invertible}} and that {{c4::\(T^{-1} \)}} ={{c5::\(U S\)}}.</li></ul>

============================================================

  

    13 <br><br><ul><li>Suppose \(V\) is a finite-dimensional vector space and \(R, S, T \in \mathcal{L}(V)\) are such that {{c1::\(R S T\)}} is {{c2::surjective}}.</li></ul><ul><li>Prove that {{c5::{{c3::\(S\)}}}} is {{c5::{{c4::injective}}}}.</li></ul>

============================================================

  

    14 Suppose \(v_{1}, \ldots, v_{n}\) is {{c1::a basis of \(V\)}}. <br><br><ul><li>Prove that the map \(T:\) {{c2::\( V \)}} \( \rightarrow \) {{c3::\( \mathbf{F}^{n, 1}\)}} defined by&nbsp;</li><ul><li>\(T v\) = {{c4::\(\mathcal{M}(v)\)}}</li></ul><li>is an {{c5::isomorphism}} of \(V\) onto \(\mathbf{F}^{n, 1}\)<br></li></ul>

============================================================

  

    15<br><br><ul><li>&nbsp;Prove that every linear map from {{c1::\(\mathbf{F}^{n, 1}\)}} to {{c2::\(\mathbf{F}^{m, 1}\)}} is given by a {{c3::matrix multiplication}}. </li><li>In other words, prove that:</li><ul><li>If {{c4::\(T \in \mathcal{L}\left(\mathbf{F}^{n, 1}, \mathbf{F}^{m, 1}\right)\)}},&nbsp;</li><li>then there exists an {{c5::\(m\)-by- \(n\) matrix \(A\)}}&nbsp;</li><li>such that \(T x=A x\) for  every \(x \in \mathbf{F}^{n, 1}\).</li></ul></ul>

============================================================

  

    16 <br>Suppose \(V\) is finite-dimensional and \(T \in \mathcal{L}(V)\). <br><ul><li>Prove that \(T\) is {{c1::a scalar multiple}} {{c2::of the identity}} if and only if {{c5::{{c3::\(S T\) }}}} ={{c5::{{c3::\(T S\)}}}} for {{c5::{{c4::every \(S \in \mathcal{L}(V)\).}}}}</li></ul>

============================================================

  

    17 <br><ul><li>Suppose \(V\) is finite-dimensional and \(\mathcal{E}\) is a subspace of \(\mathcal{L}(V)\)&nbsp;</li><li>Such that {{c1::\(S T \in \mathcal{E}\)}}&nbsp; and {{c2::\(T S \in \mathcal{E}\)}} for all \(S \in \mathcal{L}(V)\) and all \(T \in \mathcal{E}\)</li><li>Prove that {{c3::\(\mathcal{E}\)}} = {{c4::\(\{0\}\)}} or {{c5::\(\mathcal{E} \)}} = \(\mathcal{L}(V)\).</li></ul>

============================================================

  

    19 Suppose \(T \in \mathcal{L}(\mathcal{P}(\mathbf{R}))\) is such that \(T\) is {{c1::injective}} and {{c2::\(\operatorname{deg} T p \leq \operatorname{deg} p\)}} for every nonzero polynomial \(p \in \mathcal{P}(\mathbf{R})\).<br><br><ul><li>(a) Prove that \(T\) is {{c3::surjective}}.</li><li>(b) Prove that {{c4::\(\operatorname{deg} T p\)}} = {{c5::\(\operatorname{deg} p\)}} for every nonzero \(p \in \mathcal{P}(\mathbf{R})\).</li></ul>

============================================================

  

    20 <br><br>Suppose \(n\) is a positive integer and \(A_{i, j} \in \mathbf{F}\) for \(i, j=1, \ldots, n\). Prove that the following are equivalent (note that in both parts below, the number of equations equals the number of variables):<br><br>(a) The {{c1::trivial}} solution {{c2::\(x_{1}=\cdots=x_{n}=0\)}} is the {{c5::{{c3::only}}}} solution to the {{c5::{{c4::homogeneous system of equations}}}}<br><br>\[<br>\begin{gathered}<br>\sum_{k=1}^{n} A_{1, k} x_{k}=0 \\<br>\vdots \\<br>\sum_{k=1}^{n} A_{n, k} x_{k}=0 .<br>\end{gathered}<br>\]<br>

============================================================

  

    20 Suppose \(n\) is a positive integer and \(A_{i, j} \in \mathbf{F}\) for \(i, j=1, \ldots, n\). Prove that the following are equivalent (note that in both parts below, the number of equations equals the number of variables):<br><br>(a) The trivial solution \(x_{1}=\cdots=x_{n}=0\) is the only solution to the homogeneous system of equations<br><br>{{c4::\[<br>\begin{gathered}<br>\sum_{k=1}^{n} A_{1, k} x_{k}=0 \\<br>\vdots \\<br>\sum_{k=1}^{n} A_{n, k} x_{k}=0 .<br>\end{gathered}<br>\]}}<br><br>(b) For every {{c1::\(c_{1}, \ldots, c_{n} \in \mathbf{F}\)}}, there exists {{c5::{{c2::a solution to the system of equations}}}}<br><br>{{c5::{{c3::\[<br>\begin{gathered}<br>\sum_{k=1}^{n} A_{1, k} x_{k}=c_{1} \\<br>\vdots \\<br>\sum_{k=1}^{n} A_{n, k} x_{k}=c_{n} .<br>\end{gathered}<br>\]}}}}<br>

============================================================

  

    1. (3 points) Consider the set of complex numbers<br><br>\[<br>G=\{a+b i \mid a, b \in \mathbb{Q}\}<br>\]<br><br><br>Is \(G\) a field (with the same addition and multiplication operations as in \(\mathbb{C}\) )?&nbsp;<br><br>Yes because:<br><ul><li>It is {{c4::closed under addition and scalar multiplication}}</li><li>Every element has {{c3::an inverse}}:</li><li>\(-(a+b i) \) = {{c5::{{c1::\(-a-b i, \)&nbsp;}}}}</li><li>\( \quad(a+b i)^{-1}\)&nbsp; = {{c5::{{c2::\(a /\left(a^{2}+b^{2}\right)-b i /\left(a^{2}+b^{2}\right)\)}}}}<br></li></ul>

============================================================

  

    \(1.5_{2}\). (3 points) Consider the set of complex numbers<br><br>\(\sigma\)<br><br>\[<br>M=\left\{r e^{\frac{\pi i}{2}}+s e^{\pi i} \mid r, s \in \mathbb{Q},\right\} .<br>\]<br><br>Is \(M\) a field?<br><br><ol><li>&nbsp;\(&nbsp; e^{\theta i} \) =&nbsp;{{c5::{{c1::\(\cos \theta\) +&nbsp;\(i sin \theta\)}}}}</li><li>\(e^{\frac{\pi i}{2}}\) =&nbsp; {{c4::{{c2::0&nbsp; + i = i}}}}</li><li>Thus the field is {{c4::{{c3::the same as the gaussian integers and thus closed}}}}</li></ol>

============================================================

  

    4. (3 points) The second edition Axler, page 12 , exercise 5 . <br><br>For each of the following subsets of \(\mathbf{F}^{3}\), determine whether it is a subspace of \(F^3\)<ul><li>(a) \(\left\{\left(x_{1}, x_{2}, x_{3}\right) \in \mathbf{F}^{3}: x_{1}+2 x_{2}+3 x_{3}=0\right\}\).&nbsp;</li><ul><li>Addition {{c1::\( (x_1+y_1) + 2(x_2+y_2) + 3(x_3+y_3) \)}} = {{c1::0}}</li><li>Mult:&nbsp;{{c1::\(\lambda x_1 + 2 (\lambda x_2) + 3(\lambda x_3)\)}} = {{c1::0}}</li></ul><li>(b) \(\left\{\left(x_{1}, x_{2}, x_{3}\right) \in \mathbf{F}^{3}: x_{1}+2 x_{2}+3 x_{3}=4\right\}\). </li><ul><li>{{c2::No origin}}</li></ul><li>(c) \(\left\{\left(x_{1}, x_{2}, x_{3}\right) \in \mathbf{F}^{3}: x_{1} x_{2} x_{3}=0\right\}\).</li><ul><li>&nbsp;Addition:&nbsp; {{c3::&nbsp;\( \prod x_i = 0 \land \prod y_i = 0\)}} {{c4::\(\not \implies\)}} {{c3::\(&nbsp; \prod (x_i+y_i) = 0 \)&nbsp;}}</li></ul><li>(d) \(\left\{\left(x_{1}, x_{2}, x_{3}\right) \in \mathbf{F}^{3}: x_{1}=5 x_{3}\right\}\).</li><ul><li>Addition:&nbsp;\(x_1 + y_1\) = {{c5::\(5x_3 + 5 y_3\)}} =&nbsp;{{c5::\(5 (x_3+y_3)\)}}</li><li>Mult:&nbsp;\(&nbsp; \lambda x_1\) =&nbsp;{{c5::\(5 \lambda x_3\)}}</li></ul></ul>

============================================================

  

    3 5. (3 points) The second edition Axler, page 19, exercise 15.<br><br>Prove or give a counterexample: if \(U_{1}, U_{2}, W\) are subspaces of \(V\) such that<br><br>\[<br>V=U_{1} \oplus W \quad \text { and } \quad V=U_{2} \oplus W<br>\]<br><br>then \(U_{1}=U_{2}\).<br><br><ul><li>{{c1::Counterexample}}:</li><ul><li>V =&nbsp;\( \{ (x,y) |&nbsp; &nbsp;x,y \in R&nbsp; \} \)</li><li>W =&nbsp;{{c2::\( \{ (x,x) |&nbsp; &nbsp;x \in R&nbsp; \} \)}}</li><li>\(U_1\) = {{c5::{{c3::\( \{ (x,0) |&nbsp; &nbsp;x \in R&nbsp; \} \)}}}}<br></li><li>\(U_2\) = {{c5::{{c4::\( \{ (0,x) |&nbsp; &nbsp;x \in R&nbsp; \} \)}}}}</li><li>{{c1::\(U_1\)&nbsp;\(\neq\)&nbsp;\(U_2\)}}</li><li>V =&nbsp;{{c1::\(U_1 \oplus W \)}} =&nbsp; {{c1::\(U_2 \oplus W\)&nbsp;}}<br></li></ul></ul>

============================================================

  

    6. (3 points) The second edition Axler, page 35, exercise 1.<br><br>Prove that if \(\left(v_{1}, \ldots, v_{n}\right)\) spans \(V\), then so does the list<br><br>\[<br>\left(v_{1}-v_{2}, v_{2}-v_{3}, \ldots, v_{n-1}-v_{n}, v_{n}\right)<br>\]<br><br>obtained by subtracting from each vector (except the last one) the following vector.<br><br>Solution:<br><br><ul><li>\(\forall x \in V\)</li><ul><li>&nbsp;\( \exists \vec{c} \)</li><li>\( x = \vec{c} \cdot \vec{v}\)</li></ul><li>\(x\) = {{c5::{{c3::\(\vec{c^\prime}\)&nbsp;}}}} \(\cdot\) \( [v_1 - v_2,\ldots, v_{n-1} - v_n, v_n]\)<br></li><ul><li>Key step: {{c4::{{c1::Create new weights from the old ones}}}}</li><li>{{c4::{{c2::\(c_i^\prime\)}}}} =&nbsp;{{c4::{{c2::\(c_i + c_{i-1}\)}}}}<br></li></ul><li>Thus {{c5::{{c3::the new list is spanning}}}}</li></ul>

============================================================

  

    (3 points) The second edition Axler, page 35, exercise 3.<br><br>Suppose \(\left(v_{1}, \ldots, v_{n}\right)\) is linearly independent in \(V\) and \(w \in V\). <br><br>Prove that if \(\left(v_{1}+\right.\) \(\left.w, \ldots, v_{n}+w\right)\) is linearly dependent, then \(w \in \operatorname{span}\left(v_{1}, \ldots, v_{n}\right)\).<br><br>Solution:<br><ul><li>\(\vec{c} \cdot \vec{v} = 0\)&nbsp;\(\implies\)&nbsp;\( \vec{c} = \vec{0}\)<br></li><li>Not linearly indp implies&nbsp;{{c1::\(\vec{c} \neq \vec{0}\)}}<br></li><ul><li>{{c1::\(\vec{c} \cdot (\vec{v} + w \cdot \vec{1})\)}}<br></li><li>=&nbsp;{{c2::\(\vec{c} \cdot \vec{v}\)}} + {{c2::\(w \times \vec{c} \cdot \vec{1}\)}}</li><li>=&nbsp;{{c5::{{c3::\(\vec{c} \cdot \vec{v}\)}}}}&nbsp; + {{c5::{{c3::\(w \sum c_i\)}}}}</li></ul><li>Which implies:</li><ul><li>w =&nbsp;{{c5::{{c4::\(\frac{\vec{c} \cdot \vec{v} }{\sum c_i}\)}}}}</li><li>w&nbsp;\(\in\)&nbsp;{{c1::\(\operatorname{span} \vec{v}\)}}</li></ul></ul><br>

============================================================

  

    \section{1. (3 points) Let \(V\) be the vector space of polynomials of degree at most five with real coefficients. Define a linear map}<br><br>\[<br>T: V \rightarrow \mathbb{R}^{3}, \quad T(p)=(p(1), p(2), p(3)) .<br>\]<br><br>That is, the coordinates of the vector \(T(p)\) are the values of \(p\) at 1,2 , and 3.<br><br><ul><li>a) Find a basis for the null space of T:</li></ul><ol><ol><li>For a polynomial p to be in the null space when evaluated at positions \(a_1 \ldots a_n\), it needs to {{c1::vanish and thus be divisible by}}&nbsp;\(z(x) \) = {{c2::\(\prod_{ 1 \leq i \leq n} (x-a_i) \)&nbsp;}}</li><ul><li>In this case \( z(x) \) = {{c2::\( (x-1)(x-2)(x-3)\)}}</li></ul><li>We can create a basis of the null space with degree at most 5 as:</li><ul><li>{{c5::{{c3::\( (z(x), x z(x), x^2 z(x)) \)}}}}<br></li><li>Because {{c5::{{c4::polynomials of different degrees}}}} are {{c5::{{c4::linearly independent}}}}</li></ul></ol></ol><ul><li>b) Find a basis of the range of T</li><ol><li>This basis is the complement of the null space basis, and can be written as:</li><ul><li>\((1,x,x^2)\) since z(x) is of degree 3<br></li></ul></ol></ul><br>

============================================================

  

    \section{1. (3 points) Let \(V\) be the vector space of polynomials of degree at most five with real coefficients. Define a linear map}<br><br>\[<br>T: V \rightarrow \mathbb{R}^{3}, \quad T(p)=(p(1), p(2), p(3)) .<br>\]<br><br>That is, the coordinates of the vector \(T(p)\) are the values of \(p\) at 1,2 , and 3.<br><br><ul><li>a) Find a basis for the null space of T:</li></ul><ol><li>For a polynomial p to be in the null space when evaluated at positions \(a_1 \ldots a_n\), it needs to vanish and thus be divisible by&nbsp;\(z(x) \) = \(\prod_{ 1 \leq i \leq n} (x-a_i) \)&nbsp;</li><ul><li>In this case \( z(x) \) = \( (x-1)(x-2)(x-3)\)</li></ul><li>We can create a basis of the null space with degree at most 5 as:</li><ul><li>\( (z(x), x z(x), x^2 z(x)) \)<br></li><li>Because polynomials of different degrees are linearly independent</li></ul></ol><ul><li>b) Find a basis of the range of T</li><ol><li>This basis is the {{c5::{{c1::complement of the null space basis}}}}, and can be written as:</li><ul><li>{{c4::{{c2::\((1,x,x^2)\)}}}} since {{c4::{{c3::z(x) is of degree 3}}}}<br></li><li></li></ul></ol></ul>

============================================================

  

    2. (3 points) Let \(V\) be the vector space of polynomials of degree at most 999 with real coefficients. Define a linear map<br><br>\[<br>T: V \rightarrow \mathbb{R}^{100}, \quad T(p)=(p(1), p(2), \ldots, p(100)) .<br>\]<br><br><ul><li>a) Find the dimension of the null space of \(T\)</li><ul><li>Since p must be divisible by the polynomial \(z(x)\) =&nbsp;{{c5::{{c1::\(\prod_{i=1}^{100} (x-i)\)}}}} with degree {{c5::{{c1::100}}}}&nbsp;</li><li>A basis of null space consists of the polynomials&nbsp;</li><ul><li>{{c4::{{c2::\( (z(x), x z(x),....,x^{899} z(x) )\)}}}}<br></li></ul><li>Meaning that the null space has dimension {{c4::{{c3::900}}}}&nbsp;</li></ul></ul>

============================================================

  

    2. (3 points) Let \(V\) be the vector space of polynomials of degree at most 999 with real coefficients. Define a linear map<br><br>\[<br>T: V \rightarrow \mathbb{R}^{100}, \quad T(p)=(p(1), p(2), \ldots, p(100)) .<br>\]<br><br><ul><li>a) Find the dimension of the null space of \(T\)</li><ul><li>Since p must be divisible by the polynomial \(z(x)\) =&nbsp;\(\prod_{i=1}^{100} (x-i)\) with degree 100&nbsp;</li><li>A basis of null space consists of the polynomials&nbsp;</li><ul><li>\( (z(x), x z(x),....,x^{899} z(x) )\)<br></li></ul><li>Meaning that the null space has dimension 900&nbsp;</li></ul><li>b) Find the dimension of the range of T</li><ul><li>dim range(T) = {{c1::1000 - 900 = 100}}</li><li>Could have also been shown by writting down a set of polynomials which {{c2::</li><li>get mapped to the standard basis of&nbsp;\(R^{100}\)}}</li><li>\(p_j(x)\) = {{c5::{{c3::\(&nbsp;\prod_{1 \leq i \leq 100, i \neq j}(x-i)\)}}}}&nbsp;\( / \) {{c5::{{c4::\( \prod_{1 \leq i \leq 100, i \neq j}(j-i)&nbsp;\)}}}}</li></ul></ul>

============================================================

  

    3. (6 points) Let \(V\) be the vector space of polynomials of degree at most 99 with real coefficients. Define a linear map<br><br>\[<br>T: V \rightarrow \mathbb{R}^{1000}, \quad T(p)=(p(1), p(2), \ldots, p(1000)) .<br>\]<br><br><ul><li>a) Find the dimension of the null space:</li><ul><li>Since for a polynomial to be in the null space it must {{c1::vanish for 1,...,1000}} then it must be of at degree {{c2::1000 at least}}</li><li>Since the polynomials of V have degree {{c5::{{c3::at most 99}}}}, the null space must be have dimension {{c5::{{c4::0}}}}</li></ul></ul>

============================================================

  

    4. (3 points)<br><br>Suppose \(V\) and \(W\) are vector spaces over \(F\); that \(T \in\) \(\mathcal{L}(V, W)\); that \(\left(v_{1}, \ldots, v_{n}\right)\) is any list of vectors in \(V\); and that \(\left(T v_{1}, \ldots, T v_{n}\right)\) is linearly independent in \(W\).<ol><li>Prove that \(\left(v_{1}, \ldots, v_{n}\right)\) is linearly independent in \(V\).</li></ol><div>Solution:</div><div><ol><li>\(\forall x \in V\)&nbsp;\(x = \vec{v} \cdot \vec{v}\)<br></li><li>\(T(x)\) =&nbsp;\(T(\vec{c} \cdot \vec{v}) \)<br></li><li>Assume that&nbsp;{{c1::\(\vec{v}\) is not lin indp}}:</li><ol><li>Meaning {{c2::\(\exists \vec{b}\) such that}} :</li><ol><li>{{c2::\(\vec{b} \cdot \vec{v} = 0\)}}</li><li>And&nbsp;{{c2::\(\vec{b} \neq \vec{0}\)}}</li></ol><li>Which implies&nbsp;</li><ol><li>{{c5::{{c3::\(T(\vec{b}\cdot \vec{v}) \)}}}} = {{c5::{{c3::\( 0\)}}}}</li><li>&nbsp;Thus {{c5::{{c3::\(\sum b_i Tv_i = 0\)}}}} and&nbsp;{{c5::{{c4::\(\vec{b} \neq \vec{0}\)}}}}</li><li>Which would imply that&nbsp;{{c5::{{c4::\(\vec{Tv}\) was not linearly independent, which would be a contradiction}}}}</li></ol></ol></ol></div>

============================================================

  

    17 <br><br>Suppose \(p_{0}, p_{1}, \ldots, p_{m}\) are polynomials in \(\mathcal{P}_{m}(\mathbf{F})\) such that \(p_{j}(2)=0\) for each \(j\). <br><ul><li>Prove that \(p_{0}, p_{1}, \ldots, p_{m}\) is not linearly independent in \(\mathcal{P}_{m}(\mathbf{F})\).</li></ul><br>Solution:<br><ol><li>Assume that&nbsp;{{c1::\(\vec{p}\) is lin indp}}</li><ol><li>Since&nbsp;{{c2::\(\mathrm{len} \, \vec{p} \) = m}} then {{c3::the list \(1, p_0,...,p_m\)}} {{c1::must be linearly dependent}}</li><li>{{c3::\(1 \)}}&nbsp;\(\in\)&nbsp;{{c4::\(\operatorname{span} \vec{p}\)}}<br></li><ol><li>So {{c3::1}} =&nbsp;{{c4::\(\vec{c} \cdot \vec{p}\)}}</li></ol><li>However:</li><ol><li>{{c5::&nbsp;\(1 x^0 (2)\)}} =&nbsp;\(\vec{c} \cdot \vec{p}\)(2)</li><li>\(\implies\)&nbsp;{{c5::\(1\)}} = 0<br></li><li>So {{c1::the initial list must have been linearly dependent}}</li></ol></ol></ol>

============================================================

  

    <br>14 Suppose \(U_{1}, \ldots, U_{m}\) are finite-dimensional subspaces of \(V\). Prove that<br><br>\(U_{1}+\cdots+U_{m}\) is finite-dimensional and<br><br>\(\operatorname{dim}\left(U_{1}+\cdots+U_{m}\right) \leq \operatorname{dim} U_{1}+\cdots+\operatorname{dim} U_{m}\).<br><br><ul><li>Proof method: induction</li><ol><li>Base case: {{c1::dim\((U_1+U_2)\)}} =</li><ol><li>= {{c2::dim(\(U_1\)) + dim(\(U_2\)) - dim(\(U_1 \cap U_2\))&nbsp;}}</li><li>\(\leq\) {{c3::dim(\(U_1\)) + dim(\(U_2\))}}</li></ol><li>Inductive hypothesis: {{c4::dim (\(\sum_{i=1}^{k} U_i\))}}&nbsp;\(\leq\)&nbsp;{{c5::\( \sum_{i=1}^{k} \operatorname{dim} U_i\)}}</li><ol><li>{{c4::dim (\(\sum_{i=1}^{k+1} U_i\))}} =&nbsp;</li><ol><li>= {{c2::dim (\(\sum_{i=1}^{k} U_i\)) + dim (\(U_{k+1}\)) - dim(\(&nbsp; \sum_{i=1}^{k} U_i \cap U_{k+1} \))}}</li><li>\(\leq\) {{c3::\( \underbrace{\operatorname{dim} (\sum_{i=1}^{k} U_i) + \operatorname{dim} (U_k)}_{\operatorname{dim}\sum_{i=1}^{k+1} U_i&nbsp;} \)}}</li></ol></ol></ol></ul>

============================================================

  

    16 Suppose \(U_{1}, \ldots, U_{m}\) are finite-dimensional subspaces of \(V\) such that \(U_{1}+\cdots+U_{m}\) is a direct sum. Prove that \(U_{1} \oplus \cdots \oplus U_{m}\) is finite-<br><br>\(\operatorname{dim} U_{1} \oplus \cdots \oplus U_{m}=\operatorname{dim} U_{1}+\cdots+\operatorname{dim} U_{m}\).<br><br><ul><li>Proof method: induction</li><ol><li>Base case:&nbsp;\(\operatorname{dim} (U_1\oplus U_2)\) =&nbsp;&nbsp;</li><ol><li>=&nbsp;{{c1::\( \operatorname{dim} U_1\)&nbsp; + \(\operatorname{dim} U_2\) - \(\underbrace{\operatorname{dim} (U_1 \cap U_2)}_{0}&nbsp;\)}}</li><li>= {{c2::\( \operatorname{dim} U_1\)&nbsp; + \(\operatorname{dim} U_2\)}}</li></ol><li>Inductive hypothesis:&nbsp;{{c5::{{c3::\(\operatorname{dim} \sum_{i=1}^{k} \oplus U_i \)}}}} =&nbsp;{{c5::{{c4::\(\sum_{i=1}^{k} dim(U_i)\)}}}}</li><ol><li>{{c5::{{c3::\(\operatorname{dim} \sum_{i=1}^{k+1} \oplus U_i \)}}}} =&nbsp;</li><li>= {{c1::\(\operatorname{dim} \sum_{i=1}^{k} \oplus U_i \) +&nbsp;\(\) \(\operatorname{dim} U_{k+1} \) - \( \underbrace{\operatorname{dim} \sum_{i=1}^{k} \oplus U_i \ \cap U_{k+1}&nbsp;}_{\oplus \implies 0} \)}}<br></li><li>= {{c2::\(\operatorname{dim} \sum_{i=1}^{k} \oplus U_i \) +&nbsp;\(\) \(\operatorname{dim} U_{k+1} \)}}</li><li>= {{c5::{{c3::\(\sum_{i=1}^{k+1} dim(U_i)\)}}}}</li></ol></ol></ul>

============================================================

  

    10 <br>Suppose \(v_{1}, \ldots, v_{n}\) spans \(V\) and \(T \in \mathcal{L}(V, W)\). Prove that the list&nbsp;\(Tv_1,\ldots Tv_n\) spans range T<br><br><br>Solution:<ol><li>{{c1::\(Tv_1,\ldots,Tv_n\)}} \(\in \){{c1::&nbsp;\(span(Tv_1,\ldots,Tv_n)\)}}<br></li><li>For {{c2::all&nbsp;\(x \in V\)}}, {{c3::\(x = \vec{c} \cdot \vec{v}\)}}</li><ol><li>\(\implies\)&nbsp;{{c4::\(T(x) = T(\vec{c} \cdot \vec{v})\)}} for {{c3::all Tx in range T}}</li><li>Thus {{c5::range T&nbsp;\(\in\)&nbsp;\(span(Tv_1,\ldots,Tv_n)\)}}</li></ol><li>By 1 and 2, \(Tv_1,\ldots Tv_n\) spans range T</li></ol>

============================================================

  

    1 Suppose \(T\) is a function from \(V\) to \(W\). The graph of \(T\) is the subset of \(V \times W\) defined by<br><br>{{c5::{{c1::\(\text { graph of } T\)}}}}&nbsp;= {{c4::{{c3::\(\{(v, T v) \in V \times W: v \in V\} .\)}}}}<br><br>Prove that \(T\) is a linear map if and only if {{c5::{{c1::the graph of \(T\)}}}} is {{c4::{{c2::a subspace of \(V \times W\).}}}}

============================================================

  

    2 <br><br>Suppose \(V_{1}, \ldots, V_{m}\) are vector spaces such that {{c5::{{c1::\(V_{1} \times \cdots \times V_{m}\)}}}} is {{c4::{{c2::finite dimensional.}}}} <br><ul><li>Prove that \(V_{j}\) is {{c4::{{c2::finite-dimensional}}}} for {{c4::{{c3:: each \(j=1, \ldots, m\).}}}}</li></ul>

============================================================

  

    4 <br>Suppose \(V_{1}, \ldots, V_{m}\) are vector spaces. <br><ul><li>Prove that {{c5::{{c1::\(\mathcal{L}\left(V_{1} \times \cdots \times V_{m}, W\right)\)}}}} and {{c4::{{c2::\(\mathcal{L}\left(V_{1}, W\right) \times \cdots \times \mathcal{L}\left(V_{m}, W\right)\)}}}} are {{c4::{{c3::isomorphic vector spaces}}}}.</li></ul>

============================================================

  

    5 <br><br>Suppose \(W_{1}, \ldots, W_{m}\) are vector spaces. <br><br><ul><li>Prove that {{c5::{{c1::\(\mathcal{L}\left(V, W_{1} \times \cdots \times W_{m}\right)\)}}}} and {{c4::{{c2::\(\mathcal{L}\left(V, W_{1}\right) \times \cdots \times \mathcal{L}\left(V, W_{m}\right)\)}}}} are {{c4::{{c3::isomorphic}}}} vector spaces.</li></ul>

============================================================

  

    6 <br>For \(n\) a positive integer, define {{c2::\(V^{n}\}}) by<br><br><ul><li>{{c2::\(V^{n}\)}} = {{c1::\(\underbrace{V \times \cdots \times V}_{n \text { times } } .\)}}</li></ul><br>Prove that {{c2::\(V^{n}\)}} and \(\mathcal{L}\){{c5::{{c3::\(\left(\mathbf{F}^{n}, V\right)\)}}}} are {{c5::{{c4::isomorphic}}}} vector spaces.<br>

============================================================

  

    7 <br>Suppose \(v, x\) are vectors in \(V\) and \(U, W\) are subspaces of \(V\) such that {{c5::{{c1::\(v+U\)}}}} = {{c4::{{c2::\(x+W\)}}}}. <br><ul><li>Prove that {{c4::{{c3::\(U=W\)}}}}.</li></ul>

============================================================

  

    8 <br><br><ul><li>Prove that a nonempty subset \(A\) of \(V\) is {{c5::{{c1::an affine subset of \(V\)}}}}</li><li>&nbsp;If and only if {{c4::{{c2::\(\lambda v+(1-\lambda) w \in A\)}}}} for {{c4::{{c3::all \(v, w \in A\)}}}} and {{c4::{{c3::all \(\lambda \in \mathbf{F}\)}}}}.</li></ul>

============================================================

  

    9 <br>Suppose \(A_{1}\) and \(A_{2}\) are {{c5::{{c2::affine subsets of \(V\)}}}}. <br><ul><li>Prove that {{c5::{{c2::the intersection \(A_{1} \cap A_{2}\)}}}} is either an {{c4::{{c1::affine subset of \(V\)}}}} or {{c4::{{c3::the empty set.}}}}</li></ul>

============================================================

  

    10<br>Prove that {{c1::the intersection}} of {{c2::every collection}} of {{c5::{{c3::affine subsets of \(V\)}}}} is either {{c5::{{c3::an affine subset of \(V\)}}}} or {{c5::{{c4::the empty set.}}}}

============================================================

  

    11 Suppose \(v_{1}, \ldots, v_{m} \in V\). Let<br><br>\(A=\left\{\lambda_{1} v_{1}+\cdots+\lambda_{m} v_{m}: \lambda_{1}, \ldots, \lambda_{m} \in \mathbf{F}\right.\) and \(\left.\lambda_{1}+\cdots+\lambda_{m}=1\right\}\).<br><br><ul><li>(a) Prove that \(A\) is {{c1::an affine subset of \(V\)}}.</li><li><br></li><li>(b) Prove that every {{c1::affine subset of \(V\)}} that {{c2::contains \(v_{1}, \ldots, v_{m}\)}} also {{c3::contains \(A\).}}</li><li><br></li><li>(c) Prove that \(A\) = {{c1::\(v+U\)}} for {{c1::some \(v \in V\)}} and {{c4::some subspace \(U\) of \(V\)}} with {{c5::\(\operatorname{dim} U \leq m-1\)}}.</li></ul>

============================================================

  

    12 <br><br>Suppose \(U\) is a subspace of \(V\) such that \(V / U\) is {{c5::{{c1::finite}}}}-dimensional<br><ul><li>Prove that \(V\) is {{c4::{{c2::isomorphic}}}} to {{c4::{{c3::\(U \times(V / U)\)}}}}.</li></ul>

============================================================

  

    13 <br>Suppose \(U\) is a subspace of \(V\) and {{c5::{{c3::\(v_{1}+U, \ldots, v_{m}+U\)}}}} is a basis of \(V / U\) and {{c5::{{c3::\(u_{1}, \ldots, u_{n}\)}}}} is a basis of \(U\). <br><ul><li>Prove that {{c4::{{c1::\(v_{1}, \ldots, v_{m}, u_{1}, \ldots, u_{n}\)}}}} is {{c4::{{c2::a basis of \(V\)}}}}.</li></ul>

============================================================

  

    14 <br><br>Suppose \(U=\{\left(x_{1}, x_{2}, \ldots\right) \in \mathbf{F}^{\infty}:\) {{c1::\(x_{j} \neq 0\)}} for only {{c2::finitely many \(\left.j\right\}\)}}.<br><br><ul><li>(a) Show that \(U\) is {{c3::a subspace of \(\mathbf{F}^{\infty}\)}}.</li><li><br></li><li>(b) Prove that {{c4::\(\mathbf{F}^{\infty} / U\)}} is {{c5::infinite}}-dimensional.</li></ul>

============================================================

  

    15 <br><br>Suppose \(\varphi \in \mathcal{L}(V, \mathbf{F})\) and {{c5::{{c1::\(\varphi \neq 0\)}}}}. <br><br><ul><li>Prove that {{c4::{{c2::\(\operatorname{dim} V /(\) null \(\varphi)\)}}}} = {{c4::{{c3::\(1\)}}}}.</li></ul>

============================================================

  

    16 <br>Suppose \(U\) is a subspace of \(V\) such that {{c1::\(\operatorname{dim} V / U\)}}= {{c2::\(1\)}}. <br><br><ul><li>Prove that there exists {{c3::\(\varphi \in \mathcal{L}(V, \mathbf{F})\)}} such that {{c4::null \(\varphi\)}} = {{c5::\(U\)}}.</li></ul>

============================================================

  

    17 <br>Suppose \(U\) is a subspace of \(V\) such that \(V / U\) is {{c1::finite}}-dimensional. <br><ul><li>Prove that there exists a subspace \(W\) of \(V\) such that {{c2::\(\operatorname{dim} W\)}} ={{c3::\(\operatorname{dim} V / U\)}} and {{c4::\(V\)}} = {{c5::\(U \oplus W\)}}.</li></ul>

============================================================

  

    18<br><br>Suppose \(T \in \mathcal{L}(V, W)\) and \(U\) is a subspace of \(V\). Let \(\pi\) denote the quotient map from \(V\) onto \(V / U\). <br><ul><li>Prove that there exists {{c1::\(S \in \mathcal{L}(V / U, W)\)}} such that {{c2::\(T\)}} = {{c2::\(S \circ \pi\)}} if and only if {{c3::\(U\)}}&nbsp; {{c4::\(\subset\)}}&nbsp; {{c5::null \(T\)}}.</li></ul>

============================================================

  

    20 Suppose \(U\) is a subspace of \(V\). Define \(\Gamma: \mathcal{L}(V / U, W) \rightarrow \mathcal{L}(V, W)\) by<br><br><ul><li>\(\Gamma(S)\) = {{c1::\(S \circ \pi\)}}</li></ul><br>(a) Show that \(\Gamma\) is {{c2::a linear map.}}<br><br>(b) Show that \(\Gamma\) is {{c3::injective}}.<br><br>(c) Show that {{c4::range \(\Gamma\)}} ={{c5::\(\{T \in \mathcal{L}(V, W): T u=0\) for every \(u \in U\}\).}}<br>

============================================================

  

    1 Explain why every {{c5::{{c3::linear functional}}}} is either {{c4::{{c1::surjective}}}} or {{c4::{{c2::the zero map}}}}.

============================================================

  

    3 <br>Suppose \(V\) is finite-dimensional and \(v \in V\) with {{c5::{{c1::\(v \neq 0\)}}}}. <br><ul><li>Prove that there exists {{c4::{{c2::\(\varphi \in V^{\prime}\)}}}} such that {{c4::{{c2::\(\varphi(v)\)}}}} = {{c4::{{c3::\(1\)}}}}.</li></ul>

============================================================

  

    4 <br><br>Suppose \(V\) is finite-dimensional and \(U\) is a subspace of \(V\) such that {{c1::\(U \neq V\}})<div><br></div><div><br><br><ul><li>Prove that there exists {{c2::\(\varphi \in V^{\prime}\)}} such that {{c3::\(\varphi(u)\)}} = {{c4::\(0\)}} for {{c2::every \(u \in U\)}} but {{c5::\(\varphi \neq 0\)}}.</li></ul></div>

============================================================

  

    5 <br>Suppose \(V_{1}, \ldots, V_{m}\) are vector spaces. <br><ul><li>Prove that {{c5::{{c1::\(\left(V_{1} \times \cdots \times V_{m}\right)^{\prime}\)}}}} and {{c4::{{c2::\(V_{1}^{\prime} \times \cdots \times V_{m}{ }^{\prime}\)}}}} are {{c4::{{c3::isomorphic vector spaces}}}}.</li></ul>

============================================================

  

    6 <br>Suppose \(V\) is finite-dimensional and \(v_{1}, \ldots, v_{m} \in V\). Define a linear map \(\Gamma: V^{\prime} \rightarrow \mathbf{F}^{m}\) by<br><br><ul><li>\( \Gamma(\varphi)\) ={{c1::\(\left(\varphi\left(v_{1}\right), \ldots, \varphi\left(v_{m}\right)\right) .\)}}</li></ul><br><ul><li>(a) Prove that \(v_{1}, \ldots, v_{m}\) {{c2::spans \(V\)}} if and only if \(\Gamma\) is {{c3::injective}}.</li><li><br></li><li>(b) Prove that \(v_{1}, \ldots, v_{m}\) is {{c4::linearly independent}} if and only if \(\Gamma\) is {{c5::surjective}}.</li></ul>

============================================================

  

    7 <br>Suppose \(m\) is a {{c1::positive}} integer. <br><ul><li>Show that the dual basis of the basis {{c2::\(1, x, \ldots, x^{m}\)}} of \(\mathcal{P}_{m}(\mathbf{R})\) is {{c5::{{c3::\(\varphi_{0}, \varphi_{1}, \ldots, \varphi_{m}\)}}}}, where {{c5::{{c3::\(\varphi_{j}(p)\)}}}} ={{c5::{{c4::\(\frac{p^{(j)}(0)}{j !}\)}}}}.&nbsp;</li><li><i>Here \(p^{(j)}\) denotes the \(j^{\text {th }}\) derivative of \(p\)</i></li></ul>

============================================================

  

    9 <br><ul><li>Suppose \(v_{1}, \ldots, v_{n}\) is a basis of \(V\) and \(\varphi_{1}, \ldots, \varphi_{n}\) is the corresponding dual basis of \(V^{\prime}\).</li><li>Suppose {{c1::\(\psi\)}} \(\in\) {{c2::\(V^{\prime}\)}}. </li><li><b>Prove that</b></li><ul><li>{{c1::\(\psi\)}} = {{c5::{{c3::\(\psi\left(v_{1}\right) \varphi_{1}\)}}}} + \(\cdots\) + {{c5::{{c4::\(\psi\left(v_{n}\right) \varphi_{n} .\)}}}}</li></ul></ul>

============================================================

  

    11 <br><br>Suppose \(A\) is an \(m\)-by- \(n\) matrix with A {{c1::\(\neq 0\)}}. <br><ul><li>Prove that the rank of \(A\) is {{c2::1}} if and only if there exist:</li><ul><li>&nbsp;{{c3::\(\left(c_{1}, \ldots, c_{m}\right) \in \mathbf{F}^{m}\)}}&nbsp;</li><li>And {{c4::\(\left(d_{1}, \ldots, d_{n}\right) \in \mathbf{F}^{n}\)}}&nbsp;</li><li>Such that {{c5::\(A_{j, k}=c_{j} d_{k}\)}} for {{c5::every \(j=1, \ldots, m\)}} and {{c5::every \(k=1, \ldots, n\)}}.</li></ul></ul>

============================================================

  

    12 Show that the dual map of {{c1::the identity}} map on {{c3::\(V\)}} is {{c5::{{c2::the identity}}}} map on {{c5::{{c4::\(V^{\prime}\)}}}}.

============================================================

  

    15<br><br>&nbsp;Suppose \(W\) is finite-dimensional and \(T \in \mathcal{L}(V, W)\). <br><ul><li>Prove that {{c1::\(T^{\prime}\)}} = {{c2::\(0\)}} if and only if {{c5::{{c3::\(T\)}}}} ={{c5::{{c4::\(0\)}}}}.</li></ul>

============================================================

  

    16 <br><br>Suppose \(V\) and \(W\) are finite-dimensional. <br><ul><li>Prove that the map that takes {{c1::\(T \in \mathcal{L}(V, W)\)}} to {{c2::\(T^{\prime} \in \mathcal{L}\left(W^{\prime}, V^{\prime}\right)\)&nbsp;}}</li><li>Is an {{c3::isomorphism of }}&nbsp; {{c4::\(\mathcal{L}(V, W)\)}} {{c5::onto \(\mathcal{L}\left(W^{\prime}, V^{\prime}\right)\)}}.</li></ul>

============================================================

  

    17 <br>Suppose \(U \subset V\). <br><br><ul><li>Explain why {{c5::{{c1::\(U^{0}\)}}}} =\(\{\) {{c4::{{c2::\(\varphi \in V^{\prime}\)}}}} : {{c4::{{c3::\(U \subset.\) null \(\varphi\)}}}} \(\}\).</li></ul>

============================================================

  

    18 Suppose \(V\) is finite-dimensional and \(U \subset V\). Show that {{c1::\(U=\{0\}\)}} if and only if {{c2::\(U^{0}\)}} {{c5::{{c4::=}}}} {{c5::{{c3::\(V^{\prime}\)}}}}.

============================================================

  

    19 Suppose \(V\) is finite-dimensional and \(U\) is a subspace of \(V\). Show that {{c5::{{c1::\(U=V\)}}}} if and only if {{c4::{{c2::\(U^{0}\)}}}} ={{c4::{{c3::\(\{0\}\)}}}}.

============================================================

  

    20 Suppose \(U\) and \(W\) are subsets of \(V\) with \(U \subset W\). Prove that {{c5::{{c1::\(W^{0}\)}}}} {{c4::{{c2::\(\subset\)}}}} {{c4::{{c3::\(U^{0}\)}}}}.

============================================================

  

    21 Suppose \(V\) is finite-dimensional and \(U\) and \(W\) are subspaces of \(V\) with {{c1::\(W^{0} \subset U^{0}\)}}. <br><ul><li>Prove that {{c2::\(U\)}} {{c5::{{c3::\(\subset\)}}}} {{c5::{{c4::\(W\)}}}}.</li></ul>

============================================================

  

    22 Suppose \(U, W\) are subspaces of \(V\). Show that {{c5::{{c1::\((U+W)^{0}\)}}}} {{c4::{{c2::=}}}} {{c4::{{c3::\(U^{0} \cap W^{0}\)}}}}.

============================================================

  

    23 <br>Suppose \(V\) is finite-dimensional and \(U\) and \(W\) are subspaces of \(V\). <br><ul><li>Prove that {{c5::{{c1::\((U \cap W)^{0}\)}}}} {{c4::{{c2::=}}}} {{c4::{{c3::\(U^{0}+W^{0}\)}}}}.</li></ul>

============================================================

  

    25 Suppose \(V\) is finite-dimensional and \(U\) is a subspace of \(V\). Show that<br><br><ul><li>\(U\) = \(\{\) {{c1::\(v \in V\)}} : {{c2::\(\varphi(v)=0\)}} {{c5::{{c3::\(\text { for every }\)}}}} {{c5::{{c4::\(\varphi \in U^{0}\)}}}}&nbsp;\(\}\)</li></ul>

============================================================

  

    26 Suppose \(V\) is finite-dimensional and \(\Gamma\) is a subspace of \(V^{\prime}\). Show that<br><br><ul><li>\(\Gamma\) ={{c4::\(\{\)}} {{c1::\(v \in V\)}} : {{c5::{{c2::\(\varphi(v)=0\)}}}} {{c5::{{c3::\(\text { for every } \varphi \in \Gamma .\)}}}}&nbsp; {{c4::\(\}^{0}\)}}</li></ul>

============================================================

  

    28 Suppose \(V\) and \(W\) are finite-dimensional, \(T \in \mathcal{L}(V, W)\), and there exists \(\varphi \in W^{\prime}\) such that:<br><ul><li>{{c1::&nbsp;null \(T^{\prime}\)}} = {{c2::\(\operatorname{span}(\varphi)\)}}.</li></ul>Prove that:<br><ul><li>{{c5::{{c3::&nbsp;range \(T\)}}}} = {{c5::{{c4::\(\operatorname{null} \varphi\)}}}}.</li></ul>

============================================================

  

    29 Suppose \(V\) and \(W\) are finite-dimensional, \(T \in \mathcal{L}(V, W)\), and there exists \(\varphi \in V^{\prime}\) such that:<br><ul><li>{{c1::&nbsp;range \(T^{\prime}\)}} = {{c2::\(\operatorname{span}(\varphi)\)}}.&nbsp;</li></ul>Prove that:<br><ul><li>{{c5::{{c3::&nbsp;null \(T\)}}}} = {{c5::{{c4::\(\operatorname{null} \varphi\)}}}}.</li></ul>

============================================================

  

    30 Suppose \(V\) is finite-dimensional and \(\varphi_{1}, \ldots, \varphi_{m}\) is a {{c4::linearly independent}} list in \(V^{\prime}\).<br><br><ul><li>Prove that:</li><ul><li>\(\operatorname{dim}\) {{c1::\(\left(\left(\operatorname{null} \varphi_{1}\right) \cap \cdots \cap\left(\operatorname{null} \varphi_{m}\right)\right)\)}} {{c5::{{c3::=}}}} {{c5::{{c2::\((\operatorname{dim} V)-m .\)}}}}</li></ul></ul>

============================================================

  

    31 <br>Suppose \(V\) is finite-dimensional and \(\varphi_{1}, \ldots, \varphi_{n}\) is a {{c1::basis of \(V^{\prime}\)}}. <br><ul><li>Show that there exists {{c2::a basis of \(V\)}} {{c5::{{c3::whose dual basis}}}} {{c5::{{c4::is \(\varphi_{1}, \ldots, \varphi_{n}\)}}}}.</li></ul>

============================================================

  

    32 <br><br>Suppose \(T \in \mathcal{L}(V)\), and \(u_{1}, \ldots, u_{n}\) and \(v_{1}, \ldots, v_{n}\) are bases of \(V\). Prove that the following are equivalent:<br><br><ul><li>(a) \(T\) is&nbsp;{{c1::invertible}}.</li><li><br></li><li>(b) {{c2::The columns of \(\mathcal{M}(T)\) are linearly independent in \(\mathbf{F}^{n, 1}\)}}.</li><li><br></li><li>(c) {{c3::The columns of \(\mathcal{M}(T) \operatorname{span} \mathbf{F}^{n, 1}\)}}.</li><li><br></li><li>(d) {{c4::The rows of \(\mathcal{M}(T)\) are linearly independent in \(\mathbf{F}^{1, n}\)}}.</li><li><br></li><li>(e) {{c5::The rows of \(\mathcal{M}(T) \operatorname{span} \mathbf{F}^{1, n}\)}}.</li></ul><br>

============================================================

  

    33 <br>Suppose \(m\) and \(n\) are {{c4::positive}} integers. <br><ul><li>Prove that the function that takes {{c1::\(A\)}} to {{c1::\(A^{\mathrm{t} }\)}} is a linear map from {{c5::{{c2::\(\mathbf{F}^{m, n}\)}}}} to {{c5::{{c2::\(\mathbf{F}^{n, m}\)}}}}.&nbsp;</li><li>Furthermore, prove that this linear map is {{c5::{{c3::invertible}}}}.</li></ul>

============================================================

  

    34 <br>The double dual space of \(V\), denoted \(V^{\prime \prime}\), is defined to be the dual space of \(V^{\prime}\). In other words, \(V^{\prime \prime}=\left(V^{\prime}\right)^{\prime}\). <br><br><ul><li>Define \(\Lambda: V \rightarrow V^{\prime \prime}\) by</li><ul><li>\((\Lambda v)(\varphi)=\varphi(v)\) for \(v \in V\) and \(\varphi \in V^{\prime}\).</li></ul></ul><br><ul><li>(a) Show that \(\Lambda\) is {{c1::a linear map from \(V\) to \(V^{\prime \prime}\)}}.</li><li><br></li><li>(b) Show that if T \(\in\) {{c2::\(\mathcal{L}(V)\)}}, then {{c3::\(T^{\prime \prime} \circ \Lambda=\Lambda \circ T\)}}, where \(T^{\prime \prime}\) = {{c4::\(\left(T^{\prime}\right)^{\prime}\)}}.</li><li><br></li><li>(c) Show that if \(V\) is finite-dimensional, then \(\Lambda\) is {{c5::an isomorphism from \(V\) onto \(V^{\prime \prime}\)}}.</li></ul><br><br>

============================================================

  

    35 Show that {{c5::{{c2::\((\mathcal{P}(\mathbf{R}))^{\prime}\)}}}} and {{c4::{{c1::\(\mathbf{R}^{\infty}\)}}}} are {{c4::{{c3::isomorphic}}}}.

============================================================

  

    36 <br><br>Suppose \(U\) is a subspace of \(V\). <br><ul><li>Let \(i: U \rightarrow V\) be the inclusion map defined by \(i(u)=u\).&nbsp;</li><li>Thus \(i^{\prime} \in \mathcal{L}\left(V^{\prime}, U^{\prime}\right)\).</li></ul>Show that:<br><br><ul><li>(a) Show that {{c1::null}} \(i^{\prime}\) = {{c2::\(U^{0}\)}}.</li><li><br></li><li>(b) Prove that if \(V\) is finite-dimensional, then {{c3::range}} \(i^{\prime}\) = {{c4::\(U^{\prime}\)}}.</li><li><br></li><li>(c) Prove that if \(V\) is finite-dimensional, then {{c5::\(\widetilde{i^{\prime} }\)}} is {{c5::an isomorphism from \(V^{\prime} / U^{0}\) onto \(U^{\prime}\)}}.</li></ul><br>

============================================================

  

    37 <br><br>Suppose \(U\) is a subspace of \(V\). <br><ul><li>Let \(\pi: V \rightarrow V / U\) be the usual quotient map.&nbsp;</li><li>Thus \(\pi^{\prime}\)&nbsp; \(\in\)&nbsp; {{c5::\(\mathcal{L}\left((V / U)^{\prime}, V^{\prime}\right)\).}}</li></ul>Show that:<br><ul><li>(a) Show that \(\pi^{\prime}\) is {{c1::injective}}.</li><li><br></li><li>(b) Show that {{c2::range}} \(\pi^{\prime}\) ={{c3::\(U^{0}\)}}.</li><li><br></li><li>(c) Conclude that \(\pi^{\prime}\) is an {{c4::isomorphism from \((V / U)^{\prime}\) onto \(U^{0}\)}}.</li></ul><br><br>

============================================================

  

    <b>Definition&nbsp;</b>{{c1::\(\operatorname{Re} z\)}}<b>&nbsp;,&nbsp;</b>{{c3::\(\operatorname{Im} z\)}}<br><br>Suppose \(z\) = {{c5::\(a+b i\)}}, where \(a\) and \(b\) are real numbers.<br><ul><li>- The real part of \(z\), denoted {{c1::\(\operatorname{Re} z\)}}, is defined by {{c1::\(\operatorname{Re} z\)}} = {{c2::\(a\)}}.<br></li><li><br></li><li>- The imaginary part of \(z\), denoted {{c3::\(\operatorname{Im} z\)}}, is defined by {{c3::\(\operatorname{Im} z\)}} = {{c4::\(b\)}}.</li></ul>

============================================================

  

    Thus for every complex number \(z\), we can write it based on its {{c4::real}} and {{c4::imaginary}} {{c5::parts}}<br><br><ul><li>\(z\) = {{c1::\(\operatorname{Re} z\)}} {{c2::+}} {{c3::\((\operatorname{Im} z) i .\)}}</li></ul>

============================================================

  

    <b>Definition complex conjugate, absolute value</b><br><br>Suppose \(z \in \mathbf{C}\).<br><br>- The complex conjugate of \(z \in \mathbf{C}\), denoted {{c1::\(\bar{z}\)}}, is defined by<br><br><ul><li>{{c1::\(\bar{z}\)}} = {{c2::\(\operatorname{Re} z-(\operatorname{Im} z) i .\)}}</li></ul><br>- The absolute value of a complex number \(z\), denoted {{c5::{{c3::\(|z|\)}}}}, is defined by<br><br><ul><li>{{c3::\(|z|\)}} = {{c5::{{c4::\(\sqrt{ (\operatorname{Re} z)^{2}+(\operatorname{Im} z)^{2} }&nbsp;.\)  }}}}</li></ul>

============================================================

  

    <b>Properties of complex numbers</b><br><br>Suppose \(w, z \in \mathbf{C}\). Then<br><ul><li><b>sum of&nbsp;</b>\(z\) and \(\bar{z}\)</li><ul><li>\(z+\bar{z}\) ={{c1::\(2 \operatorname{Re} z\)}}</li></ul></ul><ul><li><b>difference of \(z\) and \(\bar{z}\)</b></li><ul><li>\(z-\bar{z}\) = {{c2::\(2(\operatorname{Im} z) i\)}}</li></ul></ul><ul><li><b>product of \(z\) and \(\bar{z}\)</b></li><ul><li>\(z \bar{z}\) = {{c5::{{c3::\(|z|^{2} \text {; }\)}}}}</li></ul></ul><ul><li><b>additivity and multiplicativity of complex conjugate</b></li><li>\(\overline{w+z}\) = {{c5::{{c4::\(\bar{w}+\bar{z} \)}}}} and \( \overline{w z}\) = {{c5::{{c4::\(\bar{w} \bar{z} ;\)}}}}</li></ul><br>

============================================================

  

    <b>Properties of complex numbers</b><br>Suppose \(w, z \in \mathbf{C}\). Then<br><ul><li><b>conjugate of conjugate</b></li><li>\(\overline{\bar{z}}\) = {{c1::\(z ;\)}}</li></ul><ul><li><b>real and imaginary parts are </b>{{c2::bounded by \(|z|\)}}</li><ul><li>\(|\operatorname{Re} z|\)&nbsp; {{c3::\( \leq\) \(|z|\)}}&nbsp; &nbsp;and \( |\operatorname{Im} z|\)&nbsp; {{c3::\(\leq|z|\)}}</li></ul></ul><ul><li><b>absolute value of the complex conjugate</b></li><ul><li>\(|\bar{z}|\) =\(|z| \text {; }\)</li></ul></ul><ul><li><b>multiplicativity of absolute value</b></li><ul><li>\(|w z|\) = {{c4::\(|w||z|\)}}</li></ul></ul><ul><li><b>Triangle Inequality</b></li><li>{{c5::\(|w+z| \leq|w|+|z| .\)}}</li></ul><br><img src="paste-e93009042e0528957ccb7f71354b12974fe963d3.jpg"><br>

============================================================

  

    <b>Proof for triangle inequality of complex numbers:<br></b>\[<br>|w+z| \leq|w|+|z| .<br>\]<br>Proof:<br><ul><li>\(|w+z|^{2}\)&nbsp; &nbsp;</li><li>= {{c1::\( (w+z)(\bar{w}+\bar{z}) \)&nbsp;}}</li><li>= {{c2::\( w \bar{w}+z \bar{z}+w \bar{z}+z \bar{w} \)}}<br></li><li>= {{c3::\(|w|^{2}+|z|^{2}+w \bar{z}+\overline{w \bar{z} } \)}}<br></li><li>= {{c4::\( |w|^{2}+|z|^{2}+2 \operatorname{Re}(w \bar{z}) \)}}</li><li>\(\leq\) {{c5::\(|w|^{2}+|z|^{2}+2|w \bar{z}| \)}}</li><li>= {{c5::\( |w|^{2}+|z|^{2}+2|w||z| \)}}</li><li>= \( (|w|+|z|)^{2} \)<br></li></ul>

============================================================

  

    Recall that a function \(p: \mathbf{F} \rightarrow \mathbf{F}\) is called a polynomial with coefficients in \(\mathbf{F}\) if there exist {{c5::{{c1::\(a_{0}, \ldots, a_{m} \in \mathbf{F}\)}}}} such that<br><br><ul><li>\(\quad p(z)\) = {{c4::{{c2::\(a_{0}+a_{1} z+a_{2} z^{2}+\cdots+a_{m} z^{m}\)}}}}</li></ul><br>for {{c4::{{c3::all \(z \in \mathbf{F}\)}}}}.<br>

============================================================

  

    4.7 If a polynomial is the zero function, then {{c1::all coefficients are 0}}<br><br><ul><li>Suppose \(a_{0}, \ldots, a_{m} \in \mathbf{F}\). If</li><ul><li>{{c2::\(a_{0}+a_{1} z+\cdots+a_{m} z^{m}\)}} = {{c5::{{c3::\(0\)}}}}</li><li>for {{c5::{{c4::every \(z \in \mathbf{F}\)}}}}, then {{c1::\(a_{0}=\cdots=a_{m}=0\)}}.</li></ul></ul>

============================================================

  

    <b>Division Algorithm for Polynomials</b><br><br>Suppose that \(p, s \in \mathcal{P}(\mathbf{F})\), with {{c4::\(s \neq 0\)}}. Then there exist {{c3::unique polynomials \(q, r \in \mathcal{P}(\mathbf{F})\)}} such that<br><br><ul><li>\(p\) = {{c5::{{c1::\(s q+r\)}}}}</li><li>and {{c5::{{c2::\(\operatorname{deg} r&lt;\operatorname{deg} s\)}}}}.</li></ul>

============================================================

  

    <b>Proof for the division algorithm for polynomials part 1<br><br><br></b>Proof Let \(n=\operatorname{deg} p\) and \(m=\operatorname{deg} s\). If \(n&lt;m\), then take \(q=0\) and \(r=p\) to get the desired result. Thus we can assume that \(n \geq m\).<br><br>Define \(T: \mathcal{P}_{n-m}(\mathbf{F}) \times \mathcal{P}_{m-1}(\mathbf{F}) \rightarrow \mathcal{P}_{n}(\mathbf{F})\) by<br><br><ul><li>{{c1::\(T(q, r)\)}} = {{c2::\(s q+r .\)}}</li><li>If \((q, r) \in\) {{c3::null \(T\)}}, then {{c4::\(s q+r=0\)}}, which implies that {{c4::\(q=0\) and \(r=0\)&nbsp;}}</li><li>&nbsp;Thus {{c5::\(\operatorname{dim}\) null \(T=0\)}} (proving the "unique" part of the result).</li></ul><br><br>

============================================================

  

    <b>Definition zero of a polynomial</b><br><br>A number \(\lambda \in \mathbf{F}\) is called a zero (or root) of a polynomial \(p \in \mathcal{P}(\mathbf{F})\) if<br><ul><li>{{c5::{{c2::\(p(\lambda)\)}}}} {{c4::{{c3::=}}}} {{c4::{{c1::\(0\)}}}}</li></ul>

============================================================

  

    <b>Definition&nbsp;</b>{{c5::{{c1::factor}}}}<br><br>A polynomial \(s \in \mathcal{P}(\mathbf{F})\) is called a {{c1::factor}} of \(p \in \mathcal{P}(\mathbf{F})\) if:<br><ul><li>There exists {{c4::{{c2::a polynomial \(q \in \mathcal{P}(\mathbf{F})\)}}}} such that {{c4::{{c3::\(p=s q\)}}}}.</li></ul>

============================================================

  

    Each {{c1::zero}} of a polynomial corresponds to a {{c2::degree-1 factor}}<br><br>Suppose \(p \in \mathcal{P}(\mathbf{F})\) and \(\lambda \in \mathbf{F}\). <br><ul><li>Then {{c4::\(p(\lambda)=0\)}} if and only if there is {{c5::a polynomial \(q \in \mathcal{P}(\mathbf{F})\)}} such that</li><ul><li>&nbsp;\(p(z)\) = {{c3::\((z-\lambda) q(z)\)}}</li></ul><li>for every \(z \in \mathbf{F}\).<br></li></ul>

============================================================

  

    A polynomial has {{c5::{{c1::at most}}}} {{c4::{{c2::as many zeros}}}} as {{c4::{{c3::its degree}}}}<br>

============================================================

  

    <b>A polynomial has</b> {{c2::at most}}<b>&nbsp;as&nbsp;</b>{{c5::many zeros}}<b>&nbsp;as&nbsp;</b>{{c1::its degree}}<br><br>Suppose \(p \in \mathcal{P}(\mathbf{F})\) is a polynomial with degree \(m \geq 0\). <br><ul><li>Then \(p\) has {{c3::at most \(m\)}} {{c4::distinct zeros}} in \(\mathbf{F}\).</li></ul>

============================================================

  

    {{c1::Fundamental Theorem of Algebra}}<br><br>Every {{c2::nonconstant}} polynomial with {{c5::{{c3::complex}}}} coefficients has {{c5::{{c4::a zero}}}}.

============================================================

  

    Factorization of a polynomial over \(\mathbf{C}\)<br><br>If \(p \in \mathcal{P}(\mathbf{C})\) is a {{c3::nonconstant}} complex polynomial, then \(p\) has a {{c4::unique factorization}} (except for {{c5::the order of the factors}}) of the form<br><br><ul><li>\(p(z)\) = {{c1::\(c\left(z-\lambda_{1}\right) \cdots\left(z-\lambda_{m}\right)\)}}</li><ul><li>where {{c2::\(c, \lambda_{1}, \ldots, \lambda_{m} \in \mathbf{C}\)}}.</li></ul></ul>

============================================================

  

    Polynomials with {{c1::real}} coefficients have {{c2::zeros in pairs}}<br><ul><li>Suppose \(p \in \mathcal{P}(\mathbf{C})\) is a polynomial with {{c1::real}} coefficients. If {{c4::\(\lambda \in \mathbf{C}\)}} is {{c3::a zero of \(p\)}}, then so is {{c5::\(\bar{\lambda}\)}}.<br></li></ul>

============================================================

  

    Factorization of a {{c1::quadratic}} polynomial<br><br>Suppose \(b, c \in \mathbf{R}\). Then there is a polynomial factorization of the form<br><ul><li>{{c2::\(x^{2}+b x+c\)}} = {{c3::\(\left(x-\lambda_{1}\right)\left(x-\lambda_{2}\right)\)}}</li><ul><li>with {{c4::\(\lambda_{1}, \lambda_{2} \in \mathbf{R}\)}}&nbsp;</li></ul><li>if and only if {{c5::\(b^{2} \geq 4 c\)}}.</li></ul>

============================================================

  

    To show why we can factorise real-coefficient polynomials as {{c3::quadratic}} + {{c4::linear}} polynomials:<br><ul><li>Because:&nbsp;{{c5::{{c1::\( (x-\lambda) (x-\bar{\lambda})\)}}}} =<br>{{c5::{{c2::\( \left(x^{2}-2(\operatorname{Re} \lambda) x+|\lambda|^{2}\right)<br>\)}}}}<br></li><li>Gives us a {{c3::quadratic}} term of the required form</li></ul>

============================================================

  

    <b>Factorization of a polynomial over \(\mathbf{R}\)</b><br><br>Suppose \(p \in \mathcal{P}(\mathbf{R})\) is a nonconstant polynomial. Then \(p\) has a unique factorization (except for the order of the factors) of the form<br><ul><li>\(p(x)\) = {{c1::\(c\)}} {{c2::\((x-\lambda_{1})\)}} {{c3::\(\cdots\)}} {{c2::\((x-\lambda_{m})\)}} {{c4::\((x^{2}+b_{1} x+c_{1})\)}} {{c3::\(\cdots\)}} {{c5::\((x^{2}+b_{M} x+c_{M}),\)}}</li><li>where \(c, \lambda_{1}, \ldots, \lambda_{m}, b_{1}, \ldots, b_{M}, c_{1}, \ldots, c_{M} \in \mathbf{R}\), with \(b_{j}{ }^{2}&lt;4 c_{j}\) for each \(j\).</li></ul>

============================================================

  

    4 <br>Suppose \(m\) and \(n\) are positive integers with \(m \leq n\), and suppose \(\lambda_{1}, \ldots, \lambda_{m} \in \mathbf{F}\). <br><ul><li>Prove that there exists a polynomial \(p \in \mathcal{P}(\mathbf{F})\) with degree &nbsp;{{c1::\(\operatorname{deg} p\)}} = {{c1::\(n\)}} such that {{c2::\(0\)}} ={{c3::\(p\left(\lambda_{1}\right)\)}} =\(\cdots\) = {{c4::\(p\left(\lambda_{m}\right)\)}} and such that {{c5::\(p\) has no other zeros.}}</li></ul>

============================================================

  

    5 Suppose \(m\) is a nonnegative integer, \(z_{1}, \ldots, z_{m+1}\) are distinct elements of \(\mathbf{F}\), and \(w_{1}, \ldots, w_{m+1} \in \mathbf{F}\). <br><ul><li>Prove that there exists a {{c1::unique}} polynomial \(p \in \mathcal{P}_{m}(\mathbf{F})\) such that</li><ul><li>{{c2::\(p\left(z_{j}\right)\)}} = {{c5::{{c3::\(w_{j}\)}}}}</li><li>for {{c5::{{c4::\(j=1, \ldots, m+1\)}}}}.</li></ul></ul>

============================================================

  

    6 Suppose \(p \in \mathcal{P}(\mathbf{C})\) has degree \(m\). Prove that \(p\) has {{c1::\(m\) distinct zeros}} if and only if {{c2::\(p\)}} and {{c5::{{c3::its derivative \(p^{\prime}\)}}}} have {{c5::{{c4::no zeros in common}}}}.

============================================================

  

    Prove that {{c4::every}} polynomial of {{c1::odd}} {{c5::{{c2::degree}}}} with {{c5::{{c3::real}}}} coefficients has a {{c5::{{c3::real zero}}}}.

============================================================

  

    10 <br>Suppose \(m\) is a nonnegative integer and \(p \in \mathcal{P}_{m}(\mathbf{C})\) is such that:<br><ul><li>There exist distinct {{c1::real }} numbers {{c2::\(x_{0}, x_{1}, \ldots, x_{m}\)}} such that {{c3::\(p\left(x_{j}\right) \in \mathbf{R}\)}} for {{c4::\(j=0,1, \ldots, m\)}}.&nbsp;</li><li>Prove that all the coefficients of \(p\) are {{c5::real}}.</li></ul>

============================================================

  

    11 <br>Suppose \(p \in \mathcal{P}(\mathbf{F})\) with {{c1::\(p \neq 0\)}}. <br><ul><li>Let \(U\) = {{c2::\(\{p q: q \in \mathcal{P}(\mathbf{F})\}\)}}.</li><ul><li>(a) Show that \(\operatorname{dim}\) {{c5::{{c3::\(\mathcal{P}(\mathbf{F}) / U\)}}}} = {{c5::{{c4::\(\operatorname{deg} p\).}}}}</li></ul></ul>

============================================================

  

    Why is it difficult to understand&nbsp;\(T \in L(V)\) by decomposing V as follows<br>\[<br>V=U_{1} \oplus \cdots \oplus U_{m},<br>\]<br>Ans:<br><ul><li>Because T {{c5::{{c1::may not map each&nbsp;\(U_i\) onto itself}}}} and the decomposition may {{c4::{{c2::lose meaning after the first application of T}}}}</li><li>Thus not allowing us to {{c4::{{c3::take powers}}}}</li></ul>

============================================================

  

    Definition {{c1::invariant subspace<br>}}<br>Suppose \(T \in \mathcal{L}(V)\). A subspace \(U\) of \(V\) is called {{c1::invariant}} under {{c2::\(T\)}} if {{c3::\(u \in U\)}} {{c4::implies}} {{c5::\(T u \in U\)}}.

============================================================

  

    5.3 Example Suppose \(T \in \mathcal{L}(V)\). Show that each of the following subspaces of \(V\) is invariant under \(T\) :<br><ul><li>(a) \(\{0\}\);</li><li>(b) \(V\);</li><li>(c) \(\operatorname{null} T\);</li><li>(d) range \(T\).</li></ul><br>Solution:<br><br><ul><li>(a) If \(u \in\{0\}\), then {{c1::\(u=0\)}} and hence {{c2::\(T u=0 \in\{0\}\)}}. Thus \(\{0\}\) is invariant under \(T\).</li><li><br></li><li>(b) If \(u \in V\), then {{c3::\(T u \in V\)}}. Thus \(V\) is invariant under \(T\).</li><li><br></li><li>(c) If \(u \in \operatorname{null} T\), then {{c4::\(T u=0\)}}, and hence {{c4::\(T u \in \operatorname{null} T\)}}. Thus null \(T\) is invariant under \(T\).</li><li><br></li><li>(d) If \(u \in \operatorname{range} T\), then {{c5::\(T u \in \operatorname{range} T\)}}. Thus range \(T\) is invariant under \(T\).</li></ul>

============================================================

  

    Definition {{c1::eigenvalue}}<br><br>Suppose \(T \in \mathcal{L}(V)\). A {{c2::number \(\lambda \in \mathbf{F}\)}} is called an {{c1::eigenvalue}} of \(T\) if there exists {{c3::\(v \in V\)}} such that {{c4::\(v \neq 0\)}} and {{c5::\(T v=\lambda v\).}}

============================================================

  

    <b>Equivalent conditions to be an eigenvalue</b><br><br>Suppose \(V\) is finite-dimensional, \(T \in \mathcal{L}(V)\), and \(\lambda \in F\). Then the following are equivalent:<br><br><ul><li>(a) {{c1::\(\lambda\)}} is {{c1::an eigenvalue of \(T\)}};</li><li>(b) {{c2::\(T-\lambda I\)}} is {{c3::not injective}};</li><li>(c) {{c2::\(T-\lambda I\)}} is {{c4::not surjective}};</li><li>(d) {{c2::\(T-\lambda I\)}} is {{c5::not invertible}}.</li></ul>

============================================================

  

    Proof for equivalent eigenvalue conditions:<br><img src="paste-38d4b9ad6e26a1f8d2de5f708d2ec91e4d41cc4e.jpg"><br><br>Proof:<br><ul><li>Conditions (a) and (b) are equivalent because the equation {{c1::\(T v=\lambda \nu\)}} is equivalent to the equation {{c2::\((T-\lambda I) v=0\).&nbsp;}}</li><li>Conditions (b), (c), and (d) are equivalent by {{c5::{{c3::the injectivity-surjectivity-invertibility}}}} {{c5::{{c4::equivalence}}}}</li></ul>

============================================================

  

    <b>Definition</b> {{c1::eigenvector}}<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(\lambda \in \mathbf{F}\) is an eigenvalue of \(T\). A vector \(v \in V\) is called an {{c2::eigenvector}} of \(T\) {{c5::corresponding to \(\lambda\)}} if {{c3::\(v \neq 0\)}} and {{c4::\(T v=\lambda v\)}}.

============================================================

  

    <ul><li>Because \(T v=\lambda v\) if and only if {{c5::{{c1::\((T-\lambda I) v=0\)}}}},&nbsp;</li><li>A vector \(v \in V\) with {{c4::{{c2::\(v \neq 0\)}}}} is an eigenvector of \(T\) corresponding to \(\lambda\) if and only if {{c4::{{c3::\(v \in \operatorname{null}(T-\lambda I)\)}}}}.</li></ul>

============================================================

  

    {{c1::Linearly independent}} {{c2::eigenvectors}}<br><br>Let \(T \in \mathcal{L}(V)\). Suppose {{c5::{{c3::\(\lambda_{1}, \ldots, \lambda_{m}\)}}}} are {{c2::distinct eigenvalues}} of \(T\) and {{c5::{{c4::\(v_{1}, \ldots, v_{m}\)}}}} are {{c2::corresponding eigenvectors}}. Then \(v_{1}, \ldots, v_{m}\) is {{c1::linearly independent}}.

============================================================

  

    Now we show that {{c5::{{c3::eigenvectors}}}} corresponding to {{c4::{{c1::distinct eigenvalues}}}} are {{c4::{{c2::linearly independent}}}}.

============================================================

  

    <b>Number of eigenvalues</b><br><br>Suppose \(V\) is finite-dimensional. Then each operator on \(V\) has {{c1::at most}} {{c2::\(\operatorname{dim} V\)}} {{c5::{{c3::distinct}}}} {{c5::{{c4::eigenvalues}}}}.

============================================================

  

    Proof that a vector space V has at most {{c5::{{c3::dim V}}}} distinct eigenvalues<br><ul><li>Because eigenvectors {{c4::{{c1::form a linearly independent list}}}}</li><li>The maximum length of an eigenvectors list is {{c4::{{c2::dim V}}}}</li></ul>

============================================================

  

    Definition {{c1::\(\left.T\right|_{U}\)}} and {{c2::\(T / U\)}}<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(U\) is a subspace of \(V\) invariant under \(T\).<br><br><ul><li>The {{c2::restriction}} operator {{c1::\(\left.T\right|_{U}\)}}&nbsp; \(\in\) {{c4::\( \mathcal{L}(U)\)}} is defined by</li><ul><li>{{c1::\(\left.T\right|_{U}\)}} {{c4::(u)}}= {{c3::\(T u\)}}</li><li>for {{c4::\(u \in U\)}}.</li></ul></ul><ul><li>The {{c1::quotient}} operator {{c2::\(T / U\)}}&nbsp; \(\in\) {{c5::\( \mathcal{L}(V / U)\)}} is defined by</li><ul><li>{{c2::\((T / U)\)}} {{c5::(v+U)}}= {{c3::\(T v+U\)}}</li><li>for {{c5::\(v \in V\)}}.</li></ul></ul><br>

============================================================

  

    Verify that the quotien operator over invariant subspaces is well defined by checking if \(v+U = w+U\) implies \(Tv+U = Tw + U\)?<br>\[<br>(T / U)(v+U)=T v+U<br>\]<br>Proof:<br><ul><li>Because v+U=w+U then&nbsp;{{c1::\(v-w\in U\)}}</li><li>Because {{c5::U is invariant under T}} we also have&nbsp;{{c2::\(T(v-w) \in U\)}}</li><li>Which implies that&nbsp;{{c3::\(Tv - Tw \in U\)}}</li><li>Which then implies that&nbsp;{{c4::\(Tv +U = Tw + U\)}} as desired</li></ul>

============================================================

  

    &nbsp;1 Suppose \(T \in \mathcal{L}(V)\) and \(U\) is a subspace of \(V\).<br><br><ul><li>(a) Prove that if {{c3::\(U\)}} \(\subset\) {{c2::null \(T\)}}, then \(U\) is {{c1::invariant under \(T\).}}</li><li><br></li><li>(b) Prove that if {{c4::range \(T\)}}&nbsp; \(\subset\)&nbsp; {{c5::\(U\)}}, then \(U\) is {{c1::invariant under \(T\).}}</li></ul>

============================================================

  

    2 <br>Suppose \(S, T \in \mathcal{L}(V)\) are such that {{c1::\(S T\)}} = {{c2::\(T S\)}}. Prove that {{c3::null \(S\)::null/range}} is {{c4::invariant}} {{c5::under \(T\)}}.

============================================================

  

    3 Suppose \(S, T \in \mathcal{L}(V)\) are such that {{c1::\(S T\)}} = {{c2::\(T S\)}}. Prove that {{c5::{{c3::range \(S\)}}}} is {{c5::{{c4::invariant under}}}} \(T\).

============================================================

  

    4 Suppose that \(T \in \mathcal{L}(V)\) and \(U_{1}, \ldots, U_{m}\) are {{c5::{{c3::subspaces of \(V\)}}}} {{c4::{{c1::invariant under \(T\)}}}}. Prove that {{c4::{{c2::\(U_{1}+\cdots+U_{m}\)}}}} is {{c1::invariant under \(T\)}}.

============================================================

  

    5 Suppose \(T \in \mathcal{L}(V)\). Prove that the {{c5::{{c1::intersection}}}} of {{c4::{{c2::every collection}}}} of subspaces of \(V\) {{c4::{{c3::invariant under \(T\)}}}} is {{c3::invariant under \(T\)}}.

============================================================

  

    13 Suppose \(V\) is finite-dimensional, \(T \in \mathcal{L}(V)\), and \(\lambda \in \mathbf{F}\). Prove that there exists \(\alpha \in \mathbf{F}\) such that {{c1::\(|\alpha-\lambda|\)}} &lt; {{c2::\(\frac{1}{1000}\)}} and {{c5::{{c3::\(T-\alpha I\)}}}} is {{c5::{{c4::invertible}}}}.

============================================================

  

    15 Suppose \(T \in \mathcal{L}(V)\). Suppose \(S \in \mathcal{L}(V)\) is invertible.<br><br><ul><li>(a) Prove that {{c1::\(T\)}} and {{c2::\(S^{-1} T S\)}} have {{c4::the same}} {{c5::eigenvalues}}.</li><li>(b) The eigenvectors of T are the {{c3::mappings of the eigenvectors of \(S^{-1} T S\) through S}}</li></ul>

============================================================

  

    6 <br><br>If \(V\) is finite-dimensional and \(U\) is a subspace of \(V\) that is invariant {{c5::{{c1::under every operator on \(V\)}}}}, then {{c4::{{c2::\(U=\{0\}\)}}}} or {{c4::{{c3::\(U=V\)}}}}.

============================================================

  

    16 Suppose \(V\) is a complex vector space, \(T \in \mathcal{L}(V)\), and the matrix of \(T\) with respect to some basis of \(V\) contains only real entries. <br><ul><li>Show that if {{c1::\(\lambda\)}} is an {{c2::eigenvalue of \(T\)}}, then {{c5::{{c3::so is}}}} {{c5::{{c4::\(\bar{\lambda}\)}}}}.</li></ul>

============================================================

  

    21 Suppose \(T \in \mathcal{L}(V)\) is invertible.<br><br><ul><li>(a) Suppose \(\lambda \in \mathbf{F}\) with \(\lambda \neq 0\).&nbsp;</li><ul><li>Prove that \(\lambda\) is an eigenvalue of \(T\) if and only if {{c1::\(\frac{1}{\lambda}\)}} {{c2::is an eigenvalue of \(T^{-1}\)}}.</li></ul><li>(b) Prove that {{c3::\(T\)}} and {{c4::\(T^{-1}\)}} have {{c5::the same eigenvectors.}}</li></ul>

============================================================

  

    23 Suppose \(V\) is finite-dimensional and \(S, T \in \mathcal{L}(V)\). Prove that {{c1::\(S T\)}} and {{c2::\(T S\)}} have {{c5::{{c3::the same}}}} {{c5::{{c4::eigenvalues}}}}.

============================================================

  

    24 <br><br>Suppose \(A\) is an \(n\)-by- \(n\) matrix with entries in \(\mathbf{F}\). Define \(T \in \mathcal{L}\left(\mathbf{F}^{n}\right)\) by \(T x=A x\), where elements of \(\mathbf{F}^{n}\) are thought of as \(n\)-by-1 column vectors.<br><br><ol><li>(a) Suppose {{c1::the sum of the entries}} in {{c2::each row}} of \(A\) equals {{c4::1}} . Prove that {{c4::1}} is {{c5::an eigenvalue of \(T\)}}.</li><li>(b) Suppose {{c1::the sum of the entries}} in {{c3::each column}} of \(A\) equals {{c4::1}} . Prove that {{c4::1}} is {{c5::an eigenvalue of \(T\)}}.</li></ol>

============================================================

  

    25 Suppose \(T \in \mathcal{L}(V)\) and \(u, v\) are eigenvectors of \(T\) such that {{c1::\(u+v\)}} is {{c2::also}} {{c5::{{c3::an eigenvector of \(T\)}}}}. <br><ul><li>Prove that \(u\) and \(v\) are eigenvectors of \(T\) {{c5::{{c4::corresponding to the same eigenvalue}}}}.</li></ul>

============================================================

  

    26 Suppose \(T \in \mathcal{L}(V)\) is such that {{c1::every}} {{c2::nonzero vector}} in \(V\) is an {{c3::eigenvector}} of \(T\). Prove that \(T\) is {{c4::a scalar multiple}} {{c5::of the identity operator}}.

============================================================

  

    27 <br>Suppose \(V\) is finite-dimensional and \(T \in \mathcal{L}(V)\) is such that {{c1::every}} subspace of \(V\) with dimension {{c2::\(\operatorname{dim} V-1\)}} is {{c5::{{c3::invariant under \(T\)}}}}. <br><ul><li>Prove that \(T\) is a {{c5::{{c4::scalar multiple of the identity operator}}}}.</li></ul>

============================================================

  

    28 <br>Suppose \(V\) is finite-dimensional with \(\operatorname{dim}\) \(V\)&nbsp; {{c1::\(\geq 3\)}} and \(T \in \mathcal{L}(V)\) is such that {{c2::every}} {{c3::2}}-dimensional subspace of \(V\) is {{c4::invariant under \(T\)}}. <br><ul><li>Prove that \(T\) is {{c5::a scalar multiple of the identity operator}}.</li></ul>

============================================================

  

    29 Suppose \(T \in \mathcal{L}(V)\) and {{c1::\(\operatorname{dim} \operatorname{range} T\)}} = {{c2::\(k\)}}. Prove that \(T\) has {{c5::{{c3::at most \(k+1\)}}}} {{c5::{{c4::distinct eigenvalues}}}}.

============================================================

  

    31 <br>Suppose \(V\) is finite-dimensional and \(v_{1}, \ldots, v_{m}\) is a list of vectors in \(V\). <br><ul><li>Prove that \(v_{1}, \ldots, v_{m}\) is {{c1::linearly independent}} if and only if there exists {{c2::\(T \in \mathcal{L}(V)\)}} such that {{c3::\(v_{1}, \ldots, v_{m}\)}} are {{c4::eigenvectors of \(T\)}} {{c5::corresponding to distinct eigenvalues.}}</li></ul>

============================================================

  

    32 Suppose \(\lambda_{1}, \ldots, \lambda_{n}\) is a list of {{c1::distinct real numbers}}.<br><ul><li>&nbsp;Prove that the list {{c2::\(e^{\lambda_{1} x}, \ldots, e^{\lambda_{n} x}\)}} is {{c5::{{c3::linearly independent}}}} in the vector space of {{c5::{{c4::real-valued functions on \(\mathbf{R}\)}}}}.</li></ul><br>

============================================================

  

    34 <br>Suppose \(T \in \mathcal{L}(V)\). <br><ul><li>Prove that {{c1::\(T /(\) null \(T)\)}} is {{c2::injective}} if and only if {{c3::\((\) null \(T)\)}}\(\cap\){{c4::(range \(T)\)}}={{c5::\(\{0\}\)}}.</li></ul>

============================================================

  

    35 Suppose \(V\) is finite-dimensional, \(T \in \mathcal{L}(V)\), and \(U\) is {{c1::invariant under \(T\)}}. Prove that {{c2::each}} eigenvalue of {{c5::{{c3::\(T / U\)}}}} is an eigenvalue of {{c5::{{c4::\(T\).}}}}

============================================================

  

    <b>Definition</b> {{c1::\(T^{m}\)}}<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(m\) is a positive integer.<ul><li>- {{c1::\(T^{m}\)}} is defined by</li><ul><li>{{c1::\(T^{m}\) }} = {{c4::\(\underbrace{T \cdots T}_{m \text { times } } \text {. }\)}}</li></ul><li>- \(T^{0}\) is defined to be the identity operator \(I\) on \(V\).</li><li>- If \(T\) is invertible with inverse \(T^{-1}\), then {{c5::{{c3::\(T^{-m}\)}}}} is defined by</li><ul><li>{{c3::\(T^{-m}\)}} = {{c5::{{c5::\(\left(T^{-1}\right)^{m} \text {. }\)}}}}</li></ul></ul>

============================================================

  

    You should verify that if \(T\) is an {{c1::operator}}, then<br><br><ul><li>{{c1::\(T^{m} T^{n}\)}} = {{c2::\(T^{m+n}\)}}</li><li>{{c5::{{c3::\( \quad\left(T^{m}\right)^{n}\)}}}} = {{c5::{{c4::\(T^{m n},\)}}}}</li></ul>

============================================================

  

    You should verify that if \(T\) is an operator, then<br><br>\[<br>T^{m} T^{n}=T^{m+n} \quad \text { and } \quad\left(T^{m}\right)^{n}=T^{m n},<br>\]<br><br>where \(m\) and \(n\) are allowed to be {{c1::arbitrary integers}} if \(T\) is {{c2::invertible}} and {{c5::{{c3::nonnegative integers}}}} if \(T\) is {{c5::{{c4::not invertible}}}}.

============================================================

  

    <b>Definition</b> {{c1::\(p(T)\)}}<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(p \in \mathcal{P}(\mathbf{F})\) is a polynomial given by<br><br><ul><li>{{c3::\(p(z)\)}} = {{c5::{{c4::\(a_{0}+a_{1} z+a_{2} z^{2}+\cdots+a_{m} z^{m}\)}}}}</li><li>for \(z \in \mathbf{F}\).&nbsp;</li><li>Then {{c1::\(p(T)\)}} is the operator defined by<br></li><ul><li>{{c1::\(p(T)\)}} = {{c5::{{c2::\(a_{0} I+a_{1} T+a_{2} T^{2}+\cdots+a_{m} T^{m} .\)}}}}</li></ul></ul>

============================================================

  

    <b>Definition&nbsp;</b>{{c4::product of polynomials}}<br><br>If {{c3::\(p, q \in \mathcal{P}(\mathbf{F})\)}}, then {{c5::{{c1::\(p q\) \(\in\) \(\mathcal{P}(\mathbf{F})\)}}}} is the polynomial defined by<br><br><ul><li>{{c5::{{c1::\((p q)(z)\)}}}} = {{c5::{{c2::\(p(z) q(z)\)}}}}</li><li>for \(z \in \mathbf{F}\).<br></li></ul>

============================================================

  

    Multiplicative properties of polynomials of operartors<br><br>Suppose \(p, q \in \mathcal{P}(\mathbf{F})\) and \(T \in \mathcal{L}(V)\). Then<br><ul><li>(a) {{c1::\((p q)(T)\)}} = {{c2::\(p(T) q(T)\)}};</li><li>(b) {{c5::{{c3::\(p(T) q(T)\)}}}} = {{c5::{{c4::\(q(T) p(T)\)}}}}.</li></ul>

============================================================

  

    Operators on complex vector spaces have {{c1::an}} {{c2::eigenvalue}}<br><br>{{c3::Every}} operator on a finite-dimensional, {{c4::nonzero}}, {{c5::complex}} vector space has {{c1::an}} {{c2::eigenvalue}}.

============================================================

  

    Proof for:<br><img src="paste-29472791a9796491856f717c4879d5bea2733742.jpg"><br><br>Proof:<br>Proof Suppose \(V\) is a complex vector space with dimension \(n&gt;0\) and \(T \in \mathcal{L}(V)\). Choose \(v \in V\) with \(v \neq 0\). Then the list<br><br>\[<br>v, T v, T^{2} v, \ldots, T^{n} v<br>\]<br><br>is not linearly independent, because \(V\) has dimension \(n\) and we have \(n+1\) vectors. Thus there exist complex numbers \(a_{0}, \ldots, a_{n}\), not all 0 , such that<br><br>\[<br>0=a_{0} v+a_{1} T v+\cdots+a_{n} T^{n} v<br>\]<br><br>Note that \(a_{1}, \ldots, a_{n}\) cannot all be 0 , because otherwise the equation above would become \(0=a_{0} v\), which would force \(a_{0}\) also to be 0 .<br><br>Make the \(a\) 's the coefficients of a polynomial, which by the Fundamental Theorem of Algebra (4.14) has a factorization<br><br>\[<br>a_{0}+a_{1} z+\cdots+a_{n} z^{n}=c\left(z-\lambda_{1}\right) \cdots\left(z-\lambda_{m}\right),<br>\]<br><br>where \(c\) is a nonzero complex number, each \(\lambda_{j}\) is in \(\mathbf{C}\), and the equation holds for all \(z \in \mathbf{C}\) (here \(m\) is not necessarily equal to \(n\), because \(a_{n}\) may equal 0 ). We then have<br><br><ul><li>\(0\) = {{c1::\(a_{0} v+a_{1} T v+\cdots+a_{n} T^{n} v \)}}</li><li>&nbsp;= {{c2::\(\left(a_{0} I+a_{1} T+\cdots+a_{n} T^{n}\right) v \)}}</li><li>&nbsp;= {{c3::\(c\left(T-\lambda_{1} I\right) \cdots\left(T-\lambda_{m} I\right) v \)}}</li></ul><br>Thus {{c4::\(T-\lambda_{j} I\)}} is {{c5::not injective for at least one \(j\)}}. In other words, \(T\) has an eigenvalue.<br>

============================================================

  

    Proof for:<br><img src="paste-6c07f8b1c24a23de4a68a99207f7ab9621248463.jpg"><br><br>&nbsp;Suppose \(V\) is a complex vector space with dimension \(n&gt;0\) and \(T \in \mathcal{L}(V)\). Choose \(v \in V\) with \(v \neq 0\). Then<br><br>{{c1::\[<br>v, T v, T^{2} v, \ldots, T^{n} v<br>\]}}<br><br>is {{c2::not linearly independent}}, because {{c3::\(V\) has dimension \(n\) and we have \(n+1\) vectors}}. Thus there exist {{c4::complex numbers \(a_{0}, \ldots, a_{n}\), not all 0}} , such that<br><br>{{c5::\[<br>0=a_{0} v+a_{1} T v+\cdots+a_{n} T^{n} v<br>\]}}<br><br>Note that {{c4::\(a_{1}, \ldots, a_{n}\) cannot all be 0}} , because {{c2::otherwise the equation above would become \(0=a_{0} v\), which would force \(a_{0}\) also to be 0 .}}

============================================================

  

    Definition matrix of an {{c1::operator, \(\mathcal{M}(T)\)}}<br><br>Suppose \(T\) \(\in\) {{c1::\(\mathcal{L}(V)\)}} and \(v_{1}, \ldots, v_{n}\) is a basis of \(V\). The matrix of \(T\) with respect to this basis is the {{c2::\(n\)}}-by- {{c2::\(n\)}} matrix<br><br>{{c5::{{c3::\[<br>\mathcal{M}(T)=\left(\begin{array}{ccc}<br>A_{1,1} &amp; \ldots &amp; A_{1, n} \\<br>\vdots &amp; &amp; \vdots \\<br>A_{n, 1} &amp; \ldots &amp; A_{n, n}<br>\end{array}\right)<br>\]}}}}<br><br>whose entries \(A_{j, k}\) are defined by<br><br><ul><li>\(T v_{k}\) = {{c5::{{c4::\(A_{1, k} v_{1}+\cdots+A_{n, k} v_{n} .\)}}}}</li></ul><br>

============================================================

  

    For an operator&nbsp;The \(k^{\text {th }}\) column of the matrix \(\mathcal{M}(T)\) is formed from {{c5::{{c1::the coefficients}}}} used to {{c4::{{c2::write \(T v_{k}\) as a linear combination of}}}} {{c4::{{c3::\(v_{1}, \ldots, v_{n}\).}}}}

============================================================

  

    Definition {{c5::{{c1::diagonal}}}} of a matrix<br><br>The {{c1::diagonal}} of a {{c4::{{c2::square}}}} matrix consists of {{c4::{{c3::the entries along the line from the upper left corner to the bottom right corner.}}}}

============================================================

  

    <b>Definition</b> {{c5::{{c1::upper-triangular}}}} <b>matrix</b><br><br>A matrix is called {{c5::{{c1::upper triangular}}}} if {{c4::{{c2::all the entries below the diagonal}}}} {{c4::{{c3::equal 0}}}} .

============================================================

  

    <b>Conditions for upper-triangular matrix</b><br><br>Suppose \(T \in \mathcal{L}(V)\) and \(v_{1}, \ldots, v_{n}\) is a basis of \(V\). Then the following are equivalent:<br><br><ul><li>(a) {{c5::{{c1::the matrix of \(T\) with respect to \(v_{1}, \ldots, v_{n}\) is upper triangular;}}}}</li><li>(b) {{c4::{{c2::\(\quad T v_{j} \in \operatorname{span}\left(v_{1}, \ldots, v_{j}\right)\)}}}} for each {{c4::{{c2::\(j=1, \ldots, n\);}}}}</li><li>(c) {{c4::{{c3::\(\operatorname{span}\left(v_{1}, \ldots, v_{j}\right)\) is invariant under \(T\) for each \(j=1, \ldots, n\).}}}}</li></ul>

============================================================

  

    Proof for:<br><img src="paste-54fc0b1bbcbf72e499eafb761477d9d3fc16d294.jpg"><br>Proof The equivalence of (a) and (b) follows easily from the definitions and a moment's thought. Obviously (c) implies (b). Hence to complete the proof, we need only prove that (b) implies (c).<br><br>Thus suppose (b) holds. Fix \(j \in\{1, \ldots, n\}\). From (b), we know that<br><br><ul><li>{{c1::\( T v_{1} \in \operatorname{span}\left(v_{1}\right) \subset \operatorname{span}\left(v_{1}, \ldots, v_{j}\right) ; \)}}</li><li>{{c2::\( T v_{2} \in \operatorname{span}\left(v_{1}, v_{2}\right) \subset \operatorname{span}\left(v_{1}, \ldots, v_{j}\right) \)&nbsp;}}</li><li>...</li><li>{{c5::{{c3::\(T v_{j} \in \operatorname{span}\left(v_{1}, \ldots, v_{j}\right) .\)}}}}</li></ul><br>Thus if \(v\) is a linear combination of \(v_{1}, \ldots, v_{j}\), then<br><br>{{c5::{{c4::\[<br>T v \in \operatorname{span}\left(v_{1}, \ldots, v_{j}\right) .<br>\]}}}}<br><br>In other words, \(\operatorname{span}\left(v_{1}, \ldots, v_{j}\right)\) is invariant under \(T\), completing the proof.<br>

============================================================

  

    If an operator on a real vector space {{c1::has no eigenvalues}} then there is no {{c2::basis to respect to which}} the operator has {{c5::{{c3::an upper-triangular matrix}}}} because {{c5::{{c4::the first vector in basis for an upper-triangular matrix is an eigenvector}}}}

============================================================

  

    Over {{c5::{{c1::\(\mathbf{C}\)}}}}, every operator has {{c4::{{c2::an upper-triangular}}}} matrix<br><br>Suppose \(V\) is a finite-dimensional {{c5::{{c1::complex}}}} vector space and \(T \in \mathcal{L}(V)\). Then \(T\) has {{c2::an upper-triangular}} matrix with respect to {{c4::{{c3::some basis of \(V\).}}}}

============================================================

  

    Determination of {{c1::invertibility}} from {{c2::upper-triangular matrix}}<br><br>Suppose \(T \in \mathcal{L}(V)\) has an {{c2::upper-triangular matrix}} with respect to some basis of \(V\). Then \(T\) is {{c1::invertible}} if and only if {{c5::{{c3::all the entries}}}} {{c5::{{c4::on the diagonal of that}}}} {{c2::upper-triangular matrix}} {{c5::{{c3::are nonzero.}}}}

============================================================

  

    Determination of {{c5::{{c1::eigenvalues}}}} from {{c4::{{c2::upper-triangular}}}} matrix<br><br>Suppose \(T \in \mathcal{L}(V)\) has an {{c2::upper-triangular}} matrix with respect to some basis of \(V\). Then the {{c1::eigenvalues}} of \(T\) {{c4::{{c3::are precisely the entries on the diagonal}}}} {{c4::{{c2::of that upper-triangular matrix.}}}}

============================================================

  

     Suppose \(T \in \mathcal{L}(V)\) and there exists {{c4::a positive integer \(n\)}} such that {{c4::\(T^{n}\)}}={{c4::&nbsp;\(0\)}}.<ul "=""><li>(a) Prove that {{c1::\(I-T\)}} is {{c2::invertible}} and that</li><ul><li>{{c1::\((I-T)\)}}{{c5::\(^{-1}\)}} = {{c3::\(I+T+\cdots+T^{n-1} .\)}}</li><li></li></ul></ul>

============================================================

  

    2 Suppose \(T \in \mathcal{L}(V)\) and {{c4::\((T-2 I)(T-3 I)(T-4 I)=0\)}}. Suppose \(\lambda\) is an eigenvalue of \(T\). Prove that \(\lambda\) = {{c1::\(2\)}} or \(\lambda\) = {{c5::{{c2::\(3\)}}}} or \(\lambda\) = {{c5::{{c3::\(4\)}}}}.

============================================================

  

    3 Suppose \(T \in \mathcal{L}(V)\) and {{c1::\(T^{2}=I\)}} and {{c2::-1}} is {{c5::{{c3::not an eigenvalue of \(T\)}}}}. Prove that \(T\) = {{c5::{{c4::\(I\)}}}}.

============================================================

  

    4 Suppose \(P \in \mathcal{L}(V)\) and {{c1::\(P^{2}\)}} = {{c2::\(P\)}}. Prove that \(V=\) {{c3::null \(P\)}} {{c4::\(\oplus\)}} {{c5::range \(P\).}}

============================================================

  

    6 Suppose \(T \in \mathcal{L}(V)\) and \(U\) is a subspace of \(V\) invariant under \(T\). Prove that \(U\) is {{c5::{{c1::invariant}}}} {{c4::{{c2::under \(p(T)\)}}}} for {{c4::{{c3::every polynomial \(p \in \mathcal{P}(\mathbf{F})\).}}}}

============================================================

  

    7 Suppose \(T \in \mathcal{L}(V)\). Prove that {{c1::9}} is an {{c2::eigenvalue}} of {{c5::{{c3::\(T^{2}\)}}}} if and only if {{c5::{{c4::3 or -3}}}} is an {{c2::eigenvalue}} of \(T\).

============================================================

  

    9 Suppose \(V\) is finite-dimensional, \(T \in \mathcal{L}(V)\), and \(v \in V\) with \(v \neq 0\). Let \(p\) be a nonzero polynomial of {{c1::smallest}} degree such that {{c2::\(p(T) v=0\)}}. Prove that every {{c5::{{c3::zero of \(p\)}}}} is {{c5::{{c4::an eigenvalue of \(T\).}}}}

============================================================

  

    10 Suppose \(T \in \mathcal{L}(V)\) and \(v\) is an {{c3::eigenvector}} of \(T\) with {{c4::eigenvalue \(\lambda\)}}. Suppose \(p \in \mathcal{P}(\mathbf{F})\). Prove that {{c5::{{c1::\(p(T) v\)}}}} = {{c5::{{c2::\(p(\lambda) v\).}}}}

============================================================

  

    11 Suppose \(\mathbf{F}=\mathbf{C}, T \in \mathcal{L}(V), p \in \mathcal{P}(\mathbf{C})\) is a polynomial, and \(\alpha \in \mathbf{C}\). Prove that \(\alpha\) is an {{c1::eigenvalue}} {{c2::of \(p(T)\)}} if and only if {{c3::\(\alpha\)}} = {{c4::\(p(\lambda)\)}} for {{c5::some eigenvalue \(\lambda\) of \(T\).}}

============================================================

  

    13 Suppose \(W\) is a complex vector space and \(T \in \mathcal{L}(W)\) has {{c1::no}} {{c2::eigenvalues}}. Prove that every subspace of \(W\) invariant under \(T\) is either {{c5::{{c3::\(\{0\}\)}}}} or {{c5::{{c4::infinitedimensional}}}}.

============================================================

  

    19 Suppose \(V\) is finite-dimensional with \(\operatorname{dim} V&gt;1\) and \(T \in \mathcal{L}(V)\). Prove that<br><br><ul><li>{{c5::{{c1::\(\{p(T): p \in \mathcal{P}(\mathbf{F})\}\)}}}} {{c4::{{c2::\( \neq \)}}}} {{c4::{{c3::\(\mathcal{L}(V) .\)}}}}</li></ul>

============================================================

  

    20 Suppose \(V\) is a finite-dimensional complex vector space and \(T \in \mathcal{L}(V)\). Prove that \(T\) has an {{c5::{{c1::invariant subspace}}}} {{c4::{{c2::of dimension \(k\)}}}} for each {{c4::{{c3::\(k=\) \(1, \ldots, \operatorname{dim} V\).}}}}

============================================================

  

    <b>Definition</b> {{c1::diagonal}} <b>matrix</b><br><br>A {{c2::diagonal}} matrix is a {{c3::square}} matrix that is {{c4::0 everywhere}} {{c5::except possibly along the diagonal.}}

============================================================

  

    If an operator has a {{c3::diagonal}} or {{c4::upper-triangular}} matrix with respect to some basis, then the {{c5::{{c2::entries along the diagonal}}}} are {{c5::{{c1::precisely the eigenvalues of the operator}}}}

============================================================

  

    Definition {{c1::eigenspace}}, {{c2::\(E(\lambda, T)\)}}<br><br><ul><li>Suppose \(T \in \mathcal{L}(V)\) and \(\lambda \in \mathbf{F}\). The {{c1::eigenspace}} of \(T\) corresponding to \(\lambda\), denoted {{c2::\(E(\lambda, T)\)}}, is defined by&nbsp;</li><ul><li>{{c2::\(E(\lambda, T)\)}} = {{c3::\(\operatorname{null}(T-\lambda I) .\)}}</li></ul><li>In other words, {{c2::\(E(\lambda, T)\)}} is the set of {{c4::all eigenvectors of \(T\)}} {{c5::corresponding to \(\lambda\)}}, along with {{c5::the 0 vector.}}<br></li></ul>

============================================================

  

    \(\lambda\) is an {{c4::eigenvalue}} of \(T\) if and only if {{c1::\(E(\lambda, T)\)}} {{c5::{{c2::\( \neq\)}}}} {{c5::{{c3::\(\{0\}\)}}}}.

============================================================

  

    If \(\lambda\) is an {{c1::eigenvalue}} of an operator \(T \in \mathcal{L}(V)\), then \(T\) {{c2::restricted to}} {{c5::{{c3::\(E(\lambda, T)\)}}}} is just {{c5::{{c4::the operator of multiplication by \(\lambda\).}}}}

============================================================

  

    {{c1::Sum of eigenspaces}} is {{c2::a direct sum}}<br><br>Suppose \(V\) is finite-dimensional and \(T \in \mathcal{L}(V)\). Suppose also that \(\lambda_{1}, \ldots, \lambda_{m}\) are distinct eigenvalues of \(T\). Then<br><br><ul><li>\(E\left(\lambda_{1}, T\right)+\cdots+E\left(\lambda_{m}, T\right)\)</li><ul><li>is {{c2::a direct sum}},</li></ul><li>Furthermore</li><li>{{c3::\(\operatorname{dim} E\left(\lambda_{1}, T\right)+\cdots+\operatorname{dim} E\left(\lambda_{m}, T\right)\)}}&nbsp; {{c4::\(\leq\)}} {{c5::\( \operatorname{dim} V\)}}</li></ul>

============================================================

  

    Proof for:<br><br><img src="paste-8dbb41588801155f230a3280607badb34fe56765.jpg"><br><br>Proof To show that \(E\left(\lambda_{1}, T\right)+\cdots+E\left(\lambda_{m}, T\right)\) is a direct sum, suppose<br><br>{{c1::\[<br>u_{1}+\cdots+u_{m}=0<br>\]}}<br><br>where {{c2::each \(u_{j}\) is in \(E(\lambda, T)\)}}. Because eigenvectors corresponding to distinct eigenvalues are l{{c5::{{c3::inearly independent (see 5.10)}}}}, this implies that {{c5::{{c4::each \(u_{j}\) equals 0}}}} .&nbsp;<br><br>

============================================================

  

    <b>Definition</b> {{c1::diagonalizable}}<br><br>An {{c4::operator}} \(T \in \mathcal{L}(V)\) is called {{c1::diagonalizable}} if it has a {{c5::{{c2::diagonal}}}} matrix {{c5::{{c3::with respect to some basis of \(V\).}}}}

============================================================

  

    <b>Conditions equivalent to&nbsp;</b>{{c1::diagonalizability}}<b>:</b><br><br>Suppose \(V\) is finite-dimensional and \(T \in \mathcal{L}(V)\). Let \(\lambda_{1}, \ldots, \lambda_{m}\) denote the distinct eigenvalues of \(T\). Then the following are equivalent:<br><br><ul><li>(a) \(T\) is {{c1::diagonalizable}};</li><li>(b) {{c2::\(\quad V\)}} has {{c2::a basis consisting of eigenvectors of \(T\);}}</li><li>(c) there exist {{c3::1-dimensional subspaces \(U_{1}, \ldots, U_{n}\) of \(V\), each invariant under \(T\)}}, such that</li><ul><li>\(V\) = {{c3::\(U_{1} \oplus \cdots \oplus U_{n}\)}}</li></ul><li>(d) \(\quad V\) = {{c4::\(E\left(\lambda_{1}, T\right) \oplus \cdots \oplus E\left(\lambda_{m}, T\right)\);}}</li><li>(e) \(\operatorname{dim} V\) = {{c5::\(\operatorname{dim} E\left(\lambda_{1}, T\right)+\cdots+\operatorname{dim} E\left(\lambda_{m}, T\right)\).}}</li></ul>

============================================================

  

    {{c1::Enough eigenvalues}} implies {{c2::diagonalizability}}<br><br>If \(T \in \mathcal{L}(V)\) has {{c5::{{c3::\(\operatorname{dim} V\)}}}} {{c5::{{c4::distinct eigenvalues}}}}, then \(T\) is {{c2::diagonalizable}}.

============================================================

  

    <img src="paste-56c6ac480fcee3254347c9d29a6a8837480fd05f.jpg"><br>Proof Suppose \(T \in \mathcal{L}(V)\) has \(\operatorname{dim} V\) distinct eigenvalues \(\lambda_{1}, \ldots, \lambda_{\operatorname{dim} V}\). <br><ul><li>For each \(j\), let \(v_{j} \in V\) be an {{c1::eigenvector corresponding to the eigenvalue \(\lambda_{j}\}}).&nbsp;</li><li>Because {{c1::eigenvectors}} corresponding to {{c2::distinct eigenvalues}} are {{c5::{{c3::linearly independent}}}} \(v_{1}, \ldots, v_{\operatorname{dim} V}\) is {{c3::linearly independent}}</li><li>. A {{c3::linearly independent}} list of \(\operatorname{dim} V\) vectors in \(V\) is a {{c5::{{c4::basis of \(V\)}}}}&nbsp; thus \(v_{1}, \ldots, v_{\operatorname{dim}} V\) is a basis of \(V\). With respect to this {{c5::{{c4::basis}}}} consisting of eigenvectors, \(T\) has a diagonal matrix.</li></ul>

============================================================

  

    1 Suppose \(T \in \mathcal{L}(V)\) is {{c1::diagonalizable}}. <br><br><ul><li>Prove that</li><ul><li>&nbsp;\(V\) = {{c2::\(\operatorname{null} T\)}}&nbsp; {{c5::{{c3::\(\oplus\)}}}} {{c5::{{c4::\(\operatorname{range} T\)}}}}.</li></ul></ul>

============================================================

  

    3 Suppose \(V\) is finite-dimensional and \(T \in \mathcal{L}(V)\). Prove that the following are equivalent:<br><br><ul><li>(a) \(\quad V=\) {{c1::null \(T\)}}&nbsp; {{c2::\(\oplus\)}} {{c1::range \(T\)}}.</li><li>(b) \(\quad V\) = {{c3::\(\operatorname{null} T\)}} {{c2::+}} {{c3::\(\operatorname{range} T\)}}.</li><li>(c) {{c4::null \(T\)&nbsp; \(\cap\) range \(T\)}} = {{c5::\(\{0\}\)}}.</li></ul>

============================================================

  

    5 Suppose \(V\) is a finite-dimensional complex vector space and \(T \in \mathcal{L}(V)\). Prove that \(T\) is diagonalizable if and only if<br><ul><li>\(V\) = {{c1::\(\operatorname{null}(T-\lambda I)\)}}&nbsp; {{c2::\(\oplus\)}} {{c5::{{c3::\(\operatorname{range}(T-\lambda I)\)}}}}</li><ul><li>for {{c5::{{c4::every \(\lambda \in \mathbf{C}\)}}}}.</li></ul></ul>

============================================================

  

    6 Suppose \(V\) is finite-dimensional, \(T \in \mathcal{L}(V)\) has {{c1::\(\operatorname{dim} V\)}} distinct eigenvalues, and \(S \in \mathcal{L}(V)\) has {{c2::the same eigenvectors as \(T\)}} (not necessarily with {{c2::the same eigenvalues}}). <br><br><ul><li>Prove that {{c5::{{c3::\(S T\)}}}} = {{c5::{{c4::\(T S\)}}}}.</li></ul>

============================================================

  

    7 Suppose \(T \in \mathcal{L}(V)\) has a {{c1::diagonal}} matrix \(A\) with respect to some basis of \(V\) and that \(\lambda \in \mathbf{F}\). <br><ul><li>Prove that \(\lambda\) {{c2::appears on the diagonal of \(A\)}} {{c3::precisely}} {{c4::\(\operatorname{dim} E(\lambda, T)\)}} {{c5::times}}.</li></ul>

============================================================

  

    9 Suppose \(T \in \mathcal{L}(V)\) is invertible. <br><ul><li>Prove that&nbsp;</li><ul><li>{{c1::\(E\)}} ( {{c2::\(\lambda, T \)}} ) = {{c1::E}}({{c5::{{c3::\(\frac{1}{\lambda}, T^{-1}\)}}}} )</li><li>for {{c5::{{c4::every \(\lambda \in \mathbf{F}\)}}}} with {{c5::{{c4::\(\lambda \neq 0\)}}}}.</li></ul></ul>

============================================================

  

    10 Suppose that \(V\) is finite-dimensional and \(T \in \mathcal{L}(V)\). <br><ul><li>Let \(\lambda_{1}, \ldots, \lambda_{m}\) denote the distinct nonzero eigenvalues of \(T\). Prove that</li><ul><li>{{c1::\(\operatorname{dim} E\left(\lambda_{1}, T\right)\)}} + \(\cdots\) + {{c2::\(\operatorname{dim} E\left(\lambda_{m}, T\right)\)}} {{c5::{{c3::\(\leq\)}}}} {{c5::{{c4::\(\operatorname{dim} \text { range } T .\)}}}}</li></ul></ul>

============================================================

  

    16 The Fibonacci sequence \(F_{1}, F_{2}, \ldots\) is defined by<br><br>\[<br>F_{1}=1, F_{2}=1, \quad \text { and } F_{n}=F_{n-2}+F_{n-1} \text { for } n \geq 3 .<br>\]<br><br>Define \(T \in \mathcal{L}\left(\mathbf{R}^{2}\right)\) by \(T(x, y)=(y, x+y)\).<ul><li>(a) Show that \(T^{n}(0,1)\) = {{c3::\(\left(F_{n}, F_{n+1}\right)\)}} for each positive integer \(n\).</li><li>(b) Find the eigenvalues of \(T\).</li><li>(c) Find a basis of \(\mathbf{R}^{2}\) consisting of eigenvectors of \(T\).</li><li>(d) Use the solution to part (c) to compute \(T^{n}(0,1)\). Conclude that</li><ul><li>\(F_{n}\) = {{c1::\(\frac{1}{\sqrt{5} }\)}} {{c5::{{c2::\(\left[\left(\frac{1+\sqrt{5} }{2}\right)^{n}-\left(\frac{1-\sqrt{5} }{2}\right)^{n}\right]\)}}}}</li><li>for each positive integer \(n\).</li></ul><li>(e) Use part (d) to conclude that for each positive integer \(n\), the Fibonacci number \(F_{n}\) is the integer that is closest to</li><ul><li>{{c1::\(\frac{1}{\sqrt{5} }\)}} {{c5::{{c4::\(\left(\frac{1+\sqrt{5} }{2}\right)^{n} \text {. }\)}}}}</li></ul></ul><br>

============================================================

  

    What is the degree of a constant polynomial?<br><ul><li>For&nbsp;{{c1::\(c \neq 0 \)}} it is {{c2::0}}</li><li>For&nbsp;{{c5::{{c3::\(c = 0\)}}}} it is&nbsp;{{c5::{{c4::\(-\infty\)}}}}</li></ul>

============================================================

  

    1. (15 points) Let \(E\) consist of all real numbers of the form \(a+b \sqrt{2}\), with \(a\) and \(b\) rational numbers.<br><br><ul><li>a) Is \(E\) closed under addition (of real numbers)?&nbsp;</li><ul><li>{{c1::Yes}}</li></ul><li>b) Is \(E\) closed under multiplication (of real numbers)? {{c1::Yes}}</li><li>c) Does \((5+4 \sqrt{2})^{-1}\) belong to \(E\) ?</li><ul><li>{{c2::Yes, it's equal to \(\underline{\frac{1}{7}(4 \sqrt{2}-5)}\).}}</li></ul><li>d) Is \(E\) a field?</li><ul><li>{{c1::Yes}}, we can check the multiplication and the addition is easier.</li><ul><li>\((a+b \sqrt{2})(c+d \sqrt{2}) \) = {{c3::\((a c+2 b d)+(a d+b c) \sqrt{2},\)}}</li></ul><li>and</li><ul><li>\(1 /(a+b \sqrt{2})\) = {{c4::\(\frac{a-b \sqrt{2} }{a^{2}-2 b^{2} } .\)}}</li></ul></ul><ul><li>All expressions are {{c5::rational numbers as long as \(a, b, c, d\) are rational numbers.}}</li></ul></ul>

============================================================

  

    2.(15 points) Give an example of a finite-dimensional vector space \(V\) and three subspaces \(U_{1}, U_{2}\), and \(U_{3}\) for which<br><br>\[<br>\begin{aligned}<br>\operatorname{dim}\left(U_{1}+U_{2}+U_{3}\right) \neq &amp; \operatorname{dim} U_{1}+\operatorname{dim} U_{2}+\operatorname{dim} U_{3} \\<br>&amp; -\operatorname{dim} U_{1} \cap U_{2}-\operatorname{dim} U_{2} \cap U_{3}-\operatorname{dim} U_{1} \cap U_{3} \\<br>&amp; +\operatorname{dim} U_{1} \cap U_{2} \cap U_{3} .<br>\end{aligned}<br>\]<ul><li>V = {{c1::(x,y)}}</li><li>\(U_1\) = {{c2::(x,x)}}<br></li><li>\(U_2\) = {{c5::{{c3::(x,0)}}}}<br></li><li>\(U_3\) = {{c5::{{c4::(0,x)}}}}<br></li></ul>

============================================================

  

    3 (20 points) Let \(\left(v_{1}, v_{2}, v_{3}, v_{4}\right)\) be a linearly independent set of vectors of a vector space over \(\mathbb{R}\). Find out<br><br>a) Do there exist \(a, b \in \mathbb{R}\) such that<br><br>\[<br>\operatorname{dim} \operatorname{span}\left(v_{1}+3 v_{2}-v_{3}, 2 v_{1}+a v_{2}, b \cdot v_{3}, v_{4}\right)=2 ?<br>\]<br>Solution: {{c1::No}}<br><ul><li>We know \(\operatorname{span}\) {{c3::\(\left(v_{1}+3 v_{2}-v_{3}, v_{4}\right)\)}} = {{c4::\(2\)}},&nbsp;</li><li>We want to show \(2 v_{1}+a v_{2}\) {{c2::can't be in the span for any \(a\)}}.&nbsp;</li><li>If {{c3::\(c\left(v_{1}+3 v_{2}-v_{3}\right)+d v_{4}\)}} = \(2 v_{2}+a v_{2}\),&nbsp;</li><ul><li>Then we know {{c5::\(c=0\); look at the coefficient in front of \(v_{4}\), we know \(d=0\)}}. {{c1::A contradiction}}.</li></ul></ul>

============================================================

  

    3 (20 points) Let \(V\) be the real vector space consisting of all polynomials of degree less than or equal to three, and vanishing at \(x=1\). In the notation of the text, this is<br><br>\[<br>V=\left\{p \in \mathcal{P}_{3}(\mathbb{R}) \mid p(1)=0\right\}<br>\]<br><br>Find a basis for \(V\).<br><br>Solution:<br><ul><li>Basis</li><ul><li>{{c1::\(&nbsp; (x-1), x(x-1), x^2(x-1)\)}}<br></li></ul><li>Lin indp:</li><ul><li>If {{c2::\(a(x-1)+b(x-1)^{2}+c(x-1)^{3}\)}} = \(0\)&nbsp;<br></li><ul><li>it requires that {{c2::a,b,c be all 0}} since {{c5::{{c3::they correspond to different powers of x}}}}</li></ul></ul><li>Spanning:</li><ul><li>Since {{c5::{{c4::dim V is 4 and a constant polynomial cannot vanish at 1, the dim must be 3 making the list a spanning list}}}}</li></ul></ul>

============================================================

  

    4 (30 points). Assume \(\left(v_{1}, \ldots, v_{n}\right)\) is a list of vectors in \(V \operatorname{such}\) that \(\operatorname{dim} \operatorname{span}\left(v_{1}, \ldots, v_{n}\right)=\) \(k\). Let \(w\) be another vector in \(V\).<br><br><br>b) Prove that<br><br><ul><li>\(\operatorname{dim} \operatorname{span}\left(v_{1}+w, \ldots, v_{n}+w\right)\)&nbsp; \(\leq\)&nbsp; \(k+1\)</li></ul><br>Proof:<br><ul><li>Let</li><ul><li>&nbsp;\(V_1\) = span ({{c1::\(\vec{v}\)}})</li><li>\(V_2\) = span ({{c1::\(\vec{v} + w\cdot \vec{1}\)}})<br></li></ul><li>Then:</li><ul><li>{{c2::\(V_2\)}}&nbsp;{{c3::\(\subset\)}}&nbsp;{{c4::\(V_1+span(w)\)}}<br></li></ul><li>So</li><ul><li>{{c5::dim&nbsp;\(V_2\)&nbsp;\(\leq\) dim&nbsp;\(V_1\) + 1}} = k+1</li></ul></ul>

============================================================

  

    4 (30 points). Assume \(\left(v_{1}, \ldots, v_{n}\right)\) is a list of vectors in \(V \operatorname{such}\) that \(\operatorname{dim} \operatorname{span}\left(v_{1}, \ldots, v_{n}\right)=\) \(k\). Let \(w\) be another vector in \(V\).<br><br>c) Prove that<br><ul><li>{{c5::{{c1::\(\operatorname{dim} \operatorname{span}\left(v_{1}+w, \ldots, v_{n}+w\right)\)}}}}&nbsp; {{c4::{{c2::\(\geq\)}}}}&nbsp; {{c4::{{c3::\(k-1 .\)}}}}</li></ul><div><ul></ul></div>

============================================================

  

    What is a system of linear equation in n unknowns?<br><br><ul><li>{{c5::{{c1::\(a_{11} x_{1}+a_{12} x_{2}+\cdots+a_{1 n} x_{n}\)}}}} = {{c5::{{c1::\(b_{1} \)}}}}</li><li>{{c4::{{c2::\(a_{21} x_{1}+a_{22} x_{2}+\cdots+a_{2 n} x_{n} \)}}}} = {{c4::{{c2::\(b_{2} \)}}}}</li><li>\(\vdots \)</li><li>{{c4::{{c3::\( a_{m 1} x_{1}+a_{m 2} x_{2}+\cdots+a_{m n} x_{n}\)}}}} = {{c4::{{c3::\(b_{m}\)}}}}</li></ul>

============================================================

  

    The idea of systems of linear equations is that:<br><ul><li>We are given {{c1::the matrix \(A \in M_{m \times n}\)}}</li><ul><li>&nbsp;(for instance {{c2::some kind of model of a business operation}}),</li></ul><li>And {{c3::the vector \(b \in F^{m}\)}}&nbsp;</li><ul><li>(for instance {{c4::some kind of desired set of outcomes}})</li></ul><li>And we wish to {{c5::solve for an unknown vector \(x \in F^{n}\)}}&nbsp;</li><ul><li>(for instance {{c4::the input conditions we should put into the model to get the desired outcomes}}).</li></ul></ul>

============================================================

  

    How would you phrase this equation in terms of abstract linear algebra?<br>\[<br>\begin{gathered}<br>a_{11} x_{1}+a_{12} x_{2}+\cdots+a_{1 n} x_{n}=b_{1} \\<br>a_{21} x_{1}+a_{22} x_{2}+\cdots+a_{2 n} x_{n}=b_{2} \\<br>\vdots \\<br>a_{m 1} x_{1}+a_{m 2} x_{2}+\cdots+a_{m n} x_{n}=b_{m}<br>\end{gathered}<br>\]<br><ul><li>Given {{c1::\(A \in \mathcal{L}(V, W)\)}} and {{c2::\(b \in W\)}}, {{c5::{{c3::find \(x \in V\)}}}} so that {{c5::{{c4::\(A x=b\)}}}}.<br></li></ul>

============================================================

  

    <b>Definition</b> 2.6. The {{c1::null space}} of {{c2::an \(m \times n\) matrix \(A\)}} is<br><br><ul><li>{{c5::{{c3::\(\operatorname{Null}(A)\)}}}} = {{c5::{{c4::\(\{v \in F^{n} \mid A v=0\}\)}}}}</li></ul>

============================================================

  

    The {{c5::{{c3::range}}}} of an m by n matrix \(A\) is<br><br><ul><li>{{c4::{{c1::\(\operatorname{Range}(A)\)}}}} = {{c4::{{c2::\(\left\{A v \in F^{m} \mid v \in F^{n}\right\} . \quad-J\)}}}}</li></ul>

============================================================

  

    Definition: the {{c1::column space}} of an m-by-n matrix A:<br><br><ul><li>{{c1::\(\operatorname{Col}(A)\)}}&nbsp;= {{c2::\(\operatorname{span}\left(c_{1}, \ldots, c_{n}\right)=\operatorname{Range}(A) \subset F^{m} .\)}}</li><ul><li>Where&nbsp;{{c5::{{c3::\(c_j \in F^{m}\)}}}} {{c5::{{c4::represents the j-th column of A}}}}</li></ul></ul>

============================================================

  

    <b>Definition</b>: the {{c1::row space}} of an m-by-n matrix A is:<br><br><ul><li>{{c1::\(\operatorname{Row}(A)\)}}= {{c2::\(\operatorname{span}\left(r_{1}, \ldots, r_{m}\right) \subset F^{n} .\)}}<br></li><ul><li>Where&nbsp;{{c5::{{c3::\(r_j \in F^{n}\)}}}} {{c5::{{c4::represents the j-th row of A}}}}</li></ul></ul>

============================================================

  

    The {{c1::column rank}} of a matrix A, is {{c3::the dimension of the column space}}; equivalently, {{c5::{{c4::the dimension of the range of \(A\) :}}}}<br><br><ul><li>{{c1::\(\text { c-rank }(A)\)}} = {{c5::{{c2::\(\operatorname{dim} \operatorname{Col}(A)=\operatorname{dim} \operatorname{Range}(A) .\)}}}}</li></ul>

============================================================

  

    The {{c5::{{c3::row rank}}}} of&nbsp; a matrix A is:<br><ul><li>{{c4::{{c1::\(\mathrm{r}-\operatorname{rank}(A)\)}}}}&nbsp;= {{c4::{{c2::\(\operatorname{dim} \operatorname{Row}(A) .\)}}}}<br></li></ul>

============================================================

  

    Proposition 2.7. Suppose \(A\) is an \(m \times n\) matrix.<br><br><ul><li>1. The {{c1::row rank}} and {{c1::the column rank}} of \(A\) are {{c1::equal}}, and {{c1::equal}} to {{c2::the dimension of the range of \(A\)}} :</li><ul><li>{{c3::\(\mathrm{r}-\operatorname{rank}(A)\)}} = {{c4::\(\operatorname{c}-\operatorname{rank}(A)=\operatorname{dim} \operatorname{Range}(A) .\)}}</li><li>{{c5::Their common value}} is called {{c5::the rank of \(A\)}}, and written {{c5::\(\operatorname{rank}(A)\).}}</li></ul></ul>

============================================================

  

    <ul><li>2. The {{c1::dimension}} of {{c2::the null space of \(A\)}} plus {{c5::{{c3:: the rank of \(A\)}}}} is equal to {{c5::{{c4::\(n\)}}}}.</li></ul>

============================================================

  

    Proposition 2.9. Suppose \(A\) is an \(m \times n\) matrix, with rows \(r_{1}, \ldots, r_{m} \in F^{n}\). Suppose \(B\) is a \(p \times m\) matrix.<br><br>1. Each row of \(B A\) is a {{c1::linear combination of the rows of \(A\)}}. More precisely, {{c2::the ith row of \(B A\)}} is {{c5::{{c3::the linear combination with coefficients given by the ith row of \(B\)}}}}:<br><br><ul><li>{{c5::{{c4::\(\sum_{j=1}^{m} b_{i j} r_{j}\)}}}}</li></ul>

============================================================

  

    2. The {{c1::row space of a matrix \(B A\)}} is a {{c2::subspace}} {{c5::{{c3::of the row space of \(A\)}}}} :<br><br><ul><li>Mathematical formulation: {{c5::{{c4::\(\operatorname{Row}(B A)\) \(\subset \operatorname{Row}(A) \subset F^{n} .\)}}}}</li></ul><br>

============================================================

  

    3. Each {{c1::\(1 \times n\) row vector}} \(r_{j}\) may be regarded as a linear map<br><br><ul><li>\(r_{j}\) : {{c2::\(F^{n} \rightarrow F\)}}</li><li>&nbsp;\(\quad r_{j}(v)\) = {{c3::\(r_{j} v\)}}</li></ul><br>from {{c2::column vectors to \(F\)}}, by matrix multiplication. With this notation,<br><br><ul><li>{{c5::\(A v\)}} =&nbsp;</li><ul><li>{{c4::<ul><li>\(\left(\begin{array}{c} r_{1}(v) \\ r_{2}(v) \\ \vdots \\ r_{m}(v) \end{array}\right) \in F^{m}\)</li></ul>}}<br></li></ul></ul>

============================================================

  

    The {{c1::null space}} of a matrix A is<br><br><ul><li>{{c2::\(\operatorname{Null}(A)\)&nbsp;}}&nbsp;</li><ul><li>= {{c5::{{c3::\(\left\{v \in F^{n} \mid r_{j}(v)=0 \quad(j=1, \ldots, m)\right\} \)}}}}</li><li>= {{c5::{{c4::\(\left\{v \in F^{n} \mid r(v)=0 \quad(r \in \operatorname{Row}(A))\right\} .\)}}}}</li></ul></ul>

============================================================

  

    4. The null space of \(A\) is<br><br>\[<br>\begin{aligned}<br>\operatorname{Null}(A) &amp; =\left\{v \in F^{n} \mid r_{j}(v)=0 \quad(j=1, \ldots, m)\right\} \\<br>&amp; =\left\{v \in F^{n} \mid r(v)=0 \quad(r \in \operatorname{Row}(A))\right\} .<br>\end{aligned}<br>\]<br><br>This proposition shows that:<br><ul><li>&nbsp;The {{c1::row space of \(A\)}} (consisting of {{c2::row vectors of size \(n\)}} ) can be interpreted as {{c5::{{c3::equations defining the null space of \(A\)}}}} (consisting of {{c5::{{c4::column vectors of size \(n\)}}}} ).</li></ul><br>

============================================================

  

    <span style="color: rgb(0, 0, 0);">4. The null space of \(A\) is</span><br style="color: rgb(0, 0, 0);"><br style="color: rgb(0, 0, 0);"><span style="color: rgb(0, 0, 0);">\[</span><br style="color: rgb(0, 0, 0);"><span style="color: rgb(0, 0, 0);">\begin{aligned}</span><br style="color: rgb(0, 0, 0);"><span style="color: rgb(0, 0, 0);">\operatorname{Null}(A) &amp; =\left\{v \in F^{n} \mid r_{j}(v)=0 \quad(j=1, \ldots, m)\right\} \\</span><br style="color: rgb(0, 0, 0);"><span style="color: rgb(0, 0, 0);">&amp; =\left\{v \in F^{n} \mid r(v)=0 \quad(r \in \operatorname{Row}(A))\right\} .</span><br style="color: rgb(0, 0, 0);"><span style="color: rgb(0, 0, 0);">\end{aligned}</span><br style="color: rgb(0, 0, 0);"><span style="color: rgb(0, 0, 0);">\]</span><br style="color: rgb(0, 0, 0);"><br style="color: rgb(0, 0, 0);"><span style="color: rgb(0, 0, 0);">This proposition shows that:</span><br style="color: rgb(0, 0, 0);"><ul style="color: rgb(0, 0, 0);"><li>&nbsp;The {{c1::row space of \(A\)}} (consisting of {{c2::row vectors of size \(n\)}} ) can be interpreted as {{c5::{{c3::equations defining the null space of \(A\)}}}} (consisting of {{c5::{{c4::column vectors of size \(n\)}}}} ).</li><li></li></ul>

============================================================

  

    The strategy of {{c4::Gaussian elimination}} is to {{c5::transform any systems of equations}} into {{c1::special}} {{c3::forms}} that are {{c2::easy to solve}}

============================================================

  

    <b>Definition</b> 2.10. An \(m \times n\) matrix \(A\) is said to be in {{c1::row-echelon}} form if {{c2::the nonzero entries are restricted to an inverted staircase shape}}. More precisely, we require<br><br>1. {{c3::the first nonzero entry in each row}} {{c4::is strictly to the right of the first nonzero entry in each earlier row}}; and<br><br>2. {{c5::any rows consisting entirely of zeros}} {{c4::must follow any nonzero rows}}.

============================================================

  

    Definition 2.10. An \(m \times n\) matrix \(A\) is said to be in {{c1::row-echelon}} form if {{c2::the nonzero entries are restricted to an inverted staircase shape}}.&nbsp; More precisely, we require<br><br>1. {{c3::the first nonzero entry in each row}} {{c4::is strictly to the right of the first nonzero entry in each earlier row}}; and<br><br>2. {{c5::any rows consisting entirely of zeros}} {{c4::must follow any nonzero rows.}}

============================================================

  

    Definition 2.10. An \(m \times n\) matrix \(A\) is said to be in row-echelon form if the nonzero entries are restricted to an inverted staircase shape. More precisely, we require<br><br><ul><li>1. the first nonzero entry in each row is strictly to the right of the first nonzero entry in each earlier row; and</li><li><br></li><li>2. any rows consisting entirely of zeros must follow any nonzero rows.</li></ul><br>The second requirement may be thought of as a {{c5::{{c1::special case of the first}}}}, if the "first nonzero entry" of a zero row is defined to be in position {{c4::{{c2::\(+\infty\)}}}}, and one says that {{c4::{{c3::\(+\infty&gt;+\infty&gt;j\) for any finite position \(j\)}}}}.&nbsp;<br>

============================================================

  

    The {{c2::pivots}} of a {{c3::row-echelon}} matrix are the {{c1::(finite) positions \((i, j(i))\)}} {{c4::of the first nonzero entries of the nonzero rows \(i=1, \cdots, r\)}}, with {{c5::\(r \leq m\) the number of nonzero rows.}}

============================================================

  

    Here is a row-echelon matrix, with the three pivots at position {{c5::{{c1::\((1,2)\)}}}} , \{{c4::{{c2::((2,4)\)}}}}, and {{c4::{{c3::\((3,5)\)}}}} shown in bold:<br><br>\[<br>\left(\begin{array}{cccccc}<br>0 &amp; \mathbf{- 2} &amp; 3 &amp; 1 &amp; 0 &amp; 1 \\<br>0 &amp; 0 &amp; 0 &amp; \mathbf{3} / \mathbf{2} &amp; -4 / 3 &amp; 17 \\<br>0 &amp; 0 &amp; 0 &amp; 0 &amp; \mathbf{1} &amp; 11 \\<br>0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\<br>0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0<br>\end{array}\right)<br>\]

============================================================

  

    The row-echelon matrix \(A\) is said to be in {{c1::reduced row-echelon}} form if in addition to the {{c1::row-echelon}}&nbsp;form:<br><br>1. {{c2::each pivot entry}} {{c3::is equal to 1}} , and<br><br>2. {{c4::all the other entries in the column of a pivot}} {{c5::are equal to zero}}.

============================================================

  

    Is this matrix in reduced row-echalon form? Why/ why not?<br>\[<br>\left(\begin{array}{cccccc}<br>0 &amp; \mathbf{- 2} &amp; 3 &amp; 1 &amp; 0 &amp; 1 \\<br>0 &amp; 0 &amp; 0 &amp; \mathbf{3} / \mathbf{2} &amp; -4 / 3 &amp; 17 \\<br>0 &amp; 0 &amp; 0 &amp; 0 &amp; \mathbf{1} &amp; 11 \\<br>0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\<br>0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0<br>\end{array}\right)<br>\]<br><ul><li>{{c5::{{c1::No}}}}, because:</li><ul><li>{{c4::{{c2::The pivots are not equal to 1}}}}</li><li>{{c4::{{c3::the pivots 3/2, and 1 have nonzero entries above them}}}}</li></ul></ul>

============================================================

  

    Suppose that the row-echelon matrix \(A\) has pivots in the first \(r\) rows, in columns<br><br>{{c5::{{c1::\[<br>1 \leq j(1)&lt;j(2)&lt;\cdots&lt;j(r) \leq n .<br>\]}}}}<br><br>We call {{c4::{{c2::\(x_{1}, x_{2}, \ldots, x_{n}\)}}}} {{c4::{{c3::the variables}}}}.&nbsp;

============================================================

  

    Suppose that the row-echelon matrix \(A\) has pivots in the first \(r\) rows, in columns<br><br>\[<br>1 \leq j(1)&lt;j(2)&lt;\cdots&lt;j(r) \leq n .<br>\]<br><br>We call \(x_{1}, x_{2}, \ldots, x_{n}\) the {{c1::variables}}, . The \(r\) {{c1::variables}} \(x_{j(i)}\) corresponding to {{c2::the pivot columns}} are called {{c2::pivot}} {{c1::variables}}. The {{c5::{{c3::remaining \(n-r\)}}}} {{c1::variables}} are called {{c5::{{c4::free}}}} {{c1::variables}}.

============================================================

  

    Proposition 2.11. Suppose that \(A\) is in {{c1::reduced row-echelon}} form, with \(r\) pivots in the entries \(\{(i, j(i)) \mid 1 \leq i \leq r\}\).<br><br><ul><li>1. The {{c2::first \(r\) standard basis vectors \(\left(f_{1}, \ldots, f_{r}\right)\) of \(F^{m}\)}} are {{c3::a basis of Range \((A)\).}}</li><ul><li>This is {{c4::the column space of \(A\)}}, so {{c5::c-rank \((A)=r\)}}</li></ul></ul>

============================================================

  

    Proposition 2.11. Suppose that \(A\) is in reduced row-echelon form, with \(r\) pivots in the entries \(\{(i, j(i)) \mid 1 \leq i \leq r\}\).<br><br><ul><li>2. The {{c1::(first) \(r\)}} {{c2::nonzero rows}} are {{c5::{{c3::a basis of the row space of \(A\)}}}}, so {{c5::{{c4::\(\mathrm{r}-\operatorname{rank}(A)=r\).}}}}<br></li></ul><br>

============================================================

  

    Proposition 2.11. Suppose that \(A\) is in reduced row-echelon form, with \(r\) pivots in the entries \(\{(i, j(i)) \mid 1 \leq i \leq r\}\).<br><br>3. For each {{c1::free variable \(x_{j}\)}}, there is a {{c2::vector in the null space}}<br><br><ul><li>\(n_{j}\) = {{c3::\(e_{j}-\sum_{i=1}^{r} a_{i j} e_{j(i)}\)}}</li><ul><li>the {{c4::\(n-r\) vectors \(n_{j}\), with \(x_{j}\) a free variable}}, are {{c5::a basis of \(\operatorname{Null}(A)\)}}.</li></ul></ul>

============================================================

  

    <b>Proposition</b> 2.11. Suppose that \(A\) is in reduced row-echelon form, with \(r\) pivots in the entries \(\{(i, j(i)) \mid 1 \leq i \leq r\}\).<br><br><ul><li>4. The equation {{c1::\(A x=b\)}}&nbsp; has a solution if and only if {{c2::\(b_{i}=0\) for all \(i&gt;r\)}}. In that case, one solution is</li><ul><li>\(x_{j(i)}\) = {{c3::\(b_{i} \)}} for {{c4::\( \quad(1 \leq i \leq r)\)&nbsp;}}</li><li>\(\quad x_{j}\) = {{c5::0}}&nbsp; for {{c4::\( \quad\left(x_{j} \right) .\) free variable}}</li></ul></ul><br>

============================================================

  

    Proposition 2.11. Suppose that \(A\) is in reduced row-echelon form, with \(r\) pivots in the entries \(\{(i, j(i)) \mid 1 \leq i \leq r\}\).<br><br><ul><li>The equation \(A x=b\) (see (2.5)) has a solution if and only if {{c1::\(b_{i}=0\) for all \(i&gt;r\)}}. In that case, one solution is</li><li>\(x_{j(i)}\) = \(b_{i} \quad(1 \leq i \leq r)\)&nbsp;</li><li>\( \quad x_{j}\) = 0 \(\quad\left(x_{j} \text { free variable }\right) .\)</li></ul><br>5. Still assuming that {{c1::\(b_{i}=0\) for all \(i&gt;r\)}}, the {{c2::most general}} solution of \(A x=b\) has {{c3::arbitrary values \(x_{j}\)}} for the {{c3::\(n-r\) free variables}}, and<br><br><ul><li>\(x_{j(i)}\) = {{c4::\(b_{i}-\sum_{j \text { free } } a_{i j} x_{j}\)}} for {{c4::\(\quad(1 \leq i \leq r) .\)}}</li></ul><br>That is, {{c5::we choose the \(n-r\) free variables}}, and then {{c5::define the \(r\) pivot variables by the equation above}}.<br><br>

============================================================

  

    5. Still assuming that \(b_{i}=0\) for all \(i&gt;r\), the most general solution of \(A x=b\) has arbitrary values \(x_{j}\) for the \(n-r\) free variables, and<br><br>\[<br>x_{j(i)}=b_{i}-\sum_{j \text { free }} a_{i j} x_{j} \quad(1 \leq i \leq r) .<br>\]<br><br>That is, we {{c1::choose}} {{c2::the \(n-r\) free variables}}, and then {{c5::{{c3::define the \(r\) pivot variables}}}} {{c5::{{c4::by the equation above.}}}}

============================================================

  

    What are the three properties of systems of linear equations which have undergon gaussian elimination?<br>\[<br>(A, b) \rightsquigarrow(C, d)<br>\]<br><ol><li>\(A\) and \(C\) are {{c1::matrices of the same size \(n \times m\), over the same field \(F\)}}, and \(b\) and \(d\) are {{c2::vectors in \(F^{m}\)}};</li><li>The two systems \(A x=b\) and \(C x=d\) have {{c3::exactly the same solutions}};&nbsp;</li><ol><li>that is, for \(x \in F^{n}\), {{c4::the equation \(A x=b\) is true if and only if \(C x=d\) is true}}; and</li></ol><li>The matrix \(C\) is in {{c5::reduced row-echelon}} form.</li></ol>

============================================================

  

    Definition 3.2. Suppose \(A x=b\) is a system of \(m\) equations in \(n\) unknowns \(((2.5))\). An elementary row operation is one of the four procedures below.<br><br>1. {{c1::Multiply the \(i\) th equation by a nonzero scalar \(\lambda\)}}. That is, {{c2::multiply the \(i\) th row of \(A\) and the \(i\) th entry of \(b\) each by \(\lambda\)}} :<br><br><ul><li>{{c3::\(\left(a_{i 1}, a_{i 2}, \ldots, a_{i n}\right)\)}} \( \rightsquigarrow \) {{c4::\(\left(\lambda a_{i 1}, \lambda a_{i 2}, \ldots, \lambda a_{i n}\right) \)&nbsp;}}</li><li>{{c5::\( \quad b_{i} \)}} \(\rightsquigarrow\)&nbsp; {{c4::\( \lambda b_{i} .\)}}</li></ul>

============================================================

  

    Definition 3.2. Suppose \(A x=b\) is a system of \(m\) equations in \(n\) unknowns \(((2.5))\). An elementary row operation is one of the four procedures below.<br><br>2. {{c1::Add a multiple \(\mu\) of the \(j\) th equation}} {{c2::to a later equation \(i\)}}, with {{c3::\(1 \leq\) \(j&lt;i \leq m\)}}. That is<br><br><ul><li>\(\left(a_{i 1}, a_{i 2}, \ldots, a_{i n}\right) \) \( \rightsquigarrow \) {{c4::\(\left(a_{i 1}+\mu a_{j, 1}, a_{i 2}+\mu a_{j 2}, \ldots, a_{i n}+\mu a_{j n}\right), \)}}</li><li>\(b_{i}\)&nbsp; &nbsp;\(\rightsquigarrow\)&nbsp; {{c5::\(b_{i}+\mu b_{j} .\)}}</li></ul>

============================================================

  

    Definition 3.2. Suppose \(A x=b\) is a system of \(m\) equations in \(n\) unknowns \(((2.5))\). An elementary row operation is one of the four procedures below.<br><br>3. {{c1::Add a multiple \(\mu\) of the \(j\) th equation}} to {{c2::an earlier equation \(i\)}}, with {{c3::\(1 \leq i&lt;j \leq m\)}}. That is<br><br><ul><li>\(\left(a_{i 1}, a_{i 2}, \ldots, a_{i n}\right)&nbsp; \) \(\rightsquigarrow\) {{c4::\(\left(a_{i 1}+\mu a_{j, 1}, a_{i 2}+\mu a_{j 2}, \ldots, a_{i n}+\mu a_{j n}\right), \)}}</li><li>\(b_{i}\) \( \rightsquigarrow\) {{c5::\(b_{i}+\mu b_{j} .\)}}</li></ul>

============================================================

  

    Definition 3.2. Suppose \(A x=b\) is a system of \(m\) equations in \(n\) unknowns \(((2.5))\). An elementary row operation is one of the four procedures below.<br><br>4. {{c1::Exchange equations \(i\) and \(j\)}}.<br><br><ul><li>\(\left(a_{j 1}, a_{j 2}, \ldots, a_{j n}\right)\) \(\rightsquigarrow\) {{c2::\(\left(a_{i 1}, a_{i 2}, \ldots, a_{i n}\right)\)&nbsp;}}</li><ul><li>\(\quad b_{j}\) \(\rightsquigarrow\) {{c3:: \(b_{i} \)}}</li></ul><li>&nbsp;\(\left(a_{i 1}, a_{i 2}, \ldots, a_{i n}\right)\) \(\rightsquigarrow\) {{c4::\(\left(a_{j 1}, a_{j 2}, \ldots, a_{j n}\right),\)&nbsp;}}</li><ul><li>\(\quad b_{i} \) \(\rightsquigarrow\)&nbsp; {{c5::\(b_{j} \)}}</li></ul></ul><br>

============================================================

  

    In short, the elementary row operations allowed by gausian elimination are:<br><ol><li>{{c1::Multiply the \(i\) th equation by a nonzero scalar \(\lambda\).}}</li><li>{{c2::Add a multiple \(\mu\) of the \(j\) th equation to a later equation \(i\)}}<br></li><li>{{c5::{{c3::Add a multiple \(\mu\) of the \(j\) th equation to an earlier equation \(i\)}}}}<br></li><li>{{c5::{{c4::Exchange equations \(i\) and \(j\).}}}}<br></li></ol>

============================================================

  

    The elementary row opeartions used in gaussian elimination can be shorthanded to:<br><ul><li>Multiply equation:&nbsp;</li><ul><li>{{c1::\(M(i ; \lambda)&nbsp; (1 \leq i \leq m, \lambda \in F-\{0\}) \)}}</li></ul><li>Add equation multiple to later equation:</li><ul><li>{{c2::\( L(i, j ; \mu)&nbsp; (1 \leq j&lt;i \leq m, \mu \in F) \)}}<br></li></ul><li>Add equation multiple to earlier equation</li><ul><li>{{c5::{{c3::\( U(i, j ; \mu)&nbsp; (1 \leq i&lt;j \leq m, \mu \in F) \)}}}}<br></li></ul><li>Exhcange equations i and j</li><ul><li>{{c5::{{c4::\( E(i, j)&nbsp; (1 \leq j&lt;i \leq m)\)}}}}<br></li></ul></ul><div><br></div>

============================================================

  

    The elementary row matrix of the multiply elementary row operation:<br><br>{{c1::\(M(i ; \lambda)\)}} = {{c2::\(\left(\begin{array}{ccccccc}1 &amp; 0 &amp; &amp; \ldots &amp; &amp; 0 &amp; \\ 0 &amp; 1 &amp; &amp; \ldots &amp; &amp; 0 &amp; \\ &amp; &amp; \ddots &amp; &amp; &amp; &amp; \\ 0 &amp; 0 &amp; \ldots &amp; \lambda &amp; \ldots &amp; &amp; 0 \\ &amp; &amp; &amp; &amp; \ddots &amp; &amp; \\ 0 &amp; 0 &amp; &amp; &amp; \ldots &amp; &amp; 1\end{array}\right)\)}}<br><br><ul><li>with {{c5::{{c3::&nbsp;\(\lambda\)}}}} {{c5::{{c4::appearing in the&nbsp;\(i,i\) place}}}}</li></ul>

============================================================

  

    The elementary row matrix of {{c4::adding a multiple of an equation to a lower/later equation}}<br><br>{{c1::\[<br>L(i, j ; \mu)=\left(\begin{array}{ccccccc}<br>1 &amp; 0 &amp; &amp; \cdots &amp; &amp; &amp; 0 \\<br>0 &amp; 1 &amp; &amp; \cdots &amp; &amp; &amp; 0 \\<br>&amp; &amp; &amp; \ddots &amp; &amp; &amp; \\<br>0 &amp; \cdots &amp; \mu &amp; \cdots &amp; 1 &amp; \cdots &amp; 0 \\<br>&amp; &amp; &amp; &amp; &amp; \ddots &amp; \\<br>0 &amp; 0 &amp; &amp; \cdots &amp; &amp; &amp; 1<br>\end{array}\right)<br>\]}}<br>with {{c5::{{c2::\(\mu\)}}}} {{c5::{{c3::appearing in the \((i, j)\) position \((i&gt;j)\)}}}};

============================================================

  

    The elementary row matrix of the elementary row operation of adding a multiple of an equation to an eaerlier/upper equation<br><br>{{c1::\(U(i, j ; \mu)\)}} = {{c2::\(\left(\begin{array}{cccccccc}1 &amp; 0 &amp; &amp; &amp; \cdots &amp; &amp; &amp; 0 \\ 0 &amp; 1 &amp; &amp; &amp; \cdots &amp; &amp; &amp; 0 \\ &amp; &amp; \ddots &amp; &amp; &amp; &amp; &amp; \\ 0 &amp; 0 &amp; \cdots &amp; 1 &amp; \cdots &amp; \mu &amp; \cdots &amp; 0 \\ &amp; &amp; &amp; &amp; &amp; \ddots &amp; &amp; \\ 0 &amp; 0 &amp; &amp; &amp; \cdots &amp; &amp; &amp; 1\end{array}\right)\)}}<br><br><ul><li>with {{c5::{{c3::\(\mu\)}}}} {{c5::{{c4::appearing in the \((i, j)\) position \((i&lt;j)\)}}}}<br></li></ul>

============================================================

  

    The elementary row matrix of the elementary row operation of exchanging two equations:<br><br>{{c2::\(E(i, j)\)}} = {{c1::\(\left(\begin{array}{cccccccc}1 &amp; 0 &amp; &amp; &amp; \cdots &amp; &amp; &amp; 0 \\ 0 &amp; 1 &amp; &amp; &amp; \cdots &amp; &amp; &amp; 0 \\ &amp; &amp; \ddots &amp; &amp; &amp; &amp; &amp; \\ 0 &amp; 0 &amp; \cdots &amp; 0 &amp; \cdots &amp; 1 &amp; \cdots &amp; 0 \\ &amp; &amp; &amp; &amp; \ddots &amp; &amp; &amp; \\ 0 &amp; 0 &amp; \cdots &amp; 1 &amp; \cdots &amp; 0 &amp; \cdots &amp; 0 \\ &amp; &amp; &amp; &amp; &amp; &amp; \ddots &amp; \\ 0 &amp; 0 &amp; &amp; &amp; \cdots &amp; &amp; &amp; 1\end{array}\right)\)}}<br><ul><li>with the {{c5::{{c3::off-diagonal ones}}}} appearing in positions {{c5::{{c4::\((i, j)\) and \((j, i)(i&lt;j)\).}}}}</li></ul>

============================================================

  

    Proposition 3.4. Suppose that we are give a system of \(m\) simultaneous linear equations in \(n\) unknowns \(A x=b((2.5))\).<br><br>1. Performing an elementary row operation (Definition 3.2) is the same as {{c1::multiplying \(A\) and \(b\)}} {{c2::on the left by the corresponding elementary row matrix ((3.3)).}}<br><br>2. Multiplying \(A\) and \(b\) on the left by any \(p \times m\) matrix \(C\) can only {{c3::enlarge the set of solutions}}. That is, {{c3::any solution \(x\) of \(A x=b\) is also a solution of \((C A) x=C b\).}}<br><br>3. The elementary row matrices are all {{c4::invertible}}. Explicitly,<br><br>{{c4::\[<br>\begin{aligned}<br>M(i ; \lambda)^{-1} &amp; =M\left(i ; \lambda^{-1}\right) ; &amp; L(i, j ; \mu)^{-1} &amp; =L(i, j,-\mu) ; \\<br>U(i, j ; \mu)^{-1} &amp; =U(i, j,-\mu) ; &amp; E(i, j)^{-1} &amp; =E(i, j) .<br>\end{aligned}<br>\]}}<br><br>4. Elementary row operations {{c5::do not change the solutions of \(A x=b\)}}.<br><br>Consequently any finite sequence of elementary row operations amounts to {{c5::left multiplication of \(A\) and \(b\) by an invertible \(m \times m\) matrix \(L\), and does not change the set of solutions.}}

============================================================

  

    Proposition 3.4. Suppose that we are give a system of \(m\) simultaneous linear equations in \(n\) unknowns \(A x=b((2.5))\).<br><br>1. Performing an elementary row operation (Definition 3.2) is the same as {{c1::multiplying \(A\) and \(b\)}} {{c2::on the left by the corresponding elementary row matrix ((3.3)).}}<br><br>2. Multiplying \(A\) and \(b\) on the left by any \(p \times m\) matrix \(C\) can only {{c3::enlarge the set of solutions}}. That is, {{c3::any solution \(x\) of \(A x=b\) is also a solution of \((C A) x=C b\).}}<br><br>3. The elementary row matrices are all {{c4::invertible}}. Explicitly,<br><br>{{c4::\[<br>\begin{aligned}<br>M(i ; \lambda)^{-1} &amp; =M\left(i ; \lambda^{-1}\right) ; &amp; L(i, j ; \mu)^{-1} &amp; =L(i, j,-\mu) ; \\<br>U(i, j ; \mu)^{-1} &amp; =U(i, j,-\mu) ; &amp; E(i, j)^{-1} &amp; =E(i, j) .<br>\end{aligned}<br>\]}}<br><br>4. Elementary row operations {{c5::do not change the solutions of \(A x=b\)}}.<br><br>Consequently any finite sequence of elementary row operations amounts to {{c5::left multiplication of \(A\) and \(b\) by an invertible \(m \times m\) matrix \(L\), and does not change the set of solutions.}}

============================================================

  

    Proposition 3.4. Suppose that we are give a system of \(m\) simultaneous linear equations in \(n\) unknowns \(A x=b((2.5))\).<br><br>1. Performing an {{c1::elementary row operation }} is {{c2::the same}} as {{c5::{{c3::multiplying \(A\) and \(b\) on the left}}}} by {{c5::{{c4::the corresponding elementary row}}}} matrix&nbsp;

============================================================

  

    Proposition 3.4. Suppose that we are give a system of \(m\) simultaneous linear equations in \(n\) unknowns \(A x=b((2.5))\).<br><br>2. Multiplying \(A\) and \(b\) on {{c1::the left}} by {{c2::any \(p \times m\) matrix \(C\)}} can {{c3::only enlarge the set of solutions}}. That is, {{c4::any solution \(x\) of \(A x=b\)}} is {{c5::also a solution of \((C A) x=C b\)}}.

============================================================

  

    Proposition 3.4. Suppose that we are give a system of \(m\) simultaneous linear equations in \(n\) unknowns \(A x=b((2.5))\).<br><br>3. The elementary row matrices are all invertible. Explicitly,<br><br><ul><li>\(M(i ; \lambda)^{-1}\)&nbsp; &nbsp;= {{c1::\(M\left(i ; \lambda^{-1}\right) ; \)}}</li><li>\( L(i, j ; \mu)^{-1}\)&nbsp; = {{c2::\(L(i, j,-\mu) ; \)}}</li><li>\(U(i, j ; \mu)^{-1}\)&nbsp; &nbsp;= {{c5::{{c3::\(U(i, j,-\mu)&nbsp; \)}}}} </li><li>\( E(i, j)^{-1} \)&nbsp; = {{c5::{{c4::\(E(i, j) \)}}}}</li></ul>

============================================================

  

    Proposition 3.4. Suppose that we are give a system of \(m\) simultaneous linear equations in \(n\) unknowns \(A x=b((2.5))\).<br><br>4. {{c5::{{c1::Elementary row operations}}}} {{c4::{{c2::do not}}}} {{c4::{{c3::change the solutions of \(A x=b\).}}}}

============================================================

  

    Any {{c1::finite sequence of elementary row operations}} amounts to {{c2::left multiplication of \(A\) and \(b\)}} by {{c5::{{c3::an invertible \(m \times m\) matrix \(L\)}}}}, and does {{c5::{{c4::not change the set of solutions}}}}.

============================================================

  

    Why is assertion 2 true?<br><br>Proposition 3.4. Suppose that we are give a system of \(m\) simultaneous linear equations in \(n\) unknowns \(A x=b((2.5))\).<br><br>2. Multiplying \(A\) and \(b\) on the left by any \(p \times m\) matrix \(C\) can only enlarge the set of solutions. That is, any solution \(x\) of \(A x=b\) is also a solution of \((C A) x=C b\).<br><br>The second statement in (2) is obvious (by {{c5::{{c1::applying \(C\) to the equation \(A x=b\)}}}}, and&nbsp;using the {{c4::{{c2:: associative}}}} law). The first statement in (2) {{c4::{{c3::follows from the second.}}}}

============================================================

  

    The elementary row operations as described in Definition 3.2 are obviously {{c1::reversible}}, and in each case {{c2::the inverse is another elementary row operation of the same kind.}} For example, to {{c1::reverse}} the operation of adding \(\mu\) times the \(j\) th row to the \(i\) th row, we simply {{c5::{{c3::add \(-\mu\) times the \(j\) th row to the \(i\) th row.}}}} From this it follows that<br>{{c5::{{c4::\[<br>L(i, j,-\mu) L(i, j ; \mu)=I_{m} .<br>\]}}}}

============================================================

  

    Why is assertion 4 true?<br><br><img src="paste-e045e6074886b0bf6784386937b90586e3062ef8.jpg"><br><br>For (4), part (2) says that {{c1::an elementary row operation \(L\) can only increase the set of solutions}}. So by (3),<br><br><ul><li>\(\text { (solutions of } A x=b)\)&nbsp;&nbsp;</li><li>\(\subset\) {{c2::\((\text { solutions of } L A x=L b) \)}}</li><li>&nbsp;\(\subset\) {{c3::\(\left(\text { solutions of } L^{-1} L A x=L^{-1} L b\right) \)}}</li><li>&nbsp;= {{c4::\( (\text { solutions of } A x=b) .\)}}</li></ul><br>So the containments must be {{c5::equalities}}.<br>

============================================================

  

    The first part of Gaussian elimination {{c1::finds (in succession) \(r\) special entries}}<br><br><ul><li>{{c2::\( (i(1), j(1)),(i(2), j(2)), \ldots,(i(r), j(r)), \)}}</li><ul><li>such that</li><li>{{c3::&nbsp;\(1 \leq j(1)&lt;j(2)&lt;\cdots&lt;j(r) \leq n, \)}}</li><li>{{c4::\( 1 \leq i(p) \leq m \text { all distinct }\)}}</li></ul></ul>These {{c1::entries}} {{c5::will become the pivots in the row-echelon form&nbsp;}}<br>

============================================================

  

    After we perform the row operations in the first part of Guassian Elimination, we want to have a matrix with the following properties:<br><br><ul><li>{{c1::\( \text { the first entry of row } i(p) \) \( \text { is a } 1 \text {, in column } j(p) \text {; } \)}}</li><li>{{c2::\( \text { entries in column } j(p)\) \(\text { above row } i(p) \text {, } \)}}</li><ul><li>{{c5::{{c3::\( \text { except in rows } i(q) \) \(\text { with } q&lt;p \text {, they are zero; and } \)}}}}</li></ul><li>{{c5::{{c4::&nbsp;\( \text { entries in column } j(p) \) \( \text { below row } i(p) \text { are zero. } \)}}}}</li></ul>

============================================================

  

    After we perform the row operations in the first part of Guassian Elimination, we want to have a matrix with the following properties:<br><br><ul><li>{{c1::\( \text { the first entry of row } i(p) \) \( \text { is a } 1 \text {, in column } j(p) \text {; } \)}}</li><li>{{c2::\( \text { entries in column } j(p)\) \(\text { above row } i(p) \text {, } \)}}</li><ul><li>{{c5::{{c3::\( \text { except in rows } i(q) \) \(\text { with } q&lt;p \text {, they are zero; and } \)}}}}</li></ul><li>{{c5::{{c4::&nbsp;\( \text { entries in column } j(p) \) \( \text { below row } i(p) \text { are zero. } \)}}}}</li></ul>

============================================================

  

    After we perform the row operations in the first part of GE, we will have a matrix with the following properties&nbsp;<br><br>\[<br>\begin{aligned}<br>&amp; \text { the first entry of row } i(p) \text { is a } 1 \text {, in column } j(p) \text {; } \\<br>&amp; \text { entries in column } j(p) \text { above row } i(p) \text {, } \\<br>&amp; \text { except in rows } i(q) \text { with } q&lt;p \text {, are zero; and } \\<br>&amp; \text { entries in column } j(p) \text { below row } i(p) \text { are zero. }<br>\end{aligned}<br>\]<br><br>We will find these entries and arrange for the these vanishing conditions {{c1::one row at a time::in what sequence?}}. We know we are finished when we finally have<br><br><ul><li>{{c2::\(\text { all entries of } A \)}} {{c5::{{c3::\(\text { outside rows } i(1), \ldots, i(r)\)}}}} {{c5::{{c4::\( \text { are zero. } \)}}}}</li></ul>

============================================================

  

    The row operations to achieve the first part of GE are of types {{c5::{{c1::M and L}}}}, meaning {{c4::{{c2::multiplying rows}}}} and {{c4::{{c3::subtracting pivot rows from sucessor rows lower down the equations matrix.}}}}

============================================================

  

    A typical 3x3 case of making zeroes for GE<br><br><ul><li>\(\left(\begin{array}{lll}\mathbf{2} &amp; 3 &amp; 4 \\2 &amp; 2 &amp; 2 \\1 &amp; 2 &amp; 1\end{array}\right)\)&nbsp;</li><li>{{c5::{{c1::\( \stackrel{M(1 ; 1 / 2)} {\longrightarrow} \)}}}}</li><li>\(\left(\begin{array}{ccc}\mathbf{1} &amp; 3 / 2 &amp; 2 \\2 &amp; 2 &amp; 2 \\1 &amp; 2 &amp; 1\end{array}\right) \)&nbsp;</li><li>{{c4::{{c2::\(\stackrel{L(2,1 ;-2)}{\longrightarrow} \)}}}}</li><li>\(\left(\begin{array}{ccc}\mathbf{1} &amp; 3 / 2 &amp; 2 \\0 &amp; -1 &amp; -2 \\1 &amp; 2 &amp; 1\end{array}\right)\)&nbsp;&nbsp;</li><li>{{c4::{{c3::\(\stackrel{L(3,1 ;-1)}{\longrightarrow}\)}}}}</li><li>\(\left(\begin{array}{ccc}\mathbf{1} &amp; 3 / 2 &amp; 2 \\0 &amp; -1 &amp; -2 \\0 &amp; 1 / 2 &amp; -1\end{array}\right)\)</li></ul><br>

============================================================

  

    A typical 3x3 case of making zeroes for GE<br><br><ul><li>\(\left(\begin{array}{lll}\mathbf{2} &amp; 3 &amp; 4 \\2 &amp; 2 &amp; 2 \\1 &amp; 2 &amp; 1\end{array}\right)\)&nbsp;</li><li>\( \stackrel{M(1 ; 1 / 2)} {\longrightarrow} \)</li><li>{{c5::{{c1::\(\left(\begin{array}{ccc}\mathbf{1} &amp; 3 / 2 &amp; 2 \\2 &amp; 2 &amp; 2 \\1 &amp; 2 &amp; 1\end{array}\right) \)&nbsp;}}}}</li><li>\(\stackrel{L(2,1 ;-2)}{\longrightarrow} \)</li><li>{{c4::{{c2::\(\left(\begin{array}{ccc}\mathbf{1} &amp; 3 / 2 &amp; 2 \\0 &amp; -1 &amp; -2 \\1 &amp; 2 &amp; 1\end{array}\right)\)&nbsp;&nbsp;}}}}</li><li>\(\stackrel{L(3,1 ;-1)}{\longrightarrow}\)</li><li>{{c4::{{c3::\(\left(\begin{array}{ccc}\mathbf{1} &amp; 3 / 2 &amp; 2 \\0 &amp; -1 &amp; -2 \\0 &amp; 1 / 2 &amp; -1\end{array}\right)\)}}}}</li><li></li></ul>

============================================================

  

    &nbsp;A typical 3x3 case of making zeroes for GE<br><br><ul><li>\(\left(\begin{array}{lll}\mathbf{2} &amp; 3 &amp; 4 \\2 &amp; 2 &amp; 2 \\1 &amp; 2 &amp; 1\end{array}\right)\)&nbsp;</li><li>\( \stackrel{M(1 ; 1 / 2)} {\longrightarrow} \)</li><li>\(\left(\begin{array}{ccc}\mathbf{1} &amp; 3 / 2 &amp; 2 \\2 &amp; 2 &amp; 2 \\1 &amp; 2 &amp; 1\end{array}\right) \)&nbsp;</li><li>\(\stackrel{L(2,1 ;-2)}{\longrightarrow} \)</li><li>\(\left(\begin{array}{ccc}\mathbf{1} &amp; 3 / 2 &amp; 2 \\0 &amp; -1 &amp; -2 \\1 &amp; 2 &amp; 1\end{array}\right)\)&nbsp;&nbsp;</li><li>\(\stackrel{L(3,1 ;-1)}{\longrightarrow}\)</li><li>\(\left(\begin{array}{ccc}\mathbf{1} &amp; 3 / 2 &amp; 2 \\0 &amp; -1 &amp; -2 \\0 &amp; 1 / 2 &amp; -1\end{array}\right)\)</li></ul><div>Here:</div><div><ul><li>&nbsp;I {{c1::pick the first nonzero entry in the first nonzero column}}, and {{c2::mark it in bold}}.&nbsp;</li><li>This {{c2::marks}} {{c3::the first row}} as the first of our special rows.&nbsp;</li><li>I then {{c4::multiply this special row by the inverse of the first entry}}, to {{c4::make the first entry 1.&nbsp;}}</li><li>Then I {{c5::subtract multiples of the first row from other rows}} to {{c5::get rid of the other entries in the first column.&nbsp;}}</li></ul></div>

============================================================

  

    What steps does the first phase of gaussian elimination reduce to?<br><ol><li>{{c1::Identify pivots for each column}}</li><li>{{c2::Transform the pivots to the value 1}} {{c3::by dividing by their value}}</li><li>{{c4::Make entries bellow a pivot 0}} {{c5::by subtracting the pivot row multiplied by the necessary scalar}}</li></ol>

============================================================

  

    \[<br>\begin{array}{r}<br>\left(\begin{array}{ccc}<br>\mathbf{1} &amp; 3 / 2 &amp; 2 \\<br>0 &amp; -1 &amp; -2 \\<br>0 &amp; 1 / 2 &amp; -1<br>\end{array}\right) \stackrel{M(2 ;-1)}{\longrightarrow}\left(\begin{array}{ccc}<br>\mathbf{1} &amp; 3 / 2 &amp; 2 \\<br>0 &amp; \mathbf{1} &amp; 2 \\<br>0 &amp; 1 / 2 &amp; -1<br>\end{array}\right) \\<br>\stackrel{L(3,2 ;-1 / 2)}{\longrightarrow}\left(\begin{array}{ccc}<br>\mathbf{1} &amp; 3 / 2 &amp; 2 \\<br>0 &amp; \mathbf{1} &amp; 2 \\<br>0 &amp; 0 &amp; -2<br>\end{array}\right)<br>\end{array}<br>\]<br><br><ul><li>Here I picked {{c1::the first column that's nonzero outside the first row}}, then {{c1::marked in bold its first nonzero entry outside the first row:&nbsp;}}</li><li>now {{c2::the second row}} is {{c2::identified as the second of our special rows.&nbsp;}}</li><li>Its {{c5::{{c3::leading entry is -1}}}} , so I {{c5::{{c3::multiply the row by its inverse -1 .&nbsp;}}}}</li><li>Then I {{c5::{{c4::subtract multiples of the second row from later rows}}}} to {{c5::{{c4::get rid of the later entries in this column.}}}}</li></ul>

============================================================

  

    \[<br>\left(\begin{array}{ccc}<br>\mathbf{1} &amp; 3 / 2 &amp; 2 \\<br>0 &amp; \mathbf{1} &amp; 2 \\<br>0 &amp; 0 &amp; \mathbf{- 2}<br>\end{array}\right) \stackrel{M(3 ;-1 / 2)}{\longrightarrow}\left(\begin{array}{ccc}<br>\mathbf{1} &amp; 3 / 2 &amp; 2 \\<br>0 &amp; \mathbf{1} &amp; 2 \\<br>0 &amp; 0 &amp; \mathbf{1}<br>\end{array}\right)<br>\]<br><br><ul><li>For this last step in this first part, I notice that {{c1::the third column}} is {{c1::the first one that's nonzero outside the first two special rows.}}</li><li>&nbsp;Its {{c2::first entry outside the two special rows}} is {{c2::the lower right corner entry -2 ,&nbsp;}}</li><li>so {{c5::{{c3::that one becomes our third pivot, marked in bold.&nbsp;}}}}</li><li>I {{c5::{{c4::multiply that third row by \(-1 / 2\)}}}} to {{c5::{{c4::make the leading entry 1}}}} , and we are done with the first phase of gausisian elimination</li></ul>

============================================================

  

    <ul><li>Despite appearances, the first phase of gaussian elimination may require pivots in {{c1::ascending vertical}} postions</li><li>However, {{c2::zeroes should only be made}} {{c5::{{c3::for entries bellow their vertical position}}}} and {{c5::{{c4::not for those above}}}} in the first phase</li></ul><div><br></div>

============================================================

  

    <ul><li>The second part of GE starts with {{c1::r pivots, i(1),...,i(r)}}</li><li>It then {{c2::rearranges the rows}} so that</li><ul><li>&nbsp;{{c3::row i(1) becomes row 1}}</li><li>{{c3::...}}</li><li>{{c3::row i(r) becomes row r}}</li></ul><li>At the end of this part {{c4::the pivots will be in locations}}:</li><li>{{c5::\[<br>\begin{gathered}<br>(1, j(1)),(2, j(2)), \ldots,(r, j(r)), \\<br>1 \leq j(1)&lt;j(2)&lt;\cdots&lt;j(r) \leq n .<br>\end{gathered}<br>\]}}<br></li></ul><br>

============================================================

  

    <ul><li>The second part of GE starts with a matrix having \(r\) pivots and rearranges the rows so that row \(i(1)\) becomes row 1 , row \(i(2)\) becomes row 2 , and so on. At the end of this part, our pivots will be in locations</li><li>\(\begin{gathered}(1, j(1)),(2, j(2)), \ldots,(r, j(r)), \\1 \leq j(1)&lt;j(2)&lt;\cdots&lt;j(r) \leq n .\end{gathered}\)</li></ul><br>The matrix after this part of the algorithm will satisfy the following requirements, which mean in particular that it is in {{c1::row echelon}} form<br><br><ul><li>{{c2::\(\text { the first entry of row } p \text { is a } 1 \) \(\text { in column } j(p) \quad(1 \leq p \leq r) \text {; }\)}}</li><li>{{c3::entries in column \(j(p)\) below row \(p\) are zero.}}</li><li>{{c4::all entries of \(A\) below rows \(1, \ldots, r\) are zero.}}</li></ul><br>The big theoretical fact about this part of Gaussian elimination is<br><ul><li>The {{c1::elementary row opeartions needed to achieve the previous conditions}} {{c5::are of type E(exchaing rows)}}</li></ul>

============================================================

  

    To carry out the second phase of Gaussian Elimination:<br><ul><li>We {{c5::{{c1::exchange the first pivot row with row 1}}}}</li><li>Then {{c4::{{c2::the second pivot row with row 2}}}}</li><li>And {{c4::{{c3::so on for all pivot rows}}}}</li></ul>

============================================================

  

    The third and last part of Gaussian elimination starts with {{c5::{{c1::r pivots}}}} with {{c4::{{c2::all 0-entries bellow}}}} at locations<br><br>{{c4::{{c3::\[<br>\begin{gathered}<br>(1, j(1)),(2, j(2)), \ldots,(r, j(r)), \\<br>1 \leq j(1)&lt;j(2)&lt;\cdots&lt;j(r) \leq n .<br>\end{gathered}<br>\]}}}}

============================================================

  

    The {{c5::{{c3::final}}}} part of GE aims to {{c4::{{c2::clear the column entries}}}} {{c4::{{c1::above the pivots}}}}

============================================================

  

    The {{c5::{{c1::first}}}} part of GE aims to {{c4::{{c2::clear the column entries}}}} {{c4::{{c3::above the pivots}}}}

============================================================

  

    Afther the third and final part of GE, the matrix will satify the following requirements meaning it is in {{c1::reduced row echelon}} form:<br><ul><li>{{c2::\(\text { the first entry of row } p \text { is a } 1 \) \(\text {, in column } j(p) \quad(1 \leq p \leq r) \text {; }\)}}</li><li>{{c5::{{c3::all other entries in column \(j(p)\) are zero.}}}}</li><li>{{c5::{{c4::all entries of \(A\) below rows \(1, \ldots, r\) are zero.}}}}</li></ul>

============================================================

  

    To go from {{c1::row echelon}} form to {{c1::reduced row echelon}} form, the {{c2::final}} part of GE must use {{c3::opeartions of type U}}, meaning {{c4::reducing entries above the pivots to 0}} {{c5::by subtracting multiples of the pivot row from them}}

============================================================

  

    Applying the final phase of gaussian elimination:<br><br><ul><li>{{c1::\(\left(\begin{array}{ccc}\mathbf{1} &amp; 3 / 2 &amp; 2 \\0 &amp; \mathbf{1} &amp; 2 \\0 &amp; 0 &amp; \mathbf{1}\end{array}\right)\)&nbsp;}}</li><li>{{c2::\(\stackrel{U(1,2 ;-3 / 2)}{\longrightarrow} \)&nbsp;}}</li><li>{{c5::{{c3::\(\left(\begin{array}{ccc}\mathbf{1} &amp; 0 &amp; -1 \\0 &amp; \mathbf{1} &amp; 2 \\0 &amp; 0 &amp; \mathbf{1}\end{array}\right)\)}}}}</li></ul><div>In human text:</div><div><br></div><div><ul><li>{{c5::{{c4::Reduce the one nonzero entry above the second pivot to 0 by subtracting -3/2 times the pivot row}}}}</li></ul></div>

============================================================

  

    <b>Gaussian elimination in full&nbsp;<br><br><br></b>Theorem 4.8. Suppose \(A^{\prime}\) is an \(m \times n\) matrix with entries in a field \(F\). Then we can perform {{c5::{{c1::a finite sequence of elementary row operations on \(A\)}}}} to obtain {{c4::{{c2:: a new \(m \times n\) matrix \(A^{\prime}\)}}}} {{c4::{{c3::in reduced row-echelon form.}}}}&nbsp;

============================================================

  

    Theorem 4.8. Suppose \(A^{\prime}\) is an \(m \times n\) matrix with entries in a field \(F\). Then we can perform a finite sequence of elementary row operations on \(A^{\prime}\) to obtain a new \(m \times n\) matrix \(A^{\prime}\) in reduced row-echelon form. More precisely, we perform<br><br><ol><li>at most {{c1::\(m\)}} row operations of type {{c1::\(M\) (multiply a row by a nonzero scalar) }} interspersed with at most

&nbsp;{{c1::&nbsp;&nbsp;\(m(m-1) / 2\) operations of type \(L\) (add a multiple of a row to a later row)}}; then</li><li>at most {{c2::\(m(m-1) / 2\)}} operations of type {{c3::\(E\) (exchange two rows)}}; then</li><li>at most {{c4::\(m(m-1) / 2\)}} operations of type {{c5::\(U\) (add a multiple of a row to an earlier row)}}.</li></ol>

============================================================

  

    <ul><li>Theorem 4.8. Suppose \(A^{\prime}\) is an \(m \times n\) matrix with entries in a field \(F\). Then we can perform a finite sequence of elementary row operations on \(A^{\prime}\) to obtain a new \(m \times n\) matrix \(A^{\prime}\) in reduced row-echelon form. More precisely, we perform</li><ul><li>1. at most \(m\) row operations of type \(M\) (multiply a row by a nonzero scalar) interspersed with at most \(m(m-1) / 2\) operations of type \(L\) (add a multiple of a row to a later row); then</li><li>2. at most \(m(m-1) / 2\) operations of type \(E\) (exchange two rows); then</li><li>3. at most \(m(m-1) / 2\) operations of type \(U\) (add a multiple of a row to an earlier row).</li></ul></ul><br>Consequently, we can write<br><br><ul><li>{{c3::\(A^{\prime}\)}} = {{c1::U E L}} {{c5::{{c4::A}}}}</li><li>{{c5::{{c4::\(A\)}}}} = {{c5::{{c2::\(L^{-1}\)&nbsp; \(E^{-1}\) \(U^{-1}\)}}}} {{c3::\(A^{\prime} .\)}}</li></ul><br>

============================================================

  

    <ul><li>We can write Gaussian Elimination as&nbsp;</li><ul><li>\(A^{\prime}=U E L A\) where \(A^{\prime}\) is A but&nbsp; in reduced row-echalon form</li><li>\(\quad A=L^{-1} E^{-1} U^{-1} A^{\prime} .\)</li></ul><li>Here:</li><ul><li>&nbsp;\(L\) and \(L^{-1}\) are {{c1::\(m \times m\)}} invertible {{c1::lower-triangular::shape?}} matrices;&nbsp;</li><li>\(E\) and \(E^{-1}\) are invertible {{c2::\(m \times m\)}} {{c3::permutation}} matrices;&nbsp;</li><li>and \(U\) and \(U^{-1}\) are invertible {{c4::\(m \times m\)}} {{c5::upper-triangular}} matrices with {{c5::ones on the diagonal.&nbsp;}}</li></ul><li>The reduced row echelon matrix \(A\) is unique (independent of how the row reduction is performed).<br></li></ul>

============================================================

  

    Proposition 5.1. Suppose \(A\) is an \(m \times n\) matrix.<br><br>1. {{c1::Elementary row operations}} {{c3::do not change}} the {{c2::null space \(\operatorname{Null}(A) \subset F^{n}\)}}. In particular, they {{c3::do not change}} the {{c4::nullity}} {{c5::\(\operatorname{dim} \operatorname{Null}(A)\)}}.<br>

============================================================

  

    Proposition 5.1. Suppose \(A\) is an \(m \times n\) matrix.<br><br>3. Applying {{c1::a sequence of elementary row operations}} is equivalent to {{c2::left}} multiplication of \(A\) by an {{c5::{{c3::invertible \(m \times m\)}}}} matrix \(L\). The effect of this is to apply {{c5::{{c4:: \(L\) to Range \((A) \subset F^{m}\)}}}} :<br><br>

============================================================

  

    Proposition 5.1. Suppose \(A\) is an \(m \times n\) matrix.<br><br>3. Applying a sequence of elementary row operations is equivalent to left multiplication of \(A\) by an invertible \(m \times m\) matrix \(L\). The effect of this is to apply \(L\) to Range \((A) \subset F^{m}\) :<br><br><ul><li>{{c1::\(\operatorname{Range}(L A)\)&nbsp;}}<br></li><ul><li>= {{c2::\(\operatorname{Col}(L A)\)&nbsp;}}</li><li>= {{c5::{{c3::\(L(\operatorname{Col}(A))\)}}}}</li><li>= {{c5::{{c4::\(L(\operatorname{Range}(A)) .\)}}}}</li></ul></ul>

============================================================

  

    Proposition 5.1. Suppose \(A\) is an \(m \times n\) matrix.<br><br>4. {{c1::Elementary row operations}} {{c2::do not change}} the {{c5::{{c3::column}}}} {{c5::{{c4::rank c-rank \((A)\)}}}}.

============================================================

  

    <b>Theorem 5.2</b>. Suppose \(n\) and \(r\) are nonnegative integers. <br><ul><li>There is a {{c1::oneto-one}} correspondence between&nbsp;</li><ul><li>{{c2::\(r\)}}-dimensional {{c3::subspaces \(U \subset F^{n}\),&nbsp;}}</li><li>And {{c4::\(r \times n\)}} {{c5::matrices \(A\) in reduced row-echelon form}}, with {{c5::one pivot in each row}}; that is, with {{c5::no rows equal to zero}}.&nbsp;</li></ul><li>The correspondence sends {{c5::the matrix \(A\)}} to {{c3::the span \(\operatorname{Row}(A)\)}} of {{c1::the rows of \(A\).}}&nbsp;<br></li></ul>

============================================================

  

    Theorem 5.2. Suppose \(n\) and \(r\) are nonnegative integers. There is a oneto-one correspondence between \(r\)-dimensional subspaces \(U \subset F^{n}\), and \(r \times n\) matrices \(A\) in reduced row-echelon form, with one pivot in each row; that is, with no rows equal to zero. The correspondence sends the matrix \(A\) to the span \(\operatorname{Row}(A)\) of the rows of \(A\). To go in the other direction, suppose \(U\) is an \(r\)-dimensional subspace of \(F_{n}\). Choose a basis \(\left(u_{1}, \ldots, u_{r}\right)\) of \(U\), and let \(A^{\prime}\) be the \(r \times n\) matrix with rows \(u_{i}\). Perform Gaussian elimination on \(A^{\prime}\), getting an \(r \times n\) matrix \(A\) in reduced row echelon form; this is the matrix corresponding to the subspace \(U\).<br><br>Sketch of proof. A matrix \(A\) of the desired form clearly has \(r\) pivots, and so has rank \(r\) (Proposition 2.7). Therefore the row space \(\operatorname{Row}(A)\) is indeed an \(r\)-dimensional subspace of \(F^{n}\). <br><ul><li>Conversely, given {{c1::an \(r\)-dimensional \(U\)}}, the construction in the theorem produces an {{c2::\(r \times n\)::size?}} matrix \(A^{\prime}\) with {{c3::\(\operatorname{Row}\left(A^{\prime}\right)\)}} = {{c4::\(U\)}}. Now perform Gaussian elimination on \(A^{\prime}\) (Theorem 4.8), obtaining a reduced row echelon matrix \(A\) with {{c5::\(\operatorname{Row}(A)=\operatorname{Row}\left(A^{\prime}\right)=U\)}}, as desired.</li></ul>

============================================================

  

    Suppose \(A\) is an \(m \times n\) matrix, and \(b \in F^{m}\). Recall from (2.5) the system of \(m\) simultaneous equations in \(n\) unknowns<br><br>\[<br>A(x)=b \quad\left(x \in F^{n}, b \in F^{m}\right) .<br>\]<br><br>The {{c1::augmented}} matrix for this system is&nbsp;the {{c2::\(m \times(n+1)\)::size?}} matrix<br><br><ul><li>{{c5::{{c3::\(\widetilde{A}\)}}}}&nbsp;= {{c5::{{c4::\(\operatorname{def}(A \mid b)\)}}}}</li></ul><br>

============================================================

  

    Performing Gaussian elimination on the {{c5::{{c1::augmented}}}} matrix leads to a rowechelon matrix {{c4::{{c2::\(\left(A^{\prime} \mid b^{\prime}\right)\)::notation}}::notation}}, corresponding to an equivalent {{c4::{{c3::system of equations \(A^{\prime}(x)=b^{\prime}\)}}}}. Here's how this looks in the example of (4.2).<br><br>\[<br>\left(\begin{array}{lll}<br>2 &amp; 3 &amp; 4 \\<br>2 &amp; 2 &amp; 2 \\<br>1 &amp; 2 &amp; 1<br>\end{array}\right)\left(\begin{array}{l}<br>x_{1} \\<br>x_{2} \\<br>x_{3}<br>\end{array}\right)=\left(\begin{array}{l}<br>1 \\<br>2 \\<br>3<br>\end{array}\right)<br>\]

============================================================

  

    For this final reduced rowechalon form augmuneted matrix:<br><br>\(\left(\begin{array}{ccc|c}1 &amp; 0 &amp; 0 &amp; 1 / 2 \\ 0 &amp; 1 &amp; 0 &amp; 2 \\ 0 &amp; 0 &amp; 1 &amp; -3 / 2\end{array}\right)\).<br><br>The equivalent system of equations is<br><br><ul><li>\(x_{1}\) = {{c5::{{c1::\(1 / 2\)}}}}</li><li>\( x_{2}\) = {{c4::{{c2::\(2\)}}}} </li><li>\(x_{3}\) = {{c4::{{c3::\(-3 / 2,\)}}}}</li></ul>

============================================================

  

    When does a {{c5::{{c1::left}}}} inverse exits?<br><ul><li>When {{c4::{{c2::the null space of a linear transformation}}}} {{c4::{{c3::is 0}}}}</li></ul>

============================================================

  

    When does a {{c5::{{c1::left}}}} inverse exits?<br><ul><li>When {{c4::{{c2::the null space of a linear transformation}}}} {{c4::{{c3::is 0}}}}</li></ul>

============================================================

  

    Requering that {{c1::the null space of a matrix is 0}} and thus a {{c2::left}} inverse exists is quivalent to requiering that, from a GE perspective, :<br><ul><li>{{c3::r}}={{c3::n}}</li><li>{{c4::There are no free variables}}</li><li>{{c5::There is a pivot in every row}}</li></ul>

============================================================

  

    How is computing an inverse through GE simillar to solving a system of linear equations?<br><ul><li>&nbsp;{{c5::{{c1::Augument}}}} the matrix by {{c4::{{c2::concatenating the idenity }}}}</li><li>Or simply {{c4::{{c3::apply the same operations to both sides}}}}</li></ul>

============================================================

  

    When computing an {{c5::{{c3::inverse}}}} the reduced row-echalong form of A must then be:<br><br><ul><li>\(A^{\prime}\)&nbsp;</li><li>= {{c4::{{c1::\( \left(\begin{array}{cccc}1 &amp; 0 &amp; \cdots &amp; 0 \\0 &amp; 1 &amp; \cdots &amp; 0 \\&amp; &amp; \vdots &amp; \\0 &amp; 0 &amp; \cdots &amp; 1 \\0 &amp; 0 &amp; \cdots &amp; 0 \\&amp; &amp; \vdots &amp; \\0 &amp; 0 &amp; \cdots &amp; 0\end{array}\right)\)&nbsp;}}}}</li><li>= {{c4::{{c2::\(\left(\begin{array}{c}I_{n} \\0_{m \times n}\end{array}\right)\)}}}}</li></ul>

============================================================

  

    Proposition 6.5. Suppose that \(A\) is an \(m \times n\) matrix of rank \(r=n\) (so that \(m \geq n)\). Form an augmented matrix \(\widetilde{A}\) ={{c1::\(\left(A \mid I_{m}\right)\)}} of size {{c2::\(m \times m+n\)}}. Perform Gaussian elimination:<br><br><ul><li>\(\widetilde{A}\)= {{c1::\(\left(A \mid I_{m}\right)\)}} \( \stackrel{\text { Gauss }}{\longrightarrow}\) {{c3::\(\left(A^{\prime} \mid L\right)\)}}</li></ul><br>with \(A^{\prime}\) the matrix in (6.4) and \(L\) the \{{c4::(m \times m\)}} matrix which is {{c5::the product of all the elementary row matrices used to reduce \(A\)}}. Write \(B\) for the \(n \times m\) matrix consisting of the first \(n\) rows of \(L\). Then<br><br>\[<br>L A=A^{\prime}, \quad B A=I_{n} .<br>\]<br><br>In particular, \(B\) is a left inverse of \(A\).<br>

============================================================

  

    Proposition 6.5. Suppose that \(A\) is an \(m \times n\) matrix of rank \(r=n\) (so that \(m \geq n)\). Form an augmented matrix \(\widetilde{A}=\left(A \mid I_{m}\right)\) of size \(m \times m+n\). Perform Gaussian elimination:<br><br>\[<br>\widetilde{A}=\left(A \mid I_{m}\right) \stackrel{\text { Gauss }}{\longrightarrow}\left(A^{\prime} \mid L\right)<br>\]<br><br>with \(A^{\prime}\) the matrix in (6.4) and \(L\) the \(m \times m\) matrix which is the product of all the elementary row matrices used to reduce \(A\). Write \(B\) for the {{c1::\(n \times m\)}} matrix consisting of {{c2::the first \(n\) rows of \(L\)}}. Then<br><br><ul><li>{{c3::\(L A\)}} = {{c4::\(A^{\prime}\)&nbsp;}}</li><li>{{c3::\(B A \)}} = {{c4::\(I_{n} .\)}}</li></ul><br>In particular, \(B\) is {{c5::a left inverse of \(A\)}}.<br>

============================================================

  

    2. (20 points) List all \(2 \times 2\) matrices over \(\mathbb{R}\) which are in reduced row-echelon form.<br><ul><li>{{c1::\(\left(\begin{array}{ll}1 &amp; 0 \\ 0 &amp; 1\end{array}\right)\)}}<br></li><li>{{c2::\(\left(\begin{array}{ll}1 &amp; t \\ 0 &amp; 0\end{array}\right)\)}}<br></li><li>{{c5::{{c3::\(\left(\begin{array}{ll}0 &amp; 1 \\ 0 &amp; 0\end{array}\right)\)}}}}</li><li>{{c5::{{c4::\(\left(\begin{array}{ll}0 &amp; 0 \\ 0 &amp; 0\end{array}\right)\)}}}}<br></li></ul>

============================================================

  

    3. (20 points) Find all solutions \(X\) to the matrix equation \(A X=C\) when<br><br>a)<br><br>\[<br>A=\left(\begin{array}{lll}<br>1 &amp; 2 &amp; 3 \\<br>3 &amp; 2 &amp; 1<br>\end{array}\right)<br>\]<br><br>and<br><br>\[<br>C=\left(\begin{array}{l}<br>6 \\<br>6<br>\end{array}\right)<br>\]<br><br>We use Gaussian elimination.<br><ul><li>{{c1::\(\left(\begin{array}{lll|l}1 &amp; 2 &amp; 3 &amp; 6 \\ 3 &amp; 2 &amp; 1 &amp; 6\end{array}\right)\)}}<br></li><li>{{c2::\(\left(\begin{array}{ccc|c}1 &amp; 2 &amp; 3 &amp; 6 \\ 0 &amp; -4 &amp; -8 &amp; -12\end{array}\right)\)}}<br></li><li>{{c5::{{c3::\(\left(\begin{array}{lll|l}1 &amp; 2 &amp; 3 &amp; 6 \\ 0 &amp; 1 &amp; 2 &amp; 3\end{array}\right)\)}}}}<br></li><li>{{c5::{{c4::\(\left(\begin{array}{ccc|c}1 &amp; 0 &amp; -1 &amp; 0 \\ 0 &amp; 1 &amp; 2 &amp; 3\end{array}\right)\)}}}}</li></ul>

============================================================

  

    3. (20 points) Find all solutions \(X\) to the matrix equation \(A X=C\) when<br><br>a)<br><br>\[<br>A=\left(\begin{array}{lll}<br>1 &amp; 2 &amp; 3 \\<br>3 &amp; 2 &amp; 1<br>\end{array}\right)<br>\]<br><br>and<br><br>\[<br>C=\left(\begin{array}{l}<br>6 \\<br>6<br>\end{array}\right)<br>\]<br><br>We use Gaussian elimination.<br><br>\[<br>\begin{aligned}<br>\left(\begin{array}{lll|l}<br>1 &amp; 2 &amp; 3 &amp; 6 \\<br>3 &amp; 2 &amp; 1 &amp; 6<br>\end{array}\right) \rightarrow &amp; \left(\begin{array}{ccc|c}<br>1 &amp; 2 &amp; 3 &amp; 6 \\<br>0 &amp; -4 &amp; -8 &amp; -12<br>\end{array}\right) \rightarrow\left(\begin{array}{lll|l}<br>1 &amp; 2 &amp; 3 &amp; 6 \\<br>0 &amp; 1 &amp; 2 &amp; 3<br>\end{array}\right) \\<br>&amp; \rightarrow\left(\begin{array}{ccc|c}<br>1 &amp; 0 &amp; -1 &amp; 0 \\<br>0 &amp; 1 &amp; 2 &amp; 3<br>\end{array}\right)<br>\end{aligned}<br>\]<br><br>So the pivot variables are {{c1::\(x_{1}, x_{2}\)}} and the free variables are {{c2::\(x_{3}\)}}. One solution is {{c5::{{c3::\((0,3,0)\)}}}}, and a general solution will be {{c5::{{c4::\(\left(x_{3}, 3-2 x_{3}, x_{3}\right)\) for any \(x_{3} \in \mathbb{R}\).}}}}

============================================================

  

    3. (20 points) Find all solutions \(X\) to the matrix equation \(A X=C\) when<br><br>b)<br><br>\[<br>A=\left(\begin{array}{ll}<br>1 &amp; 3 \\<br>2 &amp; 2 \\<br>3 &amp; 1<br>\end{array}\right)<br>\]<br><br>and<br><br>\[<br>C=\left(\begin{array}{l}<br>2 \\<br>1 \\<br>2<br>\end{array}\right)<br>\]<br><br>We use Gaussian elimination:<br><ul><li>{{c1::\(\left(\begin{array}{ll|l}1 &amp; 3 &amp; 2 \\ 2 &amp; 2 &amp; 1 \\ 3 &amp; 1 &amp; 2\end{array}\right)\)}}<br></li><li>{{c2::\(\left(\begin{array}{cc|c}1 &amp; 3 &amp; 2 \\ 0 &amp; -4 &amp; -3 \\ 0 &amp; -8 &amp; -4\end{array}\right)\)}}<br></li><li>{{c5::{{c3::\(\left(\begin{array}{cc|c}1 &amp; 3 &amp; 2 \\ 0 &amp; 1 &amp; \frac{3}{4} \\ 0 &amp; 1 &amp; \frac{1}{2}\end{array}\right)\)}}}}<br></li><li>{{c5::{{c4::\(\left(\begin{array}{cc|c}1 &amp; 0 &amp; -\frac{1}{4} \\ 0 &amp; 1 &amp; \frac{3}{4} \\ 0 &amp; 0 &amp; \frac{1}{4}\end{array}\right)\)}}}}</li></ul>

============================================================

  

    4.(30 points) This problem is about the five-dimensional space of polynomials of degree less than or equal to four. The problem is to find all such polynomials \(p(x)\) satisfying the conditions<br><br>\[<br>p(1)=1, \quad p(2)=4, \quad p(3)=9, \quad p(4)=16 .<br>\]<br><br>b) The conditions \((*)\) on \(p\) can be written as a system of four simultaneous linear equations in five unknowns. Write the augmented matrix of this system of equations (explaining what the unknowns are).<br><br><ul><li>r1 = {{c1::1&nbsp; 1&nbsp; 1&nbsp; 1&nbsp; 1&nbsp; 1&nbsp;}}</li><li>r2 = {{c2::1&nbsp; 2&nbsp; 4&nbsp; 8&nbsp; 16&nbsp; 4&nbsp;}}</li><li>r3 = {{c5::{{c3::1&nbsp; 3&nbsp; 9&nbsp; 27 81&nbsp; 9}}}}&nbsp;</li><li>r4 = {{c5::{{c4::1&nbsp; 4&nbsp; 16&nbsp; 64&nbsp; 256&nbsp; 16}}}}<br></li></ul>

============================================================

  

    4.(30 points) This problem is about the five-dimensional space of polynomials of degree less than or equal to four. The problem is to find all such polynomials \(p(x)\) satisfying the conditions<br><br>\[<br>p(1)=1, \quad p(2)=4, \quad p(3)=9, \quad p(4)=16 .<br>\]<br>b) The conditions \((*)\) on \(p\) can be written as a system of four simultaneous linear equations in five unknowns. Write the augmented matrix of this system of equations (explaining what the unknowns are).<br><br>Any polynomial with degree at most 4 can be written as {{c1::\(p(x)=a_{0}+a_{1} x+a_{2} x^{2}+\) \(a_{3} x^{3}+a_{4} x^{4}\)}}, so we can consider {{c2::the coefficients \(a_{0}, \ldots, a_{4}\)}} {{c5::{{c3::as unknown variables}}}} and put in<br><br>\[<br>p(1)=1, \quad p(2)=4, \quad p(3)=9, \quad p(4)=16 .<br>\]<br><br>The augmented matrix is<br><br>{{c5::{{c4::\[<br>\left(\begin{array}{ccccc|c}<br>1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\<br>1 &amp; 2 &amp; 4 &amp; 8 &amp; 16 &amp; 4 \\<br>1 &amp; 3 &amp; 9 &amp; 27 &amp; 81 &amp; 9 \\<br>1 &amp; 4 &amp; 16 &amp; 64 &amp; 256 &amp; 16<br>\end{array}\right)<br>\]}}}}<br>

============================================================

  

    4.(30 points) This problem is about the five-dimensional space of polynomials of degree less than or equal to four. The problem is to find all such polynomials \(p(x)\) satisfying the conditions<br><br>\[<br>p(1)=1, \quad p(2)=4, \quad p(3)=9, \quad p(4)=16 .<br>\]<br><br>For the following augumented polynomial-power matrix<br><br>\(\left(\begin{array}{ccccc|c}1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 2 &amp; 4 &amp; 8 &amp; 16 &amp; 4 \\ 1 &amp; 3 &amp; 9 &amp; 27 &amp; 81 &amp; 9 \\ 1 &amp; 4 &amp; 16 &amp; 64 &amp; 256 &amp; 16\end{array}\right)\)<br><br>And the following reduced row-echalon form matrix<br><br>\(\left(\begin{array}{ccccc|c}1 &amp; 0 &amp; 0 &amp; 0 &amp; -24 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 50 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; -35 &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 10 &amp; 0\end{array}\right)\)<br><br>d) Write all the polynomials of degree less than or equal to four satisfying the conditions \((*)\) (on the first page).<br><br>So one solution as we have seen is {{c1::\((0,0,1,0,0)\)}}, equivalent to {{c2::p(x) = x^2}}. And {{c3::\(a_{4}\)}} is the free variable which makes the general solution to be<br><ul><li>{{c4::\(\left(24 a_4,-50 a_4, 1+35 a_4,-10 a_4, a_4\right)\).}}</li></ul><br><br><br><br>Thus the general polynomial will be<br><br>{{c5::\[<br>24 a_{4}-50 a_{4} x+\left(1+35 a_{4}\right) x^{2}-10 a_{4} x^{3}+a_{4} x^{4},<br>\]}}<br>

============================================================

  

    5.(20 points) Let \(T\) be an injective linear map on \(\mathcal{P}(z)\) (the polynomials with complex coefficients of any degree) to itself. Assume \(\operatorname{deg} T(p) \leq \operatorname{deg}(p)\) for any polynomial \(p \in \mathcal{P}(z)\).<br><br>a) Prove \(T\) is surjective.<br><br><ul><li>For {{c1::any finite-dim&nbsp;\(P_m(z)\)}}</li><li>{{c2::injectivity}} implies {{c5::{{c3::surjectivity}}}}</li><li>This holds {{c5::{{c4::for all m}}}} so it holds for&nbsp;\(P(z)\)</li></ul>

============================================================

  

    5.(20 points) Let \(T\) be an injective linear map on \(\mathcal{P}(z)\) (the polynomials with complex&nbsp;coefficients of any degree) to itself. Assume \(\operatorname{deg} T(p) \leq \operatorname{deg}(p)\) for any polynomial \(p \in \mathcal{P}(z)\).<br><br>b) Show \(\operatorname{deg} T(p)=\operatorname{deg}(p)\) for any \(p\).<br><br>Proof. <br><ul><li>If \(\operatorname{deg} T(p)&lt;\operatorname{deg}(p)=m\) for some \(m\), by the above, since \(T\) induces a surjective map from \(\mathcal{P}_{m-1}\) to itself, we know that {{c1::there is a polynomial \(q \in \mathcal{P}_{m-1}\)}}, such that {{c2::\(T(q)=\) \(T(p)\)}}. This contradicts {{c5::{{c3::the injectivity assumption.}}}}</li><li>Meaning that {{c5::{{c4::the degrees must be equal}}}}</li></ul>

============================================================

  

    A linear map being {{c5::{{c1::surjective}}}} corresponds to its matrices {{c4::{{c2::reduced row echelon}}}} form having {{c4::{{c3::a pivot on each row}}}}

============================================================

  

    4. (3 points) (Based on Axler, 2nd edition page 60, exercise 16 or 3rd edition page 69 , exercise 22). Suppose \(U\) is a finite-dimensional vector spaces, that \(S \in \mathcal{L}(V, W)\), and that \(T \in \mathcal{L}(U, V)\). Prove that<br><br>\[<br>\operatorname{dim} \operatorname{Null}(S T)=\operatorname{dim} \operatorname{Null}(T)+\operatorname{dim}(\operatorname{Range}(T) \cap \operatorname{Null}(S)) .<br>\]<br><br><br><br>Proof. Define<br><br><ul><li>\(U_{0}\)&nbsp;</li><ul><li>={{c1::\({}_{\operatorname{def} }\operatorname{Null}(ST)\)&nbsp;}}</li><li>= {{c2::\( \{u \in U \mid T(u) \in\operatorname{Null}(S)\}\)&nbsp;</li></ul><li>\(V_{0}\) = \(\operatorname{Null}(S) .\)}}<br></li></ul><br>The second equality is immediate from the definitions. Now it's clear that<br><br><ul><li>\(T\left(U_{0}\right)\) {{c3::\( \subset \operatorname{Null}(S),\)}}</li></ul><br>so we can think of \(T_{0} \in\) {{c4::\( \mathcal{L}\left(U_{0}, V_{0}\right)\)}} as {{c5::the restriction of \(T\) to the subspace \(U_{0}\)}}. Then<br><br><ul><li>\(\operatorname{Range}\left(T_{0}\right)\)&nbsp;</li><ul><li>= \(\operatorname{Range}(T) \cap \operatorname{Null}(S)\)</li></ul><li>&nbsp;\(\operatorname{Null}\left(T_{0}\right)\)</li><ul><li>= \(\operatorname{Null}(T) .\)</li></ul></ul><br>(Both of these statements require a little thought, but not much.) Now the rank plus nullity theorem for \(T_{0}\) gives the formula we want.<br>

============================================================

  

    4. (3 points) (Based on Axler, 2nd edition page 60, exercise 16 or 3rd edition page 69 , exercise 22). Suppose \(U\) is a finite-dimensional vector spaces, that \(S \in \mathcal{L}(V, W)\), and that \(T \in \mathcal{L}(U, V)\). Prove that<br><br>\[<br>\operatorname{dim} \operatorname{Null}(S T)=\operatorname{dim} \operatorname{Null}(T)+\operatorname{dim}(\operatorname{Range}(T) \cap \operatorname{Null}(S)) .<br>\]<br><br>Original proof:<br><ul><li>E = ST, E in L(U,W)</li><li>dim null E = {{c1::dim U - dim range E}}</li><li>dim U = {{c2::dim range T + dim null T}}</li><li>Thus dim null E = {{c5::{{c3::dim range T + dim&nbsp; null T - dim range E}}}}</li><li>As such, the question comes down to {{c5::{{c4::dim range E}}}}</li></ul>

============================================================

  

    6. (3 points) Suppose \(T \in \mathcal{L}(V, W)\), and that \(V\) is finite-dimensional.<br><br>a) Prove that \(\operatorname{Null}(T)=\{0\}\) if and only if for every linearly independent list \(\left(v_{1}, v_{2}, \ldots, v_{p}\right)\) in \(V,\left(T v_{1}, \ldots, T v_{p}\right)\) is linearly independent in \(W\).<br><br><ul><li>Two steps:</li><ul><li>If null T = {0} then {{c1::mapping a lin indp list in V via T}} can only result in {{c2::a lin indp list in W since no&nbsp;\(Tv_i\) can be 0&nbsp;}}</li><li>{{c3::\(T(\vec{c} \cdot \vec{v})\)}} = {{c4::0}}&nbsp;</li><li>{{c5::\(\vec{c} \cdot T(\vec{v})\)}} = {{c4::0}} implies&nbsp;{{c5::\(\vec{c} = \vec{0}\)}}<br></li></ul></ul>

============================================================

  

    6. (3 points) Suppose \(T \in \mathcal{L}(V, W)\), and that \(V\) is finite-dimensional.<br><br>a) Prove that \(\operatorname{Null}(T)=\{0\}\) if and only if for every linearly independent list \(\left(v_{1}, v_{2}, \ldots, v_{p}\right)\) in \(V,\left(T v_{1}, \ldots, T v_{p}\right)\) is linearly independent in \(W\).<br><br><ul><li>Two steps:</li><ul><li>If it maps lin indp lists to lin indp lists</li><li>Assume \(u\) \(\in\) {{c1::null T}}</li><li>{{c2::Extend}}&nbsp;\(u\) {{c2::to a basis of V,&nbsp;\(\vec{u}\)}}</li><li>Since {{c3::this basis is lin indp}}, then {{c3::mapping it via T can only result in another lin indp list}}</li><li>Since {{c4::no list containing 0 is lin indp}}, then {{c5::null T must have been empty except for the 0 vector}}</li></ul></ul>

============================================================

  

    6. (3 points) Suppose \(T \in \mathcal{L}(V, W)\), and that \(V\) is finite-dimensional.<br>Background:<br><ul><li>a) Prove that \(\operatorname{Null}(T)=\{0\}\) if and only if for every linearly independent list \(\left(v_{1}, v_{2}, \ldots, v_{p}\right)\) in \(V,\left(T v_{1}, \ldots, T v_{p}\right)\) is linearly independent in \(W\).</li><li>b) Prove that Range \((T)=W\) if and only if for every spanning list \(\left(v_{1}, v_{2}, \ldots, v_{q}\right)\) in \(V,\left(T v_{1}, \ldots, T v_{q}\right)\) is a spanning list in \(W\).</li></ul><b>c) Prove \(T\) is invertible if and only if \(T\) takes each basis of \(V\) to a basis of \(W\).</b><br><br>Proof: suppose it is invertible then:<br><ul><li>null T = 0</li><li>range T = W</li><li>if&nbsp;\(\vec{v}\) is {{c5::{{c1::a basis of V}}}}, then because of {{c4::{{c2::a and b}}}}&nbsp;\(T(\vec{v})\) is {{c4::{{c3::a lin indp spaning list}}}}</li></ul>

============================================================

  

    6. (3 points) Suppose \(T \in \mathcal{L}(V, W)\), and that \(V\) is finite-dimensional.<br>Background:<br><ul><li>a) Prove that \(\operatorname{Null}(T)=\{0\}\) if and only if for every linearly independent list \(\left(v_{1}, v_{2}, \ldots, v_{p}\right)\) in \(V,\left(T v_{1}, \ldots, T v_{p}\right)\) is linearly independent in \(W\).</li><li>b) Prove that Range \((T)=W\) if and only if for every spanning list \(\left(v_{1}, v_{2}, \ldots, v_{q}\right)\) in \(V,\left(T v_{1}, \ldots, T v_{q}\right)\) is a spanning list in \(W\).</li></ul><b>c) Prove \(T\) is invertible if and only if \(T\) takes each basis of \(V\) to a basis of \(W\).</b><br><br>Proof: suppose it is invertible then:<ul><li>if T takes each basis of V to a basis of W:</li><ul><li>Suppose&nbsp;\(\vec{v}_{1\to p}\) is {{c1::lin indp}}, then we can&nbsp; {{c2::extend it to a basis&nbsp;\(\vec{v}\)}}</li><li>Since {{c3::any sublist of a basis must be linearly independent}} and&nbsp;\(T(\vec{v})\) is {{c4::lin indp}}, then&nbsp;\(T(\vec{v}_{1\to p})\) {{c5::must also be lin indp so null T = 0}}</li></ul><li>The same method can be applied for {{c5::spanning lists}} to prove {{c5::surjectivity}}</li></ul>

============================================================

  

    2. (20 points) List all \(3 \times 2\) matrices over \(\mathbb{R}\) which are in reduced row-echelon form. Point out the rank of each one.<br><br>Solution:<br><br><ul><li>{{c1::\(\left(\begin{array}{ll}1 &amp; 0 \\ 0 &amp; 1 \\ 0 &amp; 0\end{array}\right)\)}}</li><li>{{c2::\(\left(\begin{array}{ll}1 &amp; a \\ 0 &amp; 0 \\ 0 &amp; 0\end{array}\right)\)}}<br></li><li>{{c5::{{c3::\(\left(\begin{array}{ll}0 &amp; 1 \\ 0 &amp; 0 \\ 0 &amp; 0\end{array}\right)\)}}}}<br></li><li>{{c5::{{c4::\(\left(\begin{array}{ll}0 &amp; 0 \\ 0 &amp; 0 \\ 0 &amp; 0\end{array}\right)\)}}}}<br></li></ul>

============================================================

  

    2. (20 points) List all \(3 \times 2\) matrices over \(\mathbb{R}\) which are in reduced row-echelon form. Point out the rank of each one.<br><br>Solution:<br><br><ul><li>\(\left(\begin{array}{ll}1 &amp; 0 \\ 0 &amp; 1 \\ 0 &amp; 0\end{array}\right)\)</li><ul><li>with rank {{c1::2}}</li></ul><li>\(\left(\begin{array}{ll}1 &amp; a \\ 0 &amp; 0 \\ 0 &amp; 0\end{array}\right)\)<br></li><ul><li>with rank {{c2::1}}</li></ul><li>\(\left(\begin{array}{ll}0 &amp; 1 \\ 0 &amp; 0 \\ 0 &amp; 0\end{array}\right)\)<br></li><ul><li>with rank {{c5::{{c3::1}}}}</li></ul><li>\(\left(\begin{array}{ll}0 &amp; 0 \\ 0 &amp; 0 \\ 0 &amp; 0\end{array}\right)\)</li><ul><li>with rank {{c5::{{c4::0}}}}</li></ul></ul>

============================================================

  

    a) Find all solutions \(X\) to the matrix equation \(A X=C\) when<br><br>\[<br>A=\left(\begin{array}{lll}<br>1 &amp; 2 &amp; 3 \\<br>2 &amp; 3 &amp; 4<br>\end{array}\right)<br>\]<br><br>and<br><br>\(C=\left(\begin{array}{l}6 \\ 8\end{array}\right)\)<br><br>Solution metod: {{c1::reduce to reduced row-echalon form}}<br><ul><li>{{c2::\(\left(\begin{array}{lll|l}1 &amp; 2 &amp; 3 &amp; 6 \\ 2 &amp; 3 &amp; 4 &amp; 8\end{array}\right)\)}}<br></li><li>{{c3::\(\left(\begin{array}{ccc|c}1 &amp; 2 &amp; 3 &amp; 6 \\ 0 &amp; -1 &amp; -2 &amp; -4\end{array}\right)\)}}<br></li><li>{{c4::\(\left(\begin{array}{lll|l}1 &amp; 2 &amp; 3 &amp; 6 \\ 0 &amp; 1 &amp; 2 &amp; 4\end{array}\right)\)}}<br></li><li>{{c5::\(\left(\begin{array}{ccc|c}1 &amp; 0 &amp; -1 &amp; -2 \\ 0 &amp; 1 &amp; 2 &amp; 4\end{array}\right)\)}}<br></li></ul>

============================================================

  

    a) Find all solutions \(X\) to the matrix equation \(A X=C\) when<br><br>\[<br>A=\left(\begin{array}{lll}<br>1 &amp; 2 &amp; 3 \\<br>2 &amp; 3 &amp; 4<br>\end{array}\right)<br>\]<br><br>and<br><br>\(C=\left(\begin{array}{l}6 \\ 8\end{array}\right)\)<br><br>Given the following augumented matrix:<br>\(\left(\begin{array}{lll|l}1 &amp; 2 &amp; 3 &amp; 6 \\ 2 &amp; 3 &amp; 4 &amp; 8\end{array}\right)\)<br><br>and reduced row-echalon form<br><br>\(\left(\begin{array}{ccc|c}1 &amp; 0 &amp; -1 &amp; -2 \\ 0 &amp; 1 &amp; 2 &amp; 4\end{array}\right)\)<br><br>What are the special and general solutions of the system of equations?<br><ul><li>Special solution: {{c1::(-2,4,0)}}</li><ul><li>Obtained by {{c2::setting the free variable to 0 and copying the C' values into the pivot rows}}</li></ul><li>General solution: {{c5::{{c3::\(<br>\left(-2+x_{3}, 4-2 x_{3}, x_{3}\right) \)}}}}</li><ul><li>Obtained by {{c5::{{c4::setting the free variable to&nbsp;\(x_3\) and then taking the pivot-row var values to be the C' value - the free variable times the coefficient from the last non-C' column of the agumuneted matrix}}}}</li></ul></ul>

============================================================

  

    b) For which \(a\), for the following \(A\) and \(C\) the equation \(A X=C\) has a solution:<br><br>\[<br>A=\left(\begin{array}{ll}<br>1 &amp; 2 \\<br>2 &amp; 3 \\<br>3 &amp; 4<br>\end{array}\right)<br>\]<br><br>and<br><br>\[<br>C=\left(\begin{array}{l}<br>2 \\<br>1 \\<br>a<br>\end{array}\right)<br>\]<br>&nbsp;Given the following reduced row-echalon form:<br><br>\(\left(\begin{array}{ll|l}1 &amp; 0 &amp; -4 \\ 0 &amp; 1 &amp; 3 \\ 0 &amp; 0 &amp; a\end{array}\right)\)<br><br>Solution:<br><ul><li>This system of linear equations only has a solution if {{c1::a=0}} since {{c2::it is is bellow the last pivot row}}</li><li>The solution is {{c5::{{c3::(-4,3)}}}} which is {{c5::{{c4::the same as the special solution}}}}]</li></ul>

============================================================

  

    4.(30 points) This problem is about the four-dimensional space of polynomials of degree less than or equal to three. The problem is to find all such polynomials \(p(x)\) satisfying the conditions<br><br>\[<br>p(1)=1, \quad p(2)=2, \quad p(3)=3,<br>\]<br><br>a) The conditions \((*)\) on \(p\) can be written as a system of three simultaneous linear equations in four unknowns. Write the augmented matrix of this system of equations (explaining what the unknowns are).<br><br>Solution: The unknowns are {{c1::the coefficients}} {{c2::in front the constant term, \(x, x^{2}, x^{3}\)}}, i.e., {{c5::{{c3::\(a_{0}+a_{1} x+a_{2} x^{2}+a_{3} x^{3}\)}}}} where {{c5::{{c3::\(a_{0}, a_{1}, a_{2}, a_{3}\)}}}} are {{c5::{{c4::the unknowns}}}}.

============================================================

  

    4.(30 points) This problem is about the four-dimensional space of polynomials of degree less than or equal to three. The problem is to find all such polynomials \(p(x)\) satisfying the conditions<br><br>\[<br>p(1)=1, \quad p(2)=2, \quad p(3)=3,<br>\]<br><br>Given the following augumented matrix for the polynomial coefficients:<br>\[<br>\left(\begin{array}{cccc|c}<br>1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\<br>1 &amp; 2 &amp; 4 &amp; 8 &amp; 2 \\<br>1 &amp; 3 &amp; 9 &amp; 27 &amp; 3<br>\end{array}\right) .<br>\]<br><br>And the following reduced row-echalon form version:<br><br>\(\left(\begin{array}{cccc|c}1 &amp; 0 &amp; 0 &amp; 6 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; -11 &amp; 1 \\ 0 &amp; 0 &amp; 1 &amp; 6 &amp; 0\end{array}\right)\).<br><br>c) Write all the polynomials of degree less than or equal to three satisfying the conditions \((*)\) (on the first page). (It's possible to solve this part without solving (a)-(b); whatever you do, explain why it works.)<br><ul><li>The special solution is: {{c1::p(x)=(x)}} i.e the coefficients {{c1::(0,1,0,0)}}</li><li>The general solution is: {{c2::\(<br>-6 a_{4}+\left(1+11 a_{4}\right) x-6 a_{4} x^{2}+a_{4} x^{3} .<br>\)}}</li><ul><li>Which is obtained by {{c5::{{c3::setting the free variable to be&nbsp;\(a_4\)}}}} and then {{c5::{{c4::subtracting the free variable coefficients from the pivot value for each elements of the list}}}}</li></ul></ul>

============================================================

  

    5.(20 points) Let \(T(p)=\left(\left(x^{2}+1\right) p\right)^{\prime \prime}\) be a map on \(\mathcal{P}(z)\) (the polynomials with complex coefficients of any degree) to itself. Prove \(T\) is surjective.<br><ul><li>T sends {{c1::all polynomials to polynomials of degree equal to themselves}}<br></li><li>T sends {{c2::only 0 to 0}} so it is {{c3::injective}}</li><li>Meaning it is {{c4::surjective}} for&nbsp;{{c5::\(P_m\)&nbsp;\(\forall m\)}}, meaning it is {{c4::surjective}} over&nbsp;{{c5::\(P(z)\)}}</li></ul>

============================================================

  

    Suppose \(V\) and \(W\) are vector spaces over a field \(F\), and \(T \in \mathcal{L}(V, W)\). <br><ol><li>A {{c1::left inverse}} for \(T\) is a linear map \(S \in \mathcal{L}(W, V)\) with the property that {{c2::\(S T=I_{V}\) (the identity map on \(V\) )}}. That is, we require</li></ol><br><ul><li>{{c5::{{c3::\(S T(v)\)}}}} ={{c5::{{c4::\( v\) \(\forall v \in V .\)}}}}</li></ul>

============================================================

  

    <ul><li>A {{c1::right inverse}} for \(T\) is a linear map \(S^{\prime} \in \mathcal{L}(W, V)\) with the property that {{c2::\(T S^{\prime}=I_{W}\) (the identity map on \(W\) )}}. That is, we require</li><li>{{c5::{{c3::\(T S^{\prime}(w)\)}}}} = {{c5::{{c4::\(w \quad(\text{ all } w \in W) .\)}}}}<br></li></ul>

============================================================

  

    Suppose \(S\) is a {{c1::left inverse}} of \(T\). Then {{c2::the only possible solution of}} <br>{{c5::{{c3::\[<br>T x=c<br>\]&nbsp;}}}}<br>is {{c5::{{c4::\(x=S c\)}}}}.

============================================================

  

    <br>For a system of linear equations Tx=c, suppose \(S^{\prime}\) is a {{c1::right}} inverse of \(T\). Then we can try {{c2::\(x=S^{\prime} c\)}} and get<br><br><ul><li>{{c3::\(T x\)}}&nbsp;= {{c4::\(T S^{\prime} c\)}} = {{c5::\( I_{W} c\)}} = {{c5::\(c .\)}}</li></ul>

============================================================

  

    Suppose \(S^{\prime}\) is a {{c5::{{c1::right inverse of \(T\)}}}}. Then {{c4::{{c2::\(x=S^{\prime} c\)}}}} is {{c4::{{c3::a solution of \(T x=c\)}}}}

============================================================

  

    We would like to "undo" differentiation, so we integrate:<br><br>\[<br>(J p)(x)=\int_{0}^{x} p(t) d t .<br>\]<br><br><ul><li>The fundamental theorem of calculus says that the derivative of this integral is \(p\); that is, \(D J=I_{\mathcal{P}}\). So \(J\) is a {{c1::right}} {{c3::inverse}} of \(D\); it provides a {{c5::{{c2::not-unique}}}} solution of {{c5::{{c4::the differential equation&nbsp;\(\frac{dq}{dx} = p\)}}}}</li></ul>

============================================================

  

    \[<br>(J p)(x)=\int_{0}^{x} p(t) d t .<br>\]<br><br><ul><li>J is a {{c1::right}} inverse of diferentiation so it provides a non-unique solution to&nbsp;{{c2::\(\frac{d q}{d x}=p\)}}</li><li>However, if we try to use it as a {{c1::left}} inverse there is a problem</li><ul><li>{{c3::\(J D(p)\)}} = {{c4::\(\int_0^x p^{\prime}(t) d t=p(x)-p(0)\).}}</li></ul><li>Which is that {{c5::J sends p to p -p(0) which is not the same as P}}</li><li><br></li></ul>

============================================================

  

    <ul><li>Integration is {{c4::only}} a {{c1::right}} {{c5::inverse}} of diferentiation</li><li>&nbsp;Since {{c2::diferentiation has a nonzero null space}} then {{c3::no left inverse can exist}}</li></ul>

============================================================

  

    <b>Theorem</b> 0.5. Suppose \(V\) and \(W\) are finite-dimensional, and that \(T \in\) \(\mathcal{L}(V, W)\).<br><br><ul><li>2) If \(S\) is a {{c1::left}} inverse of \(T\), then {{c2::\(\operatorname{Null}(S)\)}} is a {{c3::complement to Range(T)}} meaning that:</li><ul><li>{{c4::\(W\)}} = {{c5::\(\operatorname{Range}(T) \oplus \operatorname{Null}(S) .\)}}</li></ul></ul>

============================================================

  

    Suppose \(V\) and \(W\) are finite-dimensional, and that \(T \in\) \(\mathcal{L}(V, W)\).<br><br><ul><li>3) Assuming that {{c1::\(\operatorname{Null}(T)\)}} = {{c1::\(0\)}}, there is a {{c3::one-to-one correspondence}} between {{c2::left}} inverses of \(T\) and {{c4::subspaces of \(W\)}} {{c5::complementary to Range \((T)\)}}.</li></ul>

============================================================

  

    Suppose \(V\) and \(W\) are finite-dimensional, and that \(T \in\) \(\mathcal{L}(V, W)\).<br><br><ul><li>4) The operator \(T\) has a {{c1::right}} inverse if and only if {{c2::\(\operatorname{Range}(T)\)}} {{c5::{{c4::=}}}} {{c5::{{c3::\(W\)}}}}.</li></ul>

============================================================

  

    Theorem 0.5. Suppose \(V\) and \(W\) are finite-dimensional, and that \(T \in\) \(\mathcal{L}(V, W)\).<br><br><ol><li>5.) If \(S^{\prime}\) is a {{c1::right}} inverse of \(T\), then {{c2::\(\operatorname{Range}\left(S^{\prime}\right)\)}} is a {{c3::complement to \(\operatorname{Null}(T)\)}}, which means:</li><li>{{c4::\(V\)}} = {{c5::\(\operatorname{Null}(T) \oplus \operatorname{Range}\left(S^{\prime}\right) .\)}}</li></ol>

============================================================

  

    Suppose \(V\) and \(W\) are finite-dimensional, and that \(T \in\) \(\mathcal{L}(V, W)\).<br><br><ul><li>6) Assuming that {{c1::\(\operatorname{Range}(T)\)}} = {{c1::\(W\)}}, there is a {{c2::one-to-correspondence}} between {{c3::right}} inverses of \(T\) and {{c4::subspaces of \(V\)}} {{c5::complementary}}&nbsp;to \(\operatorname{Null}(T)\).</li></ul>

============================================================

  

    &nbsp;If \(T\) has both a {{c1::left}} and a {{c2::right}} inverse, then the {{c1::left}} and {{c2::right}} inverses are {{c5::{{c3::unique}}}} and {{c5::{{c4::equal to each other}}}}.&nbsp;

============================================================

  

    The four guaranteed invariant subspaces of any operator:<br><ul><li>{{c1::{0} }}</li><li>{{c2::V}}</li><li>{{c5::{{c3::null T}}}}</li><li>{{c5::{{c4::range T}}}}</li></ul>

============================================================

  

    Why are the following conditions equivalent?<br><img src="paste-5edbaacd5d55bb70f1df083447c7465548be25b7.jpg"><br>Reason:<br><ul><li>{{c1::(\(T&nbsp; - \lambda I\))(v)&nbsp;}}</li><ul><li>=&nbsp;{{c2::\(Tv - \lambda I v\)}}</li><li>=&nbsp;{{c3::\(Tv - \lambda v\)}}</li></ul><li>Since we require{{c4::&nbsp;\(v\) not to be 0}} then {{c5::the linear map (\(T&nbsp; - \lambda I\)) must not be injective, .i.e \(Tv - \lambda v\) = 0 implies v not equal to 0}}</li><li>All other conditions follow from {{c5::injectivity-surjectivity-invertibility equivalence on operators}}</li></ul>

============================================================

  

    An operator has an {{c5::{{c1::eigenvalue}}}} if and only if there exists {{c4::{{c2::a nonzero vector in its domain}}}} that {{c4::{{c3::gets sent by the operator to a scalar multiple of itself.}}}}

============================================================

  

    Example: Suppose \(T \in \mathcal{L}\left(\mathbf{C}^2\right)\) is defined by<br>\[<br>T(w, z)=(-z, w) .<br>\]<br>Then<br><ul><li>\(T\) {{c1::\((1,-i)\)&nbsp;}} &nbsp;= {{c2::\((i, 1) \)}}</li><li>&nbsp;= {{c5::{{c3::\(i(1,-i) .\)}}}}</li></ul>Thus {{c5::{{c4::\(i\) is an eigenvalue of \(T\)}}}}.<br>

============================================================

  

    Any {{c5::{{c1::nonzero scalar multiple}}}} {{c4::{{c2::of an eigenvector}}}} is {{c4::{{c3::an eigenvector}}}}

============================================================

  

    Number of eigenvalues<br>Suppose \(V\) is finite-dimensional. Then each operator on \(V\) has at most \(\operatorname{dim} V\) distinct eigenvalues.<br><br>Proof: Let \(T \in \mathcal{L}(V)\). Suppose {{c1::\(\lambda_1, \ldots, \lambda_m\)}} are {{c1::distinct eigenvalues of \(T\)}}. Let {{c2::\(v_1, \ldots, v_m\)}} be {{c2::corresponding eigenvectors}}. Then {{c3::the list \(v_1, \ldots, v_m\)}} is {{c4::linearly independent}}. Thus {{c5::\(m \leq \operatorname{dim} V\)}}, as desired.

============================================================

  

    If \(T \in \mathcal{L}(V)\), then<br><ul><li>{{c1::\(T^m T^n\)}}&nbsp;={{c2::\(T^{m+n}\)&nbsp;}}</li><li>{{c5::{{c3::\(\left(T^m\right)^n\)}}}} = {{c5::{{c4::\(T^{m n}\)}}}}</li></ul>

============================================================

  

    In the particular case of {{c1::polynomials}} {{c2::over the same operator}}, {{c5::{{c4::multiplication}}}} is {{c5::{{c3::commutative}}}}

============================================================

  

    Why is it impossible for the following operator to have an eigenvalue?<br>This result is false on infinite-dimensional complex vector spaces.<br>Example: Define \(T \in \mathcal{L}(\mathcal{P}(\mathbf{C}))\) by<br>\[<br>(T p)(z)=z p(z)<br>\]<br>Because:<br><ul><li>{{c5::{{c1::The degree of Tp is guaranteed to be one larger than the degree of p}}}}</li><ul><li>Since {{c4::{{c2::the vector space is infinite-dimensional}}}}</li></ul><li>Thus it is impossible for (Tp)(z) to {{c4::{{c3::be equivalent to scalar multiplication}}}}</li></ul>

============================================================

  

    What are the fundamental steps in prooving the following result?<br><img src="paste-a1e9f36ddbdef1933acba0233a76dcaa49043149.jpg"><br>Steps:<br><ul><li>Choose a {{c1::non-null}} vector</li><li>Build a basis by {{c2::applying powers of an operator T,}} then {{c3::add one more power to make it linearly dependent}}</li><li>Write {{c4::0 as the sum of the list}}, making it {{c5::a polynomial of T}}</li><li>The {{c5::polynomial}} can {{c5::be factored into degree-1 polynomials}} with {{c5::T not being injective for at least one root&nbsp;\(\lambda_j\)}}</li></ul>

============================================================

  

    How do the usual proofs of the existance of an eigenvalue go?<br><ul><li>Build&nbsp;{{c1::\(det (\lambda I -&nbsp; T)\)}} which is called the {{c2::characteristic polynomial}} of T</li><li>This {{c2::polynomial}} is {{c5::{{c3::only equal to 0}}}} {{c5::{{c4::if&nbsp;\(\lambda\) is an eigenvalue}}}}</li></ul>

============================================================

  

    The {{c1::first basis vector}} of an {{c2::upper triangular}} matrix must be {{c5::{{c3::an eigenvector of T}}}} with {{c5::{{c4::eigenvalue&nbsp;\(\lambda_1\)}}}}

============================================================

  

    Any vector in the eigenspace must be an eigenvector because:<br><ul><li>{{c5::{{c1::\(v \in \operatorname{null}(T-\lambda I) \)}}}} {{c4::{{c2::\(\Longleftrightarrow\)}}}}{{c4::{{c3::\( T v=\lambda v\).}}}}<br></li></ul>

============================================================

  

    For {{c1::\(T \in \mathcal{L}(V)\)}} and {{c2::\(\lambda \in \mathbf{F}\)}}, the {{c5::{{c3::eigenspace \(E(\lambda, T)\)}}}} is &nbsp;a {{c5::{{c4::subspace of \(V\).}}}}

============================================================

  

    {{c1::\(\lambda\)}} is an {{c2::eigenvalue of \(T\)}} if and only if <br><ul><li>{{c3::\(E(\lambda, T)\)}}{{c4::\( \neq\)}} {{c5::\(\{0\}\).}}</li></ul>

============================================================

  

    If {{c4::\(\lambda\) is an eigenvalue}} of an operator \(T \in \mathcal{L}(V)\), then {{c1::\(T\) restricted to \(E(\lambda, T)\)}} is {{c5::{{c2::the operator of multiplication}}}} {{c5::{{c3::by \(\lambda\).}}}}

============================================================

  

    Why is the sum of eigenspaces a direct sum?<br><img src="paste-2e1ca0e446654d2526dd049d7d605113ccb38904.jpg"><br><ul><li>{{c1::A vector&nbsp;\(u_j\)}} {{c2::from such an eigenspace}} {{c3::must be an eigenvector}} or {{c3::0}}</li><li>Because {{c4::eigenvectors are lin indp}}, {{c5::all&nbsp;\(u_j \) must be 0}}</li></ul>

============================================================

  

    Even for operators T on complex vector spaces, there may {{c5::{{c1::not be enough eigenvectors}}}} to {{c4::{{c2::form a basis of V}}}} and thus {{c4::{{c3::make T diagonalizable}}}}

============================================================

  

    Define \(T \in \mathcal{L}\left(\mathbf{C}^2\right)\) defined by<br>\[<br>T(w, z)=(z, 0) .<br>\]<br>Then {{c1::0}} is the only eigenvalue of \(T\) and furthermore<br><ul><li>{{c2::\(E(0, T)\)}} = {{c3::\(\left\{(w, 0) \in \mathbf{C}^2: w \in \mathbf{C}\right\} .\)}}</li></ul>Thus \(T\) is {{c5::not}} {{c4::diagonalizable}}.<br>

============================================================

  

    Why do enough eigenvalues implie diagonalizability?<br><ul><li>Because {{c1::eigenvectors corresponding to different eigenvalues}} {{c2::are lin indp}}</li><li>We can {{c5::{{c3::choose them as a basis}}}} {{c5::{{c4::which is equivalent to diagonalizability}}}}</li></ul>

============================================================

  

    What does it imply for a vector space to have a direct-sum decomposition?<br><ul><li>That {{c5::{{c1::each vector}}}} can {{c4::{{c2::be written uniquely}}}} {{c4::{{c3::as a sum of vectors from the constituent spaces}}}}</li></ul>

============================================================

  

    Looking for {{c5::{{c1::invariant subspaces}}}} is useful when looking for {{c4::{{c2::direct-sum decompositions of a vector space}}}} which {{c4::{{c3::are compatible with the linear transformation.}}}}

============================================================

  

    The {{c3::simplest}} {{c4::non-null}} linear transformation, from a {{c5::{{c2::matrix complexity}}}} perspective, is the {{c5::{{c1::identity map}}}}

============================================================

  

    Applying a {{c5::{{c1::diagonal}}}} matrix to a vector is equivalent to {{c4::{{c2::multiplying each element by a scalar}}}} {{c4::{{c3::from the diagonal}}}}

============================================================

  

    Applying a {{c5::{{c1::diagonal}}}} matrix to another compatible matrix is equivalent to {{c4::{{c2::multiplying each row by a scalar}}}} {{c4::{{c3::coming from the diagonal}}}}

============================================================

  

    Ideally, we would like {{c3::direct sum decompositions}} to be {{c4::compatible with operators}} by {{c5::{{c1::having the opeartor be}}}} {{c5::{{c2::invariant over the composing subspaces}}}}

============================================================

  

    Generally, a vector space V having a direct sum decomposition into subspaces of {{c1::any}} dimension which are {{c2::invariant under T}} implies that the matrix of an operator T will be {{c5::{{c3::block diagonal}}}} with {{c5::{{c4::blocks depending on the dimension of the subspaces}}}}

============================================================

  

    {{c1::Direct-sum decompositions}} of {{c2::T-invariant subspaces}} are a good way of {{c5::{{c4::characterising a vector space}}}} without reference to&nbsp;{{c5::{{c3::matrices or matrix operations}}}}

============================================================

  

    {{c5::{{c1::More than one}}}} {{c4::{{c2::eigenvector}}}} can {{c4::{{c3::share the same eigenvalue}}}}

============================================================

  

    For the reflection operation:<br><img src="paste-aa18894f26e660bccc86c908451e7ff8fa98e9ab.jpg"><br>What are the obvious eigenvectors and eigenvalues?<br><ul><li>First: eigenvalue {{c1::1}} with eigenvector {{c2::v2}}</li><li>Second: eigenvalue {{c5::{{c3::-1}}}} with eigenvector {{c5::{{c4::v1}}}}</li></ul>

============================================================

  

    For a linear map T, the eigenvectors with eigenvalue {{c5::{{c1::0}}}} are {{c4::{{c2::all the vectors}}}} {{c4::{{c3::in the null space}}}}

============================================================

  

    {{c5::{{c1::null(T)}}}} is the {{c4::{{c2::eigenspace}}}}&nbsp;{{c4::{{c3::\(E(0,T)\)}}}}

============================================================

  

    <ul><li>T(u) =&nbsp;{{c2::\(\lambda u\)}} =&nbsp;{{c1::\(\lambda I\) u}}</li><li>Which implies&nbsp;{{c1::\( (T - \lambda I)(u)\)}} =&nbsp;{{c5::{{c3::\(0\)}}}}</li><li>Which implies {{c1::\( (T - \lambda I)\)}} is {{c5::{{c4::not injective}}}} since {{c5::{{c4::u cannot be 0&nbsp;}}}}</li></ul>

============================================================

  

    For&nbsp;\(\lambda\) to be an eigenvalue it is necesary for&nbsp;\(T - \lambda I\) to not be invertible and thus {{c1::not}} {{c2::an isomorphism of}} {{c3::V onto itself}}.<br><ul><li>This implies that {{c4::removing the eigenvectors associated with&nbsp;\(\lambda\)}} {{c5::reduces the dimensionality of the range of T}} and thus {{c5::removes information from V}}</li></ul>

============================================================

  

    The existance for so many different means of checking weather&nbsp;\(\lambda\) is an eigenvalue is useful for linear maps {{c1::between very large vector spaces}} with {{c2::very large matrices}} where {{c5::{{c3::the eigenvectors}}}} would be {{c5::{{c4::unfeasible to compute}}}}

============================================================

  

    It is usually {{c1::easire}} to work with the eigenvalue equation in the {{c2::opeartor}} form&nbsp;{{c5::{{c3::\(T - \lambda I\)}}}} because it {{c5::{{c4::concerns only one opeartor}}}}

============================================================

  

    The {{c1::eigenspace}} definition can also be written as:<br><ul><li>{{c2::\(E(\lambda,T)\)&nbsp;}}</li><ul><li>=&nbsp;{{c3::\(\{&nbsp; u \in V \vert \, T(u) = \lambda v&nbsp; \}\)&nbsp;}}</li><li>= {{c4::\(\{&nbsp; u \in V \vert \, (T-\lambda I)u = 0&nbsp; &nbsp;\}\)&nbsp;}}</li><li>= {{c5::null(\(T - \lambda I\))}}</li></ul></ul>

============================================================

  

    Proof that eigenvectors&nbsp;\(\vec{v}\) corresponding to distinct eigenvalues&nbsp;\(\vec{\lambda}\) are linearly independet:<br><br>Proof by contraditction<br><ul><li>Assume they are linearly dep</li><li>Take the smallest k such that&nbsp;\(v_k\)&nbsp; {{c1::\(\in\) \( span(v_1,...,v_{k-1})\)}}</li><li>\(v_k\) =&nbsp;&nbsp;{{c2::\(\vec{c} \cdot \vec{v}_{1 \to k}\)}}<br></li><li>\(Tv_k\)&nbsp;</li><ul><li>=&nbsp;{{c3::\(\sum_{i=1}^{k-1} c_i Tv_i\)}}</li><li>= {{c4::\( \sum_{i=1}^{k-1} c_i \lambda_i v_i\)}}</li></ul><li>Then {{c5::multiply v_k by its eigenvalue&nbsp;\(\lambda_k\)}}, which is useful because&nbsp;\(Tv_k\) =&nbsp;\(\lambda v_k\), meaning we can subtract it from the second equation to get 0</li><li>\(\vec{0}\) =&nbsp;\(\sum_{i=1}^{k-1} c_i (\lambda_i - \lambda_k) v_i\)<br></li><li>However, because we chose the smallest k the list&nbsp;\(\vec{v}_{1 \to k-1}\) must be lin indp</li><li>Since the eigenvalues are assumed distinct, only the coefficients can be 0&nbsp; which contradicts the assumption that&nbsp;\(\vec{v}_{1 \to k}\) would be lin dep</li></ul>

============================================================

  

    Composition of the {{c5::{{c1::same operator}}}} is {{c4::{{c2::obviously comutative}}}}, which is part of the reason why {{c4::{{c3::multiplying polynomials}}}} of {{c5::{{c1::the same operator}}}} is {{c4::{{c2::comutative}}}}

============================================================

  

    The fundamental reasons why we can take polynomials of operators are:<br><ol><li>{{c1::Multiplication/composition of the same opeartor is comutative}}</li><li>Meaning we can {{c2::take powers without regard to order}}</li><li>We can {{c5::{{c3::add}}}} and {{c5::{{c4::scalar multiply}}}} opeartors&nbsp;</li></ol>

============================================================

  

    Given any {{c5::{{c3:: polynomial::type}}::type}} function it can be {{c4::{{c1::applied to an operator}}}} to get {{c4::{{c2::a new operator}}}}

============================================================

  

    Every {{c1::operator}} on a {{c2::finite dim}} {{c3::nonzero}} {{c4::complex}} vector space has {{c5::an eigenvalue}}

============================================================

  

    In fact, an upper-triangular matrix &nbsp; is in {{c1::the-worst}} case a&nbsp;

decomposition into&nbsp; {{c2::invariant subspaces}} {{c5::{{c3::of increasing dimension}}}}, specifically with&nbsp;&nbsp;{{c5::{{c4::\(dim = 1,2,3...,dimV\)}}}}

============================================================

  

    The {{c3::spans of column vectors}} in {{c4::upper triangular}} matrices can be said to be {{c5::{{c1::nested}}}}, with a total of {{c5::{{c2::dim V}}}} such {{c1::nested}} subspaces

============================================================

  

    Proof that every operator T over a complex vector space&nbsp; V has an upper triangular matrix:<br><br>Proof method: {{c1::induction}}<br><ul><li>For {{c2::dim V = 1}}, it is {{c3::trivial}}</li><li>Assume it holds for all {{c4::dim V &lt; k}} that a{{c5::n upper triangular matrix can be built}}</li><li>Consider the range of the opeartor&nbsp;\(T - \lambda I\)</li><ul><li>We knnow that T has an eigenvalue&nbsp;\(\lambda\)</li><li>Since it is not injective, the range must be a proper subspace of V with dim range &lt; dim V</li></ul><li>However, since the range of any operator is invariant, as such so is the range of&nbsp;\(T - \lambda I\)</li><li>Thus the map&nbsp;(\(T - \lambda I\))  restricted to its range has a basis making it upper triangular (since the range is invariant under itself)</li><li>Now we want to extend this&nbsp;\(\vec{u}\) basis to the rest of the space:&nbsp;\(\vec{u} @\vec{v}\)</li><li>Then for each&nbsp;\(v_k\) in&nbsp;\(\vec{v}\) we have \(T(v_k)\) =&nbsp;</li><ul><li>\(Tv_k - \lambda v_k + \lambda v_k\)</li><li>\( (T - \lambda I )v_k + \lambda v_k\)<br></li></ul><li>Since \( (T - \lambda I )v_k\) is in the span of&nbsp;\(\vec{u}\),&nbsp; the equation above shows that&nbsp;\(Tv_k\) is in the span of&nbsp;\(\vec{u}@\vec{v}\)</li><li>From this we conclude the operator T has an upper triangular matrix with respect to&nbsp;\(\vec{u}@\vec{v}\)</li></ul>

============================================================

  

    Definition: {{c1::trace}}<br><br>Let&nbsp;\(T \in L(V)\) be an operator and&nbsp;\(B\) be a basis making the matrix \(M_B(T)\) {{c3::upper triangular}}.<br><ul><li>The {{c2::trace}}, written {{c4::\(tr(T)\)}}, is {{c5::the sum of the diagonal entries of the upper-triangular matrix}}</li></ul>

============================================================

  

    <b>Definition</b>: {{c1::trace}}, {{c2::determinant}}<br><br>Let&nbsp;\(T \in L(V)\) be an operator and&nbsp;\(B\) a basis w.r.t which the matrix&nbsp;\(M_B(T)\) is upper-triangular:<br><ul><li>The {{c1::trace}},written {{c1::\(tr(T)\)}} is the {{c3::sum of the diagonal entries of}} {{c4::the matrix&nbsp;\(M_B(T)\)}}</li><li>The {{c2::determinant}}, written&nbsp;{{c2::\(det(T)\)}} is the {{c5::product ot the diagonal entries}} {{c4::of the matrix&nbsp;\(M_B(T)\)}}</li></ul>

============================================================

  

    The {{c5::{{c3::determinant}}}} si merely {{c4::{{c1::the product of the entries of}}}} {{c4::{{c2::an upper-triangular matrix}}}}

============================================================

  

    A {{c1::compact}} and {{c1::efficient}} way to compute the {{c2::determinant}} is to apply {{c3::Gaussian Elimination}} to get an {{c4::upper-triangular}} matrix&nbsp; and then {{c5::multiply the elements on the diagonal}}

============================================================

  

    Both the trace and the determinant are {{c5::{{c1::polynomials of}}}} {{c4::{{c2::the diagonal elements of}}}} {{c4::{{c3::an upper-triangular matrix}}}}

============================================================

  

    If one were to {{c1::swap the diagonal elements}} of an upper-triangular matrix, the {{c3::trace}} and {{c5::{{c4::determinant}}}} would {{c5::{{c2::not change}}}}

============================================================

  

    The trace and determinant are {{c1::symmetric}} polynomials since they are {{c2::invariant to}} {{c5::{{c3::permutations of}}}} {{c5::{{c4::the order of the elements of the diagonal}}}}

============================================================

  

    Suppose an operator T has an upper-triangular matrix w.r.t some basis&nbsp;\(B\).<br><br><ul><li>Then T is {{c1::invertible}} iff {{c2::all diagonal of&nbsp;\(M_B(T)\) entries are nonzero}}</li><li>Furthermore, since {{c3::a list of field elements is nonzero}} iff {{c3::their product is nonzero}} this implies that T is {{c1::invertible}} iiff {{c4::its determinant}}&nbsp;{{c5::\(det(T)\)}}&nbsp; {{c2::\( \neq\)}}&nbsp; {{c5::\(0\)&nbsp;}}</li></ul>

============================================================

  

    Proof: that an opeartor T is invertible iff the diagonal entries of its upper-triangular matrix are nonzero<br><br>Proof that T is not invertible if the diagonal entries are 0<br><ul><li>Assume there exists a basis&nbsp;\(B\) w.r.t&nbsp;\(M_B(T)\) is upper-triangular</li><li>Suppose there exists {{c1::an element of the diagonal&nbsp;\(\lambda_i\) which is zero.}}</li><ul><li>If&nbsp;{{c2::\(\lambda_1\)}} is {{c3::0}} then&nbsp;{{c4::\(T(v_1) = 0\)}} which implies&nbsp;{{c5::\(T\) is not injective}} since&nbsp;\(v_1\) is a nonzero basis vector</li><li>Iff&nbsp;\(\lambda_k = 0\) for&nbsp;\(k&gt;1\), this means that&nbsp;\(T(v_1,\ldots,v_k)\)&nbsp;\(\in\)&nbsp;\(span(v_1,...,v_{k-1})\) which is a lower dimensional space</li><li>So&nbsp;\(T_{\vert span(v_1,\ldots,v_k)}\) is not injective meaning there is a&nbsp;\(=u \in span(v_1,\ldots,v_k) \in V\) such that&nbsp;\(T(u) = 0\) meaning T is not injective and thus not invertible</li></ul></ul><br>

============================================================

  

    Proof that if T is invertible then the elements of its diagonal matrix must be nonzero:<br><br>Proof method: {{c1::contradiction}}<br><ul><li>Suppose T is {{c1::not invertible}}, this would imply it is {{c2::not injective}} and thus that there exists {{c3::a vector in the null}}</li><li>This vector can be written as a linear combination&nbsp;{{c4::\(v&nbsp; = \sum_{i=1}^k c_i v_i\)}} l{{c5::eaving off at the last nonzero basis vector}}</li><li>Then&nbsp;\(0\) =&nbsp;\(T(v)\) =&nbsp;\(T(\sum_{i=1}^k c_i v_i)\)</li><li>Thus:&nbsp;\(Tv_k\) = -\(\sum_{i=1}^{k-1} c_i v_i\)</li><li>And thus \(Tv_k \in span(v_1,\ldots,v_{k-1})\)</li><li>Thus the diagonal element corresponding to&nbsp;\(v_k\) must be 0</li></ul>

============================================================

  

    Suppose that&nbsp;\(B\) is a basis for which the operator&nbsp;\(T \in L(V)\) is {{c1::upper triangular}}.<br><ul><li>Then the {{c2::eigenvalues of T}} {{c5::{{c3::are exactly}}}} {{c5::{{c4::the entries on the diagonal of the matrix&nbsp;\(M_B(T)\)}}}}</li></ul>

============================================================

  

    Since the elements on the diagonal of an upper-triangular matrix are {{c1::eigenvalues}}, the {{c2::trace}} is the {{c3::sum}} of {{c1::eigenvalues}} while the {{c4::determinant}} is the {{c5::product}} of {{c1::eigenvalues}}

============================================================

  

    Eigenvalues can {{c1::repeat}}, meaning that the {{c4::determinant}} can have {{c2::powers}} and the {{c5::trace}} can have {{c3::repeated entries}}

============================================================

  

    Proof that the elements of the diagonal of an upper-triangular matrix of T, w.r.t base B, are eigenvalues:<br><ul><li>Consider&nbsp;\(\lambda \in F\)</li><li>To test weather it is an eigenvalue we can test whether {{c1::\(T&nbsp; - \lambda I\) is not injective}}</li><li>Put in {{c2::matrix}} form,&nbsp;{{c3::\ test \(M_B(T - \lambda I)\) is&nbsp; not invertible}}</li><li>Since \(M_B(T)\) is upper-triangular, {{c3::\(M_B(T - \lambda I)\)}} has {{c4::the same shape}} with&nbsp;{{c4::\(\lambda\) subtracted from the diagonal}}<br></li><li>Since {{c3::\(M_B(T - \lambda I)\)}} is {{c5::not invertible}} iff {{c4::one of the elements is 0}}, then {{c5::the elements of the diagonal of}}&nbsp; {{c3::\(M_B(T - \lambda I)\)}} must be {{c5::eigenvalues since&nbsp;\(\lambda_k - \lambda = 0\) iff&nbsp;\(\lambda = lambda_k\)}}</li></ul>

============================================================

  

    The {{c5::{{c1::trace}}}} of an operator T is the {{c4::{{c2::sum of eigenvalues}}}} counted with {{c4::{{c3::mutliplicity}}}}

============================================================

  

    The {{c1::determinant}} of an operator T is the {{c2::product of eigenvalues}} {{c5::{{c4::counted with}}}} {{c5::{{c3::mutliplicity}}}}

============================================================

  

    A diagonal matrix has {{c1::nonzero}} elements {{c2::only on the diagonal}}, however, it can still have {{c5::{{c3::zeros}}}} {{c5::{{c4::on the diagonal}}}}

============================================================

  

    A linear operator T has a diagonal matrix w.r.t the basis&nbsp;\(\vec{v}\) if&nbsp;{{c1::\(T(v_i)\)}} = {{c2::\( \lambda_i v_i\)}} which means that {{c5::{{c3::the vector space V has}}}} {{c5::{{c4::a basis formed from eigenvectors}}}}

============================================================

  

    What is the eigenspace of this transformations eigenvalues (which are {{c1::0}})?<br><img src="paste-92f48484d1c4e6295e3dbee5979bfca865f92d06.jpg"><br>Answer:<br><ul><li>The eigenspace is&nbsp;{{c2::\(nulL(T - \lambda I)\)}} which is {{c3::just&nbsp;\(null(T)\)}}, since {{c4::the null space is one-dimensional by rank-nulity}}, {{c5::no eigenbasis of dim V can exist}} and thus {{c5::the transformation has no diagonal matrix}}</li></ul>

============================================================

  

    A <b>sufficient</b> condition for a diagonalizable matrix to exist<br><ul><li>An operator T is diagonalizable if it has {{c5::{{c1::dim V distinct eigenvalues}}}} because it {{c4::{{c2::has a large enough eigenspace}}}} to {{c4::{{c3::build an eigenbasis}}}}</li></ul>

============================================================

  

    Proof that enough eigenvalues implies diagonalizability:<br><ul><li>Suppose T has {{c5::dim V distinct eigenvalues&nbsp;\(\lambda_1,\ldots,\lambda_{n=dimV}\)}}</li><li>Let&nbsp;\(\vec{v}\)&nbsp; be {{c1::the corresponding eigenvectors}}</li><li>Then&nbsp;\(\vec{v}\) is {{c2::linearly independent&nbsp;}} with {{c3::length equal to the dimension of the vector space}}</li><li>As such, they {{c4::satisfy the diagonalizable condition}} that&nbsp;{{c4::\(T(v_i)&nbsp; = \lambda_i v_i\)}}</li></ul>

============================================================

  

    How can we diagramatticaly visualise V tensor producted with F, i.e L(V,F)?<br><ul><li>Write V as {{c5::{{c1::an arrow}}}} with {{c4::{{c2::opposite direction}}}} {{c4::{{c3::from other tensor product arrows}}}}</li></ul>

============================================================

  

    We can use the idea of an evaluate map between dual vectors and normal vectors:<br><img src="paste-b21f2ee395dd92d1c84acbacfe432b9b730010fe.jpg"><br>to diagrammatically define the {{c5::{{c1::dual dual}}}} as follows:<br>{{c4::{{c2::<img src="paste-77c82a1ec0e5648ed15767befa65c99caa70538f.jpg">}}}}<br>Which is why the {{c1::dual dual}} is {{c4::{{c3::naturally isomorphic to V}}}}

============================================================

  

    Diagramatic representation of an evaluate map, ev in \(V' \oplus (V → F)\) for defining the dot product<br>{{c1::<img src="paste-39375161a45cff6cde6466427408a9bf7ecfe9d5.jpg">}}<br><br>There is a transpose concept in&nbsp;\(V \oplus V'\) in the shape of&nbsp;<br>{{c2::<img src="paste-eec77470fc3cf14efc40c5752645f8afff209bba.jpg">}}<br><br>When we combine the two maps we are just mapping circularly<br>{{c3::<img src="paste-e44a60c07096e61fa3c19b162c0bd7c83cc0ccde.jpg">}}<br>The value of the loop does not depend on the basis, this loop is in fact the {{c4::trace}}<br>With diagram<br>{{c4::<img src="paste-0c94d92fb180a1f3fe4d64152720b1f225d8a46f.jpg">}}<br>If we want to {{c5::take powers}} we have the following diagram<br><br><img src="paste-82bb39ff398c288694a06e64fac9010cfd3cb2c9.jpg">

============================================================

  

    For {{c1::2-by-2}} matrices<br><ul><li>det(T) = {{c2::\(\frac{1}{2}\)}}( {{c5::{{c3::\(tr(A)^2 \)}}}} -&nbsp; {{c5::{{c4::\(tr(A^2)\)}}}})</li></ul>

============================================================

  

    If we take the trace of {{c1::a product of linear maps}}, we are allowed to {{c2::cyclically permute the elements}} i.e<br><ul><li>tr({{c3::fgh}}) = tr({{c4::ghf}}) = tr({{c5::hfg}})</li></ul>

============================================================

  

    All properties of determinant and trace, which are {{c1::indepedent of}} {{c2::choice of basis}}, come from the {{c3::cannonical evaluation}} and {{c4::coevaluation}} {{c5::maps}}<br><br>{{c5::<img src="paste-56d3a47cbeb8d4c240c7b328982aaade1f8df52a.jpg">}}

============================================================

  

    Suppose T has m distinct eigenvalues with&nbsp;\(m\)&nbsp;\(\leq\)&nbsp;\(dimV\), then the following are equivalent:<br><ol><li>T has {{c1::a diagonal matrix w.r.t some basis}}</li><li>V has a {{c2::basis consisting of eigenvectors of T}}</li><li>There exist {{c3::one-dimensional subspaces&nbsp;\(U_1,\ldots, U_m\) of V each of which is invariant under T}} and V =&nbsp;{{c3::\(U_1 \oplus \ldots \oplus U_m\)}}</li><li>V =&nbsp;{{c4::\(nul (T - \lambda_1 I)\)&nbsp;\(\oplus\)&nbsp;\(\ldots\)&nbsp;\(\oplus\)\(null(T-\lambda_m I)\)}}</li></ol>They are sorted in order of {{c5::their dependence on a basis}}

============================================================

  

    <div>This property:</div><ol><li>V = {{c2::\(null (T - \lambda_1 I)\)&nbsp;\(\oplus\)&nbsp;\(\ldots\)&nbsp;\(\oplus\)\(null(T-\lambda_m I)\)}}</li></ol>{{c1::Abstracts away the basis}} and allows us to think of {{c5::{{c3::a diagonal matrix}}}} based entirely on {{c5::{{c4::the null space of different eigenvalues}}}}

============================================================

  

    <div>This property:</div><ol><li>V = {{c2::\(null (T - \lambda_1 I)\)&nbsp;\(\oplus\)&nbsp;\(\ldots\)&nbsp;\(\oplus\)\(null(T-\lambda_m I)\)}}</li></ol>{{c1::Abstracts away the basis}} and allows us to think of {{c5::{{c3::a diagonal matrix}}}} based entirely on {{c5::{{c4::the null space of different eigenvalues}}}}<br>

============================================================

  

    A linear map T is diagonalisable iff:<br><ul><li>{{c2::dim V}} = {{c1::dim \( null(T - \lambda_1)\) +&nbsp;\(\ldots\) + dim \( null(T - \lambda_m)\)}} where&nbsp;{{c5::{{c3::\(\lambda_1,\ldots,\lambda_m\)}}}} {{c5::{{c4::are eigenvalues}}}}</li></ul>

============================================================

  

    T having {{c5::{{c1::a number of eigenvalues equal to the dimension of the domain}}}} is a {{c4::{{c2::sufficient but not necessary}}}} condition for T to be {{c4::{{c3::diagonalisable}}}}

============================================================

  

    If we have a basis of eigenvectors we automatically know that&nbsp;<br><ul><li>V =&nbsp;{{c5::{{c1::\(null(T -\lambda_1 I)\)&nbsp;\(\oplus\)&nbsp;\(\ldots\)&nbsp;\(\oplus \)&nbsp;\(null(T- \lambda_m I)\)}}}}</li><li>By just {{c4::{{c2::grouping the eigenvectors based on the lambda to which they belong}}}}, i.e {{c4::{{c3::by the null space of the appropriate operator from the list}}}}</li></ul>

============================================================

  

    Proof that if V can be written as {{c1::the sum of eigenspace this representation}} is {{c2::unique}} i.e {{c2::a direct sum}}<br><ul><li>Suppose we could write {{c3::0 as a sum of vectors from the eigenspaces}} without {{c4::the vectors being all 0}}</li><li>This leads to a contradiction since {{c5::the vectors composing the eigenspaces must be linearly independent}} and this property would {{c5::contradict linear independence}}</li></ul>

============================================================

  

    Suppose&nbsp;<br><ul><li>dim V = {{c1::sum of the dimension of}} {{c2::eigenspaces corresponding to all of its eigenvalues}}</li></ul>We can then {{c3::choose a basis fo each eigenspace}} and {{c4::concatenate them to get a basis of V}} because {{c5::the list must be linearly independent as it is composed of eigenvectors}} and {{c5::it must be of the right length if the dim V assumption above is correct}}<br>This implies that<br><ul><li>V = {{c1::direct sum of the eigenspaces}}</li></ul>

============================================================

  

    {{c5::{{c1::Linear combinations}}}} of {{c4::{{c3::eigenvectors}}}} with {{c4::{{c2::the same eigenvalue}}}} are {{c4::{{c3::also eigenvectors}}}}

============================================================

  

    Proof that Every operator on a finite-dimensional nonzero real vector space has an invariant subspace with dimension 1 or 2<br><ul><li>dim V = n &gt; 0</li><li>Choose any {{c1::nonzero}} vector V</li><li>Construct {{c2::a list of vectors that is too long to be linearly independent&nbsp;\(T^0v, Tv, T^2v,\ldots,T^nv\)}}</li><li>Since it is {{c2::linearly dependent}} there exists&nbsp;{{c3::\(\vec{c}\)}} such that&nbsp;{{c3::\(\sum c_i T^i v = 0\)}} with&nbsp;{{c3::\(\vec{c} \neq \vec{0} \)}}</li><li>We know this {{c4::has a polynomial over R that can be factored into}}:</li><ul><li>p(x) = {{c4::c\( (x - \lambda_1)\)\(\ldots\)\((x-\lambda_m)\) \( (x^2 + \alpha_1 x + \beta_1)\)\(\ldots\)&nbsp;\((x^2+ \alpha_M x + \beta_M )\) with m and M potentially 0}}</li></ul><li>Then 0 =&nbsp;{{c5::\(\sum c_i T^iv\)}} = {{c5::c\((T-\lambda_1 I)\)\(\ldots\)\( (T-\lambda_m I) \)\((T^2 - \alpha_1 T + \beta_1 I)\)\(\ldots\)\((T^2 - \alpha_M T + \beta_M I)\)v}}</li></ul><br>

============================================================

  

    Proof that Every operator on a finite-dimensional nonzero real vector space has an invariant subspace with dimension 1 or 2<br><ul><li>dim V = n &gt; 0</li><li>Choose any nonzero vector V</li><li>Construct a list of vectors that is too long to be linearly independent&nbsp;\(T^0v, Tv, T^2v,\ldots,T^nv\)</li><li>Since it is linearly dependent there exists&nbsp;\(\vec{c}\) such that&nbsp;\(\sum c_i T^i v = 0\) with&nbsp;\(\vec{c} \neq \vec{0} \)</li><li>We know this has a polynomial over R that can be factored into:</li><li>p(x) = c\( (x - \lambda_1)\)\(\ldots\)\((x-\lambda_m)\) \( (x^2 + \alpha_1 x + \beta_1)\)\(\ldots\)&nbsp;\((x^2+ \alpha_M x + \beta_M )\) with m and M potentially 0</li><li>Then 0 =&nbsp;\(\sum c_i T^iv\) = c\((T-\lambda_1 I)\)\(\ldots\)\( (T-\lambda_m I) \)\((T^2 - \alpha_1 T + \beta_1 I)\)\(\ldots\)\((T^2 - \alpha_M T + \beta_M I)\)v</li><li>The {{c1::linear}} terms correspond to {{c3::eigenvalues}} and thus {{c3::1-dimensional invariant subspaces}}</li><li>While the {{c2::quadratic}} terms can be {{c4::not injective}}, meaning that</li><ul><li>There exists&nbsp;{{c4::\(u \in V\)}} such that&nbsp;{{c5::\(T^2u +\alpha_i Tu + \beta_i I u\) = 0}} with {{c4::u not equal to 0}}</li><li>In which case&nbsp;{{c5::\(span(u,Tu)\)}} is a {{c3::2-dimensional invariant subspace&nbsp;}}</li><li></li></ul></ul>

============================================================

  

    Direct sum {{c1::projection}} definition:<br><br><ul><li>Suppose V =&nbsp;\(U \oplus W\)</li><li>This means that&nbsp;\(\forall v \in V, \exists u \in U, w \in W \) such that&nbsp;\(v = u+w\)</li><li>We can then define a {{c1::projection}}:</li><ul><li>{{c2::\(P_{u,w}(v)\)}}&nbsp; = {{c3::\( u \)}} which is a function beacuse {{c3::u is unique}} and can be thought of as {{c4::a map from V onto U}} or {{c4::onto V}}</li></ul><li>And thus the direct sum decomposition can be rewritten:</li><li>V =&nbsp;{{c5::\(P_{u,w}(v) \) +&nbsp;\(P_{w,u}(v)\)}}</li></ul><br>

============================================================

  

    Direct sum projection definition:<br><br><ul><li>Suppose V =&nbsp;\(U \oplus W\)</li><li>This means that&nbsp;\(\forall v \in V, \exists u \in U, w \in W \) such that&nbsp;\(v = u+w\)</li><li>We can then define a projection:</li><ul><li>\(P_{u,w}(v)\)&nbsp; = u \) which is a function beacuse u is unique and can be thought of a a map from V onto U or onto V<br></li></ul><li>\(P_{u,w}\)&nbsp; = {{c1::\(v\)}} iff {{c2::v is in U}}<br></li><li>Thus it is an {{c3::idempotent}} function</li><li>Thus&nbsp;{{c4::\(P_{u,w}^2\)}} =&nbsp;{{c4::\(P_{u,w}\)}}</li><li>And thus the direct sum decomposition can be rewritten:</li><li>V =&nbsp;{{c5::\(P_{u,w}(v) \) +&nbsp;\(P_{w,u}(v)\)}}</li><li></li></ul>

============================================================

  

    A projection onto a {{c5::{{c3::subspace}}}} is {{c4::{{c1::surjective::surj/inj?}}::surj/inj?}} as a map on {{c4::{{c2::that subspace}}}}

============================================================

  

    Projections are useful for decompositions, because the {{c1::null space of one projection}} must be {{c2::complimentary}} to {{c5::{{c3::the ranges}}}} of {{c5::{{c4::the other projections}}}}

============================================================

  

    Every operator on an {{c5::{{c1::odd::parity}}::parity}}-dimensional {{c4::{{c2::real}}}} vector space has {{c4::{{c3::an eigenvalue}}}}

============================================================

  

    {{c1::range}} ({{c2::\(P_{u,w} \)}}) = {{c5::{{c3::U}}}} when&nbsp;{{c2::\(P_{u,w}\)}}&nbsp;\(\in\)&nbsp;{{c5::{{c4::\(L(V,U)\)}}}}

============================================================

  

    In making the definition of a vector space we ignored the notions of {{c5::{{c1::length}}}} and {{c4::{{c2::angle}}}}.&nbsp;These ideas are embedded in the concept of {{c4::{{c3::inner products}}}}.

============================================================

  

    <b>Definition</b> {{c5::{{c1::dot product}}}}<br><br>For \(x, y \in \mathbf{R}^{n}\), the {{c1::dot product}} of \(x\) and \(y\), denoted {{c4::{{c2::\(x \cdot y\)}}}}, is defined by<br><br><ul><li>{{c2::\(x \cdot y\)}} = {{c4::{{c3::\(x_{1} y_{1}+\cdots+x_{n} y_{n},\)}}}}</li></ul><br>where \(x=\left(x_{1}, \ldots, x_{n}\right)\) and \(y=\left(y_{1}, \ldots, y_{n}\right)\).<br>

============================================================

  

    If we think of vectors as {{c1::points}} instead of {{c2::arrows}}, then {{c5::{{c3::\(\|x\|\)}}}} should be interpreted as {{c5::{{c4::the distance from the origin to the point \(x\).}}}}

============================================================

  

    Note that the {{c1::dot product}} of two vectors in \(\mathbf{R}^{n}\) is {{c2::a number}}, not {{c3::a vector}}. <br>Obviously <br><ul><li>{{c4::\(x \cdot x\)}} = {{c5::\(\|x\|^{2}\)}} for {{c5::all \(x \in \mathbf{R}^{n}\).}}</li></ul>

============================================================

  

    The dot product on \(\mathbf{R}^{n}\) has the following properties:<br><br><ul><li>- {{c1::\(x \cdot x \geq 0\)}} for {{c1::all \(x \in \mathbf{R}^{n}\);}}</li><li>- {{c2::\(x \cdot x=0\)}} if and only if {{c2::\(x=0\)}};</li><li>- for \(y \in \mathbf{R}^{n}\) {{c3::fixed}}, the map from {{c4::\(\mathbf{R}^{n}\)}} to {{c4::\(\mathbf{R}\)}} that sends {{c4::\(x \in \mathbf{R}^{n}\)}} to {{c4::\(x \cdot y\)}} is {{c3::linear}};</li><li>- {{c5::\(x \cdot y\)}} = {{c5::\(y \cdot x\)}} for {{c5::all \(x, y \in \mathbf{R}^{n}\).}}</li></ul>

============================================================

  

    An {{c5::{{c1::inner}}}} product is a {{c4::{{c2::generalization}}}} of the {{c4::{{c3::dot}}}} product.&nbsp;

============================================================

  

    You may be tempted to guess that an inner product is defined by {{c5::{{c2::abstracting the properties of the dot product}}}} . For {{c4::{{c1::real}}}} vector spaces, that guess is {{c4::{{c3::correct}}}}.&nbsp;

============================================================

  

    Recall that if \(\lambda=a+b i\), where \(a, b \in \mathbf{R}\), then<br><br><ul><li>- the absolute value of \(\lambda\), denoted \(|\lambda|\), is defined by \(|\lambda|\) = {{c1::\(\sqrt{a^{2}+b^{2} }\)}};</li><li>- the complex conjugate of \(\lambda\), denoted \(\bar{\lambda}\), is defined by \(\bar{\lambda}\) = {{c2::\(a-b i\)}};</li><li>- {{c5::{{c3::\(|\lambda|^{2}\)}}}} = {{c5::{{c4::\(\lambda \bar{\lambda}\)}}}}</li></ul>

============================================================

  

    For \(z=\left(z_{1}, \ldots, z_{n}\right) \in\) {{c1::\(\mathbf{C}^{n}\)}}, we define the norm of \(z\) by<br><br><ul><li>\(\|z\|\) ={{c2::\(\sqrt{\left|z_{1}\right|^{2}+\cdots+\left|z_{n}\right|^{2} } .\)}}</li></ul><br>The {{c3::absolute values are needed}} because we want {{c4::\(\|z\|\) to be a nonnegative number}}. Note that<br><br><ul><li>{{c1::\(\|z\|^{2}\)}}&nbsp;= {{c5::\(z_{1} \overline{z_{1} }+\cdots+z_{n} \overline{z_{n} } .\)}}</li></ul>

============================================================

  

    \[<br>\|z\|^{2}=z_{1} \overline{z_{1} }+\cdots+z_{n} \overline{z_{n} } .<br>\]<br><br>We want to think of \(\|z\|^{2}\) as {{c1::the inner product of \(z\) with itself}}, as we did in \(\mathbf{R}^{n}\). The equation above thus suggests that {{c3::the inner product}} of {{c4::\(w=\left(w_{1}, \ldots, w_{n}\right) \in \mathbf{C}^{n}\)}} with {{c5::\(z\)}} should equal<br><br><ol><li>{{c2::\(w_{1} \overline{z_{1} }+\cdots+w_{n} \overline{z_{n} } .\)}}<br></li></ol>

============================================================

  

    The fundamental reason why we want the inner product over complex vector spaces to be&nbsp;<br>{{c5::{{c1::\[<br>w_{1} \overline{z_{1} }+\cdots+w_{n} \overline{z_{n} } .<br>\]}}}}<br><br>is because {{c4::{{c2::the norm of a complex number}}}} equals {{c4::{{c3::itself times the conjugate}}}}

============================================================

  

    Two comments about the notation used in the next definition:<br><br>- If \(\lambda\) is a {{c3::complex}} number, then the notation {{c4::\(\lambda \geq 0\)}} means that {{c5::\(\lambda\) is real and nonnegative.}}<br><br>- We use the common notation {{c1::\(\langle u, v\rangle\)}}, with {{c2::angle brackets}} denoting an inner product.&nbsp;

============================================================

  

    <br>An inner product on \(V\) is a {{c5::{{c1::function}}}} that {{c4::{{c2::takes each ordered pair \((u, v)\) of elements of \(V\) to}}}} {{c4::{{c3::a number \(\langle u, v\rangle \in \mathbf{F}\)}}}}&nbsp;

============================================================

  

    <b>Definition</b>&nbsp;inner product<br><br>An inner product on \(V\) is a function that takes each ordered pair \((u, v)\) of elements of \(V\) to a number \(\langle u, v\rangle \in \mathbf{F}\) and has the following properties:<br><br><ul><li>{{c1::positivity}}</li><li>{{c2::definiteness}}</li><li>{{c3::additivity in first slot}}</li><li>{{c4::homogeneity in first slot}}</li><li>{{c5::conjugate symmetry}}</li></ul>

============================================================

  

    <b>Definition</b>&nbsp;inner product<br><br>An inner product on \(V\) is a function that takes each ordered pair \((u, v)\) of elements of \(V\) to a number \(\langle u, v\rangle \in \mathbf{F}\) and has the following properties:<br><br><ul><li>{{c1::positivity}}</li><ul><li>{{c2::\(\langle v, v\rangle \geq 0 \text { for all } v \in V\)}}</li></ul><li>{{c5::{{c3::definiteness}}}}</li><ul><li>{{c5::{{c4::\(\langle v, v\rangle=0 \text { if and only if } v=0 ;\)}}}}</li></ul></ul>

============================================================

  

    <b>Definition</b>&nbsp;inner product<br><br>An inner product on \(V\) is a function that takes each ordered pair \((u, v)\) of elements of \(V\) to a number \(\langle u, v\rangle \in \mathbf{F}\) and has the following properties:<br><br><ul><li>{{c1::additivity in first slot}}<br></li><ul><li>{{c2::\(\langle u+v, w\rangle=\langle u, w\rangle+\langle v, w\rangle \text { for all } u, v, w \in V ;\)}}</li></ul><li>{{c5::{{c3::homogeneity in first slot}}}}</li><ul><li>{{c5::{{c4::\(\langle\lambda u, v\rangle=\lambda\langle u, v\rangle \text { for all } \lambda \in \mathbf{F} \text { and all } u, v \in V ;\)}}}}</li></ul></ul>

============================================================

  

    <b>Definition</b>&nbsp;inner product<br><br>An inner product on \(V\) is a function that takes each ordered pair \((u, v)\) of elements of \(V\) to a number \(\langle u, v\rangle \in \mathbf{F}\) and has the following properties:<br><br><ul><li>positivity</li><ul><li>{{c1::\(\langle v, v\rangle \geq 0 \text { for all } v \in V\)}}</li></ul><li>definiteness</li><ul><li>{{c2::\(\langle v, v\rangle=0 \text { if and only if } v=0 ;\)}}</li></ul><li>additivity in first slot</li><ul><li>{{c3::\(\langle u+v, w\rangle=\langle u, w\rangle+\langle v, w\rangle \text { for all } u, v, w \in V ;\)}}</li></ul><li>homogeneity in first slot</li><ul><li>{{c4::\(\langle\lambda u, v\rangle=\lambda\langle u, v\rangle \text { for all } \lambda \in \mathbf{F} \text { and all } u, v \in V ;\)}}</li></ul><li>conjugate symmetry</li><ul><li>{{c5::\(\langle u, v\rangle=\overline{\langle v, u\rangle} \text { for all } u, v \in V .\)}}</li></ul></ul>

============================================================

  

    \subsection{Definition inner product}<br><br>An inner product on \(V\) is a function that takes each ordered pair \((u, v)\) of elements of \(V\) to a number \(\langle u, v\rangle \in \mathbf{F}\) and has the following properties:<br><br>definitiveness<br><br><ul><li>{{c5::{{c1::\(\langle v, v\rangle\)}}}}&nbsp;= {{c4::{{c2::\(0\)}}}} \(\text { if and only if }\)&nbsp; {{c4::{{c3::\(v=0 ;\)}}}}<br></li></ul><br><br>

============================================================

  

    \subsection{Definition inner product}<br><br>An inner product on \(V\) is a function that takes each ordered pair \((u, v)\) of elements of \(V\) to a number \(\langle u, v\rangle \in \mathbf{F}\) and has the following properties:<br><br>homogeneity in first slot<br><br><ul><li>{{c5::{{c1::\(\langle\lambda u, v\rangle\)}}}} ={{c4::{{c2::\(\lambda\langle u, v\rangle\)}}}} \( \text { for all }\) {{c4::{{c3::\( \lambda \in \mathbf{F} \text { and all } u, v \in V ;\)}}}}</li></ul>

============================================================

  

    \subsection{Definition inner product}<br><br>An inner product on \(V\) is a function that takes each ordered pair \((u, v)\) of elements of \(V\) to a number \(\langle u, v\rangle \in \mathbf{F}\) and has the following properties:<br><br>conjugate symmetry<br><br><ul><li>{{c5::{{c1::\(\langle u, v\rangle\)}}}} = {{c4::{{c2::\(\overline{\langle v, u\rangle} \)}}}} \(\text { for all }\) {{c4::{{c3::\( u, v \in V .\)}}}}</li></ul>

============================================================

  

    <ul><li>Because every real number is equal to its cojugate we can simply state that for real vector spaces</li><ul><li>{{c5::{{c1::\(\langle u, v\rangle\)}}}} = {{c4::{{c2::\(\langle v, u\rangle\)}}}} for {{c4::{{c3::all \(v, w \in V\).}}}}</li></ul></ul>

============================================================

  

    Example inner products<br><br>(a) The {{c3::Euclidean}} inner product on {{c4::\(\mathbf{F}^{n}\)}} is defined by<br><br><ul><li>{{c5::{{c1::\(\left\langle\left(w_{1}, \ldots, w_{n}\right),\left(z_{1}, \ldots, z_{n}\right)\right\rangle\)}}}}&nbsp;= {{c5::{{c2::\(w_{1} \overline{z_{1} }+\cdots+w_{n} \overline{z_{n} } .\)}}}}</li></ul>

============================================================

  

    \subsection{Example inner products}<br><br>(b) If {{c1::\(c_{1}, \ldots, c_{n}\)}} are {{c2::positive numbers}}, then an inner product can be defined on {{c3::\(\mathbf{F}^{n}\)}} by<br><br><ul><li>{{c4::\(\left\langle\left(w_{1}, \ldots, w_{n}\right),\left(z_{1}, \ldots, z_{n}\right)\right\rangle\)}} = {{c5::\(c_{1} w_{1} \overline{z_{1} }+\cdots+c_{n} w_{n} \overline{z_{n} } .\)}}</li></ul>

============================================================

  

    \subsection{Example inner products}<br><br>(c) An inner product can be defined on the vector space of {{c1::continuous real-valued functions}} on {{c2::the interval \([-1,1]\)}} by<br><br><ul><li>{{c5::{{c3::\(\langle f, g\rangle\)}}}} = {{c5::{{c4::\(\int_{-1}^{1} f(x) g(x) d x .\)}}}}</li></ul>

============================================================

  

    \subsection{Example inner products}<br><br>(d) An inner product can be defined on {{c5::{{c1::\(\mathcal{P}(\mathbf{R})\)}}}} by<br><br><ul><li>{{c4::{{c2::\(\langle p, q\rangle\)}}}} = {{c4::{{c3::\(\int_{0}^{\infty} p(x) q(x) e^{-x} d x .\)}}}}</li></ul><br>

============================================================

  

    <b>Definition</b> {{c1::inner product space}}<br><br>An {{c2::inner product space}} is {{c5::{{c3::a vector space \(V\) along with}}}} {{c5::{{c4::an inner product on \(V\).}}}}

============================================================

  

    The most important example of an {{c5::{{c1::inner product space}}}} is {{c4::{{c2::\(\mathbf{F}^{n}\)}}}} with the {{c4::{{c3::Euclidean inner product}}}}

============================================================

  

    <b>Basic properties of an inner product</b><br><br><ul><li>(a) For {{c5::{{c1::each fixed \(u \in V\)}}}}, the function that {{c4::{{c2::takes \(v\) to \(\langle v, u\rangle\)}}}} is {{c4::{{c3::a linear map from \(V\) to \(\mathbf{F}\).}}}}</li></ul>

============================================================

  

    <b>Basic properties of an inner product</b><br><ul><li>(b) {{c5::{{c1::\(\langle 0, u\rangle\)}}}} = {{c4::{{c2::\(0\)}}}} for {{c4::{{c3::every \(u \in V\).}}}}</li></ul>

============================================================

  

    <b>Basic properties of an inner product</b><br><br><ul><li>(c) {{c5::{{c1::\(\langle u, 0\rangle\)}}}} = {{c4::{{c2::\(0\)}}}} for {{c4::{{c3::every \(u \in V\)}}}}.</li></ul>

============================================================

  

    <b>Basic properties of an inner product</b><br><br><ul><li>(d) \(\langle\) {{c1::\(u\)}}, {{c2::\(v+w\)}} \(\rangle\)= {{c3::\( \langle u, v\rangle\)}} {{c4::+}} {{c5::\(\langle u, w\rangle\)}} for {{c5::all \(u, v, w \in V\).}}<br></li><li><br></li></ul>

============================================================

  

    <b>Basic properties of an inner product</b><br><br><ul><li>(e) \(\langle\){{c1::\( u \)}}, {{c2::\(\lambda v\)}} \(\rangle\) = {{c5::{{c3::\(\bar{\lambda}\langle u, v\rangle\)}}}} for {{c5::{{c4::all \(\lambda \in \mathbf{F}\) and \(u, v \in V\).}}}}<br></li></ul>

============================================================

  

    <b>Basic properties of an inner product</b><br><br><ul><li>(a) For {{c1::each fixed \(u \in V\)}}&nbsp;the function that {{c1::takes \(v\) to \(\langle v, u\rangle\) }} is {{c1::a linear map from \(V\) to \(\mathbf{F}\).}}</li><li>(b) {{c2::\(\langle 0, u\rangle\)}} = {{c2::\(0\)}} for {{c2::every \(u \in V\).}}</li><li>(c) {{c3::\(\langle u, 0\rangle\)}} = {{c3::\(0\)}} for {{c3::every \(u \in V\).}}</li><li>(d) {{c4::\(\langle u, v+w\rangle\)}} = {{c4::\(\langle u, v\rangle+\langle u, w\rangle\)}} for {{c4::all \(u, v, w \in V\).}}</li><li>(e) {{c5::\(\langle u, \lambda v\rangle\)}} = {{c5::\(\bar{\lambda}\langle u, v\rangle\)}} for {{c5::all \(\lambda \in \mathbf{F}\) and \(u, v \in V\).}}</li></ul>

============================================================

  

    \subsection{Basic properties of an inner product}<br><br>(a) For each fixed \(u \in V\), the function that takes \(v\) to \(\langle v, u\rangle\) is a linear map from \(V\) to \(\mathbf{F}\).<br><br>Proof<br><br><ul><li>(a) Part (a) follows from the conditions of {{c5::{{c1::additivity in the first slot}}}} and {{c4::{{c2::homogeneity in the first slot}}}} in {{c4::{{c3::the definition of an inner product.}}}}</li></ul><br>

============================================================

  

    <img src="paste-22e77de52f3185e42d2973cace26edba2b9f1327.jpg"><br><br>(c) Part (c) follows from part {{c5::{{c1::(a)}}}} and {{c4::{{c2::the conjugate symmetry}}}} property in {{c4::{{c3::the definition of an inner product.}}}}

============================================================

  

    <img src="paste-04facaf5e7401b317d24758f0e08b13698f0d87b.jpg"><br>Proof for<br>(d) Suppose \(u, v, w \in V\). Then<br><br><ul><li>\(\langle u, v+w\rangle \)&nbsp; =&nbsp;</li><ul><li>={{c1::\(\overline{\langle v+w, u\rangle} \)}}</li><li>&nbsp;= {{c2::\(\overline{\langle v, u\rangle+\langle w, u\rangle} \)}}</li><li>&nbsp;= {{c5::{{c3::\( \overline{\langle v, u\rangle}+\overline{\langle w, u\rangle} \)}}}}</li><li>&nbsp;= {{c5::{{c4::\(\langle u, v\rangle+\langle u, w\rangle .\)}}}}</li></ul></ul>

============================================================

  

    <img src="paste-5e30d63e06024a238a5a76ad4f9cc7f6c2b6c2fe.jpg"><br><br>Proof for e)<br><br>(e) Suppose \(\lambda \in \mathbf{F}\) and \(u, v \in V\). Then<br><br><ul><li>\(\langle u, \lambda v\rangle\)&nbsp; =</li><ul><li>={{c1::\(\overline{\langle\lambda v, u\rangle}\)}}</li><li>&nbsp;= {{c2::\(\overline{\lambda\langle v, u\rangle} \)}}</li><li>&nbsp;= {{c5::{{c3::\(\bar{\lambda} \overline{\langle v, u\rangle} \)}}}}</li><li>&nbsp;= {{c5::{{c4::<ul><li>\(\bar{\lambda}\langle u, v\rangle,\)</li></ul>}}}}</li></ul></ul>as desired.<br>

============================================================

  

    Definition {{c5::{{c1::norm}}}}, {{c4::{{c2::\(\|v\|\)}}}}<br><br>For \(v \in V\), the {{c1::norm}} of \(v\), denoted {{c2::\(\|v\|\)}}, is defined by<br><br><ul><li>{{c2::\(\|v\|\)}} = {{c4::{{c3::\(\sqrt{\langle v, v\rangle}\)}}}}</li></ul>

============================================================

  

    \subsection{Example norms}<br><br>(a) If {{c3::\(\left(z_{1}, \ldots, z_{n}\right)\)}} \(\in\)&nbsp;{{c4::\(\mathbf{F}^{n}\)}} (with the {{c5::Euclidean}} inner product), then<br><br><ul><li>{{c1::\(\left\|\left(z_{1}, \ldots, z_{n}\right)\right\|\)}} = {{c2::\(\sqrt{\left|z_{1}\right|^{2}+\cdots+\left|z_{n}\right|^{2} } . \)}}</li></ul><br>

============================================================

  

    \subsection{Example norms}<br><br>(b) In the vector space of continuous {{c1::real-valued functions}} on {{c2::\([-1,1]\)}} , we have<br><br><ul><li>{{c5::{{c3::\(\|f\| \)}}}}&nbsp;= {{c5::{{c4::\(\sqrt{\int_{-1}^{1}(f(x))^{2} d x} .\)}}}}</li></ul><br>

============================================================

  

    <b>Basic properties of the norm</b><br><br>Suppose \(v \in V\).<br><br><ul><li>(a) {{c1::\(\|v\|\)}} = {{c2::\(0\)}} {{c5::{{c3::if and only if}}}} {{c5::{{c4::\(v=0\)}}}}.</li></ul>

============================================================

  

    <b>Basic properties of the norm</b><br><br>Suppose \(v \in V\).<br><ul><li>{{c5::{{c1::\(\|\lambda v\|\)}}}} = {{c4::{{c2::\(|\lambda|\|v\|\)}}}} for {{c4::{{c3::all \(\lambda \in \mathbf{F}\).}}}}<br></li></ul>

============================================================

  

    <img src="paste-6953387b0ab7a0b5e125af79606de79b232efd17.jpg"><br><br>Proof<br><br>(a) The desired result holds because {{c5::{{c1::\(\langle v, v\rangle=0\)}}}} {{c4::{{c2::if and only if}}}} {{c4::{{c3::\(v=0\)}}}}.

============================================================

  

    <img src="paste-ff0d1a91cf6e6f0af7c2a9e2b34f1bdb065e24fb.jpg"><br><br>Proof for (b) Suppose \(\lambda \in \mathbf{F}\). Then<br><br><ul><li>{{c2::\(\|\lambda v\|^{2}\)}}&nbsp; =</li><ul><li>={{c1::\(\langle\lambda v, \lambda v\rangle \)}}</li><li>&nbsp;= {{c3::\(\lambda\langle v, \lambda v\rangle \)}}</li><li>&nbsp;= {{c4::\(\lambda \bar{\lambda}\langle v, v\rangle \)}}</li><li>&nbsp;= {{c5::\(|\lambda|^{2}\|v\|^{2} .\)}}</li></ul></ul><br>{{c5::Taking square roots}} now gives the desired equality.<br>

============================================================

  

    <b>Definition</b> {{c1::orthogonal}}<br><br>Two vectors \(u, v \in V\) are called {{c1::orthogonal}} if {{c2::\(\langle u, v\rangle\)}} {{c5::{{c3::=}}}} {{c5::{{c4::\(0\)}}}}.

============================================================

  

    <ul><li>{{c1::\(\langle u, v\rangle\)}}&nbsp;= {{c2::\(\|u\| \)}} {{c5::{{c3::\(\|v\|\)&nbsp;}}}} {{c5::{{c4::\(\cos \theta,\)}}}}</li></ul>

============================================================

  

    Orthogonality and {{c1::0}}<br><br><ul><li>(a) {{c1::0}} is {{c2::orthogonal to every vector in \(V\).}}</li><li>(b) {{c1::0}} is also {{c5::{{c3::the only vector in \(V\)}}}} {{c5::{{c4::that is orthogonal to itself.}}}}</li></ul>

============================================================

  

    {{c1::Pythagorean}} Theorem<br><br>Suppose \(u\) and \(v\) are {{c2::orthogonal}} vectors in \(V\). Then<br><br><ul><li>{{c3::\(\|u+v\|^{2}\)}}&nbsp;{{c4::=}} {{c5::\(\|u\|^{2}+\|v\|^{2} .\)}}</li></ul>

============================================================

  

    <br><img src="paste-581fba9d875a1bccf4898efafdb1ddcf97d57bd8.jpg"><br>Proof for the pythagorean theorem:<br><ul><li>\(\|u+v\|^{2} \)&nbsp;&nbsp;</li><ul><li>=&nbsp; {{c5::{{c1::\(\langle u+v, u+v\rangle \)}}}}</li><li>= {{c4::{{c2::\(\langle u,u\rangle+\langle u, v\rangle+\langle v,u\rangle+\langle v, v\rangle \)}}}}</li><li>= {{c4::{{c3::\( \|u\|^{2}+\|v\|^{2},\)}}}}</li></ul></ul>

============================================================

  

    To discover how to write \(u\) as a scalar multiple of \(v\) plus a vector orthogonal to \(v\), let \(c \in \mathbf{F}\) denote a scalar. Then<br><br><ul><li>\(u\) = {{c5::{{c1::\(c v\)}}}} {{c4::{{c2::\(+\)}}}} {{c4::{{c3::\( (u-c v) .\)}}}}</li></ul>

============================================================

  

    To discover how to write \(u\) as a scalar multiple of \(v\) plus a vector orthogonal to \(v\), let \(c \in \mathbf{F}\) denote a scalar. Then<br><br>\[<br>u=c v+(u-c v) .<br>\]<br><br>Thus we need to choose \(c\) so that \(v\) is {{c1::orthogonal}} to {{c2::\((u-c v)\)}}. In other words, we want<br><br><ul><li>\(0\) =&nbsp;</li><ul><li>={{c5::{{c3::\(\langle u-c v, v\rangle\)&nbsp;}}}}</li><li>= {{c5::{{c4::\(\langle u, v\rangle-c\|v\|^{2} .\)}}}}</li></ul></ul>

============================================================

  

    To discover how to write \(u\) as a scalar multiple of \(v\) plus a vector orthogonal to \(v\), let \(c \in \mathbf{F}\) denote a scalar. Then<br><br>\[<br>u=c v+(u-c v) .<br>\]<br><br>Thus we need to choose \(c\) so that \(v\) is orthogonal to \((u-c v)\). In other words, we want<br><br>\[<br>0=\langle u-c v, v\rangle=\langle u, v\rangle-c\|v\|^{2} .<br>\]<br><br>The equation above shows that we should choose \(c\) to be {{c1::\(\langle u, v\rangle /\|v\|^{2}\)}}. Making this choice of \(c\), we can write<br><br><ul><li>\(u\) = {{c2::\(\frac{\langle u, v\rangle}{\|v\|^{2} } v\)}} {{c5::{{c3::+}}}} {{c5::{{c4::\(\left(u-\frac{\langle u, v\rangle}{\|v\|^{2} } v\right)\)}}}}</li></ul>

============================================================

  

    <b>An</b> {{c1::orthogonal decomposition}}<br><br>Suppose \(u, v \in V\), with \(v \neq 0\). Set :<br><ul><li>\(c\) ={{c1::\(\frac{\langle u, v\rangle}{\|v\|^{2} }\)}}&nbsp;</li><li>and \(w\) ={{c2::\(u-\frac{\langle u, v\rangle}{\|v\|^{2} } v\).&nbsp;}}</li></ul>Then<br><br><ul><li>{{c3::\(\langle w, v\rangle\)}} = {{c4::\(0\)&nbsp;}}</li><li>And {{c3::\(u\)}} = {{c5::\(c v+w .\)}}</li></ul>

============================================================

  

    <b>The</b> {{c1::Cauchy-Schwarz Inequality}}<br><br>Suppose \(u, v \in V\). Then<br><br><ul><li>{{c2::\(|\langle u, v\rangle|\)}}{{c3::\(\leq\)}}{{c4::\(\|u\|\|v\| .\)}}</li></ul><br>This {{c1::inequality}} is an {{c1::equality}} if and only if {{c5::one of \(u, v\) is a scalar multiple of the other.}}<br>

============================================================

  

    <img src="paste-a37eee220a84597b64eddca190d0a287672d6f27.jpg"><br>Proof If \(v=0\), then both sides of the desired inequality equal 0 . Thus we can assume that \(v \neq 0\). Consider the orthogonal decomposition<br><br><ul><li>\(u\) = {{c1::\(\frac{\langle u, v\rangle}{\|v\|^{2} } v+w\)}}</li></ul><br>given by 6.14 , where \(w\) is orthogonal to \(v\). By the Pythagorean Theorem,<br><br><ul><li>\(\|u\|^{2} \)&nbsp;</li><ul><li>= {{c2::\(\left\|\frac{\langle u, v\rangle}{ \|v\|^{2} } v\right\|^{2}+\|w\|^{2} \)}}</li><li>&nbsp;= {{c3::\(\frac{ |\langle u, v\rangle|^{2 } }{\|v\|^{2} }+\|w\|^{2} \)}}</li><li>&nbsp;\(\geq\) {{c4::\( \frac{|\langle u, v\rangle|^{2} }{ \|v\|^{2} } .\)}}</li></ul></ul><br>Multiplying both sides of this inequality by \(\|v\|^{2}\) and then taking square roots gives the desired inequality.<br><br>Looking at the proof in the paragraph above, note that the Cauchy-Schwarz Inequality is an equality if and only if {{c5::the last inequality is an equality.&nbsp;}}<br>

============================================================

  

    \subsection{Example examples of the Cauchy-Schwarz Inequality}<br><br>(b) If \(f, g\) are {{c1::continuous real-valued functions}} on {{c2::\([-1,1]\)}}, then<br><br><ul><li>{{c5::\(\left|\int_{-1}^{1} f(x) g(x) d x\right|^{2}\)}} \(\leq\) {{c3::\(\left(\int_{-1}^{1 }(f(x))^{2 } d x\right)\)}} {{c4::\(\left(\int_{-1}^{1}(g(x))^{2} d x\right) .\)}}</li></ul><br>

============================================================

  

    {{c1::Triangle Inequality}}<br><br>Suppose \(u, v \in V\). Then<br><br><ul><li>{{c2::\(\|u+v\|\)}}&nbsp;{{c3::\(\leq\)}} {{c4::\(\|u\|+\|v\| .\)}}</li></ul><br>This {{c1::inequality}} is an {{c1::equality}} if and only if {{c5::one of \(u, v\)}} is {{c5::a nonnegative multiple of the other.}}<br>

============================================================

  

    We have equality in the Triangle Inequality if and only if<br><br><ul><li>{{c5::{{c1::\(\langle u, v\rangle\)}}}}&nbsp;= {{c4::{{c2::\(\|u\|\|v\| .\)}}}}</li><li>Which happens only if {{c4::{{c3::one of the components is a nonegative multiple of the other}}}}</li></ul>

============================================================

  

    <b>Definition</b>: {{c1::Parallelogram Equality}}<br><br>Suppose \(u, v \in V\). Then<br><br><ul><li>{{c2::\(\|u+v\|^{2}\)}} {{c3::+}}{{c4::\(\|u-v\|^{2}\)}} ={{c5::\(2(\|u\|^{2}+\|v\|^{2}) .\)}}</li></ul>

============================================================

  

    <img src="paste-d1816ecb0211e3b519d82b39b5c6bb028b1276cd.jpg"><br><br><ul><li>\(\|u+v\|^2+\|u-v\|^2\)=<br></li><ul><li>= {{c1::\(\langle u+v, u+v\rangle\)}} +{{c2::\(\langle u-v, u-v\rangle\)}}</li><li>={{c3::&nbsp;\(\|u\|^2\)}} + {{c3::\(\|v\|^2\)}} + {{c4::\(\langle u, v\rangle+\langle v, u\rangle\)}} + {{c3::\(\|u\|^2\)}} + {{c3::\(\|v\|^2\)}} - {{c5::\(\langle u, v\rangle\)}} - {{c5::\(\langle v, u\rangle\)}}</li><li>=&nbsp;\(2\left(\|u\|^2+\|v\|^2\right)\)</li></ul></ul>

============================================================

  

    3 Suppose \(\mathbf{F}=\mathbf{R}\) and \(V \neq\{0\}\). Replace the {{c1::positivity}} condition (which states that {{c2::\(\langle v, v\rangle \geq 0\) for all \(v \in V\)}} ) in the definition of an inner product (6.3) with the condition that {{c3::\(\langle v, v\rangle&gt;0\)}} for {{c4::some \(v \in V\)}}. Show that this change in the definition does {{c5::not change the set of functions from \(V \times V\) to \(\mathbf{R}\) that are inner products on \(V\).}}

============================================================

  

    4 Suppose \(V\) is a real inner product space.<br><br>(a) Show that {{c5::{{c1::\(\langle u+v, u-v\rangle\)}}}} = {{c4::{{c2::\(\|u\|^{2}-\|v\|^{2}\)}}}} for {{c4::{{c3::every \(u, v \in V\).}}}}

============================================================

  

    4 Suppose \(V\) is a real inner product space.<br><br>(b) Show that if \(u, v \in V\) have {{c5::{{c1::the same norm}}}}, then {{c4::{{c2::\(u+v\)}}}} {{c4::{{c3::is orthogonal to \(u-v\).}}}}

============================================================

  

    5 Suppose \(T \in \mathcal{L}(V)\) is such that || {{c1::\(T v\)

}} || &nbsp; \( \leq \) || {{c1::\( v\)}} || for {{c2::every \(v \in V\)}}. Prove that {{c5::{{c3::\(T-\sqrt{2} I\)}}}} is {{c5::{{c4::invertible}}}}.

============================================================

  

    6 Suppose \(u, v \in V\). Prove that {{c1::\(\langle u, v\rangle\)}} = {{c1::\(0\)}} if and only if<br><br><ul><li>{{c2::\(\|u\|\)}}&nbsp; {{c3::\(\leq\)}} {{c4::\(\|u+a v\|\)}}</li></ul><br>for {{c5::all \(a \in \mathbf{F}\)}}.<br>

============================================================

  

    7 Suppose \(u, v \in V\). Prove that {{c1::\(\|a u+b v\|\)}} = {{c2::\(\|b u+a v\|\)}} for {{c3::all \(a, b \in \mathbf{R}\)}} if and only if {{c4::\(\|u\|\)}} = {{c5::\(\|v\|\).}}

============================================================

  

    8 Suppose \(u, v \in V\) and {{c1::\(\|u\|\)}} ={{c2::\(\|v\| \)}} = {{c3::\(1\)}} and {{c4::\(\langle u, v\rangle\)}} = {{c3::\(1\)}}. Prove that {{c5::\(u\)}} = {{c5::\(v\)}}.

============================================================

  

    9 Suppose \(u, v \in V\) and \(\|u\|\)&nbsp;<br>\(\leq\) {{c1::\(1\)}} and \(\|v\|\) \(\leq\) {{c1::\(1\)}}. Prove that<br><br><ul><li>{{c2::\(\sqrt{1-\|u\|^{2} }\)}}{{c3::\(\sqrt{1-\|v\|^{2} }\)}} \( \leq\) {{c4::\( 1\)}}{{c1::-}}{{c5::\(|\langle u, v\rangle|\)}}</li></ul>

============================================================

  

    12 Prove that<br><br><ul><li>{{c1::\(\left(x_{1}+\cdots+x_{n}\right)^{2}\)}} {{c2::\(\leq\)}} {{c5::{{c3::\( n\left(x_{1}^{2}+\cdots+x_{n}^{2}\right)\)}}}}</li></ul><br>for {{c5::{{c4::all positive integers \(n\)}}}} and {{c5::{{c4::all real numbers \(x_{1}, \ldots, x_{n}\).}}}}<br>

============================================================

  

    13 Suppose \(u, v\) are nonzero vectors in \(\mathbf{R}^{2}\). Prove that<br><br><ul><li>{{c1::\(\langle u, v\rangle\)}}&nbsp;= {{c2::\(\|u\|\|v\| \cos \theta,\)}}</li></ul><br>where {{c5::{{c3::\(\theta\)}}}} is {{c5::{{c4::the angle between \(u\) and \(v\)}}}}&nbsp;<br>

============================================================

  

    14 The angle between two vectors (thought of as arrows with initial point at the origin) in \(\mathbf{R}^{2}\) or \(\mathbf{R}^{3}\) can be defined geometrically. However, geometry is not as clear in \(\mathbf{R}^{n}\) for \(n&gt;3\). Thus {{c4::the angle between two nonzero vectors \(x, y \in \mathbf{R}^{n}\)}} is defined to be<br><br><ul><li>{{c1::\(\arccos\)}}&nbsp;\((\) {{c5::{{c2::\( \frac{\langle x, y\rangle}{\|x\|\|y\|}\)}}}}&nbsp;\( )\)</li></ul><br>where the motivation for this definition comes from the previous exercise. Explain why the {{c5::{{c3::Cauchy-Schwarz Inequality}}}} is needed to show that this definition makes sense.<br>

============================================================

  

    15 Prove that<br><br><ul><li>{{c3::\(\left(\sum_{j=1}^{n} a_{j} b_{j}\right)^{2}\)&nbsp;}}</li><ul><li>{{c4::\( \leq\)}} {{c1::\(\left(\sum_{j=1}^{n} j a_{j}^{2}\right)\)}}{{c2::\(\left(\sum_{j=1}^{n} \frac{b_{j}^{2} }{j}\right)\)}}</li></ul></ul><br>for {{c5::all real numbers \(a_{1}, \ldots, a_{n}\)}} and {{c5::\(b_{1}, \ldots, b_{n}\).}}<br>

============================================================

  

    17 Prove or disprove: there is an {{c2::inner product on \(\mathbf{R}^{2}\)}} such that {{c1::the associated norm}} is given by<br><br><ul><li>{{c3::\(\|(x, y)\|\)}} = {{c4::\(\max \{x, y\}\)}}</li></ul><br>for {{c5::all \((x, y) \in \mathbf{R}^{2}\).}}<br>

============================================================

  

    18 Suppose \(p&gt;0\). Prove that there is an inner product on \(\mathbf{R}^{2}\) such that the associated norm is given by<br><br><ul><li>{{c1::\(\|(x, y)\|\)}}&nbsp;= {{c2::\(\left(x^{p}+y^{p}\right)^{1 / p}\)}}</li></ul><br>for {{c3::all \((x, y) \in \mathbf{R}^{2}\)}} if and only if {{c4::\(p\)}} = {{c5::\(2\)}}.<br>

============================================================

  

    19 Suppose \(V\) is a {{c1::real}} inner product space. Prove that<br><br><ul><li>{{c2::\(\langle u, v\rangle\)}}&nbsp;= {{c3::\(\frac{\|u+v\|^{2} }{4}\)}} - {{c4::\(\frac{\|u-v\|^{2} }{4}\)}}</li></ul><br>for {{c5::all \(u, v \in V\).}}<br>

============================================================

  

    &nbsp;Prove that a norm (in the abstract sense) satisfying the parallelogram equality comes from {{c1::an inner product}} (in other words, show that if \|\| is a norm on \(U\) satisfying the parallelogram equality, then there is an inner product \(\langle\),\(\rangle on U\) such that {{c2::\(\|u\|\)}} = {{c5::{{c3::\(\langle u, u\rangle^{1 / 2}\)}}}} for {{c5::{{c4::all \(\left.u \in U\right)\).}}}}

============================================================

  

    21 A norm on a vector space \(U\) is a function \|\|:&nbsp; \(U\)&nbsp; \(\rightarrow\) {{c1::\([0, \infty)\)}} such that:<br><ul><li>&nbsp;\(\|u\|\) = {{c2::\(0\)}} if and only if \(u\) = {{c1::\(0\)}}</li><li>{{c2::&nbsp;\(\|\alpha u\|\)}} = {{c3::\(|\alpha|\|u\|\)}}&nbsp;</li><li>&nbsp;{{c4::\(\|u+v\|\)}}&nbsp; {{c5::\(\leq\) \(\|u\|+\|v\|\)}}&nbsp;</li></ul>

============================================================

  

    22 Show that {{c1::the square}} of {{c2::an average}} is {{c3::less than or equal to}} {{c4::the average of the squares}}. More precisely, show that if \(a_{1}, \ldots, a_{n} \in \mathbf{R}\), then {{c5::the square of the average of \(a_{1}, \ldots, a_{n}\)}} is {{c5::less than or equal to the average of \(a_{1}^{2}, \ldots, a_{n}^{2}\).}}

============================================================

  

    23 Suppose \(V_{1}, \ldots, V_{m}\) are inner product spaces. Show that the equation<br><br><ul><li>\(\left\langle\left(u_{1}, \ldots, u_{m}\right),\left(v_{1}, \ldots, v_{m}\right)\right\rangle\)&nbsp;</li><li>={{c5::{{c1::\(\left\langle u_{1}, v_{1}\right\rangle\)}}}} + \(\cdots\)+ {{c4::{{c2::\(\left\langle u_{m}, v_{m}\right\rangle\)}}}}<br></li></ul><br>defines {{c4::{{c3::an inner product on \(V_{1} \times \cdots \times V_{m}\)}}}}.<br>

============================================================

  

    24 Suppose \(S \in \mathcal{L}(V)\) is an {{c1::injective::inj/surj?}} operator on \(V\). Define {{c2::\(\langle\cdot, \cdot\rangle_{1}\)}} by<br><br><ul><li>{{c3::\(\langle u, v\rangle_{1}\)}}&nbsp;= {{c4::\(\langle S u, S v\rangle\)}}</li></ul><br>for \(u, v \in V\). Show that {{c2::\(\langle\cdot, \cdot\rangle_{1}\)}} is {{c5::an inner product on \(V\).}}<br>

============================================================

  

    <img src="paste-f611c66c1bbf0c842ba5daaafa348a0e85977b3e.jpg"><br>25 Suppose \(S \in \mathcal{L}(V)\) is {{c5::{{c1::not injective}}}}. Define \(\langle\cdot, \cdot\rangle_{1}\) as in the exercise above. Explain why {{c4::{{c2::\(\langle\cdot, \cdot\rangle_{1}\)}}}} is {{c4::{{c3::not an inner product on \(V\).}}}}

============================================================

  

    26 Suppose \(f, g\) are {{c1::differentiable}} functions from \(\mathbf{R}\) to \(\mathbf{R}^{n}\).<br><br>(a) Show that<br><br><ul><li>{{c2::\(\langle f(t), g(t)\rangle^{\prime}\)}}= {{c3::\(\left\langle f^{\prime}(t), g(t)\right\rangle\)}}{{c4::+}}{{c5::\(\left\langle f(t), g^{\prime}(t)\right\rangle .\)}}</li></ul><br><br>

============================================================

  

    26 Suppose \(f\) is a differentiable function from \(\mathbf{R}\) to \(\mathbf{R}^{n}\).<br><br>(b) Suppose \(c\) {{c1::\(&gt;0\)}} and {{c2::\(\|f(t)\|\)}} = {{c3::\(c\)}} for {{c2::every \(t \in \mathbf{R}\)}}. Show that {{c4::\(\left\langle f^{\prime}(t), f(t)\right\rangle\)}} = {{c5::\(0\)}} for {{c2::every \(t \in \mathbf{R}\).}}

============================================================

  

    28 Suppose \(C\) is a subset of \(V\) with the property that \(u, v \in C\) implies {{c1::\(\frac{1}{2}(u+v)\)}}&nbsp; \(\in C\). Let \(w \in V\). Show that there is {{c2::at most one}} point in \(C\) that {{c3::is closest to \(w\)}}. In other words, show that there is {{c2::at most one \(u \in C\)}} such that<br><br><ul><li>{{c4::\(\|w-u\|\)}} {{c5::\(\leq\) \(\|w-v\|\)}} \( \quad \text { for all } v \in C .\)</li></ul>

============================================================

  

    29 For \(u, v \in V\), define \(d(u, v)=\|u-v\|\).<br><br>(c) Show that every finite-dimensional subspace of \(V\) is a {{c5::{{c1::closed}}}} {{c4::{{c2::subset of \(V\)}}}} (with respect to {{c4::{{c3::the metric \(d\)}}}} ).

============================================================

  

    30 Fix a positive integer \(n\). The {{c1::Laplacian}} \(\Delta p\) of a {{c2::twice differentiable}} function \(p\) on \(\mathbf{R}^{n}\) is the function on \(\mathbf{R}^{n}\) defined by<br><br><ul><li>\(\Delta p\)= {{c3::\(\frac{\partial^{2} p} {\partial x_{1}^{2} }\)}}+\(\cdots\)+{{c4::\(\frac{ \partial^{2}&nbsp; p} {\partial x_{n}^{2} } .\)}}</li></ul><br>The function \(p\) is called {{c5::harmonic}} if \(\Delta p\) = {{c5::\(0\)}}.<br>

============================================================

  

    30 Fix a positive integer \(n\). The {{c1::Laplacian}} \(\Delta p\) of a {{c2::twice differentiable}} function \(p\) on \(\mathbf{R}^{n}\) is the function on \(\mathbf{R}^{n}\) defined by<br><br><ul><li>\(\Delta p\)= {{c3::\(\frac{\partial^{2} p} {\partial x_{1}^{2} }\)}}+\(\cdots\)+{{c4::\(\frac{ \partial^{2}&nbsp; p} {\partial x_{n}^{2} } .\)}}</li></ul><br>The function \(p\) is called {{c5::harmonic}} if \(\Delta p\) = {{c5::\(0\)}}.<br>

============================================================

  

    30 Fix a positive integer \(n\). The Laplacian \(\Delta p\) of a twice differentiable function \(p\) on \(\mathbf{R}^{n}\) is the function on \(\mathbf{R}^{n}\) defined by<br><br>\[<br>\Delta p=\frac{\partial^{2} p}{\partial x_{1}^{2}}+\cdots+\frac{\partial^{2} p}{\partial x_{n}^{2}} .<br>\]<br><br>The function \(p\) is called harmonic if \(\Delta p=0\).<br><br>A polynomial on \(\mathbf{R}^{n}\) is a linear combination of functions of the form \(x_{1}{ }^{m_{1}} \ldots x_{n}{ }^{m_{n}}\), where \(m_{1}, \ldots, m_{n}\) are nonnegative integers.<br><br>Suppose \(q\) is a polynomial on \(\mathbf{R}^{n}\). <br>Prove that:<br><ul><li>There exists a {{c1::harmonic}} polynomial \(p\) on \(\mathbf{R}^{n}\) such that \(p(x)\) ={{c2::\(q(x)\)}} for {{c3::every \(x \in \mathbf{R}^{n}\)}} with {{c4::\(\|x\|\)}} ={{c5::\(1\)}}.</li></ul>

============================================================

  

    We can inject linearity into the norm definitions by introducing the dot product because {{c5::{{c1::if we fix a fector y in R^n}}}} then {{c4::{{c2::the map that takes x to a dot product with y}}}} is {{c4::{{c3::linear for all x}}}}

============================================================

  

    You should always put {{c5::{{c1::the absolute values}}}} into a {{c4::{{c2::norm}}}} formula in case {{c4::{{c3::complex numbers get involved}}}}

============================================================

  

    What does the &lt;u,0&gt; property follow from?<br><img src="paste-db6868fa4b6242ab69a7fed84f93397d401ede4a.jpg"><br><ul><li>From {{c5::{{c1::&lt;0,u&gt; = 0}}}}</li><li>+ {{c4::{{c2::Complex conjugate exchanging order}}}} since {{c4::{{c3::the complex conjugate of 0 is 0}}}}</li></ul>

============================================================

  

    What does the final property imply regarding real vs complex vector spaces?<br><img src="paste-7eeba26fa9e9fe5616076b6054222689b98633d5.jpg"><br><ul><li>That {{c1::homogeneity}} and thus {{c2::linearity}} {{c3::in the second slot}} can {{c4::only be achieved}} for {{c5::real}} vector spaces</li></ul>

============================================================

  

    The {{c5::{{c1::cosine angle}}}} formula {{c4::{{c3::only holds for}}}} {{c4::{{c2::R^2}}}}

============================================================

  

    A {{c2::basis}} is a {{c1::sufficient}} choice to determine an {{c5::{{c3::isomorphism between V and its dual V'}}}} and {{c1::sufficient}} to {{c5::{{c4::provide a definition for the norm}}}}

============================================================

  

    It is not necessary to define {{c1::a basis}} or the {{c2::dual space}} in order to obtain {{c5::{{c3::the length}}}} {{c5::{{c4::of a vector}}}}

============================================================

  

    {{c1::Composing vectors with their duals}} is {{c2::not}} {{c5::{{c3::the simples way of defining a dot product}}}} because it {{c5::{{c4::depends on a basis}}}}

============================================================

  

    Ideally we would like an inner product to be {{c5::{{c1::bilinear}}}} rather than {{c4::{{c2::sequilinear( linear only in the first coordinate)}}}}, but this is only possible on {{c4::{{c3::real}}}} vector spaceses

============================================================

  

    While a {{c1::basis}} can help define {{c2::an inner product}} via {{c3::composing vectors with their duals}}, {{c2::an inner product}} {{c4::can exist independent of}} {{c1::a basis}}. This is why we can {{c5::talk about the length of vectors in R^3 without specifying a basis}}

============================================================

  

    For the standard dot product in R^n:<br><ul><li>{{c1::\((x+y) \cdot z\)}} =&nbsp;{{c2::\(x \cdot z +y \cdot z\)}}<br></li><li>{{c5::{{c3::\(x \cdot (y+z) \)}}}} =&nbsp;{{c5::{{c4::\(x \cdot y + x \cdot z\)}}}}<br></li></ul>

============================================================

  

    An inner product is a {{c5::{{c1::function}}}} which takes {{c4::{{c2::vectors}}}} to {{c4::{{c3::field elements}}}}

============================================================

  

    {{c3::Additivity}} and {{c4::homogeneity}} say that the inner product is {{c5::{{c1::linear}}}} {{c5::{{c2::in the first slot}}}}

============================================================

  

    What does conjugate symmetry say about an inner product?<br><ul><li>We can {{c5::{{c1::reverse the order of the vectors}}}} as long as we {{c4::{{c2::take the conjugate of}}}} {{c4::{{c3::the outputed complex number}}}}</li></ul>

============================================================

  

    What happens to addivity and homogeneity the second slot of an inner product?<br>Properties:<br><ul><li>Additivity: is {{c5::{{c1::preserverd}}}}</li><li>Homogeneity: is {{c4::{{c2::conjugate linear}}}} {{c4::{{c3::because the scalar gets turned to the complex conjugate}}}}</li></ul>

============================================================

  

    How can we think of the following as a matrix multiplication?<br><img src="paste-ba9be1c56942edd0fa1e61920eac8d1023c0f4e8.jpg"><br>Answer:<br><ul><li>{{c5::{{c1::<img src="paste-23058756cb2fb9444401b9ae8223e78a01d89e67.jpg">}}}}<br></li><li>Where C is a {{c4::{{c2::diagonal}}}} matrix</li><li>And y is {{c4::{{c3::conjugated}}}}</li></ul>

============================================================

  

    For a matrix C to be be positive definite the following equation must be obeyed:<br><ul><li>{{c1::\(v^{t}\)}} {{c2::C}} {{c3::\(\bar{v} \)}}&nbsp;{{c4::\(\geq\)}}&nbsp;{{c5::\(0\)}} if&nbsp;{{c5::\(v \neq =0\)}}<br></li></ul>

============================================================

  

    How can we define a simple inner product for polynomials without any exponentials?<br><ul><li>Just {{c5::{{c1::integrate}}}} and {{c4::{{c2::conjugate the second polynomial on [0,1]}}}}</li><li>{{c4::{{c3::<img src="paste-872453dbaba679ccf0616d542a88792b63f534ac.jpg">}}}}<br></li></ul>

============================================================

  

    Given an inner product, we can define a norm such that:<br><ul><li>{{c1::\(\vert\vert v \vert\vert\)}} =&nbsp;{{c2::\(\sqrt{\langle v,v\rangle}\)}}</li><li>Note that the axioms of the inner product make it clear that&nbsp;{{c3::\(\vert\vert v \vert\vert\)}} =&nbsp; {{c4::\(0 \)}} requires that&nbsp;{{c5::\(v=0\)}}<br><br></li></ul>

============================================================

  

    {{c1::Taking the squared norm}} {{c2::of a vector times a scalar}} is equivalent to {{c5::{{c3::multiplying the squared norm}}}} {{c5::{{c4::by the squared norm of the scalar}}}}

============================================================

  

    The norm of {{c1::a scalar}} {{c5::times}} {{c2::a vector}} equals {{c3::the norm of the scalar}} {{c5::times}} {{c4::the norm of the vector}}

============================================================

  

    Two vectors in an {{c5::{{c3::inner product}}}} space are {{c4::{{c1::orthogonal}}}} if {{c4::{{c2::their inner product is 0}}}}

============================================================

  

    The 0 vector is {{c5::{{c1::orthogonal}}}} {{c4::{{c2::to all vectors}}}} {{c4::{{c3::including itself}}}}

============================================================

  

    {{c5::{{c1::Additivity}}}} for an {{c4::{{c3::inner product}}}} applies to {{c4::{{c2::both slots}}}}

============================================================

  

    {{c5::{{c1::Homogeneity}}}} for an {{c4::{{c2::inner product}}}} applies {{c4::{{c3::only on the first slot}}}}

============================================================

  

    To make sure that R^2 has a {{c1::single cannonical}} {{c2::direct sum decomposition}}, we must enforce {{c5::{{c3::the vectors in the subspaces}}}} {{c5::{{c4::to be orthogonal to one another}}}}

============================================================

  

    To get an orthogonal decomposition for a nonzero vector u:<br><ul><li>u = {{c5::{{c1::av + (u -av)}}}}</li><li>We can just set</li><li>{{c4::{{c2::<img src="paste-b53729bf65600ba85a3cc748a6a45243bb77debc.jpg">}}}}<br></li><li>Because of this equation for orthogonality</li><li>{{c4::{{c3::<img src="paste-841614acd9df683459d17c9935a1b62bc06fe51a.jpg">}}}}<br></li></ul>

============================================================

  

    {{c5::Two vectors}} being {{c4::orthogonal}} implies that {{c1::all vectors}} {{c2::in their span}} {{c3::are orthogonal}}

============================================================

  

    Explain this first part of the orthogonal decomposition formula via a metaphor?<br><img src="paste-ce86ba28b97c39f8494c6293af121fac146a1f02.jpg"><br>Metaphor:<br><ul><li>The inner product means {{c1::the amount of u}} {{c2::which is the direction v}}</li><li>The {{c3::norm}} just {{c4::normalizes}} this {{c5::by the length of v}}</li></ul>

============================================================

  

    A list of vectors&nbsp;\(\vec{v}\) is called {{c2::orthonormal}} if&nbsp;<br><ul><li>{{c3::\(\langle v_i,v_j \rangle\)}}&nbsp;</li><li>=&nbsp;{{c5::{{c4::\(\delta_{i,j}\)}}}}&nbsp;&nbsp;</li><li>{{c5::{{c1::1 if i&nbsp; j and 0 if&nbsp;\(i\neq j\)}}}}</li></ul>

============================================================

  

    A list of vectors&nbsp;\(\vec{v}\) is called {{c2::orthonormal}} if&nbsp;<br><ul><li>{{c3::\(\langle v_i,v_j \rangle\)}}&nbsp;</li><li>=&nbsp;{{c5::{{c4::\(\delta_{i,j}\)}}}}&nbsp;&nbsp;</li><li>{{c5::{{c1::1 if i&nbsp; j and 0 if&nbsp;\(i\neq j\)}}}}</li></ul>

============================================================

  

    A list of vectors is orthonormal:<br><ul><li>If they are {{c1::pairwise orthogonal}} for {{c2::distinct vectors}}</li><li>And {{c5::{{c3::each vector}}}} has {{c5::{{c4::norm 1}}}}</li></ul>

============================================================

  

    The {{c5::{{c3::standard basis of R^3}}}} is {{c4::{{c2::cannonical}}}} because it is {{c4::{{c1::orthonormal}}}}

============================================================

  

    If&nbsp;\(\vec{v}\) is an orthonormal list with length m:<br><ul><li>For any vector x in V</li><li>{{c5::{{c1::\(\vert\vert \vec{c} \cdot \vec{v} \vert\vert^2 \)}}}} =&nbsp;{{c4::{{c2::\(\sum c_i^2 \)}}}}<br></li><li>By the {{c4::{{c3::pythagorean}}}} theorem</li></ul>

============================================================

  

    If&nbsp;\(\vec{v}\) is an orthonormal list with length m:<br><ul><li>\(\vert\vert \vec{c} \cdot \vec{v} \vert\vert^2 \) =&nbsp;\(\sum c_i^2 \)<br></li><li>By the pythagorean theorem</li><ul><li>Since&nbsp;{{c1::\(\vert\vert c_i v_i \vert\vert^2\)}} =&nbsp;{{c2::\(\vert\vert c_i \vert\vert^2\)&nbsp;\(\vert\vert v_i \vert\vert^2\)}} = {{c3::\(\vert\vert c_i \vert\vert^2\)}}</li><li>And {{c4::all the vectors in the list are orthogonal}} so {{c5::we can induction it}}</li></ul></ul>

============================================================

  

    <b>Definition</b>&nbsp;{{c1::orthonormal}}<br><br>- A list of vectors is called {{c1::orthonormal}} if {{c3::each vector}} in the list {{c2::has norm 1}} and {{c3::is orthogonal to all the other vectors in the list.}}<br><br>- In other words, a list \(e_{1}, \ldots, e_{m}\) of vectors in \(V\) is {{c1::orthonormal}} if<br><br><ul><li>{{c4::\(\left\langle e_{j}, e_{k}\right\rangle\)}}&nbsp;= {{c5::\( \begin{cases}1 &amp; \text { if } j =k, \\ 0 &amp; \text { if } j \neq k\end{cases}\)}}</li></ul>

============================================================

  

    The norm of an {{c1::orthonormal}} {{c2::linear combination}}<br><br>If \(e_{1}, \ldots, e_{m}\) is an {{c1::orthonormal}} list of vectors in \(V\), then<br><br><ul><li>{{c5::{{c3::\(\left\|a_{1} e_{1}+\cdots+a_{m} e_{m}\right\|^{2}\)}}}} ={{c5::{{c4::\(\left|a_{1}\right|^{2}+\cdots+\left|a_{m}\right|^{2}\)}}}}</li></ul><br>for {{c5::{{c4::all \(a_{1}, \ldots, a_{m} \in \mathbf{F}\).}}}}<br>

============================================================

  

    <img src="paste-bb774a6a85c05ac46cd141dfe89a60f3134793bb.jpg"><br><br>Proof Because each \(e_{j}\) has {{c5::{{c1::norm 1,}}}} this follows easily from {{c4::{{c3::repeated applications of}}}} {{c4::{{c2::the Pythagorean Theorem (6.13)}}}}.

============================================================

  

    <img src="paste-e0586b195c66fcbc4fd4eb849d77f9a9ee71287c.jpg"><br><br>The result above has the following important corollary:<br><br><ul><li><br></li><li>{{c5::{{c3::Every}}}} {{c4::{{c1::orthonormal list of vectors}}}} is {{c4::{{c2::linearly independent}}}}.</li></ul>

============================================================

  

    <br><img src="paste-e6c142620eca603031853f00dc00bebb31bf2cd1.jpg"><br>Proof: Suppose \(e_{1}, \ldots, e_{m}\) is an orthonormal list of vectors in \(V\) and \(a_{1}, \ldots, a_{m} \in \mathbf{F}\) are such that<br><br><ul><li>{{c1::\(a_{1} e_{1}+\cdots+a_{m} e_{m}\)}}&nbsp;= {{c2::\(0 .\)}}</li></ul><br>Then:<br><ul><li>&nbsp;{{c3::\(\left|a_{1}\right|^{2}+\cdots+\left|a_{m}\right|^{2}\)}} = {{c4::\(0\)}} ,&nbsp;</li><li>Which means that {{c5::all the \(a_{j}\) 's are 0}} .&nbsp;</li><li>Thus \(e_{1}, \ldots, e_{m}\) is linearly independent.</li></ul>

============================================================

  

    An {{c5::{{c1::orthonormal}}}} list of {{c4::{{c2::the right length}}}} is an {{c1::orthonormal}} {{c4::{{c3::basis}}}}<br><br>Every {{c1::orthonormal}} list of vectors in \(V\) with {{c4::{{c2::length \(\operatorname{dim} V\)}}}} is an {{c1::orthonormal}} {{c4::{{c3::basis of \(V\)}}}}.

============================================================

  

    <img src="paste-40b9cbbebf36acc9276c669c51d1742568636a3d.jpg"><br>Proof:<br><ul><li>Orthonormal lists are {{c5::{{c1::linearly independent}}}}</li><li>A {{c4::{{c2::lin indp list of the right length}}}} is {{c4::{{c3::a basis}}}}</li></ul>

============================================================

  

    Writing a vector as linear combination of orthonormal basis<br><br>Suppose \(e_{1}, \ldots, e_{n}\) is an orthonormal basis of \(V\) and \(v \in V\). Then<br><br><ul><li>{{c1::\(v\)}} = {{c2::\(\left\langle v, e_{1}\right\rangle e_{1}+\cdots+\left\langle v, e_{n}\right\rangle e_{n}\)}}</li></ul><br>and<br><br><ul><li>{{c5::{{c3::\(\|v\|\)\(^{2}\)}}}}= {{c5::{{c4::\(\left|\left\langle v, e_{1}\right\rangle\right|^{2}+\cdots+\left|\left\langle v, e_{n}\right\rangle\right|^{2} .\)}}}}</li></ul>

============================================================

  

    <img src="paste-49a105300a311d7f6d4fc6b366d22406dcae3bdc.jpg"><br>Proof Because \(e_{1}, \ldots, e_{n}\) is a basis of \(V\), there exist scalars \(a_{1}, \ldots, a_{n}\) such that<br><br>\[<br>v=a_{1} e_{1}+\cdots+a_{n} e_{n} .<br>\]<br><br><ul><li>Because \(e_{1}, \ldots, e_{n}\) is orthonormal, {{c1::taking the inner product of both sides of this equation}} {{c2::with \(e_{j}\)}} gives:&nbsp;</li><li>{{c3::\(\left\langle v, e_{j}\right\rangle\)}} = {{c4::\(a_{j}\)}}.&nbsp;</li><li>Thus the first equation in 6.30 holds.</li></ul><br>The second equation in 6.30 follows immediately from the first equation and {{c5::the norm equation for orthonormal linear combinations}}<br>

============================================================

  

    <img src="paste-49a105300a311d7f6d4fc6b366d22406dcae3bdc.jpg"><br>Proof Because \(e_{1}, \ldots, e_{n}\) is a basis of \(V\), there exist scalars \(a_{1}, \ldots, a_{n}\) such that<br><br>\[<br>v=a_{1} e_{1}+\cdots+a_{n} e_{n} .<br>\]<br><br><ul><li>Because \(e_{1}, \ldots, e_{n}\) is orthonormal, {{c1::taking the inner product of both sides of this equation}} with&nbsp;{{c2::\(e_{j}\)}} gives:&nbsp;</li><li>{{c3::\(\left\langle v, e_{j}\right\rangle\)}} = {{c4::\(a_{j}\)}}.&nbsp;</li><li>Thus the first equation in 6.30 holds.</li></ul><br>The second equation in 6.30 follows immediately from the first equation and {{c5::the norm equation for orthonormal linear combinations}}<br>

============================================================

  

    The Gram-Schmidt Procedure gives a method for turning a {{c5::{{c1::linearly independent}}}} list into an {{c4::{{c2::orthonormal}}}} list with {{c4::{{c3::the same span as the original list}}}}.

============================================================

  

    <b>Gram-Schmidt Procedure</b><br><br>Suppose \(v_{1}, \ldots, v_{m}\) is a linearly independent list of vectors in \(V\).:<br><ul><li>&nbsp;Let \(e_{1}\) = {{c1::\(v_{1} \) \(/\) \(\left\|v_{1}\right\|\).&nbsp;}}</li><li>For \(j=2, \ldots, m\), define \(e_{j}\) inductively by</li><ul><li>\(e_{j}\) =&nbsp;</li><ul><li>{{c2::\( (v_{j}-\left\langle v_{j}, e_{1}\right\rangle e_{1}- \) \(\cdots\) \(-\left\langle v_{j}, e_{j-1}\right\rangle e_{j-1})\)&nbsp;}}</li><li>Operation: {{c3::Divided by}}</li><li>{{c4::\(\|v_{j}-\left\langle v_{j}, e_{1}\right\rangle e_{1}-\) \(\cdots\) \(-\left\langle v_{j}, e_{j-1}\right\rangle e_{j-1}\| .\)}}</li></ul></ul></ul><br>Then \(e_{1}, \ldots, e_{m}\) is an orthonormal list of vectors in \(V\) such that<br><br><ul><li>{{c5::\(\operatorname{span}\left(v_{1}, \ldots, v_{j}\right)\)}}&nbsp;= {{c5::\(\operatorname{span}\left(e_{1}, \ldots, e_{j}\right)\)}}</li></ul><br>for \(j=1, \ldots, m\).<br>

============================================================

  

    \subsection{Gram-Schmidt Procedure}<br><br>Suppose \(v_{1}, \ldots, v_{m}\) is a linearly independent list of vectors in \(V\). Let \(e_{1}=v_{1} /\left\|v_{1}\right\|\). For \(j=2, \ldots, m\), define \(e_{j}\) inductively by<br><br><ul><li>\(e_{j}\) =&nbsp;</li><ul><li>{{c1::\((v_{j}-\left\langle v_{j}, e_{1}\right\rangle e_{1}\)}} {{c5::\(-\)}}&nbsp;</li><li>\(\cdots\)&nbsp;</li><li>{{c5::\(-\)}} {{c2::\(\left\langle v_{j}, e_{j-1}\right\rangle e_{j-1}) \)}}&nbsp;&nbsp;</li><li>{{c5::\(/\)&nbsp;&nbsp;}}</li><ul><li>{{c3::\(\|v_{j}-\left\langle v_{j}, e_{1}\right\rangle e_{1} \)}} {{c5::\(-\)}}&nbsp;</li><li>\(\cdots\)&nbsp;</li><li>{{c5::\(-\)}} {{c4::\(\left\langle v_{j}, e_{j-1}\right\rangle e_{j-1}\| .\)}}</li></ul></ul></ul>

============================================================

  

    Compact version of gram-schmidt process for orthonormal basis creation:<br><ul><li>\(e_{1}\) = {{c1::\(v_{1} /\left\|v_{1}\right\|\)}}.<br></li><li>\(e_j\) =&nbsp;{{c2::\(v_j\)}} {{c3::\(-\)}}&nbsp;{{c4::\((\sum_{i=1}^{j-1} \left\langle v_j, e_i\right\rangle e_i) \)}}&nbsp;{{c5::\(/\)}}&nbsp;\(\) {{c2::\(\|v_j\)}} {{c3::\(-\)}}&nbsp; {{c4::\((\sum_{i=1}^{j-1} \left\langle v_j, e_i\right\rangle e_i)&nbsp;\|\)}}<br></li></ul>

============================================================

  

    <img src="paste-a9f5908cd8a6d0004a5aa232b3985ef5b4d9a9c8.jpg"><br><br>Proof We will show by induction on \(j\) that the desired conclusion holds. To get started with \(j=1\), note that {{c1::\(\operatorname{span}\left(v_{1}\right)\)}} = {{c1::\(\operatorname{span}\left(e_{1}\right)\)}} because {{c2::\(v_{1}\) is a positive multiple of \(e_{1}\).}}<br><br>Suppose \(1&lt;j&lt;m\) and we have verified that<br><br>6.32<br><br>\[<br>\operatorname{span}\left(v_{1}, \ldots, v_{j-1}\right)=\operatorname{span}\left(e_{1}, \ldots, e_{j-1}\right) .<br>\]<br><br>Note that \(v_{j} \notin \operatorname{span}\left(v_{1}, \ldots, v_{j-1}\right)\) (because \(v_{1}, \ldots, v_{m}\) is linearly independent). Thus \(v_{j} \notin \operatorname{span}\left(e_{1}, \ldots, e_{j-1}\right)\). Hence we are {{c3::not dividing by 0 in the definition of \(e_{j}\) given in 6.31}}. Dividing a vector by its norm produces a new vector with {{c4::norm 1}} ; thus \(\left\|e_{j}\right\|\) = {{c4::\(1\)}}.<br><br>Let \(1 \leq k&lt;j\). Then<br><br><ul><li>\(</li><li><br></li><li>\left\langle e_{j}, e_{k}\right\rangle\)</li><li>&nbsp; = \(\left\langle\frac{v_{j}-\left\langle v_{j}, e_{1}\right\rangle e_{1}-\cdots-\left\langle v_{j}, e_{j-1}\right\rangle e_{j-1}}{\left\|v_{j}-\left\langle v_{j}, e_{1}\right\rangle e_{1}-\cdots-\left\langle v_{j}, e_{j-1}\right\rangle e_{j-1}\right\|}, e_{k}\right\rangle \)</li><li>&nbsp;= {{c5::\(\frac{\left\langle v_{j}, e_{k}\right\rangle-\left\langle v_{j}, e_{k}\right\rangle}{\left\|v_{j}-\left\langle v_{j}, e_{1}\right\rangle e_{1}-\cdots-\left\langle v_{j}, e_{j-1}\right\rangle e_{j-1}\right\|} \)}}</li><li>&nbsp;= {{c5::\(0 .\)}}</li></ul><br>Thus \(e_{1}, \ldots, e_{j}\) is an orthonormal list.<br>

============================================================

  

    Existence of {{c1::orthonormal}} {{c5::basis}}<br><br>Every {{c2::finite}}-dimensional {{c3::inner product space}} has an {{c4::orthonormal}} {{c5::basis}}.

============================================================

  

    <img src="paste-76a9a7c35aedfe8ef636dcc0b4b6658d8b7e340c.jpg"><br>Proof:<br><ul><li>{{c5::{{c1::Choose a basis&nbsp;}}}}</li><li>{{c4::{{c2::Apply Gram-Schmidt process}}}} {{c4::{{c3::to get an orthonormal basis with the same span}}}}</li></ul>

============================================================

  

    {{c1::Orthonormal}} {{c2::list}} extends to {{c1::orthonormal}} {{c5::{{c3::basis}}}}<br><br>Suppose \(V\) is finite-dimensional. Then every {{c1::orthonormal}} {{c2::list of vectors in \(V\)}} can {{c5::{{c4::be extended to an}}}} {{c1::orthonormal}} {{c5::{{c3::basis of \(V\).}}}}

============================================================

  

    {{c1::Upper-triangular}} matrix with respect to {{c2::orthonormal}} {{c4::basis}}<br><br>Suppose \(T \in \mathcal{L}(V)\). If \(T\) has an {{c1::upper-triangular}} matrix with respect to {{c4::some basis of \(V\)}}, then \(T\) has an {{c3::upper-triangular}} matrix with respect to {{c5::some}} {{c2::orthonormal}} {{c5::basis of \(V\)}}.

============================================================

  

    Why is the first reduction justified? In the proof of Gram-Schmidt<br><img src="paste-0448a057c52d7e682e35c5d48e55869c0ccc7c30.jpg"><br>Ans:<br><ul><li>{{c1::All&nbsp;\(e_j, j \neq k\)}} {{c2::are ortogonal to&nbsp;\(e_k\)}} {{c3::except for&nbsp;\(e_k\)}}</li><li>Thus {{c4::all other elements}} {{c5::reduce to 0}}</li></ul>

============================================================

  

    <img src="paste-a61abfd3f93d0e9d4db3cb3eee2744d97162d572.jpg"><br>Proof:<br><ul><li>{{c1::\(\operatorname{span}(v_1,\ldots,v_j)\)}} is i{{c2::nvariant under T}} for {{c3::each&nbsp;\(j=1,\ldots,n\)}}<br></li><li>{{c4::Gram-Schimdt}} {{c5::produces only lists of vectors with identical spans at every step}}</li><li>Thus {{c5::the basis can be converted to an orthonormal basis}} without {{c5::losing the upper-triangular shape}}</li></ul>

============================================================

  

    {{c1::Schur's}} Theorem<br><br>Suppose \(V\) is a finite-dimensional complex vector space and \(T \in \mathcal{L}(V)\). Then \(T\) has an {{c2::upper-triangular}} matrix {{c3::with respect to}} {{c4::some}} {{c5::orthonormal}} {{c4::basis of \(V\).}}

============================================================

  

    Definition {{c5::{{c1::linear functional}}}}<br><br>A {{c1::linear functional}} on \(V\) is a {{c4::{{c2::linear map from \(V\) to \(\mathbf{F}\)}}}}. In other words, a {{c1::linear functional}} is {{c4::{{c3::an element of \(\mathcal{L}(V, \mathbf{F})\).}}}}

============================================================

  

    If {{c4::\(u \in V\)}}, then the map that sends {{c1::\(v\)}} to {{c5::{{c2::\(\langle v, u\rangle\)}}}} is a {{c5::{{c3::linear functional}}}} on \(V\).

============================================================

  

    {{c1::Riesz Representation}} Theorem<br><br>Suppose \(V\) is finite-dimensional and \(\varphi\) is a linear functional on \(V\). Then there is a {{c2::unique vector \(u \in V\)}} such that<br><br><ul><li>{{c3::\(\varphi(v)\)}} = {{c4::\(\langle v, u\rangle\)}}</li></ul><br>for {{c5::every \(v \in V\).}}<br>

============================================================

  

    <img src="paste-ab614a105cf4a82e7934b5e216070bee5202a5a0.jpg"><br>Proof First we show there exists a vector \(u \in V\) such that \(\varphi(v)=\langle v, u\rangle\) for every \(v \in V\). Let \(e_{1}, \ldots, e_{n}\) be an orthonormal basis of \(V\). Then<br><ul><li>\(\varphi(v)\)&nbsp;</li><li>= {{c1::\(\varphi\left(\left\langle v, e_1\right\rangle e_1+\cdots+\left\langle v, e_n\right\rangle e_n\right)\)}}<br></li><li>=&nbsp;{{c2::\(\left\langle v, e_1\right\rangle \varphi\left(e_1\right)+\cdots+\left\langle v, e_n\right\rangle \varphi\left(e_n\right)\)}}</li><li>= {{c5::{{c3::\(\left\langle v, \overline{\varphi\left(e_1\right)} e_1+\cdots+\overline{\varphi\left(e_n\right)} e_n\right\rangle\)}}}}<br></li><li>Thus setting&nbsp;</li><li>\(u\) = {{c5::{{c4::\(\overline{\varphi\left(e_1\right)} e_1+\cdots+\overline{\varphi\left(e_n\right)} e_n\)}}}}<br></li><li>Satisfies the condition</li></ul>

============================================================

  

    <img src="paste-cfcd6e590022102504c02a0eedc7c40ebaec55ef.jpg"><br><br>Proof First we show there exists a vector \(u \in V\) such that \(\varphi(v)=\langle v, u\rangle\) for every \(v \in V\). Let \(e_{1}, \ldots, e_{n}\) be an orthonormal basis of \(V\). Then<br><br>\[<br>\begin{aligned}<br>\varphi(v) &amp; =\varphi\left(\left\langle v, e_{1}\right\rangle e_{1}+\cdots+\left\langle v, e_{n}\right\rangle e_{n}\right) \\<br>&amp; =\left\langle v, e_{1}\right\rangle \varphi\left(e_{1}\right)+\cdots+\left\langle v, e_{n}\right\rangle \varphi\left(e_{n}\right) \\<br>&amp; =\left\langle v, \overline{\varphi\left(e_{1}\right)} e_{1}+\cdots+\overline{\varphi\left(e_{n}\right)} e_{n}\right\rangle<br>\end{aligned}<br>\]<br><br>for every \(v \in V\), where the first equality comes from 6.30. Thus setting<br><br>\[<br>u=\overline{\varphi\left(e_{1}\right)} e_{1}+\cdots+\overline{\varphi\left(e_{n}\right)} e_{n},<br>\]<br><br>we have \(\varphi(v)=\langle v, u\rangle\) for every \(v \in V\), as desired.<br><br>Now we prove that only one vector \(u \in V\) has the desired behavior. Suppose \(u_{1}, u_{2} \in V\) are such that<br><br>\[<br>\varphi(v)=\left\langle v, u_{1}\right\rangle=\left\langle v, u_{2}\right\rangle<br>\]<br><br>for every \(v \in V\). Then<br><br><ul><li>{{c1::\(0\)}}&nbsp;= {{c2::\(\left\langle v, u_{1}\right\rangle-\left\langle v, u_{2}\right\rangle\)}} = {{c3::\(\left\langle v, u_{1}-u_{2}\right\rangle\)}}</li></ul><br>for every \(v \in V\). Taking \(v\) = {{c4::\(u_{1}-u_{2}\)}} shows that {{c5::\(u_{1}-u_{2}\)}} = {{c5::\(0\)}}. In other words, {{c5::\(u_{1}=u_{2}\)}}, completing the proof of the uniqueness part of the result.<br>

============================================================

  

    Suppose \(V\) is finite-dimensional and \(\varphi\) a linear functional on \(V\). Then 6.43 gives a formula for the vector \(u\) that satisfies \(\varphi(v)=\langle v, u\rangle\) for all \(v \in V\). Specifically, we have<br><br><ul><li>\(u\) = {{c1::\(\overline{\varphi\left(e_{1}\right)} e_{1}\)}} {{c2::\(+\)}}{{c5::{{c4::\(\cdots\)}}}}{{c2::\(+\)}} {{c5::{{c3::\(\overline{\varphi\left(e_{n}\right)} e_{n} .\)}}}}</li></ul>

============================================================

  

    1 (a) Suppose \(\theta \in \mathbf{R}\). Show that {{c1::\((\cos \theta, \sin \theta)\)}} ,{{c2::\((-\sin \theta, \cos \theta)\)}} and {{c3::\((\cos \theta, \sin \theta)\)}},{{c4::\((\sin \theta,-\cos \theta)\)}} are {{c5::orthonormal bases of \(\mathbf{R}^{2}\).}}

============================================================

  

    1 (a) Suppose \(\theta \in \mathbf{R}\). Show that \((\cos \theta, \sin \theta),(-\sin \theta, \cos \theta)\) and \((\cos \theta, \sin \theta),(\sin \theta,-\cos \theta)\) are orthonormal bases of \(\mathbf{R}^{2}\).<br><br>(b) Show that {{c5::{{c1::each orthonormal basis of \(\mathbf{R}^{2}\)}}}} is {{c4::{{c2::of the form given}}}} {{c4::{{c3::by one of the two possibilities of part (a).}}}}

============================================================

  

    2 Suppose \(e_{1}, \ldots, e_{m}\) is an orthonormal list of vectors in \(V\). Let \(v \in V\). Prove that<br><br><ul><li>{{c1::\(\|v\|^{2}\)}}&nbsp; = {{c2::\(\left|\left\langle v, e_{1}\right\rangle\right|^{2}+\cdots+\left|\left\langle v, e_{m}\right\rangle\right|^{2}\)}}</li></ul><br>if and only if {{c5::{{c3::\(v\)}}}}&nbsp; \(\in\) {{c5::{{c4::\(\operatorname{span}\left(e_{1}, \ldots, e_{m}\right)\).}}}}<br>

============================================================

  

    10 Suppose \(V\) is a real inner product space and \(v_{1}, \ldots, v_{m}\) is a linearly independent list of vectors in \(V\). Prove that:<br><ul><li>&nbsp;There exist exactly {{c1::\(2^{m}\)}} {{c2::orthonormal}} lists \(e_{1}, \ldots, e_{m}\) of vectors in \(V\) such that</li><li>{{c3::\(\operatorname{span}\left(v_{1}, \ldots, v_{j}\right)\)}} = {{c4::<ul><li>\(\operatorname{span}\left(e_{1}, \ldots, e_{j}\right)\)</li></ul>}}</li></ul>for {{c5::all \(j \in\{1, \ldots, m\}\).}}<br>

============================================================

  

    11 Suppose \(\langle\cdot, \cdot\rangle_{1}\) and \(\langle\cdot, \cdot\rangle_{2}\) are inner products on \(V\) such that:<br><ul><li>{{c1::&nbsp;\(\langle v, w\rangle_{1}\)}} = {{c2::\(0\)}} if and only if {{c1::\(\langle v, w\rangle_{2}\)}} = {{c2::\(0\)}}.&nbsp;</li><li>Prove that:</li><ul><li>&nbsp;There is a {{c3::positive number \(c\)}} such that&nbsp;</li><li>{{c4::\(\langle v, w\rangle_{1}\)}} ={{c5::\(c\langle v, w\rangle_{2}\)}} for {{c5::every \(v, w \in V\).}}</li></ul></ul>

============================================================

  

    12 Suppose \(V\) is finite-dimensional and \(\langle\cdot, \cdot\rangle_{1},\langle\cdot, \cdot\rangle_{2}\) are inner products on \(V\) with corresponding norms \(\|\cdot\|_{1}\) and \(\|\cdot\|_{2}\). <br><ul><li>Prove that there exists {{c5::a positive number \(c\)}} such that</li><li>{{c1::\(\|v\|_{1}\)}} {{c2::\(\leq\)}} {{c3::\(c\|v\|_{2}\)}}</li><li>for {{c4::every \(v \in V\).}}</li></ul>

============================================================

  

    13 Suppose \(v_{1}, \ldots, v_{m}\) is a {{c1::linearly independent}} list in \(V\). Show that there exists {{c2::\(w \in V\)}} such that {{c3::\(\left\langle w, v_{j}\right\rangle\)}} {{c4::&gt;}} {{c5::\(0\)}} for {{c5::all \(j \in\{1, \ldots, m\}\).}}

============================================================

  

    14 Suppose \(e_{1}, \ldots, e_{n}\) is an {{c1::orthonormal}} basis of \(V\) and \(v_{1}, \ldots, v_{n}\) are vectors in \(V\) such that<br><br><ul><li>{{c2::\(\left\|e_{j}-v_{j}\right\|\)}} {{c3::\(&lt;\)}} {{c4::\(\frac{1}{\sqrt{n} }\)}}</li></ul><br>for {{c4::each \(j\)}}. <br><ul><li>Prove that \(v_{1}, \ldots, v_{n}\) is {{c5::a basis of \(V\).}}</li></ul>

============================================================

  

    15 Suppose \(C_{\mathbf{R}}([-1,1])\) is the vector space of continuous real-valued functions on the interval \([-1,1]\) with inner product given by<br><br>\[<br>\langle f, g\rangle=\int_{-1}^{1} f(x) g(x) d x<br>\]<br><br>for \(f, g \in C_{\mathbf{R}}([-1,1])\). Let \(\varphi\) be the linear functional on \(C_{\mathbf{R}}([-1,1])\) defined by \(\varphi(f)=f(0)\). Show that there does not exist \(g \in C_{\mathbf{R}}([-1,1])\) such that<br><br>\[<br>\varphi(f)=\langle f, g\rangle<br>\]<br><br>for every \(f \in C_{\mathbf{R}}([-1,1])\).<br><br>[The exercise above shows that the Riesz Representation Theorem (6.42) {{c5::{{c2::does not hold}}}} on {{c4::{{c1::infinite}}}}-dimensional vector spaces {{c4::{{c3::without additional hypotheses on \(V\) and \(\varphi\).}}}}]

============================================================

  

    15 Suppose \(C_{\mathbf{R}}([-1,1])\) is the vector space of continuous real-valued functions on the interval \([-1,1]\) with inner product given by<br><br>\[<br>\langle f, g\rangle=\int_{-1}^{1} f(x) g(x) d x<br>\]<br><br>for \(f, g \in C_{\mathbf{R}}([-1,1])\). Let \(\varphi\) be the linear functional on {{c4::\(C_{\mathbf{R} }([-1,1])\)}} defined by \(\varphi(f)\) = {{c5::\(f(0)\)}}. Show that there does not exist {{c1::\(g \in C_{\mathbf{R} }([-1,1])\)}} such that<br><br><ul><li>{{c2::\(\varphi(f)\)}} = {{c3::\(\langle f, g\rangle\)}}</li></ul>

============================================================

  

    16 Suppose \(\mathbf{F}=\mathbf{C}, V\) is finite-dimensional, \(T \in \mathcal{L}(V)\), all the eigenvalues of \(T\) have {{c1::absolute value less than 1}} , and \(\epsilon\) {{c2::&gt; \(0\)}}. <br><br>Prove that:<br><ul><li>&nbsp;There exists a {{c3::positive integer \(m\)}} such that:</li><li>&nbsp;{{c4::\(\left\|T^{m} v\right\|\)}}&nbsp; {{c2::\(\leq\)}} {{c5::\( \epsilon\|v\|\)}} for every \(v \in V\).</li></ul>

============================================================

  

    17 For \(u \in V\), let \(\Phi u\) denote the linear functional on \(V\) defined by<br><br>\[<br>(\Phi u)(v)=\langle v, u\rangle<br>\]<br><br>for \(v \in V\).<br><br><ul><li>(b) Show that if \(\mathbf{F}\) = {{c5::{{c1::\(\mathbf{C}\)}}}} and V {{c4::{{c2::\(\neq\{0\}\)}}}}, then \(\Phi\) is {{c4::{{c3::not a linear map.}}}}</li></ul>

============================================================

  

    17 For \(u \in V\), let \(\Phi u\) denote the linear functional on \(V\) defined by<br><br>\[<br>(\Phi u)(v)=\langle v, u\rangle<br>\]<br><br>for \(v \in V\).<br><br><ul><li>(d) Suppose \(\mathbf{F}\) = {{c1::\(\mathbf{R}\)}} and \(V\) is {{c2::finite}}-dimensional. Use a {{c3::dimension-counting}} argument (but without using 6.42) to show that \(\Phi\) is an {{c4::isomorphism}} {{c5::from \(V\) onto \(V^{\prime}\)}}.</li></ul>

============================================================

  

    <img src="paste-e8cb5601fa61951f034884ef72e2ea1267f089c5.jpg"><br>[Part \((d)\) gives an alternative proof of the {{c1::Riesz Representation}} Theorem (6.42) when \(\mathbf{F}\) = {{c2::\(\mathbf{R}\)}}. Part (d) also gives a {{c3::natural isomorphism (meaning that it does not depend on a choice of basis)}} {{c4::from a finite-dimensional real inner product space}} {{c5::onto its dual space.}}]

============================================================

  

    Importantly, the Gram-Schmidt procedure produces vectors with {{c5::{{c1::the same span}}}} {{c4::{{c2::up to}}}} j for {{c4::{{c3::all 1&lt;= j &lt;= n}}}}

============================================================

  

    Given the following example:<br><br><img src="paste-e37edd2ae6eb8fc664815cae8bdbeac6bd78c4e3.jpg"><br><br>How does Gram-Schmidt working with innter products easier?<br><ul><li>For {{c1::any inner product}}, it allows us to obtain {{c2::orthonormal bases}} which {{c5::{{c3::behave nicely under the inner product}}}}, just like {{c5::{{c4::how the standard basis of F^n behaves under the dot product}}}}</li></ul>

============================================================

  

    {{c5::{{c1::Inner products}}}} with {{c4::{{c2::the second vector fixed}}}} {{c4::{{c3::are linear functionals}}}}

============================================================

  

    What is remarkable about the following representation of u?<br><img src="paste-9bf6f49f14b88afe77ac396e0c612601e13b9b4d.jpg"><br><ul><li>{{c1::Orthonornal bases}} {{c2::are not unique}}</li><li>However {{c3::u is}}, meaning that i{{c4::it is fixed regardlss of basis}} and {{c5::this property holds for all of them}}</li></ul>

============================================================

  

    A lis of vectors is {{c5::{{c3::orthonormal}}}} if all vectors in the list have {{c4::{{c1::norm 1}}}} and {{c4::{{c2::they are pairwise orthogonal}}}}

============================================================

  

    The {{c2::squared norm}} of a vector expressed in terms of an {{c1::orthonormal basis}} is {{c5::{{c3::the sum of the squared norms}}}} {{c5::{{c4::of the coefficients}}}}

============================================================

  

    Why do we know an orthonormal list is linearly indepdent?<br><ul><li>Norm of {{c5::{{c1::0 vector squared}}}} is {{c4::{{c2::the sum&nbsp; of the coefficient norms squared}}}}</li><li>Meaning {{c4::{{c3::all coeficients must be 0}}}}</li></ul>

============================================================

  

    While Gram-Schmidt {{c1::re-angles}} vectors so they are {{c2::perpendicular to previous vectors in the list}}, it {{c3::preserves}} {{c4::the span of}} {{c5::the combination of vectors}}

============================================================

  

    The first step of Gram-Schmidt is a mere {{c5::{{c1::normalization of the first vector}}}}, since {{c4::{{c2::a list of one element}}}} {{c4::{{c3::is already orthogonal}}}}

============================================================

  

    The way the inductive step of Gram-Schmidt works is:<br><ul><li>For&nbsp;\(e_j\), {{c1::subtract from&nbsp;\(v_j\)}} {{c2::the part of&nbsp;\(v_j\)}} {{c3::which lies in the direction of all previous vectors in the orthogonal list}} via {{c4::dot-producting&nbsp;}}</li><li>{{c5::Make this new vector have norm 1&nbsp;}}</li></ul>

============================================================

  

    Why is subtracting from&nbsp;\(v_j\) equivalent to removing those components of&nbsp;\(v_j\) in the gram-schmidt equation?<br><img src="paste-ceeab9062538e1df49379e7b7f4cccc710c9fd08.jpg"><br>Because:<br><ul><li>For an orthonormal basis {{c1::the dot product}} gives you {{c2::the coefficient of&nbsp;\(e_i\) in&nbsp;\(v_j\)}}</li><li>Thus {{c3::subtracting&nbsp;\(&lt;v_j,e_i&gt;e_i\) from&nbsp;\(v_j\)}} means {{c4::removing&nbsp;\(e_i\) from the sum which composes&nbsp;\(v_j\).}}</li><li>Since the original list was linearly independent, this {{c5::removes all information related to&nbsp;\(e_i\)}}</li></ul>

============================================================

  

    The fact that every orthonormal list can be {{c5::{{c3::extended to an orthonormal basis}}}} is particularly useful when we have {{c4::{{c2::orthonormal bases}}}} of {{c4::{{c1::subspaces}}}}

============================================================

  

    <ul><li>To transform an upper-triangular matrix into an upper-triangular matrix with respect to an orthonormal basis,&nbsp;</li><li>Just {{c1::apply the gram-schmidt procedure}} {{c2::to the current basis}} {{c3::in order}}</li><li>This will preserve the upper-triangular nature since {{c4::gram-schmidt lets span \(v_1,\dots,v_j\) unchanged}}&nbsp;for {{c5::all j &lt;= dim V}}</li></ul>

============================================================

  

    Gram-schmidt preserves the {{c1::nested}} {{c2::structure}} {{c5::{{c3::of subspaces}}}} for {{c5::{{c4::upper-triangular}}}} matrices

============================================================

  

    If V is a {{c2::complex}} {{c3::inner-product}} space. Then T has an {{c5::{{c1::upper-triangular}}}} matrix with respect to some {{c5::{{c4::orthonormal}}}} basis of V

============================================================

  

    Why is the following direct-sum decomposition unsatisfying?<br><img src="paste-485c48e5a32eeeae1564471d2f66f6edce925cfb.jpg"><br><ul><li>The {{c5::{{c1::angle between u and w}}}} can be {{c4::{{c2::aribtrarily small as long as it is non-zero}}}}</li><li>Which makes {{c4::{{c3::figuring out coefficients of vectors difficult}}}}</li></ul>

============================================================

  

    What makes distinguishing the following direct-sum decompositions possible?<br><img src="paste-0612a8ecf1d89773c9e5c1d5329e2c35eed1d28e.jpg"><br><ul><li>{{c1::Inner products}}, {{c2::norms}} and {{c5::{{c3::angles}}}}</li><li>Allowing us to {{c5::{{c4::breakdown vectors into orthogonal components}}}}</li></ul>

============================================================

  

    <b>Definition</b>: The {{c1::orthogonal complement}}<br><br>The {{c1::orthogonal complement}} of a subspace U of an inner product space V, denoted {{c2::<img src="paste-e44b8da29e4394a8ff7a653eb7b4ad1189fcd710.jpg">,}}&nbsp;&nbsp;is {{c5::{{c3::the set of vectors in V}}}} which {{c5::{{c4::are orthogonal to every vector in U}}}}

============================================================

  

    The {{c1::orthogonal complement}} of U is {{c2::the set of vectors in V}} such that {{c3::the inner product}} of {{c2::those ectors}} and {{c4::all vectors in U}} is {{c5::0}}

============================================================

  

    The orthogonal complement of a subspace is a subspace because:<br><ul><li>Origin: {{c5::{{c3::is orthogonal to all vectors so it is in the orthogonal complement}}}}</li><li>Additivity: {{c4::{{c1:: additivity in first slot<img src="paste-09e7b8d78b9794d9be537fe04d7b72538cc88c13.jpg">}}}}</li><li>Homogeneity:{{c4::{{c2:: homogeneity in first slot&nbsp;<img src="paste-00c4961a699b0270763776f73b2f365d2fb58a96.jpg">}}}}</li></ul>

============================================================

  

    The {{c5::{{c2::orthogonal}}}} {{c4::{{c3::complement}}}} of a subspace is {{c4::{{c1::a subspace}}}}

============================================================

  

    The {{c1::orthogonal}} {{c2::complement}} {{c5::{{c3::of an entire vector space V}}}} is {{c5::{{c4::{0} }}}}

============================================================

  

    The {{c1::orthogonal}} {{c2::complement}} of {{c5::{{c3::{0} }}}} is {{c5::{{c4::the entire ambient vector space V}}}}

============================================================

  

    If {{c1::U}} is a {{c3::subspace of an inner product space V}} then {{c3::V}} can be {{c5::decomposed}} into {{c1::a direct sum}} of {{c2::U}} and {{c4::its orthogonal complement}}

============================================================

  

    <b>Definition</b> {{c1::orthogonal complement}}, {{c2::\(U^{\perp}\)}}<br><br>If \(U\) is a subset of \(V\), then the {{c1::orthogonal complement}} of \(U\), denoted {{c2::\(U^{\perp}\)}}, is {{c3::the set of all vectors in \(V\)}} that {{c4::are orthogonal to every vector in \(U\)}} :<br><br><ul><li>{{c2::\(U^{\perp}\)}} ={{c5::\(\{v \in V:\langle v, u\rangle=0 \text { for every } u \in U\} \)}}</li></ul>

============================================================

  

    For example, if \(U\) is a line in \(\mathbf{R}^{3}\), then \(U^{\perp}\) is {{c5::{{c1::the plane containing the origin}}}} that {{c4::{{c2::is perpendicular to \(U\)}}}}. If \(U\) is a plane in \(\mathbf{R}^{3}\), then \(U^{\perp}\) is {{c4::{{c3::the line containing the origin}}}} that {{c4::{{c2::is perpendicular to \(U\).}}}}

============================================================

  

    <b>eBasic properties of orthogonal complement</b><br><br><ul><li>(a) If {{c1::\(U\)}}&nbsp;is a subset of \(V\) then {{c1::\(U^{\perp}\) is a subspace of \(V\).}}</li><li>(b) {{c2::\(\{0\}^{\perp}\)}} ={{c2::\(V\)}}.</li><li>(c) {{c3::\(V^{\perp}\)}} = {{c3::\(\{0\}\)}}.</li><li>(d) If {{c4::\(U\)}}&nbsp;is a {{c4::subset of \(V\)}}, then {{c5::\(U \cap U^{\perp} \subset\{0\}\)}}.</li><li>(e) If {{c5::\(U\) and \(W\)}}&nbsp;are {{c5::subsets of \(V\)}}&nbsp;and {{c5::\(U \subset W\)}}, then {{c4::\(W^{\perp} \subset U^{\perp}\).}}</li></ul>

============================================================

  

    <b>Basic properties of&nbsp;orthogonal complement</b><br><br><ol><li>(a) If {{c1::\(U\)}} {{c2::is a subset of \(V\)}}, then {{c5::{{c3::\(U^{\perp}\)}}}} is {{c5::{{c4::a subspace of \(V\)}}}}.</li></ol><br>

============================================================

  

    <b>Basic properties of&nbsp;</b>{{c5::{{c3::orthogonal complement}}}}<br><br><br>(c) {{c4::{{c1::\(V^{\perp}\)}}}} = {{c4::{{c2::\(\{0\}\)}}}}.<br>

============================================================

  

    <b>Basic properties of orthogonal complement</b><br><br><br><br><ul><li>(d) If {{c1::\(U\)}} is {{c2::a subset of \(V\)}}, then {{c3::\(U\)}} {{c4::\(\cap\)}} {{c3::\( U^{\perp}\)}}&nbsp; {{c4::\(\subset\)}} {{c5::\(\{0\}\)}}.</li></ul>

============================================================

  

    <b>Basic properties of orthogonal complement</b><br><br><ul><li>(e) If {{c1::\(U\)}} and {{c1::\(W\)}} {{c2::are subsets of \(V\)}} and {{c3::\(U \subset W\)}}, then {{c4::\(W^{\perp}\)}} {{c3::\( \subset\)}}&nbsp; {{c5::\(U^{\perp}\)}}</li></ul>

============================================================

  

    \subsection{Basic properties of orthogonal complement}<br><br>(a) If \(U\) is a subset of \(V\), then \(U^{\perp}\) is a subspace of \(V\).<br><br>Proof:<br><ul><li>Origin: {{c3::0 is orthogonal to all vectors}}</li><li>Additivity: &nbsp;if {{c1::two vectors are orthogonal to u in U}} then {{c2::their sum also is}} because of {{c3::addivity in the first componennt}}</li><li>Homogeneity: if {{c4::a vector is orthogonal to u in U}} then {{c5::any scalar multiple of it also is}} because of {{c3::homogeneity in the first component}}</li></ul>

============================================================

  

    \subsection{Basic properties of orthogonal complement}<br><br><br>(b) \(\{0\}^{\perp}=V\).<br><br><br>Proof:<br><ul><li>(b) Suppose \(v \in V\). Then {{c1::\(\langle v, 0\rangle\)}} = {{c2::\(0\)}}, which implies that \(v\) \(\in\) {{c3::\(\{0\}^{\perp}\)}}. Thus {{c4::\(\{0\}^{\perp}\)}} = {{c5::\(V\)}}.<br></li></ul><br>

============================================================

  

    \subsection{Basic properties of orthogonal complement}<br><br><br>(c) \(V^{\perp}=\{0\}\).<br><br>Proof:<br><ul><li>(c) Suppose \(v \in V^{\perp}\). Then {{c1::\(\langle v, v\rangle\)}} = {{c2::\(0\)}}, which implies that {{c3::\(v\)}} = {{c4::\(0\)}}. Thus {{c5::\(V^{\perp}\)}} = {{c5::\(\{0\}\)}}.<br></li></ul>

============================================================

  

    \subsection{Basic properties of orthogonal complement}<br><br>(d) If \(U\) is a subset of \(V\), then \(U \cap U^{\perp} \subset\{0\}\).<br><br>Proof:<br><ul><li>(d) Suppose \(U\) is a subset of \(V\) and \(v\)&nbsp; \(\in\) {{c1::\(U \cap U^{\perp}\)}}. Then {{c2::\(\langle v, v\rangle\)}} = {{c3::\(0\)}}, which implies that \(v\) = {{c4::\(0\)}}. Thus {{c5::\(U \cap U^{\perp} \subset\{0\}\)}}</li></ul>

============================================================

  

    \subsection{Basic properties of orthogonal complement}<br><br>(e) If \(U\) and \(W\) are subsets of \(V\) and \(U \subset W\), then \(W^{\perp} \subset U^{\perp}\).<br><br>Proof:<br><ul><li>(e) Suppose \(U\) and \(W\) are subsets of \(V\) and \(U \subset W\). Suppose \(v\) \(\in\) {{c1::\(W^{\perp}\)}}. Then {{c2::\(\langle v, u\rangle\)}} = {{c3::\(0\)}} for {{c2::every \(u \in W\)}}, which implies that {{c4::\(\langle v, u\rangle\)}} = {{c3::\(0\)}} for {{c2::every \(u \in U\)}}. Hence {{c5::\(v \in U^{\perp}\)}}. Thus {{c5::\(W^{\perp} \subset U^{\perp}\)}}.<br></li></ul>

============================================================

  

    {{c3::Direct sum}} of {{c4::a subspace}} and {{c5::its orthogonal complement}}<br><br>Suppose \(U\) is a finite-dimensional subspace of \(V\). Then<br><br><ul><li>{{c1::\(V\)}} = {{c2::\(U \oplus U^{\perp} .\)}}</li></ul>

============================================================

  

    <img src="paste-63bed7b30dc6453d921aa714f2982932d33a701a.jpg"><br>Proof:<br><ul><li>Proof First we will show that</li><ul><li>6.48:&nbsp;</li><li>\(V\) = \(U+U^{\perp} .\)</li></ul><li>To do this, suppose \(v \in V\). Let \(e_{1}, \ldots, e_{m}\) be an {{c1::orthonormal}} basis of \(U\). Obviously</li><ul><li>6.49:&nbsp;</li><li>\(&nbsp;v\) = {{c2::\(\underbrace{\left\langle v, e_{1}\right\rangle e_{1}+\cdots+\left\langle v, e_{m}\right\rangle e_{m} }_{u}\) }} + {{c3::\(\underbrace{v-\left\langle v, e_{1}\right\rangle e_{1}-\cdots-\left\langle v, e_{m}\right\rangle e_{m} }_{w} .\)}}</li><li>where {{c3::u is in U&nbsp;}}</li></ul><li>Because \(e_{1}, \ldots, e_{m}\) is an {{c1::orthonormal basis}}, for each j up to m we have:</li><ul><li>{{c4::\(\left\langle w, e_j\right\rangle\)&nbsp;}}</li><li>= {{c5::\(\left\langle v, e_j\right\rangle-\left\langle v, e_j\right\rangle\)}}</li><li>= {{c5::\(0\)}}</li></ul><li>Thus {{c5::\(w\) is orthogonal to every vector in \(\operatorname{span}\left(e_{1}, \ldots, e_{m}\right)\)}}. In other words,&nbsp;{{c5::\(w \in U^{\perp}\)}}.&nbsp;<br></li><li>This completes the proof that&nbsp;</li><ul><li>\(V\) = \(U+U^{\perp} .\)</li></ul></ul>

============================================================

  

    <img src="paste-2f0412f1466a04d3b2b0c69c5c6e147f4ec69860.jpg"><br>Proof:<br><ul><li>Assume \(<br>V=U+U^{\perp} .<br>\)</li><li>Since {{c1::the intersection of a subspace}} {{c2::and its orthogonal component}} is {{c5::{{c3::0}}}}, this must be {{c5::{{c4::a direct sum}}}}</li></ul>

============================================================

  

    {{c1::Dimension}} of {{c2::the orthogonal complement}}<br><br>Suppose \(V\) is finite-dimensional and \(U\) is a subspace of \(V\). Then<br><br><ul><li>{{c2::\(\operatorname{dim} U^{\perp}\)}} ={{c3::\(\operatorname{dim} V\)}}{{c4::\(-\)}}{{c5::\(\operatorname{dim} U\)}}</li></ul>

============================================================

  

    The {{c1::orthogonal complement}} of the {{c2::orthogonal complement}}<br><br>Suppose \(U\) is a finite-dimensional subspace of \(V\). Then<br><br><ul><li>{{c5::{{c3::\(U\)}}}}={{c5::{{c4::\(\left(U^{\perp}\right)^{\perp}\)}}}}</li></ul>

============================================================

  

    <img src="paste-aaaa85c9b5d7a49f37b78894a5242778b14060b7.jpg"><br><br>Proof First we will show that<br><br>\[<br>U \subset\left(U^{\perp}\right)^{\perp}<br>\]<br><br>To do this:<br><ul><li>&nbsp;Suppose \(u \in U\).&nbsp;</li><li>Then {{c1::\(\langle u, v\rangle\)}} = {{c2::\(0\)}} for {{c1::every \(v \in U^{\perp}\).&nbsp;}}</li><li>Because \(u\) is {{c3::orthogonal to every vector}} {{c4::in \(U^{\perp}\)}}, we have \(u\) \( \in\) {{c5::\(\left(U^{\perp}\right)^{\perp}\)}},&nbsp;</li></ul><br>

============================================================

  

    <b>Definition</b> {{c1::orthogonal projection}}, {{c2::\(P_{U}\)::notation}}<br><br>Suppose \(U\) is a finite-dimensional subspace of \(V\). <br><ul><li>The {{c1::orthogonal projection of \(V\) onto \(U\)}} is {{c5::the operator}} {{c2::\(P_{U} \in \mathcal{L}(V)\)}} defined as follows:&nbsp;</li><li>For \(v \in V\), write \(v\) = {{c3::\(u+w\)}}, where {{c3::\(u \in U\)}} and {{c3::\(w \in U^{\perp}\)}}. Then {{c2::\(P_{U} v\)}} = {{c4::\(u\)}}.</li></ul>

============================================================

  

    6.54 Example Suppose \(x \in V\) with \(x \) {{c5::\(\neq 0\)}} and \(U\) = {{c4::\(\operatorname{span}(x)\)}}. Show that<br><br><ul><li>{{c1::\(P_{U} v\)}} = {{c2::\(\frac{\langle v, x\rangle}{\|x\|^{2} }\)}} {{c3::\(x\)}}</li></ul><br>for {{c1::every \(v \in V\)}}.<br>

============================================================

  

    <img src="paste-1c2a6c7ced5dde517cdfd694c8adde6d2ec8db0f.jpg"><br>Solution Suppose \(v \in V\). Then<br><br><ul><li>\(v\) ={{c1::\(\frac{\langle v, x\rangle}{\|x\|^{2} } x\)}} + {{c2::\(\left(v-\frac{\langle v, x\rangle}{\|x\|^{2} } x\right),\)}}</li><li>where the first term on the right is in {{c3::\(\operatorname{span}(x)\)}} (and thus in {{c3::\(U\)}} )&nbsp;</li><li>And the second term on the right is {{c4::orthogonal to \(x\)}} (and thus is in {{c4::\(U^{\perp}\)}} ).&nbsp;</li><li>Thus \(P_{U} v\) equals {{c5::the first term on the right}}</li></ul>

============================================================

  

    <b>Properties of the orthogonal projection \(P_{U}\)</b><br><br>Suppose \(U\) is a finite-dimensional subspace of \(V\) and \(v \in V\). Then<ul><li>(a) {{c1::\(P_{U}\)}} \(\in\) {{c1::\(\mathcal{L}(V)\)}};</li><li>(b) {{c2::\(P_{U} u\)}} = {{c2::\(u\)}} for {{c2::every \(u \in U\)}};</li><li>(c) {{c3::\(P_{U} w\)}} = {{c3::\(0\)}} for {{c3::every \(w \in U^{\perp}\)}};</li><li>(d) {{c4::range \(P_{U}\)}} = {{c4::\(U\)}};</li><li>(e) {{c5::null \(P_{U}\)}} = {{c5::\(U^{\perp}\)}}</li></ul><br>

============================================================

  

    <b>Properties of the orthogonal projection \(P_{U}\)</b><br><br>Suppose \(U\) is a finite-dimensional subspace of \(V\) and \(v \in V\). Then<ul><li>(a) {{c5::{{c1::\(P_{U}\)}}}} {{c4::{{c2::\(\in\)}}}} {{c4::{{c3::\(\mathcal{L}(V)\)}}}};</li></ul>

============================================================

  

    <b>Properties of the orthogonal projection \(P_{U}\)</b><br><br>Suppose \(U\) is a finite-dimensional subspace of \(V\) and \(v \in V\). Then<ul><li>(b) {{c1::\(P_{U}\)}} {{c2::\(u\)}} = {{c5::{{c3::\(u\)}}}} for {{c5::{{c4::every \(u \in U\)}}}};</li></ul>

============================================================

  

    <b>Properties of the orthogonal projection \(P_{U}\)</b><br><br>Suppose \(U\) is a finite-dimensional subspace of \(V\) and \(v \in V\). Then<ul><li>(c) {{c1::\(P_{U}\)}} {{c2::\(w\)}} = {{c5::{{c3::\(0\)}}}} for {{c5::{{c4::every \(w \in U^{\perp}\)}}}};</li></ul>

============================================================

  

    <b>Properties of the orthogonal projection \(P_{U}\)</b><br><br>Suppose \(U\) is a finite-dimensional subspace of \(V\) and \(v \in V\). Then<ul><li>(d) {{c5::{{c1::range::range/null?}}::range/null?}} {{c4::{{c2::\(P_{U}\)}}}} = {{c4::{{c3::\(U\)}}}};</li></ul>

============================================================

  

    <b>Properties of the orthogonal projection \(P_{U}\)</b><br><br>Suppose \(U\) is a finite-dimensional subspace of \(V\) and \(v \in V\). Then<ul><li>(e) {{c5::{{c1::null}}}} {{c4::{{c2::\(P_{U}\)}}}} = {{c4::{{c3::\(U^{\perp}\)}}}};</li></ul>

============================================================

  

    \subsection{Properties of the orthogonal projection \(P_{U}\)}<br><br>Suppose \(U\) is a finite-dimensional subspace of \(V\) and \(v \in V\). Then<ul><li>(f) \(v-P_{U} v\) \(\in\) {{c1::\(U^{\perp}\)}};</li><li>(g) \(P_{U}^{2}\) = {{c2::\(P_{U}\)}};</li><li>(h) {{c3::\(\left\|P_{U} v\right\|\)}} \( \leq\) {{c3::\(\|v\|\)}};</li><li>(i) for every {{c4::orthonormal}} basis \(e_{1}, \ldots, e_{m}\) of \(U\),</li><ul><li>{{c4::\(P_{U} v\)}} = {{c4::\(\left\langle v, e_{1}\right\rangle e_{1}\)}} + \(\cdots\)+ {{c5::\(\left\langle v, e_{m}\right\rangle e_{m} .\)}}</li></ul></ul><br>

============================================================

  

    \subsection{Properties of the orthogonal projection \(P_{U}\)}<br><br>Suppose \(U\) is a finite-dimensional subspace of \(V\) and \(v \in V\). Then<br><br><ul><li>(f) {{c1::\(v\)}}{{c2::-}}{{c3::\(P_{U}\) \(v\)}} {{c4::\(\in\)}} {{c5::\(U^{\perp}\)}};</li></ul>

============================================================

  

    \subsection{Properties of the orthogonal projection \(P_{U}\)}<br><br>Suppose \(U\) is a finite-dimensional subspace of \(V\) and \(v \in V\). Then<br><ul><li>(g) {{c5::{{c1::\(P_{U}\)}}}}{{c4::{{c2::\(^{2}\)}}}} = {{c4::{{c3::\(P_{U}\)}}}}</li></ul>

============================================================

  

    \subsection{Properties of the orthogonal projection \(P_{U}\)}<br><br>Suppose \(U\) is a finite-dimensional subspace of \(V\) and \(v \in V\). Then<br><ul><li>(h) {{c5::{{c1::\(\left\|P_{U} v\right\|\)}}}}{{c4::{{c2::\( \leq\)}}}}{{c4::{{c3::\(\|v\|\)}}}};</li></ul>

============================================================

  

    \subsection{Properties of the orthogonal projection \(P_{U}\)}<br><br>Suppose \(U\) is a finite-dimensional subspace of \(V\) and \(v \in V\). Then<br><br><ul><li>(i) for every {{c1::orthonormal}} basis \(e_{1}, \ldots, e_{m}\) of \(U\),<br></li><li>\(P_{U} v\)= {{c2::\(\left\langle v, e_{1}\right\rangle e_{1}\)}}{{c3::\(+\)}}{{c4::\(\cdots\)}}{{c3::\(+\)}}{{c5::\(\left\langle v, e_{m}\right\rangle e_{m} .\)}}<br></li></ul>

============================================================

  

    <div>The remarkable simplicity of the solution to this {{c5::{{c1::minimization}}}} {{c4::{{c2::problem}}}} has led to many important applications of {{c4::{{c3::inner product spaces}}}} outside of pure mathematics.<br></div>

============================================================

  

    <br>The following problem often arises: given a subspace \(U\) of \(V\) and {{c1::a point \(v \in V\)}}, find {{c2::a point \(u \in U\)}} such that {{c3::\(\|v-u\|\)}} is {{c4::as small as possible}}. The next proposition shows that this problem is solved by {{c5::taking \(u=P_{U} v\).}}

============================================================

  

    Suppose \(U\) is a finite-dimensional subspace of \(V, v \in V\), and \(u \in U\). Then<br><ul><li>{{c1::\(\left\|v-P_{U} v\right\|\)}}{{c2::\(\leq\)}}{{c5::{{c3::\(\|v-u\|\)}}}}</li><li>Furthermore, the {{c2::inequality}} above is an {{c2::equality}} if and only if \(u\) = {{c5::{{c4::\(P_{U} v\)}}}}.</li></ul>

============================================================

  

    <img src="paste-81d56a8739ee61a20b8ed7889ba69c4b1a1ab3be.jpg"><br>Proof:<br><ul><li>\(\left\|v-P_U v\right\|^2\)&nbsp;</li><li>\(\leq\) {{c1::\(\left\|v-P_U v\right\|^2\)}} +{{c2::\(\left\|P_U v-u\right\|^2\)}}<br></li><li>=&nbsp;\(\|\) {{c3::\(\left(v-P_U v\right)\)}} +{{c4::\(\left(P_U v-u\right)\)}}\(\|\){{c5::\(^2\)}}</li><li>=&nbsp;{{c5::\(\|v-u\|\)}} {{c5::\(^2\)}}</li></ul>

============================================================

  

    <img src="paste-84b0e020fe2186f50e76eefd74455ffd0b11cee3.jpg" style="width: 420.127px;"><br><ul><li>Where the first line above holds because {{c1::\(0 \leq\left\|P_{U} v-u\right\|^{2}\)}},&nbsp;</li><li>The second line above comes from the {{c2::Pythagorean Theorem}} [which applies because {{c3::\(v-P_{U} v \in U^{\perp}\)}} , and {{c3::\(P_{U} v-u \in U\)}} ],&nbsp;</li><li>Our inequality above is an equality if and only if 6.57 is an equality, which happens if and only if {{c4::\(\left\|P_{U} v-u\right\|\)}} = {{c4::\(0\)}}, which happens if and only if {{c5::\(u\)}} = {{c5::\(P_{U} v\)}}.<br></li></ul>

============================================================

  

    <img src="paste-9e144d32be3f7af966a245d215b3dd27ab6bbc79.jpg"><br><img src="paste-979b25a833fc59ac50fadf1d1d7a8dff79c21254.jpg"><br>The last result is often combined with the formula 6.55({{c5::{{c1::i}}}}) to {{c4::{{c2::compute explicit solutions}}}} {{c4::{{c3::to minimization problems.}}}}

============================================================

  

    2 Suppose \(U\) is a finite-dimensional subspace of \(V\). <br><ul><li>Prove that {{c1::\(U^{\perp}\)}} = {{c2::\(\{0\}\)}} if and only if {{c5::{{c3::\(U\)}}}} = {{c5::{{c4::\(V\)}}}}.</li></ul>

============================================================

  

    3 Suppose \(U\) is a subspace of \(V\) with basis \(u_{1}, \ldots, u_{m}\) and<br><br>\[<br>u_{1}, \ldots, u_{m}, w_{1}, \ldots, w_{n}<br>\]<br><br>is a basis of \(V\). <br><ul><li>Prove that if the Gram-Schmidt Procedure is applied to the basis of \(V\) above, producing a list \(e_{1}, \ldots, e_{m}, f_{1}, \ldots, f_{n}\),&nbsp;</li><li>Then {{c3::\(e_{1}, \ldots, e_{m}\)}} is {{c1::an orthonormal basis of \(U\)}}&nbsp;</li><li>And {{c5::{{c4::\(f_{1}, \ldots, f_{n}\)}}}} is {{c5::{{c2::an orthonormal basis of \(U^{\perp}\).}}}}</li></ul>

============================================================

  

    5 Suppose \(V\) is finite-dimensional and \(U\) is a subspace of \(V\). <br><ul><li>Show that {{c5::{{c1::\(P_{U^\perp}\)}}}} = {{c4::{{c2::\(I-P_{U}\)}}}}, where {{c4::{{c3::\(I\)}}}} is {{c4::{{c3::the identity operator on \(V\)}}}}.</li></ul>

============================================================

  

    6 Suppose \(U\) and \(W\) are finite-dimensional subspaces of \(V\). <br><ul><li>Prove that {{c1::\(P_{U}\)}}{{c2::\(P_{W}\)}} = {{c3::\(0\)}} if and only if {{c4::\(\langle u, w\rangle\)}}={{c5::\(0\)}} for {{c4::all \(u \in U\)}} and {{c4::all \(w \in W\).}}</li></ul>

============================================================

  

    7 <br><br><ul><li>Suppose \(V\) is finite-dimensional&nbsp;</li><li>And \(P \in \mathcal{L}(V)\) is such that {{c1::\(P^{2}\)}} = {{c1::\(P\)}}&nbsp;</li><li>And every vector in {{c2::null \(P\)}} is {{c3::orthogonal}} to every vector in {{c2::range \(P\)}}.&nbsp;</li><li>Prove that there exists a subspace \(U\) of \(V\) such that {{c4::\(P\)}}={{c5::\(P_{U}\)}}.</li></ul>

============================================================

  

    8 Suppose \(V\) is finite-dimensional and \(P \in \mathcal{L}(V)\) is such that:<br><ul><li>{{c1::&nbsp;\(P^{2}\)}}={{c1::\(P\)}} and</li><li>{{c2::\(\|P v\|\)}} {{c5::\(\leq\)}}{{c2::\(\|v\|\)}} for every \(v \in V\).&nbsp;</li><li>Prove that there exists a subspace \(U\) of \(V\) such that {{c3::\(P\)}}={{c4::\(P_{U}\)}}.</li></ul>

============================================================

  

    9 Suppose \(T \in \mathcal{L}(V)\) and \(U\) is a finite-dimensional subspace of \(V\). <br><ul><li>Prove that \(U\) is {{c5::{{c1::invariant under \(T\)}}}} if and only if {{c4::{{c2::\(P_{U} T P_{U}\)}}}} = {{c4::{{c3::\(T P_{U}\).}}}}</li></ul>

============================================================

  

    10 Suppose \(V\) is finite-dimensional, \(T \in \mathcal{L}(V)\), and \(U\) is a subspace of \(V\). <br><ul><li>Prove that {{c1::\(U\)}} and {{c1::\(U^{\perp}\)}} are both {{c2::invariant under \(T\)}}&nbsp;</li><li>If and only if {{c5::{{c3::\(P_{U} T\)}}}} ={{c5::{{c4::\(T P_{U}\)}}}}.</li></ul>

============================================================

  

    14 Suppose \(C_{\mathbf{R}}([-1,1])\) is the vector space of continuous real-valued functions on the interval \([-1,1]\) with inner product given by<br><br><ul><li>\(\langle f, g\rangle\)={{c5::\(\int_{-1}^{1} f(x) g(x) d x\)}}</li></ul><br>for \(f, g \in C_{\mathbf{R}}([-1,1])\). Let \(U\) be the subspace of \(C_{\mathbf{R}}([-1,1])\) defined by<br><br><ul><li>\(U\)= \(\{f \in C_{\mathbf{R}}([-1,1]):\) {{c3::\(f(0)\)}}={{c4::\(0\)}} \(\} \)</li></ul><br>(a) Show that {{c1::\(U^{\perp}\)}} = {{c2::\(\{0\}\)}}.<br>

============================================================

  

    <br><img src="paste-a9110c170597ff48324963e75ff74c2c45d1c669.jpg"><br>Solution Let \(C_{\mathbf{R}}[-\pi, \pi]\) denote the real inner product space of continuous real-valued functions on \([-\pi, \pi]\) with inner product<br><br><ul><li>\(\langle f, g\rangle\) = {{c1::\(\int_{-\pi}^{\pi} f(x) g(x) d x .\)}}</li><li>Let \(v \in C_{\mathbf{R}}[-\pi, \pi]\) be the function defined by \(v(x)\) ={{c2::\(\sin x\)}}.&nbsp;</li><li>Let \(U\) denote the subspace of \(C_{\mathbf{R}}[-\pi, \pi]\) consisting of the polynomials with real coefficients and degree at most 5&nbsp;</li><ul><li>Find \(u \in U\) such that {{c5::{{c3::\(\|v-u\|\)}}}} is {{c5::{{c4::as small as possible}}}}.</li></ul></ul><div><br></div>

============================================================

  

    <br><img src="paste-a9110c170597ff48324963e75ff74c2c45d1c669.jpg"><br>Solution Let \(C_{\mathbf{R}}[-\pi, \pi]\) denote the real inner product space of continuous real-valued functions on \([-\pi, \pi]\) with inner product<br><br><ul><li>\(\langle f, g\rangle\) = \(\int_{-\pi}^{\pi} f(x) g(x) d x .\)</li><li>Let \(v \in C_{\mathbf{R}}[-\pi, \pi]\) be the function defined by \(v(x)\) =\(\sin x\).&nbsp;</li><li>Let \(U\) denote the subspace of \(C_{\mathbf{R}}[-\pi, \pi]\) consisting of the polynomials with real coefficients and degree at most 5&nbsp;</li><ul><li>Find \(u \in U\) such that \(\|v-u\|\) is as small as possible.</li></ul></ul><div>To compute the solution to our approximation problem:</div><div><ul><li>First apply the {{c1::Gram-Schmidt Procedure}} to the basis {{c2::\(1, x, x^{2}, x^{3}, x^{4}, x^{5}\)}} of \(U\), producing an {{c3::orthonormal}} basis \(e_{1}, e_{2}, e_{3}, e_{4}, e_{5}, e_{6}\) of \(U\).&nbsp;</li><li>Then, again using the inner product given by 6.59, compute \(P_{U} v\) using 6.55:</li><ul><li>\(P_{U} v\) = {{c4::\(\left\langle v, e_{1}\right\rangle e_{1}+\cdots+\left\langle v, e_{m}\right\rangle e_{m} .\)}}</li></ul><li>Then using this explicit formula, apply {{c5::\(<br>\left\|v-P_{U} v\right\|\)}}&nbsp;\(\leq\) {{c5::\(\|v-u\|\)}}</li></ul></div>

============================================================

  

    <img src="paste-637adc9d149d02dd2e94c3d87049d9f2eb48078b.jpg"><br><img src="paste-7d5ca477d34fe4b78e0036f5a985dff7a051661c.jpg"><br>How does aproximating this using subspace minimization compare to the taylor series graphically?<br><ul><li>Graph for lin alg:</li><li>{{c1::<img src="paste-635817ada9afabf87cdc79f4e38787b28d6c633f.jpg">}}<br></li><li>Graph for taylor</li><li>{{c2::<img src="paste-f8f1361cfb37e4f1a394ae2c21e3cd5079f9b01e.jpg">}}<br></li><li>The taylor polynomial is an {{c3::excellent aproximation near 0}} {{c4::but for |x| &gt; 2}} it {{c3::stops being so acurate}}</li><li>For example {{c5::at x=3}} the error of the taylor series is {{c5::hundreds of times larger than the lin alg solution}}</li></ul>

============================================================

  

    The {{c5::{{c2::orthogonal}}}} {{c4::{{c3::complement}}}} is a {{c4::{{c1::subspace}}}}

============================================================

  

    The {{c5::{{c3::orthogonal complement}}}} of {{c4::{{c1::{0} }}}} is {{c4::{{c2::the whole vector space V}}}}

============================================================

  

    The {{c5::{{c1::orthogonal complement}}}} of {{c4::{{c2::the whole vector space V}}}} is {{c4::{{c3::{0} }}}}

============================================================

  

    If \(U\) is a subset of \(V\), then \(U \cap U^{\perp} \subset\{0\}\).<br><br>Why does this only require containment and not equality?<br><ul><li>Because we only require U to be a {{c1::subset}} and not a {{c2::subspace}}, {{c5::{{c3::there is no guarantee the origin is in U}}}}</li><li>If we were require it to be a {{c1::subset}} then {{c5::{{c4::we could use equiality}}}}</li></ul>

============================================================

  

    <img src="paste-1815e15cf6af5f878ec0f57d0642884cc05a4946.jpg"><br>Why is this true?<br>Because:<br><ul><li>We can use an {{c1::orthonormal basis of U}} to show that {{c2::v - u is in the orthogonal complement}} meaning that {{c3::V}} = {{c3::\(U\) +&nbsp;\(U^{\perp}\)}}</li><li>We also know that {{c4::the intersection of \(U\) and \(U^{\perp}\)}} is {{c5::0}}, thus {{c5::the sum is a direct sum}}</li></ul>

============================================================

  

    <img src="paste-f0ee3eb59c1cdc0d47dfd909a0162907a48105fa.jpg"><br>Why is the property useful?<br><ul><li>Because{{c1::&nbsp;\(P_U v\)}} is an {{c2::element of U}}</li><li>This means that {{c5::{{c3::the element of U}}}} which {{c5::{{c4::minimizes the norm/distance}}}} must be&nbsp;{{c1::\(P_U v\)}}</li></ul>

============================================================

  

    <img src="paste-77b23a9b1797e386997b28d0e1b85f9639183340.jpg"><br>We can cast this as a {{c1::minimization}} problem with linear algebra because {{c2::both sin}} and {{c3::polynomials}} are in {{c4::the vector space of continuous real-value functions}} {{c5::on the interval}}

============================================================

  

    Why does {{c1::any}} {{c2::polynomial}} {{c3::decomposition}} of sin x {{c4::only use odd powers}}?<br>Because:<br><ul><li>sin(x) is an {{c5::odd}} function</li></ul>

============================================================

  

    Any vector space can be decomposed into a {{c5::{{c1::subspace}}}} and {{c4::{{c2::orthogonal complement}}}} {{c4::{{c3::of that subspace}}}}

============================================================

  

    Proof that vector spaces can be decomposed as a direct sum of a subspace U and its orthogonal complement \(U^\perp\):<br><ul><li>Choose {{c1::an orthonormal basis}} for U</li><li>Then, it {{c2::can be extended to a basis of V}}</li><li>As such, any vector in V can be written as the sum of&nbsp;{{c3::\(v_u\)}}, which equals&nbsp;{{c3::\(\vec{c} \cdot \vec{u}\)}}, and&nbsp;{{c4::\(v-v_u\)}}</li><li>In this case,&nbsp;{{c4::\(v-v_u\)}} must be {{c4::orthogonal to every vector in U}} because {{c5::we have removed the components of v which correspond to&nbsp;\(\vec{u}\)}}</li><li>Since {{c5::the intersection of U and its orthogonal complement is 0}}, this is a direct sum</li></ul>

============================================================

  

    Proof that the intersection of a vector space and its orthogonal complement must be {0}:<br><ul><li>If {{c5::{{c1::v were to belong to this intersection}}}}, it would have to {{c4::{{c2::be orthogonal to itself}}}} which {{c4::{{c3::only happens for the 0 vector}}}}</li></ul>

============================================================

  

    Taking the {{c5::{{c1::orthogonal complement}}}} {{c4::{{c2::twice}}}} {{c4::{{c3::gives back the original subspace}}}}

============================================================

  

    <br>Proof that taking the orthogonal complement twice gives back the original subspace:<br><ul><li>{{c5::{{c3::Any vector u in the original subspace}}}} {{c4::{{c1::is orthogonal to those in the orthogonal complement by definition}}}}</li><li>Thus {{c4::{{c1::u}}}} must be {{c4::{{c2::in&nbsp;\((U^{\perp})^{\perp}\)}}}}</li></ul>

============================================================

  

    {{c1::Orthogonal}} projections exist fundamentally because&nbsp; we can {{c2::write vector spaces}} {{c3::as direct sums}} {{c4::of subspaces}} and {{c5::their orthogonal complements}}

============================================================

  

    For a projection to be a {{c5::{{c1::function}}}}, it must require that {{c4::{{c2::the vector space can be written as a direct sum of subspaces}}}} because {{c4::{{c3::it needs to map each element to only one output}}}}

============================================================

  

    We can think of a {{c3::projection}}&nbsp;{{c4::\(P_u\)}} in two ways<br><ul><li>{{c4::\(P_u\)}}&nbsp;\(\in\)&nbsp;{{c5::{{c1::\(L(V)\)}}}}<br></li><li>{{c4::\(P_u\)}}&nbsp;\(\in\)&nbsp;{{c5::{{c2::\(L(V,U)\)}}}}<br></li></ul>

============================================================

  

    Any linear {{c2::operator}} is also a {{c3::linear map}} from {{c5::{{c4::V}}}} to {{c5::{{c1::its range}}}}

============================================================

  

    Proof that the projection operator&nbsp;\(P_u\) is linear:<br><ul><li>Addition:&nbsp;</li><ul><li>We want to show \(P_u(v+v')\)&nbsp; =&nbsp;\(P_u(v) + P_u(v')\)</li><li>\(v\) =&nbsp;{{c1::\(u + w\)}}<br></li><li>\(v'\) =&nbsp;{{c2::\(u' + w'\)}}<br></li><li>Thus&nbsp;&nbsp;\(P_u(v+v')\)&nbsp; =&nbsp;{{c3::\(P_u(u + w +u' + w')&nbsp;\)}} =&nbsp;{{c4::\(u+u'\)}} = {{c5::\(P_u(v) + P_u(v')\)}}</li></ul></ul>

============================================================

  

    Proof that the projection operator&nbsp;\(P_u\) is linear:<br><ul><li>Homogeneity:</li><ul><li>\(P_u(\alpha v)\)<br></li><li>v =&nbsp;{{c5::{{c1::\(u +w\)}}}}</li><li>\(\alpha v \) =&nbsp;{{c4::{{c2::\(\alpha u + \alpha w\)}}}}<br></li><li>Meaning</li><li>\(P_u(\alpha v)\) ={{c4::{{c3::&nbsp;\(\alpha P_u(v)&nbsp;\)}}}}</li></ul></ul>

============================================================

  

    The {{c5::{{c1::range}}}} of a {{c4::{{c2::projection operator}}}} is the {{c4::{{c3::projected-to subspace}}}}

============================================================

  

    The {{c5::{{c1::null space}}}} of a {{c4::{{c2::projection operator}}}} is {{c4::{{c3::the orthogonal complement of the projected-to subspace}}}}

============================================================

  

    v - {{c5::{{c1::\(P_u(v)\)}}}}&nbsp;{{c4::{{c2::\(\in \)&nbsp;}}}}{{c4::{{c3::\(U^\perp\)}}}}

============================================================

  

    The {{c1::orthogonal projection}} onto a subspace can be written as a {{c2::linear combiantion}} {{c3::of an orthonormal basis}} {{c4::of the projected-to subspace}} with {{c5::the dot-products as coefficients}}

============================================================

  

    The norm of an orthogonal projection of a vector must be {{c5::{{c1::smaller than the norm of the vector}}}} unless {{c4::{{c2::that vector}}}} {{c4::{{c3::is already in the projected-to subspace}}}}

============================================================

  

    How can we think of using orthogonal projections as a means of solving minimization problems conceptually?<br><img src="paste-9a3bc3fb59f7c39d492add8e0057da0f3f0b3e29.jpg"><br><ul><li>Logically: if U is {{c1::a model of reality}} and v is {{c2::a noisy observation}}, what is the closest point in {{c1::reality}} to {{c2::the noisy v.}} With U being {{c3::lower-dimensional.}}</li><ul><li>This requires that the phenomena actually be {{c3::linear}}</li></ul><li>Statistically: if U is a {{c4::staistical model}} based on a&nbsp; {{c4::lower-dimensional choice of parameters}} and v is a {{c5::phenomena}} we want {{c5::the point in the model that best explains v}}</li></ul>

============================================================

  

    For {{c5::{{c1::minimization}}}} problems using linear algebra, we define {{c4::{{c2::distance}}}} as {{c4::{{c3::the norm of the difference between two vectors}}}}

============================================================

  

    For an inner product space, two things are relevant to minimization problems:<br><ol><li>{{c1::<img src="paste-dbe9f4d90828a321145da5ed0cbfb23b865c6b12.jpg">}}<br></li><ol><li>So {{c2::the projection of v onto u is the closest point to v in the subspace}}</li></ol><li>{{c5::{{c3::<img src="paste-e91111670326bac73bd700ed6120c1ec96e9109e.jpg">}}}}<br></li><ol><li>So {{c5::{{c4::any vector in u which reaches equality must be the closest vector}}}}</li></ol></ol><br>

============================================================

  

    <b>Definition</b> {{c1::adjoint}}, {{c2::\(T^{*}\)}}<br><br>Suppose \(T \in \mathcal{L}(V, W)\). The {{c1::adjoint}} of \(T\) is the function {{c2::\(T^{*}\)}} : {{c5::\(W\)}}&nbsp; \(\rightarrow\) {{c5::\(V\)}} such that<br><br><ul><li>{{c3::\(\langle T v, w\rangle\)}} = {{c4::\(\left\langle v, T^{*} w\right\rangle\)}}</li></ul><br>for {{c5::every \(v \in V\)}} and {{c5::every \(w \in W\)}}.<br>

============================================================

  

    \subsection{Example Define \(T: \mathbf{R}^{3} \rightarrow \mathbf{R}^{2}\) by}<br><br>\[<br>T\left(x_{1}, x_{2}, x_{3}\right)=\left(x_{2}+3 x_{3}, 2 x_{1}\right) .<br>\]<br><br>Find a formula for \(T^{*}\).<br><br>Solution :<br><ul><li>Here \(T^{*}\) will be a function from \(\mathbf{R}^{2}\) to \(\mathbf{R}^{3}\). To compute \(T^{*}\),</li><li>Solution method: {{c1::fix a point \(\left(y_{1}, y_{2}\right) \in \mathbf{R}^{2}\)::replace T adj with this}}. Then for every \(\left(x_{1}, x_{2}, x_{3}\right) \in \mathbf{R}^{3}\) we have</li></ul><br><ul><li>\(\langle\left(x_{1}, x_{2}, x_{3}\right),\) {{c1::\( T^{*}\left(y_{1}, y_{2}\right)\)::fixed point}} \(\rangle&nbsp; \)</li><li>= {{c2::\(&nbsp;\langle T(x_{1} \)}}&nbsp;,&nbsp; {{c2::\( (x_{2}, x_{3}), (y_{1}, y_{2})\)}} \(\rangle \)</li><li>=\( \langle \) {{c3::\((x_{2}+3 x_{3}\)}}, {{c3::\( 2 x_{1}),(y_{1}, y_{2})\)}} \(\rangle \)</li><li>= {{c4::\( x_{2} y_{1}+3 x_{3} y_{1}+2 x_{1} y_{2} \)}}</li><li>=\(\langle\) {{c5::\((x_{1}, x_{2}, x_{3})\)}}, {{c5::\((2 y_{2}, y_{1}, 3 y_{1})\)}} \( \rangle .\)</li></ul><br>Thus<br><br><ul><li>\(T^{*}\left(y_{1}, y_{2}\right)\) = {{c5::\(\left(2 y_{2}, y_{1}, 3 y_{1}\right) \text {. }\)}}</li></ul><br>

============================================================

  

    7.4 Example Fix \(u \in V\) and \(x \in W\). Define \(T \in \mathcal{L}(V, W)\) by<br><br>\[<br>T v=\langle v, u\rangle x<br>\]<br><br>for every \(v \in V\). Find a formula for \(T^{*}\).<br><br>Solution Fix \(w \in W\). Then for every \(v \in V\) we have<br><br><ul><li>\(\left\langle v, T^{*} w\right\rangle\)&nbsp;</li><li>= {{c1::\(\langle T v, w\rangle \)::inner prod}}</li><li>= {{c2::\(\langle\langle v, u\rangle x, w\rangle \)::inner prod}}</li><li>= {{c5::{{c3::\( \langle v, u\rangle\langle x, w\rangle \)::inner prod}}::inner prod}}</li><li>= {{c5::{{c4::\(\langle v,\langle w, x\rangle u\rangle .\)::inner prod}}::inner prod}}</li></ul><br>Thus<br><br><ul><li>\(T^{*} w\) = {{c5::{{c4::\(\langle w, x\rangle u\)}}}}</li></ul>

============================================================

  

    The {{c1::adjoint}} is a {{c2::linear map}}<br><br>If \(T\)&nbsp; \(\in\) {{c5::{{c4::\(\mathcal{L}(V, W)\)}}}}, then {{c1::\(T^{*}\)}}&nbsp; \(\in\)&nbsp; {{c5::{{c3::\(\mathcal{L}(W, V)\)}}}}.

============================================================

  

    The common way to prove something about the {{c5::{{c3::adjoint}}}} is to use the following identity:<br><br><ul><li>{{c4::{{c1::\(\langle T v, w\rangle\)}}}} = {{c4::{{c2::\(\left\langle v, T^{*} w\right\rangle\)}}}}</li></ul>

============================================================

  

    <img src="paste-928c7b4d4ec4d336029cb4efab83abd2f673a4c8.jpg"><br>Proving additivity:<br><br>Proof Suppose \(T \in \mathcal{L}(V, W)\). Fix \(w_{1}, w_{2} \in W\). If \(v \in V\), then<br><br><ul><li>{{c1::\(\left\langle v, T^{*}\left(w_{1}+w_{2}\right)\right\rangle \)::inner prod}}</li><li>= {{c2::\(\left\langle T v, w_{1}+w_{2}\right\rangle \)::inner prod}}</li><li>&nbsp;= {{c3::\(\left\langle T v, w_{1}\right\rangle+\left\langle T v, w_{2}\right\rangle \)::inner prod}}</li><li>= {{c4::\(\left\langle v, T^{*} w_{1}\right\rangle+\left\langle v, T^{*} w_{2}\right\rangle \)::inner prod}}</li><li>= {{c5::\(\left\langle v, T^{*} w_{1}+T^{*} w_{2}\right\rangle,\)::inner prod}}</li></ul><br>which shows that \(T^{*}\left(w_{1}+w_{2}\right)=T^{*} w_{1}+T^{*} w_{2}\).<br>

============================================================

  

    <img src="paste-05f616a623aeee78b939fee6a40f46048d2933b7.jpg"><br>Proving homogeneity<br>Fix \(w \in W\) and \(\lambda \in \mathbf{F}\). If \(v \in V\), then<br><br><ul><li>{{c1::\(\left\langle v, T^{*}(\lambda w)\right\rangle\)::inner prod}}</li><li>= {{c2::\(\langle T v, \lambda w\rangle \)::inner prod}}</li><li>= {{c3::\(\bar{\lambda}\langle T v, w\rangle \)::inner prod}}</li><li>= {{c4::\(\bar{\lambda}\left\langle v, T^{*} w\right\rangle \)::inner prod}}</li><li>= {{c5::\(\left\langle v, \lambda T^{*} w\right\rangle,\)::inner prod}}</li></ul><br>which shows that \(T^{*}(\lambda w)=\lambda T^{*} w\).<br>

============================================================

  

    Properties of the adjoint<br><br><ul><li>(a) {{c1::\(\quad(S+T)^{*}\)}} = {{c2::\(S^{*}+T^{*}\)}} for {{c1::all \(S, T \in \mathcal{L}(V, W)\);}}</li><li><br></li><li>(b) {{c2::\((\lambda T)^{*}\)}} = {{c1::\(\bar{\lambda} T^{*}\)}} for {{c2::all \(\lambda \in \mathbf{F}\)}} and {{c2::\(T \in \mathcal{L}(V, W)\);}}</li><li><br></li><li>(c) {{c3::\(\left(T^{*}\right)^{*}\)}} = {{c4::\(T\)}} for {{c3::all \(T \in \mathcal{L}(V, W)\)}};</li><li><br></li><li>(d) {{c4::\(I^{*}\)}} = {{c3::\(I\)}}, where {{c4::\(I\)}} is {{c4::the identity operator on \(V\)}};</li><li><br></li><li>(e) {{c5::\((S T)^{*}\)}} = {{c5::\(T^{*} S^{*}\)}} for {{c5::all \(T \in \mathcal{L}(V, W)\)}} and {{c5::\(S \in \mathcal{L}(W, U)\)}}.</li></ul>

============================================================

  

    \subsection{Properties of the adjoint}<br><br>(a) {{c5::{{c1::\(\quad(S+T)^{*}\)}}}} = {{c4::{{c2::\(S^{*}+T^{*}\)}}}} for {{c4::{{c3::all \(S, T \in \mathcal{L}(V, W)\)}}}};

============================================================

  

    \subsection{Properties of the adjoint}<br><br>(b) {{c1::\((\lambda T)^{*}\)}} = {{c2::\(\bar{\lambda} T^{*}\)}} for all {{c5::{{c3::\(\lambda \in \mathbf{F}\)}}}} and {{c5::{{c4::\(T \in \mathcal{L}(V, W)\)}}}};

============================================================

  

    \subsection{Properties of the adjoint}<br><br><br>(c) {{c5::{{c1::\(\left(T^{*}\right)^{*}\)}}}} = {{c4::{{c2::\(T\)}}}} for all {{c4::{{c3::\(T \in \mathcal{L}(V, W)\)}}}};

============================================================

  

    \subsection{Properties of the adjoint}<br><br>(d) {{c1::\(I^{*}\)}} = {{c2::\(I\)}}, where {{c2::\(I\)}} is {{c5::{{c3::the identity operator}}}} {{c5::{{c4::on \(V\)}}}};

============================================================

  

    \subsection{Properties of the adjoint}<br><br><br>(e) {{c1::\((S T)^{*}\)}} = {{c2::\(T^{*} S^{*}\)}} for all {{c5::{{c3::\(T \in \mathcal{L}(V, W)\)}}}} and {{c5::{{c4::\(S \in \mathcal{L}(W, U)\)}}}}

============================================================

  

    <img src="paste-209e543b8b7113c0d02f25384bafdb066510358b.jpg"><br>Proof:<br><br>(b) Suppose \(\lambda \in \mathbf{F}\) and \(T \in \mathcal{L}(V, W)\). If \(v \in V\) and \(w \in W\), then<br><br><ul><li>{{c1::\(\left\langle v,(\lambda T)^* w\right\rangle\)}}<br></li><li>=&nbsp;{{c2::\(\langle\lambda T v, w\rangle\)}}</li><li>=&nbsp;{{c3::\(\lambda\langle T v, w\rangle\)}}</li><li>=&nbsp;{{c4::\(\lambda\left\langle v, T^* w\right\rangle\)}}</li><li>=&nbsp;{{c5::\(\left\langle v, \bar{\lambda} T^* w\right\rangle\)}}</li></ul><br>Thus \((\lambda T)^{*} w=\bar{\lambda} T^{*} w\), as desired.<br>

============================================================

  

    <img src="paste-73d7ba38c7fe7e43bc46b6de81b1b35c4f040b4a.jpg"><br>Proof for c:<br><br>(c) Suppose \(T \in \mathcal{L}(V, W)\). If \(v \in V\) and \(w \in W\), then<br><br><ul><li>\(\left\langle w,\left(T^{*}\right)^{*} v\right\rangle\)&nbsp;</li><li>= {{c1::\(\left\langle T^{*} w, v\right\rangle\)&nbsp;}}</li><li>= {{c2::\(\overline{\left\langle v, T^{*} w\right\rangle}\)&nbsp;}}</li><li>= {{c5::{{c3::\(\overline{\langle T v, w\rangle} \)&nbsp;}}}}</li><li>= {{c5::{{c4::\(\langle w, T v\rangle .\)}}}}</li></ul><br>Thus \(\left(T^{*}\right)^{*} v=T v\), as desired.<br>

============================================================

  

    <img src="paste-64b5ba9754522ffa8034ed768341f012d91da116.jpg"><br>Proof:<br><br>(e) Suppose \(T \in \mathcal{L}(V, W)\) and \(S \in \mathcal{L}(W, U)\). If \(v \in V\) and \(u \in U\), then<br><br><ul><li>\(\left\langle v,(S T)^{*} u\right\rangle\)</li><li>= {{c5::{{c1::\(\langle S T v, u\rangle\)&nbsp;}}}}</li><li>= {{c4::{{c2::\(\left\langle T v, S^{*} u\right\rangle\)::recurse}}::recurse}}</li><li>= {{c4::{{c3::\(\left\langle v, T^{*}\left(S^{*} u\right)\right\rangle .\)::again}}::again}}</li></ul><br>Thus \((S T)^{*} u=T^{*}\left(S^{*} u\right)\), as desired.<br><br>

============================================================

  

    <b>Null space</b> and <b>range</b> of \(T^{*}\)<br><br>Suppose \(T \in \mathcal{L}(V, W)\). Then<br><ul><li>(a) {{c1::\(\operatorname{null} T^{*}\)}} = {{c2::\((\text { range } T)^{\perp}\)}};<br></li><li><br></li><li>(b) {{c2::range \(T^{*}\)}} = {{c1::\((\operatorname{null} T)^{\perp}\)}};</li><li><br></li><li>(c) {{c5::{{c3::\(\operatorname{null} T\)}}}} = {{c5::{{c4::\(\left(\text { range } T^{*}\right)^{\perp}\)}}}};</li><li><br></li><li>(d) {{c5::{{c4::range \(T\)}}}} = {{c5::{{c3::\(\left(\text { null } T^{*}\right)^{\perp}\)}}}}.</li></ul>

============================================================

  

    <b>Null space</b>&nbsp;and&nbsp;<b>range</b>&nbsp;of \(T^{*}\)<br><br>Suppose \(T \in \mathcal{L}(V, W)\). Then<br><ul><li>(a) {{c1::\(\operatorname{null} T^{*}\)}} = {{c1::\((\text { range } T)^{\perp}\)}};<br></li><li><br></li><li>(b) {{c2::range \(T^{*}\)}}= {{c2::\((\operatorname{null} T)^{\perp}\)}};</li><li><br></li><li>(c) {{c5::{{c3::\(\operatorname{null} T\)}}}} = {{c5::{{c3::\(\left(\text { range } T^{*}\right)^{\perp}\)}}}};</li><li><br></li><li>(d) {{c5::{{c4::range \(T\)}}}} = {{c5::{{c4::\(\left(\text { null } T^{*}\right)^{\perp}\)}}}}.</li></ul>

============================================================

  

    {{c3::Null space}}&nbsp;and&nbsp;{{c4::range}}&nbsp;of \(T^{*}\)<br><br>Suppose \(T \in \mathcal{L}(V, W)\). Then<br><ul><li>(a) {{c5::{{c1::\(\operatorname{null} T^{*}\)}}}} = {{c5::{{c2::\((\text { range } T)^{\perp}\)}}}};</li></ul>

============================================================

  

    {{c1::Null space}}&nbsp;and&nbsp;{{c2::range}}&nbsp;of \(T^{*}\)<br><br>Suppose \(T \in \mathcal{L}(V, W)\). Then<ul><li>(b) {{c5::{{c3::range \(T^{*}\)}}}}= {{c5::{{c4::\((\operatorname{null} T)^{\perp}\)}}}};</li></ul>

============================================================

  

    {{c1::Null space}}&nbsp;and&nbsp;{{c2::range}}&nbsp;of \(T^{*}\)<br><br>Suppose \(T \in \mathcal{L}(V, W)\). Then<ul><li>(c) {{c5::{{c3::\(\operatorname{null} T\)}}}} = {{c5::{{c4::\(\left(\text { range } T^{*}\right)^{\perp}\)}}}};</li></ul>

============================================================

  

    {{c1::Null space}}&nbsp;and&nbsp;{{c2::range}}&nbsp;of \(T^{*}\)<br><br>Suppose \(T \in \mathcal{L}(V, W)\). Then<br><ul><li>{{c5::{{c3::(d) range \(T\)}}}} = {{c5::{{c4::\(\left(\text { null } T^{*}\right)^{\perp}\)}}}}.<br></li></ul>

============================================================

  

    <img src="paste-4679dd4a3c60e3380cda6e152f581258b49de687.jpg"><br><br>Proof We begin by proving (a). Let \(w \in W\). Then<br><br><ul><li>\(w \in \operatorname{null} T^{*} \)&nbsp;&nbsp;</li><li>\(\Longleftrightarrow\) {{c1::\( T^{*} w\)::by def}} = {{c1::\(0 \)::by def}}</li><li>\(\Longleftrightarrow\) {{c2::\(\left\langle v, T^{*} w\right\rangle\)::expand def}} = {{c2::\(0\)}} \(\text { for all } v \in V \)</li><li>\(\Longleftrightarrow\) {{c5::{{c3::\(\langle T v, w\rangle\)::further expand def}}::further expand def}} = {{c5::{{c3::\(0\)}}}} \(\text { for all } v \in V \)</li><li>\(\Longleftrightarrow\) \(w\) \(\in\) {{c5::{{c4::\((\text { range } T)^{\perp} .\)}}}}</li></ul><br>Thus null \(T^{*}=(\operatorname{range} T)^{\perp}\), proving (a).<br><br>

============================================================

  

    <img src="paste-3a17ead3b362bcf2c581832d899b7b869839ee01.jpg"><br>Proof We begin by proving (a). Let \(w \in W\). Then<br><br>\[<br>\begin{aligned}<br>w \in \operatorname{null} T^{*} &amp; \Longleftrightarrow T^{*} w=0 \\<br>&amp; \Longleftrightarrow\left\langle v, T^{*} w\right\rangle=0 \text { for all } v \in V \\<br>&amp; \Longleftrightarrow\langle T v, w\rangle=0 \text { for all } v \in V \\<br>&amp; \Longleftrightarrow w \in(\text { range } T)^{\perp} .<br>\end{aligned}<br>\]<br><br>Thus null \(T^{*}=(\operatorname{range} T)^{\perp}\), proving (a).<br><br>THen:<br><br><ul><li>If we {{c1::take the orthogonal complement of both sides}} of (a), we get {{c2::(d)}},&nbsp;</li><li>Replacing {{c3::\(T\)}} with {{c3::\(T^{*}\)}} in (a) gives {{c4::(c)}}</li><li>Finally, replacing {{c3::\(T\)}} with {{c3::\(T^{*}\)}} in {{c5::(d)}} gives {{c5::(b)}}.</li></ul>

============================================================

  

    <b>Definition</b> {{c1::conjugate transpose}}<br><br>The {{c1::conjugate transpose}} of an {{c2::\(m\)-by- \(n\)::size}} matrix is the {{c2::\(n\)-by- \(m\)::size}} matrix obtained by {{c5::{{c3::interchanging the rows and columns}}}} and then {{c5::{{c4::taking the complex conjugate of each entry.}}}}

============================================================

  

    If \(\mathbf{F}=\mathbf{R}\), then the {{c5::{{c1::conjugate transpose}}}} of a matrix is {{c4::{{c2::the same as its transpose}}}}, which is the matrix obtained by {{c4::{{c3::interchanging the rows and columns.}}}}

============================================================

  

    <b>Definition</b> {{c1::conjugate transpose}}<br><br>The {{c1::conjugate transpose}} of an {{c2::\(m\)-by- \(n\)::size?}} matrix is the {{c2::\(n\)-by- \(m\)::size?}} matrix obtained by {{c5::{{c3::interchanging the rows and columns::step 1}}::step 1}} and {{c5::{{c4::then taking the complex conjugate of each entry.::step 2}}::step 2}}

============================================================

  

    The adjoint of a linear map does {{c1::not depend::depend or not?}} on {{c2::a choice of basis}}. This explains why this book emphasizes {{c5::{{c3::adjoints of linear maps}}}} instead of {{c5::{{c4::conjugate transposes of matrices.}}}}

============================================================

  

    &nbsp;With respect to {{c1::nonorthonormal}} bases, the matrix of {{c2::\(T^{*}\)}} {{c3::does not::does/does not?}} {{c4::necessarily}} {{c5::equal the conjugate transpose of the matrix of \(T\)}}.

============================================================

  

    The matrix of \(T^{*}\)<br><br>Let \(T \in \mathcal{L}(V, W)\). Suppose \(e_{1}, \ldots, e_{n}\) is an {{c1::orthonormal}} basis of \(V\) and \(f_{1}, \ldots, f_{m}\) is an {{c1::orthonormal}} basis of \(W\). Then<br><br><ul><li>{{c2::\(\mathcal{M}\left(T^{*},\left(f_{1}, \ldots, f_{m}\right),\left(e_{1}, \ldots, e_{n}\right)\right)\)}}</li></ul><br>is {{c5::{{c3::the conjugate transpose of}}}}<br><br><ul><li>{{c5::{{c4::\(\mathcal{M}\left(T,\left(e_{1}, \ldots, e_{n}\right),\left(f_{1}, \ldots, f_{m}\right)\right) .\)}}}}</li></ul>

============================================================

  

    <b>Definition</b> {{c1::self-adjoint}}<br><br>An operator \(T \in \mathcal{L}(V)\) is called {{c1::self-adjoint}} if {{c2::\(T\)}} = {{c3::\(T^{*}\)}}. In other words, \(T \in \mathcal{L}(V)\) is {{c1::self-adjoint}} if and only if<br><br><ul><li>{{c4::\(\langle T v, w\rangle\)}} = {{c5::\(\langle v, T w\rangle\)}}</li></ul><br>for {{c5::all \(v, w \in V\).}}<br>

============================================================

  

    You should verify that the {{c1::sum}} of two {{c1::self-adjoint}} operators is {{c2::self-adjoint}} and that the {{c5::{{c3::product}}}} of a {{c5::{{c4::real scalar}}}} and a {{c1::self-adjoint}} operator is {{c2::self-adjoint}}.

============================================================

  

    {{c1::Eigenvalues}} of {{c2::self-adjoint}} operators are {{c5::{{c3::real}}}}<br><br>{{c5::{{c4::Every}}}} {{c1::eigenvalue}} of a {{c2::self-adjoint}} operator is {{c3::real}}.

============================================================

  

    Proof Suppose \(T\) is a self-adjoint operator on \(V\). Let \(\lambda\) be an eigenvalue of \(T\), and let \(v\) be a nonzero vector in \(V\) such that \(T v=\lambda v\). Then<br><br><ul><li>\(\lambda\|v\|^{2}\)</li><li>= {{c1::\(\langle\lambda v, v\rangle \)}}</li><li>= {{c2::\(\langle T v, v\rangle\)}}</li><li>= {{c3::\(\langle v, T v\rangle \)}}</li><li>= {{c4::\(\langle v, \lambda v\rangle=\bar{\lambda}\|v\|^{2} .\)}}</li></ul><br>Thus {{c5::\(\lambda=\bar{\lambda}\)}}, which means that {{c5::\(\lambda\) is real, as desired.}}<br>

============================================================

  

    Over \(\mathbf{C}\) , {{c2::\(T v\)}} is {{c1::orthogonal}} to {{c5::{{c3::\(v\)}}}} for {{c5::{{c3::all \(v\)}}}} {{c5::{{c4::only for the 0 operator}}}}<br>

============================================================

  

    Suppose \(V\) is a complex inner product space and \(T \in \mathcal{L}(V)\). Suppose<br><br><ul><li>{{c1::\(\langle T v, v\rangle\)}} = {{c2::\(0\)}}</li></ul><br>for {{c5::{{c3::all \(v \in V\)}}}}. Then {{c5::{{c4::\(T=0\)}}}}.<br>

============================================================

  

    Over {{c1::\(\mathbf{C}\)}} {{c2::\(\langle T v, v\rangle\)}} is {{c3::real}} for {{c4::all \(v\)}} {{c5::only for self-adjoint operators}}

============================================================

  

    Suppose \(V\) is a complex inner product space and \(T \in \mathcal{L}(V)\). Then \(T\) is {{c1::self-adjoint}} if and only if<br><ul><li>{{c2::\(\langle T v, v\rangle\)}}&nbsp; {{c3::\(\in \)}}&nbsp; {{c4::\(\mathbf{R}\)}}<br></li></ul><br>for {{c5::every \(v \in V\).}}<br>

============================================================

  

    If {{c1::\(T\)}} = {{c2::\(T^{*}\)}} and {{c3::\(\langle T v, v\rangle\)}} = {{c4::\(0\)}} for all \(v\), then {{c5::\(T\)}} = {{c4::\(0\)}}

============================================================

  

    Suppose \(T\) is a {{c1::self-adjoint}} operator on \(V\) such that<br><br><ul><li>{{c2::\(\langle T v, v\rangle\)}} = {{c3::\(0\)}}</li></ul><br>for {{c4::all \(v \in V\)}}. Then {{c5::\(T\)}} = {{c3::\(0\).}}<br>

============================================================

  

    On a {{c1::real}} inner product space \(V\), a {{c2::nonzero operator \(T\)}} might {{c3::satisfy \(\langle T v, v\rangle=0\)}} for {{c4::all \(v \in V\)}}. However, this cannot happen for a {{c5::self-adjoint}} operator.

============================================================

  

    Definition {{c1::normal}}<br><br>- An operator on an inner product space is called {{c1::normal}} if it {{c2::commutes with its adjoint.}}<br><br>- In other words, \(T \in \mathcal{L}(V)\) is {{c1::normal}} if<br><br><ul><li>{{c5::{{c3::\(T T^{*}\)}}}} = {{c5::{{c4::\(T^{*} T \text {. }\)}}}}</li></ul>

============================================================

  

    Obviously every {{c1::self-adjoint}} operator is {{c2::normal}}, because if \(T\) is {{c1::self-adjoint}} then {{c5::{{c3::\(T^{*}\)}}}} = {{c5::{{c4::\(T\).}}}}

============================================================

  

    &nbsp;{{c1::null::null/range?}} {{c2::\(T\)}} = {{c3::\(\operatorname{null}\)::null/range?}} {{c4::\(T^{*}\)}} for every {{c5::normal}} operator \(T\).

============================================================

  

    \(T\) is {{c1::normal}} if and only if {{c2::\(\|T v\|\)}} = {{c5::{{c3::\(\left\|T^{*} v\right\|\)}}}} for {{c5::{{c4::all \(v\)}}}}<br><br>

============================================================

  

    An operator \(T \in \mathcal{L}(V)\) is {{c1::normal}} if and only if<br><br><ul><li>{{c2::\(\|T v\|\)}} = {{c5::{{c3::\(\left\|T^{*} v\right\|\)}}}}</li></ul><br>for {{c5::{{c4::all \(v \in V\).}}}}<br>

============================================================

  

    A {{c1::normal}} operator and its {{c2::adjoint}} have {{c5::{{c3::the same}}}} {{c5::{{c4::eigenvectors}}}}.

============================================================

  

    The {{c1::eigenvalues}} of the {{c2::adjoint}} of each operator are equal (as a {{c3::set}}) to {{c4::the complex conjugates}} of {{c5::the eigenvalues of the operator}}. Howerver, an operator and its adjoint may have different {{c5::eigenvectors}}.

============================================================

  

    For \(T\) {{c1::normal}}, {{c2::\(T\)}} and {{c3::\(T^{*}\)}} have {{c4::the same}} {{c5::eigenvectors}}<br>

============================================================

  

    Suppose \(T \in \mathcal{L}(V)\) is {{c1::normal}} and \(v \in V\) is an eigenvector of {{c5::\(T\)}} with eigenvalue {{c2::\(\lambda\)}}. Then \(v\) is also an eigenvector of {{c3::\(T^{*}\)}} with eigenvalue {{c4::\(\bar{\lambda}\)}}.

============================================================

  

    {{c1::Orthogonal}} {{c2::eigenvectors}} for {{c3::normal}} operators<br><br>Suppose \(T \in \mathcal{L}(V)\) is {{c3::normal}}. Then {{c2::eigenvectors}} of \(T\) {{c5::corresponding to}} {{c4::distinct eigenvalues}} are {{c1::orthogonal}}.

============================================================

  

    2 Suppose \(T \in \mathcal{L}(V)\) and \(\lambda \in \mathbf{F}\). Prove that \(\lambda\) is an eigenvalue of \(T\) if and only if {{c5::{{c1::\(\bar{\lambda}\)}}}} is {{c4::{{c2::an eigenvalue of}}}} {{c4::{{c3::\(T^{*}\)}}}}.

============================================================

  

    3 Suppose \(T \in \mathcal{L}(V)\) and \(U\) is a subspace of \(V\). Prove that \(U\) is {{c1::invariant}} under \(T\) if and only if {{c2::\(U^{\perp}\)}} is {{c5::{{c3::invariant under}}}} {{c5::{{c4::\(T^{*}\).}}}}

============================================================

  

    4 Suppose \(T \in \mathcal{L}(V, W)\). Prove that<br><br>(a) {{c1::\(T\)}} is {{c2::injective}} if and only if {{c5::{{c3::\(T^{*}\)}}}} is {{c5::{{c4::surjective}}}};<br><br>(b) {{c1::\(T\)}} is {{c4::surjective}} if and only if {{c3::\(T^{*}\)}} is {{c2::injective}}.

============================================================

  

    5 Prove that<br><br><ul><li>\(\operatorname{dim}\) {{c1::\(n u l l T^{*}\)&nbsp;}}</li><li>= \(\operatorname{dim}\) {{c2::\(n u l l T\)}} {{c5::+}} \(\operatorname{dim}\)&nbsp; {{c3::\(W\)}} {{c5::-}} \(\operatorname{dim}\) {{c4::\(V\)}}</li></ul>

============================================================

  

    5 Prove that<br><br><ul><li>\(\operatorname{dim}\)&nbsp; {{c1::\(n u l l\)}} {{c2::\(T^{*}\)&nbsp;}}</li><li>= {{c3::\(\operatorname{dim} n u l l T\)}} {{c5::+}} {{c4::\(\operatorname{dim} W\)}} {{c5::-}} {{c5::\(\operatorname{dim} V\)}}</li></ul>

============================================================

  

    5 Prove that<br><br><ul><li>dim {{c1::range}}{{c2::\(T^*\)}} = dim {{c5::{{c3::range}}}}{{c5::{{c4::T}}}}</li></ul><br>

============================================================

  

    7 Suppose \(S, T \in \mathcal{L}(V)\) are {{c1::self-adjoint}}. Prove that {{c3::\(S T\)}} is {{c5::{{c2::self-adjoint}}}} if and only if {{c3::\(S T\)}} = {{c5::{{c4::\(T S\)}}}}.

============================================================

  

    8 Suppose \(V\) is a {{c1::real}} inner product space. Show that the set of {{c2::self-adjoint}} operators on \(V\) is a {{c5::{{c3::subspace}}}} {{c5::{{c4::of \(\mathcal{L}(V)\).}}}}

============================================================

  

    Suppose \(V\) is a {{c1::complex}} inner product space with V {{c2::\(neq\{0\}\)}}. Show that the set of {{c3::self-adjoint}} operators on \(V\) is {{c4::not a subspace}} {{c5::of \(\mathcal{L}(V)\).}}

============================================================

  

    10 Suppose \(\operatorname{dim} V\)&nbsp; {{c1::\(\geq 2\)}}. Show that the set of {{c2::normal}} operators on \(V\) is {{c5::{{c3::not a subspace}}}} {{c5::{{c4::of \(\mathcal{L}(V)\).}}}}

============================================================

  

    Suppose \(P \in \mathcal{L}(V)\) is such that {{c1::\(P^{2}\)}} = {{c2::\(P\)}}. Prove that there is a {{c3::subspace \(U\) of \(V\)}} such that {{c4::\(P\)}} = {{c4::\(P_{U}\)}} if and only if \(P\) is {{c5::self-adjoint}}.

============================================================

  

    15 Fix \(u, x \in V\). Define \(T \in \mathcal{L}(V)\) by<br><br><ul><li>\(T v\) = {{c1::\(\langle v, u\rangle x\)}}</li></ul><br>for every \(v \in V\).<br><br>(a) Suppose \(\mathbf{F}\) = {{c2::\(\mathbf{R}\)}}. Prove that \(T\) is {{c3::self-adjoint}} if and only if {{c4::\(u, x\)}} is {{c5::linearly dependent.}}<br>

============================================================

  

    16 Suppose \(T \in \mathcal{L}(V)\) is {{c1::normal}}. Prove that<br><br><ul><li>{{c2::\(\text { range }\)}}{{c3::\(T\)}} = {{c4::\(\operatorname{range}\)}}{{c5::\(T^{*} \text {. }\)}}</li></ul>

============================================================

  

    17 Suppose \(T \in \mathcal{L}(V)\) is normal. Prove that<br><br><ul><li>{{c1::\(\text { null }\)}} {{c2::\(T^{k}\)}} = {{c1::\(\operatorname{null}\)}} {{c5::{{c3::\(T\)&nbsp;}}}}</li><li>{{c5::{{c4::\( \operatorname{range }\)}}}}{{c2::\(T^{k}\)}}= {{c5::{{c4::\(\operatorname{range}\)}}}}{{c5::{{c3::\(T\)}}}}</li></ul><br>for every {{c2::positive integer \(k\)}}.<br>

============================================================

  

    5. Define \(T \in \mathcal{L}\left(\mathbf{F}^{2}\right)\) by<br><br>\[<br>T(w, z)=(z, w)<br>\]<br><br>Find all eigenvalues and eigenvectors of \(T\).<br><br><ul><li>\(w\)&nbsp; =&nbsp; \( \lambda z\)<br></li><li>\(z\) =&nbsp;\(\lambda w\)<br></li><li>Thus:</li><li>{{c1::\(\lambda^2 w \)}} = {{c1::\(w\)}}<br></li><li>{{c2::\(w (\lambda -1) (\lambda + 1)\)}} = {{c2::0}}<br></li><li>This implies that&nbsp;\(\lambda\) = {{c3::1}} or&nbsp;\(\lambda\) = {{c4::-1}}</li><li>For&nbsp;\(\lambda \) = {{c3::1}}</li><ul><li>The eigenvector is {{c5::(w,w)}}</li></ul><li>For&nbsp;\(\lambda\) = {{c4::-1}}</li><ul><li>The eigenvector is {{c5::(w,-w)}}</li></ul></ul>

============================================================

  

    7. Suppose \(n\) is a positive integer and \(T \in \mathcal{L}\left(\mathrm{F}^{n}\right)\) is defined by<br><br>\[<br>T\left(x_{1}, \ldots, x_{n}\right)=\left(x_{1}+\cdots+x_{n}, \ldots, x_{1}+\cdots+x_{n}\right) ;<br>\]<br><br>in other words, \(T\) is the operator whose matrix (with respect to the standard basis) consists of all 1's. Find all eigenvalues and eigenvectors of \(T\).<br><br>Solution:<br>Solution: Suppose \(\lambda\) is an eigenvalue of \(T\). For this particular operator, the eigenvalue-eigenvector equation \(T x=\lambda x\) becomes the system of equations<br><br><ul><li>{{c1::\(x_{1}+\cdots+x_{n}\)}} = {{c1::\(\lambda x_{1} \)}}</li><li>\(\vdots \)</li><li>{{c2::\(x_{1}+\cdots+x_{n}\)}} = {{c2::\(\lambda x_{n} .\)}}</li></ul><br>Thus<br><br><ul><li>{{c3::\(\lambda x_{1}\)}} = \(\cdots\) = {{c3::\(\lambda x_{n} .\)}}</li></ul><div>Then either:</div><div><br></div><div><ul><li>\(\lambda\) = {{c4::0}}<br></li><ul><li>In which case the corresponding set of eigenvectors is&nbsp;</li><li>{{c5::\[<br>\left\{\left(x_{1}, \ldots, x_{n}\right) \in \mathbf{F}^{n}: x_{1}+\cdots+x_{n}=0\right\} .<br>\]}}<br></li></ul><li>\(\lambda\) = {{c5::n}}<br></li><ul><li>In which cas the corresponding set of eigenvectors equals:</li><li>{{c4::\[<br>\left\{\left(x_{1}, \ldots, x_{n}\right) \in \mathbf{F}^{n}: x_{1}=\cdots=x_{n}\right\} .<br>\]}}<br></li></ul></ul></div>

============================================================

  

    11. Suppose \(S, T \in \mathcal{L}(V)\). Prove that \(S T\) and \(T S\) have the same eigenvalues.<br><br>Solution: Suppose that \(\lambda \in \mathbf{F}\) is an eigenvalue of \(S T\). We want to prove that \(\lambda\) is an eigenvalue of \(T S\). Because \(\lambda\) is an eigenvalue of \(S T\), there exists a nonzero vector \(v \in V\) such that<br><br><ul><li>\((ST)v\) =&nbsp;\( \lambda v\)</li><li>If&nbsp;</li><li>\((TS)v\) =&nbsp;\( \alpha v\)</li><li>Then</li><li>{{c1::\(T(ST)v\)}} =&nbsp;{{c1::\( T \lambda v\)}}</li><li>{{c2::\(T(ST)v\)}} =&nbsp;{{c2::\(&nbsp; \lambda Tv\)}}</li><li>{{c3::\(TST v\)}} =&nbsp;{{c3::\(&nbsp; \lambda Tv\)}}</li><li>Which implies that&nbsp;{{c4::\(Tv\) is an eigvenvector of&nbsp;\(TS\)}} and&nbsp;{{c5::\(\lambda\) is also an eigenvalue of T}}</li><li>Mutatis mulandis</li></ul>

============================================================

  

    15. Suppose \(\mathbf{F}=\mathbf{C}, T \in \mathcal{L}(V), p \in \mathcal{P}(\mathbf{C})\), and \(a \in \mathbf{C}\). Prove that \(a\) is an eigenvalue of \(p(T)\) if and only if \(a=p(\lambda)\) for some eigenvalue \(\lambda\) of \(T\).<br><br>Solution:<br><br><br>SOLUTION: First suppose that \(a\) is an eigenvalue of \(p(T)\). Thus \(p(T)-a I\) is not injective. Write the polynomial \(p(z)-a\) in factored form:<br><br>\[<br>p(z)-a=c\left(z-\lambda_{1}\right) \ldots\left(z-\lambda_{m}\right),<br>\]<br><br>where \(c, \lambda_{1}, \ldots, \lambda_{m} \in \mathrm{C}\). We can assume that \(c \neq 0\) (otherwise \(p\) is a constant polynomial, in which case the desired result clearly holds). The equation above implies that<br><br><ul><li>{{c1::\(p(T)-a I\)}} = {{c2::\(c\left(T-\lambda_{1} I\right) \ldots\left(T-\lambda_{m} I\right) .\)}}</li><li>Because {{c1::\(p(T)-a I\)}} is {{c3::not injective}}</li><ul><li>This implies that {{c4::\(T-\lambda_{j} I\)}} is {{c3::not injective}} for some \(j\).&nbsp;</li><li>In other words, {{c5::some \(\lambda_{j}\) is an eigenvalue of \(T\)}}.&nbsp;</li><li>The formula above for \(p(z)-a\) shows that {{c5::\(p\left(\lambda_{j}\right)-a\)}} = {{c5::\(0\).&nbsp;}}</li><li>Hence \(a=p\left(\lambda_{j}\right)\), as desired.</li></ul></ul>

============================================================

  

    15. Suppose \(\mathbf{F}=\mathbf{C}, T \in \mathcal{L}(V), p \in \mathcal{P}(\mathbf{C})\), and \(a \in \mathbf{C}\). Prove that \(a\) is an eigenvalue of \(p(T)\) if and only if \(a=p(\lambda)\) for some eigenvalue \(\lambda\) of \(T\).<br><br>Solution:<br>For the other direction, now suppose that \(a\) = \(p(\lambda)\) for some eigenvalue \(\lambda\) of \(T\). Thus there exists a nonzero vector \(v \in V\) such that<br><br><ul><li>{{c1::\(T v\)}} = {{c1::\(\lambda v .\)}}</li></ul><br>{{c2::Repeatedly applying \(T\) to both sides of this equation}} shows that {{c3::\(T^{k} v\)}} = {{c3::\(\lambda^{k} v\)}} for {{c3::every positive integer \(k\)}}. Thus<br><br><ul><li>\(p(T) v \)&nbsp;</li><li>= {{c4::\(p(\lambda) v \)}}</li><li>= {{c5::\(a v\)}}</li></ul><br>Thus \(a\) is an eigenvalue of \(p(T)\).<br>

============================================================

  

    5. (6 points) (Axler 3rd edition Page 161, exercise 16). The Fibonacci sequence \(F_{1}, \ldots, F_{n}\) is defined by<br><br>\[<br>F_{1}=1, F_{2}=1, \text { and } F_{n}=F_{n-2}+F_{n-1} \text { for } n \geq 3<br>\]<br><br>Define \(T \in \mathcal{L}\left(\mathbb{R}^{2}\right)\) by \(T(x, y)=(y, x+y)\)<br><br>1. Show that \(T^{n}(0,1)=\left(F_{n}, F_{n+1}\right)\) for each positive integer \(n\).<br><br>Solution:&nbsp;<br><ul><li>Proof by induction, for n=1 easy then:</li><li>{{c5::{{c1::\(T\left(F_{n-1}, F_{n}\right)\)&nbsp;}}}}</li><li>= {{c4::{{c2::\(\left(F_{n}, F_{n-1}+F_{n}\right)\)}}}}</li><li>&nbsp;= {{c4::{{c3::\(\left(F_{n}, F_{n+1}\right)\).}}}}<br></li></ul>

============================================================

  

    5. (6 points) (Axler 3rd edition Page 161, exercise 16). The Fibonacci sequence \(F_{1}, \ldots, F_{n}\) is defined by<br><br>\[<br>F_{1}=1, F_{2}=1, \text { and } F_{n}=F_{n-2}+F_{n-1} \text { for } n \geq 3<br>\]<br><br>Define \(T \in \mathcal{L}\left(\mathbb{R}^{2}\right)\) by \(T(x, y)=(y, x+y)\)<br><br>2. Find the eigenvalues of \(T\).<br><br>Solution:<br><ul><li>For&nbsp;\(T(u,v) \) =&nbsp; {{c1::\(\lambda u, \lambda v \)}} = {{c1::\( v, v+u\)}}</li><li>{{c2::\(v\)}} ={{c2::&nbsp;\(\lambda u\)}}<br></li><li>{{c3::\(\lambda v\)}} =&nbsp;{{c3::\(v +u\)}}<br></li><li>Thus&nbsp;</li><li>{{c4::\(u\left(\lambda^{2}-\lambda-1\right)\)}}&nbsp;= \(0\)<br></li><li>Since eigenvectors must be nonzero, the following is implied based on {{c4::the quadratic formula}}</li><li>\(\lambda_{1}\) = {{c5::\(\frac{1+\sqrt{5} }{2}\)&nbsp;}}</li><li>\(\lambda_{2}\) = {{c5::\(\frac{1-\sqrt{5} }{2} .\)}}<br></li></ul>

============================================================

  

    5. (6 points) (Axler 3rd edition Page 161, exercise 16). The Fibonacci sequence \(F_{1}, \ldots, F_{n}\) is defined by<br><br>\[<br>F_{1}=1, F_{2}=1, \text { and } F_{n}=F_{n-2}+F_{n-1} \text { for } n \geq 3<br>\]<br><br>Define \(T \in \mathcal{L}\left(\mathbb{R}^{2}\right)\) by \(T(x, y)=(y, x+y)\)<br><br>With eigenvalues:<br>\[<br>\lambda_{1}=\frac{1+\sqrt{5}}{2}, \lambda_{2}=\frac{1-\sqrt{5}}{2} .<br>\]<br><br><br>3. Find a basis of \(\mathbb{R}^{2}\) consisting of eigenvectors of \(T\).<br><br><br>To find the eigenvector for \(\lambda_{1}\), we just compute the null space of<br><br>{{c5::{{c1::\[<br>\left(\begin{array}{ll}<br>0 &amp; 1 \\<br>1 &amp; 1<br>\end{array}\right)-\left(\begin{array}{cc}<br>\frac{1+\sqrt{5} }{2} &amp; 0 \\<br>0 &amp; \frac{1+\sqrt{5} }{2}<br>\end{array}\right)=\left(\begin{array}{cc}<br>-\frac{1+\sqrt{5} }{2} &amp; 1 \\<br>1 &amp; \frac{1-\sqrt{5} }{2}<br>\end{array}\right)<br>\]}}}}<br><br>which is spanned by \(v_{1}\) = {{c4::{{c2::\(\left(\frac{\sqrt{5}-1}{2}, 1\right)\).}}}}<br><br>Similarly we find the eigenvector for \(v_{2}\) = \(\lambda_{2}\) is {{c4::{{c3::\(\left(\frac{\sqrt{5}+1}{2},-1\right)\).}}}}

============================================================

  

    In a few applications (like {{c1::quantum mechanics}}) the need for a basis vector to have {{c2::length one}} also has a clear and natural meaning: the length of a vector can sometimes be related to {{c5::{{c3::a probability}}}}, and "{{c5::{{c4::probability one}}}}" is a very natural condition.

============================================================

  

    There are also costs associated to working with {{c1::orthonormal}} lists. A central strength of linear algebra is that almost all the computations required are {{c1::arithmetic}}: {{c2::addition}}, {{c3::subtraction}}, {{c4::multiplication}}, and {{c5::division}}

============================================================

  

    The Gram-Schmidt process to create orthonormal lists requires {{c1::m::how many?}} {{c2::square root::what kind?}} operations, which are {{c5::{{c3::computationally expensive to do}}}} {{c5::{{c4::without error::how?}}::how?}}

============================================================

  

    Proposition 0.1. If \(\left(f_{1}, \ldots, f_{m}\right)\) is an orthogonal list of vectors in \(V\), then<br><br><ul><li>\(\|\) {{c1::\(b_{1} f_{1}+\cdots b_{m} f_{m}\)}} \(\|\) = {{c2::\(\left|b_{1}\right|^{2}\left\|f_{1}\right\|^{2}\)}} {{c5::{{c4::+ \(\cdots\) +}}}} {{c5::{{c3::\(\left|b_{m}\right|^{2}\left\|f_{m}\right\|^{2}\)}}}}</li></ul><br>for {{c5::{{c4::all \(b_{1}, \ldots, b_{m} \in F\)}}}}.<br>

============================================================

  

    Corollary 0.2. The list of {{c5::{{c3::nonzero}}}} vectors in an {{c4::{{c1::orthogonal}}}} list is {{c4::{{c2::linearly independent}}}}.

============================================================

  

    An {{c1::orthogonal}} {{c2::basis}} of \(V\) is an {{c5::{{c3::orthogonal list of vectors in \(V\)}}}} that is {{c5::{{c4::also a basis of \(V\).&nbsp;}}}}

============================================================

  

    Theorem 0.3. Suppose \(\left(f_{1}, \ldots, f_{n}\right)\) is an {{c1::orthogonal}} basis of \(V\). Then<br><br><ul><li>\(v\) = {{c2::\(\frac{\left\langle v, f_{1}\right\rangle}{\left\langle f_{1}, f_{1}\right\rangle} f_{1}\)}} {{c4::+\(\cdots\)+}}{{c2::\(\frac{\left\langle v, f_{n}\right\rangle}{\left\langle f_{n}, f_{n}\right\rangle} f_{n}\)}}</li></ul><br>and<br><br><ul><li>\(\|v\|^{2}\) = {{c3::\(\frac{\left|\left\langle v, f_{1}\right\rangle\right|^{2} }{\left\langle f_{1}, f_{1}\right\rangle}\)}}{{c5::+\(\cdots\)+}}{{c3::\(\frac{\left|\left\langle v, f_{n}\right\rangle\right|^{2} }{\left\langle f_{n}, f_{n}\right\rangle}\)}}</li></ul><br>for every \(v \in V\).<br>

============================================================

  

    Theorem 0.3. Suppose \(\left(f_{1}, \ldots, f_{n}\right)\) is an {{c5::orthogonal}} basis of \(V\). Then<br><br><ul><li>{{c3::\(v\)}} ={{c1::\(\frac{\left\langle v, f_{1}\right\rangle}{\left\langle f_{1}, f_{1}\right\rangle} f_{1}+\cdots+\frac{\left\langle v, f_{n}\right\rangle}{\left\langle f_{n}, f_{n}\right\rangle} f_{n}\)}}</li></ul><br>and<br><br><ul><li>{{c4::\(\|v\|^{2}\)}} = {{c2::\(\frac{\left|\left\langle v, f_{1}\right\rangle\right|^{2} }{\left\langle f_{1}, f_{1}\right\rangle}+\cdots+\frac{\left|\left\langle v, f_{n}\right\rangle\right|^{2} }{\left\langle f_{n}, f_{n}\right\rangle} \)}}</li></ul><br>for {{c5::every \(v \in V\).}}<br>

============================================================

  

    Proposition 0.4. If \(\left(v_{1}, \ldots, v_{m}\right)\) is a list of vectors in \(V\), then there is an {{c1::orthogonal}} list \(\left(f_{1}, \ldots, f_{m}\right)\) of vectors in \(V\) such that<br><br>\(6.21^{\prime}\)<br><br><ul><li>{{c2::\(\operatorname{span}\left(v_{1}, \ldots, v_{j}\right)\)}} = {{c5::{{c3::\(\operatorname{span}\left(f_{1}, \ldots, f_{j}\right)\)}}}}</li></ul><br>for {{c5::{{c4::\(j=1, \ldots, m\)}}}}.<br>

============================================================

  

    The inductive step of Gram Schmidt when it is used to construct an {{c4::orthogonal}} {{c5::rather than orthonormal}} list:<br><ul><li>\(f_{j}\) = {{c1::\(v_{j}\)}} {{c3::-}} {{c2::\(\sum_{1 \leq i \leq j-1, f_{i} \neq 0} \frac{\left\langle v_{j}, f_{i}\right\rangle}{\left\langle f_{i}, f_{i}\right\rangle} f_{i}\)}}</li></ul><br>

============================================================

  

    How can we think of Gram Schmidt in the language of Gaussian elimination?<br><br><ul><li>Take each&nbsp;\(v_j\) to be {{c1::the rows of a matrix}}</li><li>Operations we are performing correspond to {{c2::subtracting multiples of rows from lower (later) rows}}</li><li>Thus, the fact that&nbsp;</li><ul><li>{{c5::{{c3::\[<br>\operatorname{span}\left(v_{1}, \ldots, v_{j}\right)=\operatorname{span}\left(f_{1}, \ldots, f_{j}\right)<br>\]::mathematical relation}}::mathematical relation}}<br></li></ul><li>Follows directly from {{c5::{{c4::the nature of the opeartions}}}}</li></ul>

============================================================

  

    To decompose v as {{c4::two orthogonal}} {{c5::rather than orthonormal}} vectors we can say that:<br><ul><li>\(v\) = {{c1::\(\underbrace{\frac{\left\langle v, f_{1}\right\rangle}{\left\langle f_{1}, f_{1}\right\rangle} f_{1}+\cdots+\frac{\left\langle v, f_{m}\right\rangle}{\left\langle f_{m}, f_{m}\right\rangle} f_{m} }_{u}\) ::u}} {{c3::+}} {{c2::\(\underbrace{v-\frac{\left\langle v, f_{1}\right\rangle}{\left\langle f_{1}, f_{1}\right\rangle} f_{1}-\cdots-\frac{\left\langle v, f_{m}\right\rangle}{\left\langle f_{m}, f_{m}\right\rangle} f_{m} }_{w} \)::w}}</li></ul><br>

============================================================

  

    Let \(\left(f_{1}, \ldots, f_{m}\right)\) be an orthogonal basis of \(U\). Obviously<br><br>\[<br>v=\underbrace{\frac{\left\langle v, f_{1}\right\rangle}{\left\langle f_{1}, f_{1}\right\rangle} f_{1}+\cdots+\frac{\left\langle v, f_{m}\right\rangle}{\left\langle f_{m}, f_{m}\right\rangle} f_{m}}_{u}+\underbrace{v-\frac{\left\langle v, f_{1}\right\rangle}{\left\langle f_{1}, f_{1}\right\rangle} f_{1}-\cdots-\frac{\left\langle v, f_{m}\right\rangle}{\left\langle f_{m}, f_{m}\right\rangle} f_{m}}_{w} .<br>\]<br><br>Clearly \(u \in U\). Because \(\left(f_{1}, \ldots, f_{m}\right)\) is an orthogonal list, for each \(j\) we have<br><br><ul><li>\(\langle\)&nbsp; {{c1::\(w, f_{j}\)}} \(\rangle\)&nbsp;</li><li>= {{c2::\(\left\langle v, f_{j}\right\rangle\)}} {{c4::-}} {{c3::\(\frac{\left\langle v, f_{j}\right\rangle}{\left\langle f_{j}, f_{j}\right\rangle}\left\langle f_{j}, f_{j}\right\rangle\)&nbsp;}}</li><li>= {{c4::\(0\)}}</li></ul><br>Thus \(w\) is {{c5::orthogonal to every vector in \(\operatorname{span}\left(f_{1}, \ldots, f_{m}\right)\)}}.<br>

============================================================

  

    If \(\left(f_{1}, \ldots, f_{m}\right)\) is an {{c3::orthogonal}} basis of \(U\), then the formula for {{c2::the projection \(P_{U}\)}} becomes<br><br><ul><li>{{c2::\(P_{U}(v)\)}} = {{c5::{{c1::\( \sum_{i=1}^m \frac{\langle v,f_i \rangle}{\langle f_i, f_i \rangle} f_i \)<br>}}}}</li></ul>for {{c5::{{c4::every \(v \in V\).}}}}<br>

============================================================

  

    There several "simpler" notions of the "size" of a vector in \(\mathbb{R}^{n}\). Here are three such notions<br><ul><li>{{c1::\(\left\|\left(x_{1}, \ldots, x_{n}\right)\right\|_{1}\)}} = {{c2::<ul><li>\(\left|x_{1}\right|+\left|x_{2}\right|+\cdots+\left|x_{n}\right|\)</li></ul>}}</li></ul><br><ul><li>{{c3::\(\left\|\left(x_{1}, \ldots, x_{n}\right)\right\|_{\infty}\)}} = {{c4::\(\text { largest of }\left|x_{1}\right|,\left|x_{2}\right|, \ldots,\left|x_{n}\right|\)}}</li><li><br></li><li>(useful for example if the different coordinates correspond to {{c5::different kinds of bad thing}}, and you want to {{c5::be sure that all of them are small}}); and</li></ul><br><br>

============================================================

  

    1. (3 points) This problem is about the \(x\)-axis \(L=\{(x, 0) \mid x \in \mathbb{R}\}\).<br><br>a) Find all the points on \(L\) that are as close as possible to the point \((3,2)\) in terms of the \(\|\cdot\|_{1}\) size: that is, all the \((x, 0)\) so that \(\|(3,2)-(x, 0)\|_{1}\) is as small as possible.<br><br>Solution:<br><ul><li>We want to find all \(x\) making {{c1::\(|3-x|+|2|\)::formula}} as small as possible.&nbsp;<br></li><li>Because {{c2::the second summand is constant}}, it's {{c3::the same thing to find all \(x\) making \(|3-x|\) as small as possible.&nbsp;}}</li><li>Clearly {{c4::there is just one}}, \(\mathbf{x}\) = {{c5::\(\mathbf{3}\).}}<br></li></ul>

============================================================

  

    1. (3 points) This problem is about the \(x\)-axis \(L=\{(x, 0) \mid x \in \mathbb{R}\}\).<br><br>b) Find all the points on \(L\) that are as close as possible to the point \((3,2)\) in terms of the \(\|\cdot\|_{\infty}\) size.<br><br>Solution:<br><ul><li>This time we want to make {{c1::the larger of the two numbers}} {{c2::\(|3-x|\) and \(|2|\)}}. as small as possible. T</li><li>The way to achieve that smallest value&nbsp; is to make sure that {{c2::\(|3-x| \leq 2\)::formula}}.&nbsp;</li><li>This is true for x in {{c3::\(\mathbf{1} \leq \mathbf{x} \leq \mathbf{5}\)::bounds}}</li><li>The closest points (for \(\|\cdot\|_{\infty}\) ) on the \(x\)-axis to \((3,2)\):</li><ul><li>{{c4::&nbsp;All the points \((x, 0)\) for \(x\) between 1 and 5}} ; {{c5::they are all at distance 2 .}}</li></ul></ul>

============================================================

  

    1. (3 points) This problem is about the \(x\)-axis \(L=\{(x, 0) \mid x \in \mathbb{R}\}\).<br><br>c) Find all the points on \(L\) that are as close as possible to the point \((3,2)\) in terms of the \(\|\cdot\|_{2}\) size.<br><br>Solution:<br><ul><li>We want to make {{c5::{{c1::\((3-x)^{2}+2^{2}\)::formula}}::formula}} as small as possible.</li><li>Clearly {{c4::{{c3::the only way}}}} to achieve that is to take \(\mathbf{x}\) = {{c4::{{c2::\(\mathbf{3}\)}}}}.</li></ul>

============================================================

  

    2. (6 points) This problem is about the diagonal line \(D=\{(t, t) \mid t \in \mathbb{R}\}\).<br>c) Find all the points on \(D\) that are as close as possible to the point \((3,2)\) in terms of the \(\|\cdot\|_{2}\) size.<br>Solution:<br><br><br><ul><li>This time we want to minimize</li><ul><li>{{c1::\((3-t)^{2}+(2-t)^{2}\)}} = {{c2::\(2 t^{2}-10 t+13\)}} = {{c3::\(2(t-5 / 2)^{2}+1 / 2\)}}</li></ul><li>The minimum is acheived when t = {{c4::5/2}}</li><li>So there is/are {{c5::exactly one::how many?}} closest point {{c5::(5/2,5/2)::which point/s?}} at distance/s {{c5::\(\sqrt{2}/2\)}}</li></ul>

============================================================

  

    4. (3 points) Suppose \(r_{1}, r_{2}\), and \(r_{3}\) are real numbers such that \(r_{1}^{2}+r_{2}^{2}+r_{3}^{2}=1\). Given \((x, y, z) \in \mathbb{R}^{3}\), find a formula for the point on the line \(L=\left\{\left(t r_{1}, t_{2}, t r_{3}\right) \mid t \in\right.\) \(\mathbb{R}\}\) that is closest to \((x, y, z)\) in the \(\|\cdot\|_{2}\) size. (You can find this formula in many places. You're also being asked to write an explanation of why it's true.)<br><br>Solution steps:<br><ul><li><br></li><li>\( \vert \vert(tr_1 - x, tr_2 - y, tr_3 - z)\vert \vert_2 ^2\)&nbsp;</li><li>=&nbsp;{{c1::\(&nbsp; t^2(r_1^2+r^2+r^3) - 2t(x r_1+y r_2+z r_3) + x^2 +y^2 + z^2&nbsp; \)}}</li><li>= {{c2::\(&nbsp; t^2 - 2t(x r_1+y r_2+z r_3)&nbsp;&nbsp;+ x^2 +y^2 + z^2&nbsp; \)}}</li><li>Since {{c3::this is a parabola}}, we can find the minimum as :</li><li>{{c4::\(&nbsp; -b / 2a\)}} =&nbsp; {{c4::\(2(x r_1+y r_2+z r_3) / 2\)}}&nbsp; = {{c4::\( x r_1+y r_2+z r_3\)}}<br></li><li>Explanation: This is {{c5::the projection of (x,y,z) onto the&nbsp;\((tr1, tr_2, tr_3)\) line}}</li></ul>

============================================================

  

    5. (4 points) This problem is about the inner product space \(C[0,1]\) of real-valued continuous functions on the interval \([0,1]\), with inner product<br><br>\[<br>\langle p, q\rangle=\int_{0}^{1} p(x) q(x) d x<br>\]<br><br>It's like the example in the text about finding a good polynomial approximation to \(\sin (x)\).<br><br>a) Let \(U=\mathcal{P}_{3}(\mathbb{R})\) be the four-dimensional subspace of \(C[0,1]\) consisting of polynomials of degree less than or equal to three. Apply the Gram-Schmidt process described in the notes on orthogonal bases on the course web site to convert the basis \(\left(1, x, x^{2}, x^{3}\right)\) of \(U\) to an orthogonal basis having the same span. (I suggest the notes because it's easier than the text.)<br><br>The procedure is<br><br><ul><li>\(f_{1}\) = {{c1::\(v_{1}\)}} =&nbsp; {{c1::1}}</li><li>\(f_{2}\) = {{c2::\(v_{2}-\frac{\left\langle v_{2}, f_{1}\right\rangle}{\left\langle f_{1}, f_{1}\right\rangle} f_{1}\)&nbsp;}}</li><ul><li>= {{c5::{{c3::\(x-\frac{\langle x, 1\rangle}{\langle 1,1\rangle} \cdot 1\)&nbsp;}}}}</li><li>= {{c5::{{c4::\(x-\frac{1 / 2}{1} \cdot 1\)= \(x-1 / 2\)}}}}</li></ul></ul><br><br>

============================================================

  

    5. (4 points) This problem is about the inner product space \(C[0,1]\) of real-valued continuous functions on the interval \([0,1]\), with inner product<br><br>\[<br>\langle p, q\rangle=\int_{0}^{1} p(x) q(x) d x<br>\]<br><br>It's like the example in the text about finding a good polynomial approximation to \(\sin (x)\).<br><br>a) Let \(U=\mathcal{P}_{3}(\mathbb{R})\) be the four-dimensional subspace of \(C[0,1]\) consisting of polynomials of degree less than or equal to three. Apply the Gram-Schmidt process described in the notes on orthogonal bases on the course web site to convert the basis \(\left(1, x, x^{2}, x^{3}\right)\) of \(U\) to an orthogonal basis having the same span. (I suggest the notes because it's easier than the text.)<br><br>The procedure is<br><br>\[<br>\begin{gathered}<br>f_{1}=v_{1}=1 \\<br>f_{2}=v_{2}-\frac{\left\langle v_{2}, f_{1}\right\rangle}{\left\langle f_{1}, f_{1}\right\rangle} f_{1}=x-\frac{\langle x, 1\rangle}{\langle 1,1\rangle} \cdot 1=x-\frac{1 / 2}{1} \cdot 1=x-1 / 2<br>\end{gathered}<br>\]<br><br>Here and in many places we use<br><br><ul><li>\(\left\langle x^{p}, x^{q}\right\rangle\)&nbsp;</li><li>= {{c5::{{c1::\(\int_{0}^{1} x^{p+q} d x\)&nbsp;}}}}</li><li>= {{c4::{{c2::\(\left[\frac{x^{p+q+1} }{p+q+1} \right]_{0}^{1} \)}}}}</li><li>= {{c4::{{c3::\(\frac{1}{p+q+1} .\)}}}}</li></ul>

============================================================

  

    5. (4 points) This problem is about the inner product space \(C[0,1]\) of real-valued continuous functions on the interval \([0,1]\), with inner product<br><br>\[<br>\langle p, q\rangle=\int_{0}^{1} p(x) q(x) d x<br>\]<br><br>It's like the example in the text about finding a good polynomial approximation to \(\sin (x)\).<br><br>b) Define \(f(x)=\sqrt{x}\), regarded as a function in \(C[0,1]\). Calculate the orthogonal projection \(p_{U}(f)\) of \(f\) on the subspace \(U\). (Your answer should be a cubic polynomial in \(x\) with rational numbers as coefficients.)<br><br><br>According to the formula from the notes,<br><br><ul><li>\(P_{U}(f)\)&nbsp;</li><li>= {{c1::\(\frac{\left\langle f, f_{1}\right\rangle}{\left\langle f_{1}, f_{1}\right\rangle} f_{1}\)}} {{c5::+}} {{c2::\(\frac{\left\langle f, f_{2}\right\rangle}{\left\langle f_{2}, f_{2}\right\rangle} f_{2}\)}} {{c5::+}} {{c3::\(\frac{\left\langle f, f_{3}\right\rangle}{\left\langle f_{3}, f_{3}\right.} f_{3}\)}} {{c5::+}} {{c4::\(\frac{\left\langle f, f_{4}\right\rangle}{\left\langle f_{4}, f_{f}\right\rangle} f_{4}\)}}</li></ul><div><br></div><div>This projection is equivalent to finding {{c5::the best approximation of&nbsp;\(\sqrt{x}\)}} using {{c5::polynomials of dgree 3,2,1,0}}</div>

============================================================

  

    \subsection{Eigenvalues of self-adjoint operators are real}<br><br>Every eigenvalue of a self-adjoint operator is real.<br><br>Proof Suppose \(T\) is a self-adjoint operator on \(V\). Let \(\lambda\) be an eigenvalue of \(T\), and let \(v\) be a nonzero vector in \(V\) such that \(T v=\lambda v\). Then<br><br><ul><li>{{c5::\(\lambda\|v\|^{2}\)::norm}}</li><li>= {{c1::\(\langle\lambda v, v\rangle\):: inner prod}}</li><li>= {{c2::\(\langle T v, v\rangle\):: inner prod&nbsp;}}</li><li>= {{c3::\(\langle v, T v\rangle\):: inner prod&nbsp;}}</li><li>= {{c4::\(\langle v, \lambda v\rangle\):: inner prod}}</li><li>= {{c5::\(\bar{\lambda}\|v\|^{2} .\):: norm}}</li></ul><br>Thus \(\lambda=\bar{\lambda}\), which means that \(\lambda\) is real, as desired.<br>

============================================================

  

    1. (25 points) Let<br><br>\[<br>A=\left(\begin{array}{lll}<br>1 &amp; 1 &amp; 1 \\<br>0 &amp; 2 &amp; 2 \\<br>0 &amp; 0 &amp; 2<br>\end{array}\right)<br>\]<br><br>a matrix of real numbers.<br><br>b) For each eigenvalue (1,2), find all the corresponding eigenvectors of \(A\).<br><br>Solution:<br><ul><li>For&nbsp;\(\lambda = 1\), we have&nbsp;\(Av \) =&nbsp;\( [x+y+z, 2(y+z), 2z]\) =&nbsp;{{c1::\([x,y,z]\)}} which automatically implies {{c2::z and y must be 0}}. Thus&nbsp;\(E(1, A) \)=&nbsp; {{c2::\(\mathrm{span}(1,0,0)\)}}</li><li>For&nbsp;\(\lambda = 2\), we have&nbsp;\(Av \) =&nbsp;\( [x+y+z, 2(y+z), 2z]\) =&nbsp;{{c3::\([2x,2y,2z]\)}} which automatically implies that&nbsp;{{c4::\(2y+2z \)}} = {{c4::0}} which implies that&nbsp;{{c5::\(z\)}} = {{c5::\(0\)}} and {{c5::x,y can be any value}}.&nbsp; Thus&nbsp;\(E(2,A)\) =&nbsp;{{c5::\(\mathrm{span}(1,1,0)\)}}</li></ul>

============================================================

  

    2. (30 points) Define a sequence of real numbers by \(a_{0}=0, a_{1}=1\), and<br><br>\[<br>a_{n+1}=\left(a_{n}+a_{n-1}\right) / 2 \quad(n \geq 1)<br>\]<br><br>that is, each term is the average of the two preceding terms. The first few terms of the sequence are<br><br>\[<br>0,1,1 / 2,3 / 4,5 / 8,11 / 16,21 / 32,43 / 64,85 / 128, \ldots<br>\]<br><br>a) Find a \(2 \times 2\) real matrix \(A\) such that the following is true<br><br>\[<br>\left(\begin{array}{c}<br>a_{n+1} \\<br>a_{n}<br>\end{array}\right)=A\left(\begin{array}{c}<br>a_{n} \\<br>a_{n-1}<br>\end{array}\right)<br>\]<br><br>solution:<br><ul><li>A= {{c1::\(\left(\begin{array}{cc}1 / 2 &amp; 1 / 2 \\1 &amp; 0\end{array}\right)\)}}</li><li>Which means that to get the series:</li><li>{{c2::\(\left(\begin{array}{c}a_{n+1} \\a_{n}\end{array}\right)\)}}&nbsp;= {{c5::{{c3::\(A^{n}\)}}}}{{c5::{{c4::&nbsp;\(\left(\begin{array}{l}1 \\0\end{array}\right)\)}}}}</li></ul>

============================================================

  

    2. (30 points) Define a sequence of real numbers by \(a_{0}=0, a_{1}=1\), and<br><br>\[<br>a_{n+1}=\left(a_{n}+a_{n-1}\right) / 2 \quad(n \geq 1)<br>\]<br><br>that is, each term is the average of the two preceding terms. The first few terms of the sequence are<br><br>\[<br>0,1,1 / 2,3 / 4,5 / 8,11 / 16,21 / 32,43 / 64,85 / 128, \ldots<br>\]<br><br>a) Find a \(2 \times 2\) real matrix \(A\) such that the following is true<br><br>\[<br>\left(\begin{array}{c}<br>a_{n+1} \\<br>a_{n}<br>\end{array}\right)=A\left(\begin{array}{c}<br>a_{n} \\<br>a_{n-1}<br>\end{array}\right)<br>\]<br><br>\[<br>A=\left(\begin{array}{cc}<br>1 / 2 &amp; 1 / 2 \\<br>1 &amp; 0<br>\end{array}\right)<br>\]<br><br>b) Find all the eigenvalues and eigenvectors of your matrix \(A\).<br><br>We want to find \(\lambda\) such that<br><br><ul><li>{{c1::\(A-\lambda I\)}} = {{c2::<ul><li>\(\left(\begin{array}{cc}1 / 2-\lambda &amp; 1 / 2 \\1 &amp; -\lambda\end{array}\right)\)</li></ul>}}</li><li>is {{c3::not invertible}}.&nbsp;</li><li>Using {{c4::Gauss elimination&nbsp;}} we found this is the same as saying {{c5::\(\lambda^{2}-\frac{1}{2} \lambda-\frac{1}{2}\)}} = {{c5::\(0\)}}. So \(\lambda\) has two solutions {{c5::\(1,-\frac{1}{2}\).}}</li></ul>

============================================================

  

    2. (30 points) Define a sequence of real numbers by \(a_{0}=0, a_{1}=1\), and<br><br>\[<br>a_{n+1}=\left(a_{n}+a_{n-1}\right) / 2 \quad(n \geq 1)<br>\]<br><br>that is, each term is the average of the two preceding terms. The first few terms of the sequence are<br><br>\[<br>0,1,1 / 2,3 / 4,5 / 8,11 / 16,21 / 32,43 / 64,85 / 128, \ldots<br>\]<br><br>a) Find a \(2 \times 2\) real matrix \(A\) such that the following is true<br><br>\[<br>\left(\begin{array}{c}<br>a_{n+1} \\<br>a_{n}<br>\end{array}\right)=A\left(\begin{array}{c}<br>a_{n} \\<br>a_{n-1}<br>\end{array}\right)<br>\]<br><br>\[<br>A=\left(\begin{array}{cc}<br>1 / 2 &amp; 1 / 2 \\<br>1 &amp; 0<br>\end{array}\right)<br>\]<br><br>b) Find all the eigenvalues and eigenvectors of your matrix \(A\).<br><br>We want to find \(\lambda\) such that<br><br>\[<br>A-\lambda I=\left(\begin{array}{cc}<br>1 / 2-\lambda &amp; 1 / 2 \\<br>1 &amp; -\lambda<br>\end{array}\right)<br>\]<br><br>is not invertible. Using Gauss elimination (or just be speculating), we found this is the same as saying \(\lambda^{2}-\frac{1}{2} \lambda-\frac{1}{2}=0\). So \(\lambda\) has two solutions \(1,-\frac{1}{2}\).<br><br>The eigenvectors for 1 are the solutions of \((A-1 \cdot I)\left(\begin{array}{l}x_{1} \\ x_{2}\end{array}\right)=0\), or<br><br><ul><li>{{c1::\(-(1 / 2) x_{1}+(1 / 2) x_{2}\)}} = {{c3::\(0\)}}</li><li>{{c2::\(x_{1}-x_{2}\)}}&nbsp;= {{c3::\(0\)}}<br></li><li>Thus the solutions are&nbsp;{{c4::\(\left(\begin{array}{l}x \\ x\end{array}\right)\)}} with a basis&nbsp;\(u\)= {{c5::\(\left(\begin{array}{l}1 \\ 1\end{array}\right)\)}}</li><li>Mutatis mulandis for -1/2</li></ul><div><br></div>

============================================================

  

    2. (30 points) Define a sequence of real numbers by \(a_{0}=0, a_{1}=1\), and<br><br>\[<br>a_{n+1}=\left(a_{n}+a_{n-1}\right) / 2 \quad(n \geq 1)<br>\]<br><br>that is, each term is the average of the two preceding terms. The first few terms of the sequence are<br><br>\[<br>0,1,1 / 2,3 / 4,5 / 8,11 / 16,21 / 32,43 / 64,85 / 128, \ldots<br>\]<br><br>Find a \(2 \times 2\) real matrix \(A\) such that the following is true<br><br>\[<br>\left(\begin{array}{c}<br>a_{n+1} \\<br>a_{n}<br>\end{array}\right)=A\left(\begin{array}{c}<br>a_{n} \\<br>a_{n-1}<br>\end{array}\right)<br>\]<br><br>\[<br>A=\left(\begin{array}{cc}<br>1 / 2 &amp; 1 / 2 \\<br>1 &amp; 0<br>\end{array}\right)<br>\]<br>With eigenvalues 1&nbsp; and -1/2 and eigenvectors (1,1) and (1, -2)<br><br>c) Write a closed formula for \(a_{n}\) (not the inductive formula at the beginning, but something like \(a_{n}=3 \cdot 5^{n}-1\) ).<br><br><br>We understand how \(A\) {{c1::acts on the eigenvectors \(u\) and \(v\)}}, so we write \(\left(\begin{array}{l}1 \\ 0\end{array}\right)\) as {{c1::a linear combination of \(u\) and \(v\)}} :<br><br><ul><li>{{c2::\(\left(\begin{array}{l}1 \\0\end{array}\right)\)}} = {{c2::\((2 / 3) u+(1 / 3) v\)}}</li></ul><br>Therefore<br><br><ul><li><br></li><li>\( A^{n}\left(\begin{array}{l}1 \\0\end{array}\right)\)&nbsp;</li><li>= {{c3::\((2 / 3) A^{n} u+(1 / 3) A^{n} v \)}}</li><li>= {{c4::\( (2 / 3) \cdot 1^{n} u+(1 / 3) \cdot(-1 / 2)^{n} v \)}}</li><li>= {{c5::\( (2 / 3) u+(1 / 3) \cdot(-1 / 2)^{n} v \)}}</li><li><br></li></ul><br><br><br>d) Calculate \(\lim _{n \rightarrow \infty} a_{n}\).<br><br>Because {{c5::\((1 / 2)^{n}\) tends to zero as \(n\) tends to infinity}},<br><br><ul><li>\(\lim _{n \rightarrow \infty} a_{n}\) = {{c5::\(2 / 3\)}}</li></ul>

============================================================

  

    3. (25 points) This problem is about the subspace \(U\) of \(\mathbb{R}^{4}\) spanned by the three vectors<br><br>\[<br>\{(5,-5,0,0),(1,-3,2,0),(1,1,-5,3)\}<br>\]<br><br>a) Use Grad-Schimdt process to find an orthogonal normal basis for \(U\).<br><br>\[<br>\begin{gathered}<br>e_{1}=\frac{1}{\left\|v_{1}\right\|} v_{1}=\frac{1}{\sqrt{2}}(1,-1,0,0) . \\<br>e_{2}=\frac{v_{2}-v_{2} \cdot e_{1}}{\left\|v_{2}-v_{2} \cdot e_{1}\right\|}=\frac{(-1,-1,2,0)}{\|-1,-1,2,0) \|}=\frac{1}{\sqrt{6}}(-1,-1,2,0) . \\<br>e_{3}=\frac{v_{3}-v_{3} \cdot e_{1}-v_{3} \cdot e_{2}}{v_{3}-v_{3} \cdot e_{1}-v_{3} \cdot e_{2}}=\frac{(-1,-1,-1,3)}{\|-1,-1,-1,3) \|}=\frac{1}{\sqrt{12}}(-1,-1,-1,3) .<br>\end{gathered}<br>\]<br><br>b) In terms of the basis you found in (a), write a formula for the orthogonal projection \(P\) of \(\mathbb{R}^{4}\) on \(U\).<br><br>Solution:<br><br>Formula from the notes is<br><br><ul><li>\(P(v)\) = {{c1::\(\left\langle v, e_{1}\right\rangle e_{1}+\left\langle v, e_{2}\right\rangle e_{2}+\left\langle v, e_{3}\right\rangle e_{3}\)}}</li></ul><div>If \(v=\left(x_{1}, x_{2}, x_{3}, x_{4}\right)\),<br><br><ul><li>P(v) =&nbsp;</li><li>= {{c2::\(\frac{1}{2}\left(x_{1}-x_{2}\right)(1,-1,0,0)\)}}+{{c5::{{c3::\(\frac{1}{6}\left(2 x_{3}-x_{2}-x_{1}\right)(-1,-1,2,0)\)}}}} + {{c5::{{c4::\(\frac{1}{12}\left(3 x_{4}-x_{1}-x_{2}-x_{3}\right)(-1,-1,-1,3) \)}}}}</li></ul></div>

============================================================

  

    4. (20 points) Suppose that \(A\) is an \(n \times n\) invertible self-adjoint complex matrix.<br><br>a) Prove that every eigenvalue of \(A^{2}\) is a positive real number.<br><br><ul><li>First, we know that {{c1::every eigenvalue of a self-adjoint matrix is a real-number}}, this does not change as {{c1::it gets raised to powers}}</li><li>Let \(v\) be an eigenvalue of \(A^2\). Then</li><ul><li>{{c2::\(\lambda\langle v, v\rangle\)::inner prod}}&nbsp;=&nbsp;</li><ul><li>={{c3::\(\left\langle A^{2} v, v\right\rangle\)::inner prod}}</li><li>= {{c4::\(\langle A v, A v\rangle\)::inner prod}}</li><li>= {{c5::\(\|A v\|^{2} \geq 0\)}}</li></ul><li>So \(\lambda \geq 0\). Since A {{c5::is invertible}}, we know that \(\lambda\) {{c5::can't be 0}} .<br></li></ul></ul>

============================================================

  

    4. (20 points) Suppose that \(A\) is an \(n \times n\) invertible self-adjoint complex matrix.<br><br>b) Prove that every diagonal entry of \(A^{2}\) is a positive real number.<br><br>Then the {{c1::\((i, i)\)-entry of \(A^{2}\)}} is<br><br><ul><li>{{c2::\(\sum_{j=1}^{n} a_{i, j} a_{j, i}\)}} = {{c3::\(\sum_{j=1}^{n}\left|a_{i, j}\right|^{2}\)}}</li><ul><li>since {{c4::\(a_{j, i}\)}} = {{c5::\(\overline{a_{i, j} }\).}}&nbsp;</li></ul><li>Since \(A\) is {{c5::invertible}}, we {{c5::can't have one row to be all zero}}, so it's positive.<br></li></ul>

============================================================

  

    Why must T* exits?<br><img src="paste-eefb86190b3ba010e13d7a60ffdba064e82aff82.jpg"><br><ul><li>Because of the {{c1::representation}} theorem, every {{c2::linear function such as the inner prod on the left}} must be {{c5::{{c3::representable as an inner prod of vectors in the domain}}}}</li><li>Thus T*w is {{c5::{{c4::the vector which creates this representation}}}}</li></ul>

============================================================

  

    Proof&nbsp; for (ST*) = T*S*<br><br>Suppose \(T \in \mathcal{L}(V, W)\) and \(S \in \mathcal{L}(W, U)\). If \(v \in V\) and \(u \in U\), then<br><br><ul><li>{{c1::\(\left\langle v,(S T)^* u\right\rangle\):: inner prod}} =&nbsp;</li><ul><li>= {{c2::\(\langle S T v, u\rangle\):: inner prod}}</li><li>= {{c5::{{c3::\(\left\langle T v, S^* u\right\rangle\):: inner prod}}:: inner prod}}<br></li><li>=&nbsp;{{c5::{{c4::\(\left\langle v, T^*\left(S^* u\right)\right\rangle\).:: inner prod}}:: inner prod}}</li></ul></ul>

============================================================

  

    <img src="paste-a8ca47d946f7f00d5c54658f32e74eca4f4cc5b8.jpg"><br><br>Lets prove (a), let w be in W Then<br><ul><li>\(w \in \operatorname{null} T^*\)&nbsp;</li><li>\(\Longleftrightarrow\) {{c5::{{c1::\(T^* w=0\)}}}}</li><li>\(\Longleftrightarrow\) {{c4::{{c2::\(\left\langle v, T^* w\right\rangle\)}}}} ={{c4::{{c2::\(0\)}}}} for {{c4::{{c2::all \(v \in V\)}}}}<br></li><li>\(\Longleftrightarrow\) {{c4::{{c3::\(\langle T v, w\rangle\)}}}} = {{c4::{{c3::\(0\)}}}} for {{c2::all \(v \in V\)}}<br></li></ul>

============================================================

  

    For matrices, the {{c5::{{c1::adjoint}}}} is similar to {{c4::{{c2::taking the complex conjugate}}}} of {{c4::{{c3::a complext number}}}}

============================================================

  

    Over C, T is {{c5::{{c1::self-adjoint}}}} iff&nbsp;{{c4::{{c2::\(\langle T v, v\rangle\)}}}} is {{c4::{{c3::a real number}}}}

============================================================

  

    <img src="paste-cfb62edf1a09ee4b7ca829651b4d80b40ebd0f4e.jpg"><br><br>Proof, let v be a vector in V, then:<br><ul><li>{{c1::\(\langle T v, v\rangle\)}} - {{c1::\(\overline{\langle T v, v\rangle}\)}}&nbsp;</li><li>= {{c2::\(\langle T v, v\rangle\)}} - {{c2::\(\langle v, T v\rangle\)}}<br></li><li>=&nbsp;{{c3::\(\langle T v, v\rangle\)}} - {{c3::\(\left\langle T^* v, v\right\rangle\)}}</li><li>= {{c4::\(\left\langle\left(T-T^*\right) v, v\right\rangle\)}}.<br></li></ul><div><br></div><div><ul><li>If \(\langle T v, v\rangle \in \mathbf{R}\) for every \(v \in V\), then the left side of the {{c5::first equation}} above equals {{c5::0}} ,&nbsp;</li><li>So {{c5::\(\left\langle\left(T-T^*\right) v, v\right\rangle\)}} = {{c5::\(0\)}} for every \(v \in V\).&nbsp;</li><li>This implies that {{c5::\(T-T^*\)}} = {{c5::\(0\)}}. Hence \(T\) is self-adjoint.</li></ul></div>

============================================================

  

    <img src="paste-ad9daf29f0643679b0de40ca3b0987fb7fa127eb.jpg"><br><br><img src="paste-bd212afffdcee13ebce17dcfaca0fe822cf72b75.jpg"><br><ul><li>Conversely, if \(T\) is self-adjoint, then the {{c1::right}} side of the {{c1::last}} equation above equals {{c2::0}} ,&nbsp;</li><li>So {{c5::{{c3::\(\langle T v, v\rangle\)}}}} = {{c5::{{c4::\(\overline{\langle T v, v\rangle}\)}}}} for every \(v \in V\).&nbsp;</li><li>This implies that \(\langle T v, v\rangle \in \mathbf{R}\) for every \(v \in V\), as desired.</li></ul>

============================================================

  

    <img src="paste-a24bfef9e14eef153c19c7edcdde6d4f0f3b680e.jpg"><br><br>Proof<br><ul><li>T is normal</li><li>\(\Longleftrightarrow\)&nbsp; {{c1::\(T^* T-T T^*\)}} = {{c2::\(0\)}}<br></li><li>\(<br>\Longleftrightarrow\)&nbsp; {{c3::\(\left\langle\left(T^* T-T T^*\right) v, v\right\rangle\)::inner prod}} = {{c2::\(0\)}}&nbsp;for all \(v \in V\)<br></li><li>\(\Longleftrightarrow\) {{c4::\(\left\langle T^* T v, v\right\rangle\)::inner prod}} = {{c4::\(\left\langle T T^* v, v\right\rangle\)::inner prod}} for all v in V<br></li><li>\(\Longleftrightarrow\) {{c5::\(\|T v\|^2\)::norm}} = {{c5::\(\left\|T^* v\right\|^2\)::norm}} for all v in V<br></li></ul>

============================================================

  

    <img src="paste-3eae770354c6a56f1c62972884a8b62bcc8b3b9a.jpg"><br><br>Proof Because \(T\) is normal, so is \(T-\lambda I\), as you should verify.<br><br>Thus<br><ul><li>{{c1::\(0\)&nbsp;}}</li><li>= {{c2::\(\|(T-\lambda I) v\|\)}} because {{c2::v is an eigenvector}}<br></li><li>= {{c5::{{c3::\(\left\|(T-\lambda I)^* v\right\|\)}}}} because {{c5::{{c3::of normality}}}}</li><li>={{c5::{{c4::\(\left\|\left(T^*-\bar{\lambda} I\right) v\right\|\)}}}} because {{c5::{{c4::of adjoint distributy over addition}}}}<br></li><li>Hence \(v\) is an eigenvector of \(T^*\) with eigenvalue \(\bar{\lambda}\), as desired.<br></li></ul>

============================================================

  

    <img src="paste-990496dab7a08b80add9e720838d952103732c08.jpg"><br>Proof Suppose \(\alpha, \beta\) are distinct eigenvalues of \(T\), with corresponding eigenvectors \(u, v\). Thus \(T u=\alpha u\) and \(T v=\beta v\).<br><ul><li>We have {{c1::\(T^* v\)}} = {{c1::\(\bar{\beta} v\)}} because {{c1::eigenvectors are shared between operators and their adjoint and the eigenvalues must be the conjugate of each other}}<br></li><li>{{c3::\((\alpha-\beta)\)::scaling factor}} {{c2::\(\langle u, v\rangle\)::inner prod}}&nbsp;</li><li>= {{c4::\(\langle\alpha u, v\rangle\)::inner prod}} - {{c4::\(\langle u, \bar{\beta} v\rangle\)::inner prod}} because {{c4::of the distributive property}} + using {{c4::homogeneity in the first slot of&nbsp;\(\alpha\)}} and {{c4::homogeneity in the second slot for&nbsp;\(\beta\)}}<br></li><li>= {{c5::\(\langle T u, v\rangle-\left\langle u, T^* v\right\rangle\)}} because {{c5::eigenvectors}}&nbsp;<br></li><li>\(=0\). because {{c5::re-applying adjoint on the right}}<br></li></ul><div>Because \(\alpha \neq \beta\), the equation above implies that \(\langle u, v\rangle=0\). Thus \(u\) and \(v\) are orthogonal, as desired.<br></div>

============================================================

  

    Proving that v exists such that any linear functional&nbsp;\(\phi\) =&nbsp;\(\langle, v \rangle\)<br><ul><li>Assume we have {{c5::an orthonormal basis&nbsp;\(\vec{e}\)}}</li><li>Let&nbsp;\(u\) =&nbsp;{{c4::\(\sum \langle u, e_i \rangle e_i\)}}</li><li>Then:</li><ul><li>\(\phi(u)\) =&nbsp;</li><li>= {{c1::\(\sum&nbsp;&nbsp;\langle u, e_i \rangle \phi(e_i)&nbsp;\)}} by {{c1::linearity}}<br></li><li>= {{c2::\(\sum&nbsp;&nbsp;\langle u, \overline{\phi(e_i)} e_i \rangle&nbsp; \)}} by {{c2::conjugate homogeneity in second slot}}</li><li>&nbsp;= {{c3::\( \langle u,&nbsp;\sum&nbsp; &nbsp;\overline{\phi(e_i)} e_i \rangle&nbsp; \)}} by {{c3::linearity in the first slot}}</li></ul><li>Which gives us the desired representation of the linear functional&nbsp;</li></ul>

============================================================

  

    The reason we need the adjoint and the conjugate transpose is because the {{c1::dual}} map and the {{c2::normal transpose}} must be defined over {{c5::{{c3::the dual spaces}}}} rather than {{c5::{{c4::the original domain and codomaint}}}}

============================================================

  

    Unlike dual vector applications to their normal counterparts, {{c5::{{c1::inner products}}}} are taken in the {{c4::{{c2::real domains and codomains}}}} rather tahn {{c4::{{c3::the dual spaces}}}}

============================================================

  

    What we fundamentally want to do with the adjoint is to {{c1::inverse}} an inner product \(\langle T(v), w \rangle\) in {{c2::time}} so it can {{c5::{{c3::be taken in the domain}}}} rather than {{c5::{{c4::the codomain}}}}&nbsp;

============================================================

  

    Adjoints allow us to {{c1::translate}} {{c2::inner products}} between the {{c3::languages}} of {{c4::the domain}} and {{c5::codomain}}

============================================================

  

    Why is the adjoint of a linear map linear?<br><br><ul><li>It is the composition of two {{c1::conjugate linear}} maps, one which takes&nbsp;{{c2::f(v) to&nbsp;\(\langle f(v),w \rangle\)}} and the {{c3::riezs representation}} map which takes {{c4::this linear functional to a vector}}</li><li>Taking the {{c1::conjugate}} {{c5::twice}} gives a linear map</li></ul>

============================================================

  

    The adjoint fundamentally relies on the {{c1::riezs representation}} theorem to associate {{c2::the linear functional \(\langle T(v), w \rangle\)}} with {{c3::the unique vector in V}} which can {{c4::be represented as the inner product \(\langle&nbsp; v , v^\prime \rangle\)}}&nbsp;with&nbsp;{{c5::\(v^\prime\)}} =&nbsp;{{c5::\(T^*(w)\)}}

============================================================

  

    What is the formula of the inner product of two vectors with respect to {{c4::an orthonormal basis \(\vec{e}\)}}?<br><ul><li>\(\langle u, v \rangle\) =&nbsp;{{c3::\(\sum_i\)}}&nbsp;{{c5::{{c2::\( \langle u, e_i \rangle\)}}}} {{c5::{{c1::\( \overline{ \langle v, e_i \rangle} \)}}}}</li></ul>

============================================================

  

    Proof that<img src="paste-4e0b2f98c65820d01560c02c72972926cc9611ad.jpg"><br><br><ul><li>\(\langle u, v\rangle\) =&nbsp;<br></li><li>= {{c1::\(\langle \sum \langle u, e_i\rangle&nbsp;,&nbsp;&nbsp;&nbsp;\sum \langle v, e_i\rangle&nbsp;&nbsp;\rangle\)}}&nbsp;by {{c1::orthonormal representation}}</li><li>={{c2::\(\sum_i \sum_j\)&nbsp;\(\langle&nbsp; &nbsp;\langle u, e_i \rangle e_i, \langle v, e_j \rangle&nbsp;e_j&nbsp; &nbsp;\rangle \)}}&nbsp;by {{c2::pushing out sums due to linearity}}<br></li><li>= {{c5::{{c3::\(\sum_i \sum_j\)&nbsp;\(&nbsp; \langle u, e_i \rangle&nbsp; \overline{\langle v, e_j \rangle} \underbrace{\langle e_i, e_j}_{0 /1} \rangle&nbsp; &nbsp;\)}}}} by {{c5::{{c3::conjugate linearity&nbsp;}}}}</li><li>=&nbsp;{{c5::{{c4::\(\sum&nbsp; \langle u, e_i \rangle \overline{\langle v, e_i \rangle} \)}}}} by {{c5::{{c4::orthonormality}}}}</li></ul>

============================================================

  

    What are the steps for proving that the matrix of the adjoint is the complex conjugate?<br><ul><li>Choose {{c1::orthonormal}} bases&nbsp;\(\vec{e}\) and&nbsp;\(\vec{g}\)</li><li>Express a basis element&nbsp;\(e_k\) as&nbsp;\(T(e_k)\) =&nbsp;{{c2::\(\sum \langle T(e_k), g_i \rangle g_i\)}} for T&nbsp;</li><li>Express a basis element&nbsp;\(g_k\) as&nbsp;\(T^*(g_k)\) =&nbsp;{{c3::\(\sum \langle T^*(g_k), e_i \rangle e_i \)}} for T*</li><li>Expand this final expression into \(T^*(g_k)\)&nbsp; = {{c4::\(\sum \langle g_k, T(e_i) \rangle e_i \)}} =&nbsp;&nbsp;&nbsp;{{c5::\(\sum \overline{ \langle T(e_i)&nbsp;, g_k \rangle} e_i \)&nbsp;}} &nbsp;by {{c4::adjoint equation}} and then {{c5::conjugate homogeneity in second slot}}</li><li>Thus the matrices are conjugate transposes of each other</li></ul>

============================================================

  

    {{c5::{{c1::Adjancency}}}} matrices and {{c4::{{c2::covariance}}}} matrices are examples of {{c4::{{c3::self-adjoint}}}} operators

============================================================

  

    {{c5::{{c2::One}}}}-dimensional {{c4::{{c3::self-adjoint}}}} matrices are just {{c4::{{c1::real numbers}}}}

============================================================

  

    For complext inner product spaces<br><ul><li>{{c1::\(\langle Tv, v \rangle\)}} = {{c2::0}}<br></li><li>Implies {{c5::{{c3::T}}}} = {{c5::{{c4::0}}}}</li></ul>

============================================================

  

    Let V be a complex inner prod space and&nbsp;\(T \in L(V)\)<br><ul><li>T is {{c1::self-adjoint}} iff&nbsp;\(\langle\)&nbsp;{{c2::\(Tv,v\)}} \(\rangle\)&nbsp;{{c3::\(\in \)}}&nbsp;{{c4::\(R\)}} for {{c5::all v}}</li></ul>

============================================================

  

    Proof that \(\langle Tv,v \rangle \in R\) for all v implies that T is self-adjoint:<br><ul><li>\(\langle\)&nbsp; {{c1::\(Tv,v\)}} \(\rangle\) - \(\langle\) {{c1::\(\overline{Tv,v}\)}} \(\rangle\)</li><li>= \(\langle\)&nbsp; {{c2::\(Tv,v\)}} \(\rangle\) - \(\langle\)&nbsp; {{c2::\(v,Tv\)}} \(\rangle\)&nbsp;<br></li><li>= \(\langle\)&nbsp; {{c3::\(Tv,v\)}} \(\rangle\) -&nbsp;\(\langle\)&nbsp; {{c3::\(T^*v,v\)}} \(\rangle\)&nbsp;</li><li>= \(\langle\)&nbsp; {{c4::\( (T-T*)v,v\)}} \(\rangle\) = {{c5::0}}</li><li>Which implies&nbsp;{{c5::\(T^*\)}} = {{c5::\(T\)}}</li></ul>

============================================================

  

    <ul><li>Recall that a diagonal matrix is a {{c5::{{c1::square}}}} matrix that is {{c4::{{c2::0 everywhere except possibly along the diagonal.&nbsp;}}}}</li><li>Recall also that an operator on \(V\) has a diagonal matrix with respect to a basis if and only if {{c4::{{c3::the basis consists of eigenvectors of the operator}}}}&nbsp;</li></ul>

============================================================

  

    <ul><li>The nicest operators on \(V\) are those for which there is an {{c5::{{c1::orthonormal}}}} basis of \(V\) with respect to which the operator has a {{c4::{{c2::diagonal}}}} matrix.</li><li>&nbsp;These are precisely the operators \(T \in \mathcal{L}(V)\) such that there is an {{c1::orthonormal}} basis of \(V\) consisting of {{c4::{{c3::eigenvectors of \(T\).&nbsp;}}}}</li></ul>

============================================================

  

    <ul><li>Operators for which a {{c2::diagonal matrix exists w.rt. to an orthonormal basis}} are characterised by the {{c1::spectral}} theorem</li><li>As the {{c5::{{c3::normal}}}} operators over C and the {{c5::{{c4::self-adjoint}}}} ones over R</li></ul>

============================================================

  

    Because the conclusion of the Spectral Theorem depends on {{c5::{{c1::\(\mathbf{F}\)}}}}, we will break the Spectral Theorem into two pieces, called the {{c4::{{c2::Complex}}}} Spectral Theorem and the {{c4::{{c3::Real}}}} Spectral Theorem.&nbsp;

============================================================

  

    <ul><li>The key part of the Complex Spectral Theorem (7.24) states that if {{c1::\(\mathbf{F}\)}} = {{c2::\(\mathbf{C}\)}} and \(T \in \mathcal{L}(V)\) is {{c3::normal}},&nbsp;</li><li>Then \(T\) has a {{c4::diagonal}} matrix with respect to some {{c5::orthonormal}} basis of \(V\).&nbsp;</li></ul>

============================================================

  

    {{c1::Complex Spectral}} Theorem<br><br>Suppose \(\mathbf{F}\) = {{c1::\(\mathbf{C}\)}} and \(T \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(T\) is {{c2::normal}}.</li><li><br></li><li>(b) \(V\) has an {{c3::orthonormal}} basis consisting of {{c4::eigenvectors of \(T\).}}</li><li><br></li><li>(c) \(T\) has a {{c5::diagonal}} matrix with respect to {{c5::some orthonormal basis of \(V\).}}</li></ul>

============================================================

  

    <img src="paste-530a5e94b1b46c19656893fea1bedce3bd55f09a.jpg"><br>Proof:<br><ul><li>Proof First suppose (c) holds.&nbsp;</li><li>So \(T\) has a diagonal matrix with respect to some orthonormal basis of \(V\).&nbsp;</li><li>The matrix of \(T^{*}\) (with respect to the same basis) is obtained by {{c1::taking the conjugate transpose of the matrix of \(T\)}}&nbsp;</li><li>Hence \(T^{*}\) also has a {{c2::diagonal}} matrix.&nbsp;</li><li>Any two {{c2::diagonal}} matrices {{c3::commute}}; thus {{c5::\(T\)}} {{c3::commutes}} with {{c5::\(T^{*}\)}}, which means that \(T\) is {{c4::normal}}.&nbsp;</li><li>In other words, (a) holds.<br></li></ul>

============================================================

  

    <img src="paste-b177df9f882ef3397e176a2b7100706a58dab475.jpg"><br>Now suppose (a) holds, so \(T\) is normal. By Schur's Theorem (6.38), there is an orthonormal basis \(e_{1}, \ldots, e_{n}\) of \(V\) with respect to which \(T\) has an {{c5::{{c1::upper-triangular}}}} matrix. Thus we can write<br><ul><li>\(\mathcal{M}\left(T,\left(e_1, \ldots, e_n\right)\right)\) ={{c4::{{c3::&nbsp;\(\left(\begin{array}{ccc}a_{1,1} &amp; \cdots &amp; a_{1, n} \\ &amp; \ddots &amp; \vdots \\ 0 &amp; &amp; a_{n, n}\end{array}\right)\).}}}}<br></li><li>We will show that this matrix is actually a {{c4::{{c2::diagonal}}}} matrix.</li></ul>

============================================================

  

    <img src="paste-b177df9f882ef3397e176a2b7100706a58dab475.jpg"><br>Now suppose (a) holds, so \(T\) is normal. By Schur's Theorem (6.38), there is an orthonormal basis \(e_{1}, \ldots, e_{n}\) of \(V\) with respect to which \(T\) has an upper-triangular matrix. Thus we can write<br><ul><li>\(\mathcal{M}\left(T,\left(e_1, \ldots, e_n\right)\right)\) =&nbsp;\(\left(\begin{array}{ccc}a_{1,1} &amp; \cdots &amp; a_{1, n} \\ &amp; \ddots &amp; \vdots \\ 0 &amp; &amp; a_{n, n}\end{array}\right)\).<br></li><li>We will show that this matrix is actually a diagonal matrix.<br></li><li>We see from the matrix above that&nbsp;</li><ul><li>\(\|\) {{c1::\(T e_{1}\)}} \(\|^{2}\) = \(|\) {{c1::\(a_{1,1}\)}} \(|^{2}\)</li><li>and</li><li>\(\| \) {{c2::\(T^{*} e_{1}\)}} \(\|^{2}\) = {{c2::\(\left|a_{1,1}\right|^{2}+\left|a_{1,2}\right|^{2}+\cdots+\left|a_{1, n}\right|^{2} .\)}}</li></ul><li>Because \(T\) is {{c4::normal}}:</li><ul><li>&nbsp;\(\|\)&nbsp; {{c3::\(T e_{1}\)}}&nbsp; \( \|\) = \(\| \)&nbsp; {{c3::\(T^{*} e_{1}\)}} \(\|\).&nbsp;</li></ul><li>Thus the two equation above implity that {{c5::all entries in the first row of the matrix}}, except {{c5::possibly the first entry}}, equal {{c5::0}}</li></ul>

============================================================

  

    <img src="paste-b177df9f882ef3397e176a2b7100706a58dab475.jpg"><br>Now suppose (a) holds, so \(T\) is normal. By Schur's Theorem (6.38), there is an orthonormal basis \(e_{1}, \ldots, e_{n}\) of \(V\) with respect to which \(T\) has an upper-triangular matrix. Thus we can write<br><ul><li>\(\mathcal{M}\left(T,\left(e_1, \ldots, e_n\right)\right)\) =&nbsp;\(\left(\begin{array}{ccc}a_{1,1} &amp; \cdots &amp; a_{1, n} \\ &amp; \ddots &amp; \vdots \\ 0 &amp; &amp; a_{n, n}\end{array}\right)\).<br></li><li>We will show that this matrix is actually a diagonal matrix.<br></li><li>We see from the matrix above that&nbsp;</li><ul><li>\(\left\|T e_{1}\right\|^{2}\) = \(\left|a_{1,1}\right|^{2}\)</li><li>and</li><li>\(\left\|T^{*} e_{1}\right\|^{2}\) = \(\left|a_{1,1}\right|^{2}+\left|a_{1,2}\right|^{2}+\cdots+\left|a_{1, n}\right|^{2} .\)</li></ul><li>Because \(T\) is normal:</li><ul><li>&nbsp;\(\left\|T e_{1}\right\|\) = \(\left\|T^{*} e_{1}\right\|\) (see 7.20).&nbsp;</li></ul><li>Thus the two equations above imply that all entries in the first row of the matrix in 7.25 , except possibly the first entry \(a_{1,1}\), equal 0 .</li><li>Now from the matrix we see that</li><ul><li>\(\|\) {{c1::\(T e_{2}\)}} \(\|^{2}\) = \(|\) {{c2::\(a_{2,2}\)}} \( |^{2}\)</li><li>(because \(a_{1,2}=0\), as we showed in the paragraph above) and</li><li>\(\|\) {{c3::\(T^{*} e_{2}\)}} \(|^{2}\) ={{c4::\(\left|a_{2,2}\right|^{2}+\left|a_{2,3}\right|^{2}+\cdots+\left|a_{2, n}\right|^{2} .\)}}</li></ul><li>Because \(T\) is {{c5::normal}}, \(\| \) {{c5::\(T e_{2}\)}} \(\|\) = \(\|\) {{c5::\(T^{*} e_{2}\)}} \(\|\).&nbsp;</li><li>Thus the two equations above imply that {{c5::all entries in the second row of the matrix in 7.25, except possibly the diagonal entry \(a_{2,2}\), equal 0 .}}</li><li>Continuing in this fashion, we see that all the nondiagonal entries in the matrix 7.25 equal 0 . Thus (c) holds.</li></ul>

============================================================

  

    <img src="paste-83b3831e34c826764b0093760f2f884050344989.jpg"><br>What are the high-level points of the complex spectral theorem proof?<br><ul><li>We can create an {{c3::upper-triangular}} matrix w.r.t some {{c3::orthonormal}} basis for any vectors over C</li><li>For the first element {{c1::\(\left\|T e_1\right\|^2\)}} = {{c2::\(\left|a_{1,1}\right|^2\)}} since the column is zero<br></li><li>Because T is {{c4::normal}}, {{c5::the same applies to the adjoint/complex transpose}} and thus {{c5::the row must also be zero:}}</li><li>{{c5::<img src="paste-caa70de9bb00431a9366f94d7841a6d2253f01e0.jpg">}}<br></li><ul><li>I.e for {{c4::normal}} operators {{c5::the norms of columns and their matching rows}} must {{c5::match}}</li></ul><li>Since we know this row is zero, repeat for all other elements by induction</li></ul>

============================================================

  

    {{c1::\(x^2+b x+c\)}} =&nbsp;{{c2::\(\left(x+\frac{b}{2}\right)^2+\left(c-\frac{b^2}{4}\right)\)}} {{c5::{{c3::&gt;}}}} {{c5::{{c4::0}}}}

============================================================

  

    \[<br>x^{2}+b x+c=\left(x+\frac{b}{2}\right)^{2}+\left(c-\frac{b^{2}}{4}\right)&gt;0 .<br>\]<br><ul><li>This means that \(x^{2}+b x+c\) is an {{c1::invertible}} {{c2::non-zero}} {{c3::real}} number</li><li>To get an {{c4::equivalent expression}} for {{c4::operators}} we {{c5::replace x with T}}</li></ul>

============================================================

  

    \subsection{Invertible quadratic expressions}<br><br>Suppose \(T \in \mathcal{L}(V)\) is {{c3::self-adjoint}} and {{c4::\(b, c \in \mathbf{R}\)}} are such that {{c5::\(b^{2}\)}} {{c4::&lt;}} {{c5::\(4 c\)}}. Then<br><ul><li>{{c2::\[T^{2}+b T+c I\]}}</li><li>is {{c1::invertible}}.</li></ul>

============================================================

  

    {{c3::Self-adjoint}} operators {{c4::have eigenvalues}}<br><br>Suppose \(V\) {{c1::\(\neq\{0\}\)}} and \(T \in \mathcal{L}(V)\) is a {{c2::self-adjoint}} operator. Then \(T\) has {{c3::an eigenvalue.}}<br><br><ul><li>This tells us something new for {{c5::real vector spaces}} since {{c5::complex ones were already known to have eigenvalues}}</li></ul>

============================================================

  

    {{c1::Self-adjoint}} operators and {{c2::invariant}} subspaces<br><br>Suppose \(T \in \mathcal{L}(V)\) is {{c1::self-adjoint}} and \(U\) is a subspace of \(V\) that is {{c2::invariant}} under \(T\). Then<br><br>(a) {{c3::\(U^{\perp}\)}} is {{c3::invariant under \(T\)}};<br><br>(b) {{c4::\(\left.T\right|_{U} \in \mathcal{L}(U)\)}} is {{c4::self-adjoint}};<br><br>(c) {{c5::\(\left.T\right|_{U^{\perp} }  \in \mathcal{L}\left(U^{\perp}\right)\)}} is {{c5::self-adjoint.}}

============================================================

  

    Suppose \(T \in \mathcal{L}(V)\) is self-adjoint and \(U\) is a subspace of \(V\) that is invariant under \(T\). Then<br><br>(a) {{c1::\(\quad U^{\perp}\)}} is {{c2::invariant under \(T\);}}<br><br>(b) {{c3::\(\left.T\right|_{U}\)}} \(\in\) {{c4::\(\mathcal{L}(U)\)}} is {{c2::self-adjoint}};<br><br>(c) {{c5::\(\left.T\right|_{U^{\perp} } \)}} \(\in\) \(\mathcal{L}\left(U^{\perp}\right)\) is {{c2::self-adjoint.}}

============================================================

  

    <img src="paste-347f8df15eca2c89ed5a9727d6b582e0021fda62.jpg"><br>What are the broad steps of the proof ?<br><ul><li>Make {{c1::non-lin-indp}} list via {{c2::powers of Tv for v not 0}}</li><li>Decompose into d{{c3::egree-1 and degree-2}} polynomials&nbsp;</li><ul><li>The {{c4::degree-2}} polynomials must by-definition have {{c5::b^2 &lt; 4ac}} since otherwise they {{c5::would have been broken down into degree 1 polynomials}}</li></ul></ul><br>

============================================================

  

    <img src="paste-347f8df15eca2c89ed5a9727d6b582e0021fda62.jpg"><br>What are the broad steps of the proof ?<br><ul><li>Make non-lin-indp list via powers of Tv for v not 0</li><li>Decompose into degree-1 and degree-2 polynomials&nbsp;</li><ul><li>The degree-2 polynomials must by-definition have b^2 &lt; 4ac since otherwise they would have been broken down into degree 1 polynomials</li></ul><li>Thus using the following equation:</li><ul><li>{{c1::<img src="paste-be4282f85d4fb7f7ff19c16efed2c43c09327dcc.jpg">}}<br></li></ul><li>Each {{c2::quadratic term}} is {{c3::invertible}}, i.e {{c3::non-zero}}</li><li>Thus the {{c4::degree-1 polynomials must have one zero}} which must be {{c5::an eigenvalue}}</li><li></li></ul>

============================================================

  

    <img src="paste-365768550d3546d7498a5d6e88c498dab9f531b9.jpg"><br>Proof To prove (a), suppose \(v \in U^{\perp}\). Let \(u \in U\). Then<br><br><ul><li>\(\langle\) {{c1::\(T v, u\)}} \(\rangle\)=\(\langle\) {{c2::\(v, T u\)}}&nbsp; \(\rangle\) ={{c2::\(0\)}}</li></ul><ul><li>where the first equality above holds because \(T\) is {{c3::self-adjoint&nbsp;}}</li><li>and the second equality above holds because \(U\) is {{c4::invariant under \(T\)}}&nbsp; and because \(v \in\) {{c5::\( U^{\perp}\).&nbsp;}}</li><li>Because the equation above holds for each \(u \in U\), we conclude that \(T v \in U^{\perp}\). Thus \(U^{\perp}\) is invariant under \(T\), completing the proof of (a).</li></ul>

============================================================

  

    <img src="paste-86ec941f8e4451a31483bcf4b9d67b24f224f051.jpg"><br>To prove (b), note that if \(u, v \in U\), then<br><br><ul><li>{{c1::\(\left\langle\left(\left.T\right|_{U}\right) u, v\right\rangle\)::inner prod}}</li><li>= {{c2::\(\langle T u, v\rangle\)::inner prod}}</li><li>= {{c3::\(\langle u, T v\rangle\)::inner prod}}</li><li>= {{c4::\(\left\langle u,\left(\left.T\right|_{U}\right) v\right\rangle .\)::inner prod}}</li></ul><br>Thus \(\left.T\right|_{U}\) is {{c5::self-adjoint}}.<br>

============================================================

  

    <img src="paste-ef9d2d1232baab48c067fec1d9e8537a4d4e4c19.jpg"><br><ul><li>C follos from {{c1::replaceing U}} {{c4::with&nbsp;\(U^\perp\)}} {{c5::{{c2::in (b)}}}} which makes sense by {{c5::{{c3::(a)}}}}</li></ul>

============================================================

  

    {{c1::Real Spectral}} Theorem<br><br>Suppose \(\mathbf{F}\) = {{c1::\(\mathbf{R}\)}} and \(T \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(\quad T\) is {{c2::self-adjoint}}.</li><li><br></li><li>(b) \(\quad V\) has {{c5::{{c3::an orthonormal basis consisting of eigenvectors of \(T\).}}}}</li><li><br></li><li>(c) \(T\) has {{c5::{{c4::a diagonal matrix with respect to some orthonormal basis of \(V\).}}}}</li></ul>

============================================================

  

    Real Spectral Theorem<br><br>Suppose \(\mathbf{F}\) = \(\mathbf{R}\) and \(T \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(\quad T\) is {{c1::self-adjoint}}.</li><li><br></li><li>(b) \(\quad V\) has an {{c2::orthonormal basis}} consisting of {{c3::eigenvectors of \(T\).}}</li><li><br></li><li>(c) \(T\) has an {{c4::diagonal matrix}} {{c5::with respect to some orthonormal basis of \(V\).}}</li></ul>

============================================================

  

    Real Spectral Theorem<br><br>Suppose \(\mathbf{F}\) = \(\mathbf{R}\) and \(T \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(\quad T\) is {{c5::self-adjoint}}.</li><li><br></li><li>(b) \(\quad V\) has an {{c1::orthonormal}} basis consisting of {{c2::eigenvectors}} of \(T\).</li><li><br></li><li>(c) \(T\) has a {{c3::diagonal}} matrix with respect to {{c4::some orthonormal basis of \(V\).}}</li></ul>

============================================================

  

    <img src="paste-b0fe3b22c59e9f3e140d4e5b0ad67a429aca78cc.jpg"><br>Proof:<br><ul><li>Suppose (c) holds, then T has a diagonal matrix w.t.r some orthonormal basis of V</li><li>A diagonal matrix {{c1::equals its transpose}}</li><li>Hence&nbsp;{{c2::\(T\)}} =&nbsp;{{c3::\(T^*\)}} and thus T is {{c4::self-adjoint}} as in {{c5::(a)}}</li></ul>

============================================================

  

    4 Suppose \(\mathbf{F}=\mathbf{C}\) and \(T \in \mathcal{L}(V)\). Prove that \(T\) is normal if and only if all {{c1::pairs of eigenvectors corresponding to distinct eigenvalues of \(T\)}} {{c2::are orthogonal}} and<br><ul><li><br></li><li>{{c3::\(V\)}} = {{c4::\(E\left(\lambda_{1}, T\right) \oplus \cdots \oplus E\left(\lambda_{m}, T\right),\)}}</li></ul><br>where {{c5::\(\lambda_{1}, \ldots, \lambda_{m}\)}} denote {{c5::the distinct eigenvalues of \(T\)}}.<br>

============================================================

  

    5 Suppose \(\mathbf{F}\) = {{c3::\(\mathbf{R}\)}} and \(T \in \mathcal{L}(V)\). Prove that \(T\) is self-adjoint if and only if {{c1::all pairs of eigenvectors corresponding to distinct eigenvalues of \(T\)}} {{c2::are orthogonal}} and<br><br><ul><li>{{c5::\(V\)}} = {{c4::\(E\left(\lambda_{1}, T\right) \oplus \cdots \oplus E\left(\lambda_{m}, T\right),\)}}</li></ul><br>where {{c5::\(\lambda_{1}, \ldots, \lambda_{m}\)}} denote {{c5::the distinct eigenvalues of \(T\).}}<br>

============================================================

  

    6 Prove that a {{c4::normal}} operator on a {{c5::complex}} inner product space is {{c1::self-adjoint}} if and only if {{c3::all its eigenvalues}} {{c2::are real.}}

============================================================

  

    7 Suppose \(V\) is a complex inner product space and \(T \in \mathcal{L}(V)\) is a normal operator such that \(T^{9}=T^{8}\). Prove that \(T\) is {{c5::{{c1::self-adjoint}}}} and {{c4::{{c2::\(T^{2}\)}}}} = {{c4::{{c3::\(T\).}}}}

============================================================

  

    9 Suppose \(V\) is a complex inner product space. Prove that every normal operator on \(V\) has a {{c1::square root}}.<br><ul><li>&nbsp;(An operator \(S \in \mathcal{L}(V)\) is called a {{c1::square root}} {{c2::of \(T \in \mathcal{L}(V)\)}} if {{c5::{{c3::\(S^{2}\)}}}} = {{c5::{{c4::\(T\)}}}}.)</li></ul>

============================================================

  

    12 Suppose \(T \in \mathcal{L}(V)\) is self-adjoint, \(\lambda \in \mathbf{F}\), and \(\epsilon&gt;0\). Suppose there exists \(v \in V\) such that \(\|v\|=1\) and<br><br><ul><li>{{c5::\(\|T v-\lambda v\|\)}}{{c4::&lt;}} {{c5::\(\epsilon .\)}}</li></ul><br>Prove that \(T\) has {{c1::an eigenvalue \(\lambda^{\prime}\)}} such that:<br><ul><li>{{c2::&nbsp;\(\left|\lambda-\lambda^{\prime}\right|\)}} {{c4::&lt;}} {{c3::\(\epsilon\).}}</li></ul>

============================================================

  

    14 Suppose \(U\) is a finite-dimensional real vector space and \(T \in \mathcal{L}(U)\). <br>Prove that \(U\) has&nbsp; {{c5::{{c1::a basis consisting of eigenvectors of \(T\)}}}} if and only if there is {{c4::{{c2::an inner product on \(U\)}}}} that makes \(T\) into a {{c4::{{c3::self-adjoint}}}} operator.

============================================================

  

    <br>Definition {{c1::positive}} operator<br><br>An operator \(T \in \mathcal{L}(V)\) is called {{c1::positive}} if \(T\) is {{c2::self-adjoint}} and<br><br><ul><li>{{c3::\(\langle T v, v\rangle\)}}&nbsp; {{c4::\(\geq\)}} {{c5::\(0\)}}</li><li>for {{c5::all \(v \in V\).}}</li></ul>

============================================================

  

    <br>Definition {{c1::square root}}<br><br>An operator \(R\) is called a {{c2::square root}} {{c3::of an operator \(T\)}} if {{c4::\(R^{2}\)}} = {{c5::\(T\).}}

============================================================

  

    The {{c5::{{c3::positive}}}} operators correspond to the numbers {{c4::{{c1::\([0, \infty)\)}}}}, so better terminology would use the term {{c4::{{c2::nonnegative}}}} instead of {{c3::positive}}. However, operator theorists consistently call these the {{c3::positive}} operators, so we will follow that custom.

============================================================

  

    Specifically, a complex number \(z\) is nonnegative if and only if:<br><ul><li>&nbsp;it has {{c1::a nonnegative square root}}</li><li>&nbsp;Also,\(z\) is nonnegative if and only if it has {{c2::a real square root}}</li><li>&nbsp;Finally, \(z\) is nonnegative if and only if there exists {{c3::a complex number \(w\)}} such that {{c4::\(z\)}} = {{c5::\(\bar{w} w\)}}</li></ul><br>

============================================================

  

    Characterization of positive operators<br><br>Let \(T \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(T\) is {{c1::positive}};</li><li><br></li><li>(b) \(\quad T\) is {{c2::self-adjoint}} and {{c2::all the eigenvalues of \(T\) are nonnegative;}}</li><li><br></li><li>(c) \(T\) has {{c3::a positive square root;}}</li><li><br></li><li>(d) \(T\) has {{c4::a self-adjoint square root;}}</li><li><br></li><li>(e) there exists {{c5::an operator \(R \in \mathcal{L}(V)\)}} such that {{c5::\(T\)}} = {{c5::\(R^{*} R\).}}</li></ul>

============================================================

  

    Characterization of positive operators<br><br>Let \(T \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(T\) is {{c1::positive}};</li><li><br></li><li>(b) \(\quad T\) is {{c2::self-adjoint}} and {{c3::all}} {{c4::the eigenvalues of \(T\)}} are {{c5::nonnegative}};</li></ul>

============================================================

  

    Characterization of positive operators<br><br>Let \(T \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(c) \(T\) has a {{c1::positive}} {{c2::square root}};<br></li><li><br></li><li>(d) \(T\) has a {{c5::{{c3::self-adjoint}}}} {{c5::{{c4::square root}}}};</li></ul>

============================================================

  

    Characterization of positive operators<br><br>Let \(T \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(e) there exists an {{c1::operator}} {{c2::\(R \in \mathcal{L}(V)\)}} such that {{c5::{{c3::\(T\)}}}} ={{c5::{{c4::\(R^{*} R\).}}}}<br></li></ul>

============================================================

  

    <img src="paste-b194b61005807f9ea94d088187f4b74faf55c70a.jpg" style="width: 808.009px;"><br>Now suppose (b) holds, so that \(T\) is self-adjoint and all the eigenvalues of \(T\) are nonnegative. By the {{c1::Spectral}} Theorem (7.24 and 7.29), there is an {{c1::orthonormal}} basis \(e_{1}, \ldots, e_{n}\) of \(V\) consisting of {{c1::eigenvectors}} of \(T\). <br>Let \(\lambda_{1}, \ldots, \lambda_{n}\) be the eigenvalues of \(T\) corresponding to \(e_{1}, \ldots, e_{n}\); ; thus each \(\lambda_{j}\) is a nonnegative number. Let \(R\) be the linear map from \(V\) to \(V\) such that<br><br><ul><li>{{c2::\(R e_{j}\)}} = {{c3::\(\sqrt{\lambda_{j} }  e_{j}\)}}</li></ul><ul><li>for \(j=1, \ldots, n\) (see 3.5).&nbsp;</li><li>Then \(R\) is a positive operator, as you should verify.&nbsp;</li><li>Furthermore, {{c4::\(R^{2} e_{j}\)}} = {{c5::\(\lambda_{j} e_{j}\)}} = {{c5::\(T e_{j}\)}} for each \(j\), which implies that {{c5::\(R^{2}\)}} = {{c5::\(T\).}}&nbsp;</li><li>Thus \(R\) is a positive square root of \(T\). Hence (c) holds.<br></li></ul>

============================================================

  

    <img src="paste-2dfc5d09a708654a85610d7c51d221a1aa1bc755.jpg"><br>Now suppose (d) holds, meaning that there exists a self-adjoint operator \(R\) on \(V\) such that \(T=R^{2}\)<br><ul><li>&nbsp;Then {{c4::\(T\)}} = {{c1::\(R^{*} R\)&nbsp;}} because {{c5::{{c2::\(R^{*}\)}}}} = {{c5::{{c3::\(R\)&nbsp;}}}}</li><li>&nbsp;Hence (e) holds.</li></ul>

============================================================

  

    Each {{c5::{{c1::nonnegative}}}} number has a {{c4::{{c2::unique}}}} {{c1::nonnegative}} {{c4::{{c3::square root.}}}}

============================================================

  

    Each {{c4::positive}} operator has {{c1::only one}} {{c5::{{c2::positive}}}} {{c5::{{c3::square root}}}}<br><br>Every {{c4::positive}} operator on \(V\) has a {{c1::unique}} {{c2::positive}} {{c5::{{c3::square root.<br>}}}}

============================================================

  

    <br>Some mathematicians also use the term {{c5::{{c1::positive}}}} {{c4::{{c2::semidefinite}}}} operator, which means the same as {{c4::{{c3::positive}}}} operator.

============================================================

  

    <img src="paste-90757d8ff3cff9799c0dbabac344de5a5563ef65.jpg"><br>Finally, suppose (e) holds. Let \(R \in \mathcal{L}(V)\) be such that \(T=R^{*} R\). <br><ul><li>Then:</li><ul><li>{{c4::&nbsp;\(T^{*}\)}} =</li><li>= {{c1::\(\left(R^{*} R\right)^{*}\)&nbsp;}}</li><li>= {{c2::\(R^{*}\left(R^{*}\right)^{*}\)&nbsp;}}</li><li>={{c3::\(R^{*} R\)}}</li><li>= {{c5::\(T\).}} </li></ul><li>Hence \(T\) is self-adjoint. To complete the proof that (a) holds, note that</li><ul><li>\(\langle T v, v\rangle\) =</li><li>= \(\left\langle R^{*} R v, v\right\rangle\)&nbsp;</li><li>= \(\langle R v, R v\rangle\)&nbsp; \(\geq\)&nbsp; 0\)</li></ul><li>for every \(v \in V\). Thus \(T\) is positive.</li></ul>

============================================================

  

    <img src="paste-90757d8ff3cff9799c0dbabac344de5a5563ef65.jpg"><br>Finally, suppose (e) holds. Let \(R \in \mathcal{L}(V)\) be such that \(T=R^{*} R\).<br><ul><li>Then:</li><ul><li>&nbsp;\(T^{*}\) =</li><li>= \(\left(R^{*} R\right)^{*}\)&nbsp;</li><li>= \(R^{*}\left(R^{*}\right)^{*}\)&nbsp;</li><li>=\(R^{*} R\)</li><li>= \(T\).</li></ul><li>Hence \(T\) is self-adjoint. To complete the proof that (a) holds, note that</li><ul><li>{{c1::\(\langle T v, v\rangle\)}} =</li><li>= {{c2::\(\left\langle R^{*} R v, v\right\rangle\)&nbsp;}}</li><li>= {{c3::\(\langle R v, R v\rangle\)}}&nbsp; {{c4::\(\geq\)}}&nbsp; {{c5::\(0\)}}</li></ul><li>for every \(v \in V\). Thus \(T\) is {{c5::positive}}.</li></ul>

============================================================

  

    A positive operator can have {{c1::infinitely}} {{c2::many}} {{c3::square roots}} (although {{c4::only one of them}} can be {{c5::positive}}).

============================================================

  

    Definition {{c1::isometry}}<br><br>- An operator \(S \in \mathcal{L}(V)\) is called an {{c1::isometry}} if<br><br><ul><li>{{c2::\(\|S v\|\)}} = {{c3::\(\|v\|\)}}</li><li>for {{c4::all \(v \in V\).}}</li></ul><br>In other words, an operator is an {{c1::isometry}} if it {{c5::preserves norms.}}<br>

============================================================

  

    <ul><li>7.34 Example If \(T \in \mathcal{L}\left(\mathbf{F}^{3}\right)\) is defined by:</li><ul><li>&nbsp;\(T\left(z_{1}, z_{2}, z_{3}\right)\) = {{c5::{{c3::\(\left(z_{3}, 0,0\right)\),&nbsp;}}}}</li></ul><li>then the operator \(R \in \mathcal{L}\left(\mathbf{F}^{3}\right)\) defined by:</li><ul><li>&nbsp; \(R\left(z_{1}, z_{2}, z_{3}\right)\) = {{c4::{{c2::\(\left(z_{2}, z_{3}, 0\right)\)}}}}</li></ul><li>&nbsp;Is a {{c4::{{c1::square root}}}} of \(T\).<br></li></ul>

============================================================

  

    Characterization of {{c1::isometries}}<br><br>Suppose \(S \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(S\) is an {{c2::isometry}};</li><li>(b) {{c3::\(\langle S u, S v\rangle\)}} = {{c4::\(\langle u, v\rangle\)}} for {{c5::all \(u, v \in V\);}}</li></ul>

============================================================

  

    Characterization of isometries<br><br>Suppose \(S \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(S\) is an isometry;</li><li>(c) {{c1::\(S e_{1}, \ldots, S e_{n}\)}} is {{c2::orthonormal}} for {{c5::{{c3::every}}}} {{c5::{{c4::orthonormal list of vectors \(e_{1}, \ldots, e_{n}\) in \(V\);}}}}</li></ul>

============================================================

  

    Characterization of isometries<br><br>Suppose \(S \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(S\) is an isometry;</li><li>(b) {{c1::\(\langle S u, S v\rangle\)}} = {{c2::\(\langle u, v\rangle\)}} for all \(u, v \in V\);</li><li>(c) {{c5::{{c3::\(S e_{1}, \ldots, S e_{n}\)}}}} is {{c5::{{c4::orthonormal for every orthonormal list of vectors \(e_{1}, \ldots, e_{n}\) in \(V\);}}}}</li></ul>

============================================================

  

    Characterization of isometries<br><br>Suppose \(S \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(S\) is an isometry;</li><li>(d) there exists an {{c1::orthonormal basis \(e_{1}, \ldots, e_{n}\)}} of \(V\) such that {{c2::\(S e_{1}, \ldots, S e_{n}\)}} is {{c3::orthonormal}};</li><li>(e) {{c4::\(S^{*} S\)}} = {{c5::\(I\)}};</li></ul>

============================================================

  

    Characterization of isometries<br><br>Suppose \(S \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(S\) is an isometry;</li><li>(d) there exists an {{c1::orthonormal}} {{c2::basis \(e_{1}, \ldots, e_{n}\) of \(V\)}} such that {{c5::{{c3::\(S e_{1}, \ldots, S e_{n}\)}}}} is {{c5::{{c4::orthonormal}}}};</li></ul>

============================================================

  

    Characterization of isometries<br><br>Suppose \(S \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(S\) is an isometry;</li><li>(e) {{c1::\(S^{*} S\)}} = {{c2::\(I\)}};</li><li>(f) {{c5::{{c3::\(S S^{*}\)}}}} = {{c5::{{c4::\(I\)}}}};</li></ul>

============================================================

  

    Characterization of isometries<br><br>Suppose \(S \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) {{c1::\(S\)}} is an {{c1::isometry}};</li><li>(g) {{c1::\(S^{*}\)}} is an {{c1::isometry}};</li><li>(h) {{c2::\(S\)}} is {{c3::invertible}} and {{c4::\(S^{-1}\)}} = {{c5::\(S^{*}\).}}</li></ul>

============================================================

  

    Characterization of isometries<br><br>Suppose \(S \in \mathcal{L}(V)\). Then the following are equivalent:<br><ul><li>(b) {{c1::\(\langle S u, S v\rangle\)}} = {{c1::\(\langle u, v\rangle\)}} for {{c1::all \(u, v \in V\);}}<br></li><li>(c) {{c2::\(S e_{1}, \ldots, S e_{n}\)}} is {{c2::orthonormal for every orthonormal list of vectors \(e_{1}, \ldots, e_{n}\) in \(V\);}}</li><li>(d) there exists {{c3::an orthonormal basis \(e_{1}, \ldots, e_{n}\) of \(V\)}} such {{c3::that \(S e_{1}, \ldots, S e_{n}\) is orthonormal;}}</li><li>(e) {{c4::\(S^{*} S\)}} = {{c5::\(I\);}}</li></ul>

============================================================

  

    Characterization of isometries<br><br>Suppose \(S \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(S\) is an isometry;</li><li>(f) {{c1::\(S S^{*}\)}} = {{c1::\(I\)}};</li><li>(g) {{c2::\(S^{*}\)}} is {{c2::an isometry;}}</li><li>(h) {{c5::{{c3::\(S\)}}}} is {{c5::{{c4::invertible}}}}&nbsp;and {{c5::{{c4::\(S^{-1}\) = \(S^{*}\).}}}}</li></ul>

============================================================

  

    The next result provides several conditions that are equivalent to being an isometry. :<br><ul><li>The equivalence of (a) and (b) shows that an operator is an isometry if and only if it {{c1::preserves}} {{c2::inner products}}.&nbsp;</li><li>The equivalence of (a) and (c) [or (d)] shows that an operator is an isometry if and only if {{c3::the list of columns of its matrix}} with respect to {{c4::every [or some] basis}} is {{c4::orthonormal}}.&nbsp;</li><li>Exercise 10 implies that in the previous sentence we can replace "{{c5::columns}}" with "{{c5::rows}}".</li></ul><br><img src="paste-7b7bd39132094e660006e8fe5b104269a3c6c442.jpg"><br>

============================================================

  

    <ul><li>An isometry on a {{c3::real}} inner product space is often called an {{c1::orthogonal}} operator. An isometry on a {{c5::{{c4::complex}}}} inner product space is often called a {{c5::{{c2::unitary}}}} operator.&nbsp;</li></ul>

============================================================

  

    <ul><li>Every {{c1::isometry}} is {{c2::normal}}<br></li><li>Thus {{c4::characterization}} of {{c1::normal}} {{c5::operators}} can be used to {{c5::give descriptions of}} {{c3::isometries}}</li></ul>

============================================================

  

    Description of {{c2::isometries}} when \(\mathbf{F}\) = {{c1::\(\mathbf{C}\)}}<br><br>Suppose \(V\) is a {{c1::complex}} inner product space and \(S \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(S\) is an {{c2::isometry}}.</li><li>(b) There is an {{c3::orthonormal basis of \(V\)}} consisting of {{c4::eigenvectors of \(S\)}} whose {{c5::corresponding eigenvalues all have absolute value 1 .}}</li></ul>

============================================================

  

    Description of isometries when \(\mathbf{F}\) = {{c1::\(\mathbf{C}\)}}<br><br>Suppose \(V\) is a {{c1::complex}} inner product space and \(S \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(S\) is an isometry.</li><li>(b) There is an {{c2::orthonormal}} {{c3::basis of \(V\)}} consisting of {{c4::eigenvectors}} of \(S\) whose {{c5::corresponding eigenvalues all have absolute value 1 .}}</li></ul>

============================================================

  

    Description of isometries when \(\mathbf{F}\) = \(\mathbf{C}\)<br><br>Suppose \(V\) is a complex inner product space and \(S \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(S\) is an isometry.</li><li>(b) There is an {{c1::orthonormal}} basis of \(V\) consisting of {{c5::eigenvectors}} of \(S\) whose {{c2::corresponding}} {{c3::eigenvalues}} all have {{c4::absolute value 1 .}}</li></ul>

============================================================

  

    <img src="paste-ff8780024104e6c8c744fd5c88fcb10ac195bc47.jpg"><br><br>Suppose (a) holds, so \(S\) is an isometry. <br>By the Complex Spectral Theorem (7.24), there is an orthonormal basis \(e_{1}, \ldots, e_{n}\) of \(V\) consisting of eigenvectors of \(S\). For \(j \in\{1, \ldots, n\}\), let \(\lambda_{j}\) be the eigenvalue corresponding to \(e_{j}\). Then<br><br><ul><li>{{c1::\(\left|\lambda_{j}\right|\)}} =</li><li>= {{c2::\(\left\|\lambda_{j} e_{j}\right\|\)}}</li><li>&nbsp;= {{c3::\(\left\|S e_{j}\right\|\)&nbsp;}}</li><li>= {{c4::\(\left\|e_{j}\right\|\)&nbsp;}}</li><li>= {{c5::\(1 .\)}}</li></ul><br>Thus {{c5::each eigenvalue of \(S\) has absolute value 1, completing the proof.}}<br>

============================================================

  

    <img src="paste-ff8780024104e6c8c744fd5c88fcb10ac195bc47.jpg"><br><br>Suppose (a) holds, so \(S\) is an isometry.<br>By the {{c1::Complex Spectral}} Theorem (7.24), there is an {{c2::orthonormal basis \(e_{1}, \ldots, e_{n}\)}} of \(V\) {{c3::consisting of eigenvectors of \(S\)}}. <br>For \(j \in\{1, \ldots, n\}\), let \(\lambda_{j}\) be {{c4::the eigenvalue corresponding to \(e_{j}\)}}. Then<br><br><ul><li>\(\left|\lambda_{j}\right|\) =</li><li>= \(\left\|\lambda_{j} e_{j}\right\|\)</li><li>&nbsp;= \(\left\|S e_{j}\right\|\)&nbsp;</li><li>= \(\left\|e_{j}\right\|\)&nbsp;</li><li>= \(1 .\)</li></ul><br>Thus each eigenvalue of \(S\) has {{c5::absolute value 1}}, completing the proof.<br>

============================================================

  

    2 Suppose \(T\) is a positive operator on \(V\). Suppose \(v, w \in V\) are such that<br><br><ul><li>{{c3::\(T v\)}} = {{c3::\(w \quad \)}}</li><li>And&nbsp;</li><li>{{c4::\(T w\)}} = {{c5::\(v .\)}}</li></ul><br>Prove that {{c1::\(v\)}} = {{c2::\(w\)}}.<br>

============================================================

  

    3 Suppose \(T\) is a positive operator on \(V\) and \(U\) is a subspace of \(V\) {{c1::invariant under \(T\)}}. Prove that:<br><ul><li>{{c2::&nbsp;\(\left.T\right|_{U}\)&nbsp;}} \(\in\) {{c3::\(\mathcal{L}(U)\)}}</li><li>&nbsp;is a {{c4::positive}} {{c5::operator on \(U\).}}</li></ul>

============================================================

  

    Suppose \(T \in \mathcal{L}(V, W)\). Prove that {{c1::\(T^{*}\)}} \(\circ\) {{c2::\(T\)}} is a {{c3::positive}} operator on \(V\) and {{c4::\(T\)}} \(\circ\) {{c5::\(T^{*}\)}} is a {{c3::positive}} operator on \(W\).

============================================================

  

    5 Prove that the {{c5::{{c1::sum}}}} of two {{c4::{{c2::positive}}}} operators on \(V\) is {{c4::{{c3::positive}}}}.

============================================================

  

    Suppose \(T \in \mathcal{L}(V)\) is {{c1::positive}}. <br>Prove that {{c2::\(T^{k}\)}} is {{c1::positive}} for {{c3::every}} {{c4::positive}} {{c5::integer \(k\)}}.

============================================================

  

    7 Suppose \(T\) is a positive operator on \(V\). Prove that \(T\) is {{c1::invertible}} if and only if<br><br><ul><li>{{c4::\(\langle\)}} {{c2::\(T v,v\)}}&nbsp;{{c4::\(\rangle\)}} {{c5::&gt;}} {{c3::\(0\)}}</li></ul><br>for {{c5::every \(v \in V\)}} with {{c5::\(v \neq 0\).}}<br>

============================================================

  

    8 Suppose \(T \in \mathcal{L}(V)\). For \(u, v \in V\), define \(\langle u, v\rangle_{T}\) by<br><br><ul><li>{{c4::\(\langle u, v\rangle_{T}\)}}&nbsp;= {{c5::\(\langle T u, v\rangle .\)}}</li></ul><br>Prove that \(\langle\cdot, \cdot\rangle_{T}\) is an {{c1::inner product}} on \(V\) if and only if \(T\) is an {{c2::invertible}} {{c3::positive}} operator (with respect to the original inner product \(\langle\cdot, \cdot\rangle\) ).<br>

============================================================

  

    10 Suppose \(S \in \mathcal{L}(V)\). Prove that the following are equivalent:<br><br><ul><li>(a) \(S\) is an {{c1::isometry}};</li><li>(b) {{c2::\(\left\langle S^{*} u, S^{*} v\right\rangle\)}} = {{c2::\(\langle u, v\rangle\)}} for {{c2::all \(u, v \in V\);}}</li><li>(c) {{c3::\(S^{*} e_{1}, \ldots, S^{*} e_{m}\)}} is an {{c3::orthonormal list}} for {{c3::every orthonormal list of vectors \(e_{1}, \ldots, e_{m}\) in \(V\);}}</li><li>(d) {{c4::\(S^{*} e_{1}, \ldots, S^{*} e_{n}\)}} is an {{c5::orthonormal basis}} for {{c5::some orthonormal basis \(e_{1}, \ldots, e_{n}\) of \(V\).}}</li></ul>

============================================================

  

    10 Suppose \(S \in \mathcal{L}(V)\). Prove that the following are equivalent:<br><br><ul><li>(a) \(S\) is an {{c1::isometry}};</li><li>(b) {{c2::\(\left\langle S^{*} u, S^{*} v\right\rangle\)}} = {{c5::{{c3::\(\langle u, v\rangle\)}}}} for {{c5::{{c4::all \(u, v \in V\);}}}}</li></ul>

============================================================

  

    10 Suppose \(S \in \mathcal{L}(V)\). Prove that the following are equivalent:<br><br><ul><li>(a) \(S\) is an {{c4::isometry}};</li><li>(c) {{c1::\(S^{*} e_{1}, \ldots, S^{*} e_{m}\)}} is an {{c5::{{c2::orthonormal}}}} list for {{c5::{{c3::every}}}} {{c2::orthonormal}} {{c5::{{c3::list of vectors \(e_{1}, \ldots, e_{m}\) in \(V\);}}}}</li></ul>

============================================================

  

    10 Suppose \(S \in \mathcal{L}(V)\). Prove that the following are equivalent:<br><br><ul><li>(a) \(S\) is an {{c5::isometry}};</li><li>(d) {{c1::\(S^{*} e_{1}, \ldots, S^{*} e_{n}\)}} is an {{c2::orthonormal}} {{c3::basis}} for {{c4::some orthonormal basis \(e_{1}, \ldots, e_{n}\) of \(V\).}}</li></ul>

============================================================

  

    11 Suppose \(T_{1}, T_{2}\) are normal operators on \(\mathcal{L}\left(\mathbf{F}^{3}\right)\) and both operators have 2, 5,7 as eigenvalues. Prove that there exists an {{c1::isometry::what kind of operator?}} \(S \in \mathcal{L}\left(\mathbf{F}^{3}\right)\) such that <br><ul><li>{{c2::\(T_{1}\)}} = {{c3::\(S^{*}\)}} {{c4::\(T_{2}\)}} {{c5::\(S\).}}</li></ul>

============================================================

  

    12 Give an example of two self-adjoint operators \(T_{1}, T_{2} \in \) {{c2::\(\mathcal{L}\left(\mathbf{F}^{4}\right)\)}} such that the eigenvalues of both operators are 2,5,7 but there does {{c3::not exist}} an {{c1::isometry}} \(S \in \mathcal{L}\left(\mathbf{F}^{4}\right)\) such that:<br><ul><li>{{c4::&nbsp;\(T_{1}\)}} ={{c5::\(S^{*} T_{2} S\).&nbsp;}}</li><li>Be sure to explain why there is no {{c1::isometry}}&nbsp;with the required property.</li></ul>

============================================================

  

    - An operator on \(V\) has a {{c5::{{c2::diagonal}}}} matrix with respect to a basis if and only if {{c4::{{c3::the basis consists of}}}} {{c4::{{c1::eigenvectors of the operator.}}}}

============================================================

  

    The spectral theorems answer the question:<br><ul><li>For which operators on \(V\) is there an {{c1::orthonormal}} {{c3::basis of \(V\)}} with respect to which {{c4::the operator has a}} {{c2::diagonal}} {{c5::matrix}}?<br></li></ul>

============================================================

  

    Any two {{c5::{{c1::diagonal}}}} matrices {{c4::{{c2::commute}}}} under {{c4::{{c3::matrix multiplication}}}}

============================================================

  

    <img src="paste-c7360bede1694146732269f8d646b388bb42064d.jpg"><br>Proof First suppose (c) holds.<br>The matrix of {{c4::\(T^*\)}} is obtained by taking the {{c1::conjugate transpose of the matrix of \(T\)}}; hence {{c4::\(T^*\)}} also has a {{c2::diagonal}} matrix.<br><br>Any two diagonal matrices {{c3::commute}}; thus \(T\) {{c3::commutes}} with {{c4::\(T^*\)}}.<br><br>Thus \(T\) is {{c5::normal}}. In other words, {{c5::(a)}} holds.

============================================================

  

    <img src="paste-bb93648ae5d9fce5eacec97ab660d32e46b50030.jpg"><br>As you can verify,<br>\[<br>\frac{(1,-1,0)}{\sqrt{2}}, \frac{(1,1,1)}{\sqrt{3}}, \frac{(1,1,-2)}{\sqrt{6}}<br>\]<br>is an {{c5::{{c1::orthonormal}}}} basis of \(\mathbf{R}^3\) consisting of {{c4::{{c2::eigenvectors of \(T\)}}}}. With respect to this basis, the matrix of \(T\) is the {{c4::{{c3::diagonal}}}} matrix<br>\[<br>\left(\begin{array}{ccc}<br>27 &amp; 0 &amp; 0 \\<br>0 &amp; 9 &amp; 0 \\<br>0 &amp; 0 &amp; -15<br>\end{array}\right)<br>\]

============================================================

  

    - If \(\mathcal{M}(T)\) is a {{c1::diagonal}} matrix with {{c3::nonnegative}} {{c4::entries}} on the {{c5::diagonal}}, then \(T\) is a {{c2::positive}} operator.

============================================================

  

    Example: If \(T \in \mathcal{L}\left(\mathbf{F}^3\right)\) has matrix<br>{{c5::{{c2::\[<br>\mathcal{M}(T)=\left(\begin{array}{ccc}<br>25 &amp; 0 &amp; 0 \\<br>0 &amp; 4 &amp; 0 \\<br>0 &amp; 0 &amp; 9<br>\end{array}\right),<br>\]}}}}<br>then the operator \(R \in \mathcal{L}\left(\mathbf{F}^3\right)\) with matrix<br>{{c4::{{c3::\[<br>\mathcal{M}(R)=\left(\begin{array}{lll}<br>5 &amp; 0 &amp; 0 \\<br>0 &amp; 2 &amp; 0 \\<br>0 &amp; 0 &amp; 3<br>\end{array}\right),<br>\]}}}}<br>is a {{c4::{{c1::square root}}}} of \(T\).

============================================================

  

    {{c1::Nonegative}} numbers have a {{c2::unique}} {{c5::{{c3::positive}}}} {{c5::{{c4::square root}}}}

============================================================

  

    An {{c2::isometry}} {{c4::always}} {{c5::{{c3::preservers}}}} {{c5::{{c1::norms}}}}

============================================================

  

    <img src="paste-871b6c6d2c013b485ab795321e4b387134e2f3e4.jpg"><br><img src="paste-1beb545169ab2d85d4f9fea1eec79942df32ce63.jpg"><br><ul><li>&lt;u,v&gt; = {{c5::\(\frac{\|u+v\|^2-\|u-v\|^2}{4}\)}}</li><li>=&nbsp;{{c1::\(\frac{\|S(u+v)\|^2-\|S(u-v)\|^2}{4}\)}}</li><li>={{c2::&nbsp;\(\frac{\|S u+S v\|^2-\|S u-S v\|^2}{4}\)}}</li><li>={{c3::&nbsp;\(\langle S u, S v\rangle\),}}</li></ul>Thus {{c4::b}} holds

============================================================

  

    <img src="paste-871b6c6d2c013b485ab795321e4b387134e2f3e4.jpg"><br>Now suppose (b) holds, so \(S\) preserves inner products. Suppose that \(e_1, \ldots, e_n\) is an orthonormal list of vectors in \(V\). Then the list {{c4::\(S e_1, \ldots, S e_n\)}} is {{c3::orthonormal}} because<br><ul><li>{{c1::\(\left\langle S e_j, S e_k\right\rangle\)}} = {{c2::\(\left\langle e_j, e_k\right\rangle .\)}}</li></ul><br>Thus {{c5::(c)}} holds.<br>

============================================================

  

    <img src="paste-871b6c6d2c013b485ab795321e4b387134e2f3e4.jpg"><br>Now suppose (d) holds.<br>Let \(e_1, \ldots, e_n\) be an orthonormal basis of \(V\) such that \(S e_1, \ldots, S e_n\) is orthonormal. Thus<br><ul><li>{{c5::\(\left\langle S^* S e_j, e_k\right\rangle\)}}<br></li><li>=&nbsp;{{c1::\(\left\langle S e_j, S e_k\right\rangle\)}}</li><li>=&nbsp;{{c2::\(\left\langle e_j, e_k\right\rangle\)}}</li></ul><div>for \(j, k=1, \ldots, n\). All vectors \(u, v \in V\) can be written as linear combinations of \(e_1, \ldots, e_n\), and thus the equation above implies that<br><ul><li>{{c3::\(\left\langle S^* S u, v\right\rangle\)}} = {{c4::\(\langle u, v\rangle\)}}</li></ul>for all \(u, v \in V\).&nbsp;</div><div>Hence {{c5::\(S^* S\)}} = {{c5::\(I\)}}; in other words, {{c5::(e)}} holds.<br></div>

============================================================

  

    <img src="paste-871b6c6d2c013b485ab795321e4b387134e2f3e4.jpg"><br><div>That (e) implies (f) is a special case of the result that if \(S, T \in \mathcal{L}(V)\) and {{c1::\(T S\)}} = {{c2::\(I\)}} implies {{c5::{{c3::\(S T\)}}}} = {{c5::{{c4::\(I\).}}}}<br></div>

============================================================

  

    <img src="paste-871b6c6d2c013b485ab795321e4b387134e2f3e4.jpg"><br><div>Now suppose \((\mathrm{f})\) holds, so \(S S^*=I\). If \(v \in V\), then<br></div><div><ul><li>{{c5::\(\left\|S^* v\right\|^2\)&nbsp;}}</li><li>=&nbsp;{{c1::\(\left\langle S^* v, S^* v\right\rangle\)}}<br></li><li>=&nbsp;{{c2::\(\left\langle S S^* v, v\right\rangle\)}}</li><li>={{c3::&nbsp;\(\langle v, v\rangle\)}}</li><li>=&nbsp;{{c4::\(\|v\|^2\)}}</li></ul><div>Thus \(S^*\) is an {{c5::isometry}}, showing that {{c5::\((\mathrm{g})\)}} holds.<br></div></div>

============================================================

  

    <img src="paste-871b6c6d2c013b485ab795321e4b387134e2f3e4.jpg"><br><div>Now suppose (h) holds, so \(S\) is invertible and \(S^{-1}=S^*\).<br><br>Thus \(S^* S=I\).<br></div><div><br></div><div>If \(v \in V\), then<br><ul><li>{{c5::\(\|S v\|^2\)&nbsp;}}</li><li>={{c1::&nbsp;\(\langle S v, S v\rangle\)}}</li><li>={{c2::&nbsp;\(\left\langle S^* S v, v\right\rangle\)}}</li><li>={{c3::&nbsp;\(\langle v, v\rangle\)}}</li><li>=&nbsp;{{c4::\(\|v\|^2\).}}</li></ul><div>Thus \(S\) is an {{c5::isometry}}, showing that {{c5::(a)}} holds.<br></div></div>

============================================================

  

    {{c1::Isometries}} are by definition {{c2::normal}} since they {{c5::{{c3::commute}}}} {{c5::{{c4::with their adjoint&nbsp;}}}}

============================================================

  

    <img src="paste-0629f4e38b9bf7aa8b7c49773aaa24db716252dc.jpg"><br>Proof Suppose (a) holds, so \(S\) is an isometry. By the Complex Spectral Theorem, there is an orthonormal basis \(e_1, \ldots, e_n\) of \(V\) consisting of eigenvectors of \(S\). For \(j \in\{1, \ldots, n\}\), let \(\lambda_j\) be the eigenvalue corresponding to \(e_j\). Then<br><ul><li>\(\left|\lambda_j\right|\)</li><li>=&nbsp;{{c1::\(\left\|\lambda_j e_j\right\|\}}) because {{c2::the norm of orthonormal vectors is 1}}</li><li>= {{c3::\(\left\|S e_j\right\|\)}} because {{c3::eigenvector}}</li><li>={{c4::&nbsp;\(\left\|e_j\right\|\)}} because {{c4::isometry}}</li><li>= {{c5::1}} because {{c5::norm of orthonormal vectors is 1}}</li></ul><div>Thus {{c5::each eigenvalue of S has absolute<br></div>value 1. Hence (b) holds.}}

============================================================

  

    Every {{c5::{{c2::eigenvalue}}}} of a {{c4::{{c3::self-adjoint}}}} operator is {{c4::{{c1::real}}}}

============================================================

  

    If V is a {{c4::complex}} inner product space and T is an operator, then<br><ul><li>\(\langle\) {{c2::\(Tv, v\)}} \(\rangle\) = {{c5::{{c1::0}}}} iff&nbsp; {{c5::{{c3::T is 0}}}}<br></li></ul>

============================================================

  

    When V is a {{c4::real}} inner product space,&nbsp;\(\langle\)&nbsp;{{c1::\(Tv,v\)}} \(\rangle\) can be {{c5::{{c2::0}}}} without {{c5::{{c3::T being 0}}}}

============================================================

  

    The {{c1::spectral}} theorems provide a {{c2::weaker}} condition than being {{c3::self-adjoint}} for {{c4::diagonalizability}} of {{c5::complex}} vector spaces

============================================================

  

    An operator T is {{c1::normal}} iff :<br><ul><li>{{c2::\(T T^*\)}} =&nbsp;{{c3::\(T^* T\)}}<br></li><li>so every {{c4::self-adjoint}} operator is {{c1::normal}} {{c5::by default}}</li></ul>

============================================================

  

    An operator is also {{c4::normal}}, besides the full definition, iff:<br><ul><li>{{c1::\(&nbsp; \vert \vert\)}}&nbsp; {{c5::{{c2::\(Tv\)}}}}&nbsp;{{c1::\(&nbsp; \vert \vert\)}}&nbsp; = {{c1::\(&nbsp; \vert \vert\)}}&nbsp; {{c5::{{c3::\(T^*v\)&nbsp;}}}}{{c1::\(&nbsp; \vert \vert\)}}</li></ul>

============================================================

  

    For an arbitrary operator T and&nbsp;\(\lambda \in F\):<br><ul><li>{{c5::{{c2::&nbsp;\(\lambda\)}}}} is an {{c4::{{c1::eigenvalue}}}} of {{c4::{{c3::T}}}} iff&nbsp;{{c5::{{c2::\(\bar{\lambda}\)}}}} is an {{c1::eigenvalue}} of&nbsp;{{c4::{{c3::\(T^*\)}}}}</li></ul>

============================================================

  

    For a normal operator T:<br><ul><li>If&nbsp;\(v\) is an {{c1::eigenvector of T}} {{c2::associated to the eigenvalue&nbsp;\(\lambda\)}} then&nbsp;\(v\) is an {{c5::{{c3::eigenvector of&nbsp;\(T^{*}\)}}}} {{c5::{{c4::with eigenvalue&nbsp;\(\bar{\lambda}\)}}}}</li></ul>

============================================================

  

    If T is any operator:<br><ul><li>{{c3::The eigenvectors of T}} {{c4::corresponding to distinct eigenvalues}} are {{c5::{{c1::linearly independent}}}} for arbitray T</li></ul><div>If T is normal:</div><div><ul><li>{{c3::They are}} also {{c5::{{c2::orthogonal}}}}</li></ul></div>

============================================================

  

    Proof that for normal operators eigenvectors corresponding to different eigenvalues are orthogonal:<br><ul><li>Take two pairs of eigenvectors-eigenvalues for T and its adjoint</li><li>&nbsp;\(T v\) = \(\alpha v\)</li><li>\(T^* u\) =&nbsp;\(\beta u\)<br></li><li>(\(\alpha - \beta\)) \(\langle\) \(u,v\) \(\rangle\)<br></li><ul><li>= \(\langle\) {{c1::\(\alpha u,v\)}} \(\rangle\) -&nbsp;&nbsp;\(\langle\) {{c2::\(u, \bar{\beta} v\)}} \(\rangle\) due to {{c2::distributivity}} and {{c2::conjugate symmetry}}</li><li>= \(\langle\) {{c2::\(Tu,v\)}} \(\rangle\) -&nbsp;&nbsp;\(\langle\) {{c3::\(u, T^* v\)}} \(\rangle\) due to {{c3::eigenvectors}}</li><li>= \(\langle\) {{c4::\(Tu,v\)}} \(\rangle\) -&nbsp;&nbsp;\(\langle\) {{c5::\(Tu, v\)}} \(\rangle\) due to {{c5::adjoint definition}}</li><li>&nbsp;= 0</li></ul><li>Thus {{c5::the vectors must have been orthogonal}} since {{c5::the eigenvalues were distinct}}</li></ul>

============================================================

  

    For an operator T with eigenvalues \(\lambda_i\)&nbsp;the following are equivalent:<br><ul><li>T has a {{c1::diagonalizing basis}}</li><li>V has a {{c2::basis of eigenvectors of T}} making {{c3::T equivalent to matrix multiplication}} {{c4::on each of these invariant}} {{c5::one-dimensional subspaces}}</li></ul>

============================================================

  

    For an operator T with eigenvalues \(\lambda_i\)&nbsp;the following are equivalent:<br><ul><li>T has a {{c1::diagonalizing basis}}</li><li>V =&nbsp;{{c1::\(\oplus_{i=1}^n\)}}&nbsp;{{c2::\(U_i\)}} {{c2::each of which is is invariant under T}}</li><li>V =&nbsp;{{c1::\(\oplus_{i=1}^n\)}}&nbsp;{{c3::\(\mathrm{null}(T-\lambda_i I)\)&nbsp;}}</li><li>dim V =&nbsp;{{c4::\(\sum_{i=1}^n\)}} {{c5::\(\mathrm{dim} \,\mathrm{null} (T-\lambda_i I)\)}}</li></ul>

============================================================

  

    <ul><li>For {{c5::{{c1::complex}}}} vector spaces, the {{c4::{{c2::diagonalisable}}}} operators are the {{c4::{{c3::normal}}}} opeartors</li><li>For {{c5::{{c1::real}}}} vector spaces they are the {{c4::{{c3::self-adjoint}}}} operators</li></ul>

============================================================

  

    The {{c1::Complex}} Spectral Theorem:<br><ul><li>Suppose that V is a {{c1::complex}} inner product space and T is an operator</li><li>Then V {{c3::has an orthonormal basis}} {{c5::{{c4::consisting of the eigenvectors of T}}}} iff T is {{c5::{{c2::normal}}}}</li></ul>

============================================================

  

    {{c1::Diagonal}} {{c3::matrix}} {{c5::{{c4::multiplication}}}} is implicitly {{c5::{{c2::commutative}}}}

============================================================

  

    {{c1::Quadratic equations}} over operators correspond to {{c2::two-dimensional}} {{c5::{{c3::invariant}}}} {{c5::{{c4::subspaces}}}}

============================================================

  

    For a real product space, when is the follwing equation invertible?<br><ul><li>{{c5::\(T^2 + \alpha T + \beta\)}}<br></li></ul><div>When:</div><div><ul><li>T is {{c1::self-adjoint}}</li><li>And&nbsp;{{c2::\(\alpha^2\)}}&nbsp;{{c3::\(&lt;\)&nbsp;}}{{c4::\(4 \beta\)}}</li></ul></div>

============================================================

  

    Why is the theorem that a quadratic operator polynomial over R is invertible when the operator is self adjoint and&nbsp;\(b^2 &lt; 4c\) useful?<br><br><ul><li>It means that it is {{c1::non-negative}}</li><li>Thus if we try to prove that {{c2::T has an eigenvalue over R}}, {{c3::the quadratic terms cannot be 0}}</li><li>Thus {{c4::one of the linear terms must be 0}} and {{c5::T must have an eigenvalue despite the space being real}}</li></ul>

============================================================

  

    {{c1::Self-adjointness}} is a sufficient property to make operators over {{c2::real}} vector spaces have an {{c3::eigenvalue}}, thus an {{c4::upper-triangular}} matrix and thus a {{c5::diagonal}} matrix (due to the {{c1::transpose}}&nbsp;operation)

============================================================

  

    The {{c4::Real}} Spectral Theorem:<br><br>For V a {{c4::real}} inner product space and T an operator<ul><li>V has an {{c1::orthonormal}} {{c5::{{c2::basis of eigenvectors of T}}}} iff T is {{c5::{{c3::self-adjoint}}}}</li></ul>

============================================================

  

    <br>To prove the real spectral theorem we use an induction on V<br><ul><li>Use an inductive technique, shwoing that it is self-evident for dim V = 1</li><li>For other ones decompose the vector space into an {{c1::eigenvector}}, which exists because of {{c1::self-adjointness}},&nbsp; and {{c2::its orthogonal complement}}</li><li>Show that the {{c2::orthogonal complement}} has an {{c3::orthonormal basis formed of eigenvectors}} by showing that it is {{c4::self-adjoint}} and {{c4::using the I.H}}</li><li>Since the extra vector is {{c5::orthogonal to all vectors in the orthogonal complement}}, just {{c5::concatenate it}} to get a new orthonormal eigenbasis</li></ul>

============================================================

  

    Suppose V is a 2-dimensional inner product space over R and T is an operator. Then the following are equivalent:<br><ul><li>T is {{c1::normal}} but not {{c1::self-adjoint}}</li><li>The matirix of T is of the form &nbsp;{{c2::\(\left[\begin{array}{cc}a &amp; -b \\ b &amp; a\end{array}\right]\)}} for any {{c5::{{c3::orthonormal basis}}}} with {{c5::{{c3::b != 0}}}}</li><li>There exists an {{c3::orthonormal basis}} such that {{c5::{{c4::the matrix is of the previous form}}}} with {{c3::b != 0}}</li></ul>

============================================================

  

    The main trick for proving things about {{c5::{{c1::normal}}}} matrices is {{c4::{{c2::equating norms}}}} {{c4::{{c3::between rows and columns}}}}

============================================================

  

    <ul><li>Another distinguished subset of \(\mathbf{C}\) is the {{c4::unit}} {{c5::circle}}, which consists of {{c3::the complex numbers \(z\)}} such that {{c1::\(|z|\)}} = {{c2::\(1\).&nbsp;}}</li><li>The condition \(|z|\) = \(1\) is equivalent to the condition \(\bar{z} z\) = \(1\).&nbsp;</li><li>Under our analogy, this would correspond to the condition \(T^{*} T\) = \(I\), which is equivalent to \(T\) being an isometry.</li><li>In other words, the unit circle in \(\mathbf{C}\) corresponds to the isometries.</li></ul>

============================================================

  

    <ul><li>Another distinguished subset of \(\mathbf{C}\) is the unit circle, which consists of the complex numbers \(z\) such that \(|z|=1\).&nbsp;</li><li>The condition {{c1::\(|z|\)}} = {{c2::\(1\)}} is equivalent to the condition {{c3::\(\bar{z} z\)}} = {{c2::\(1\).&nbsp;}}</li><li>Under our analogy, this would correspond to the condition {{c4::\(T^{*} T\)}} = {{c2::\(I\)}}, which is equivalent to {{c5::\(T\) being an isometry.}}</li></ul>

============================================================

  

    Continuing with our analogy, note that each complex number \(z\) except 0 can be written in the form<br><br><ul><li>\(z\)</li><li>={{c1::\((\frac{z}{|z|})\)}} {{c2::\(|z|\)}}</li><li>={{c5::{{c3::\((\frac{z}{|z|})\)}}}}&nbsp; {{c5::{{c4::\(\sqrt{\bar{z} z}\)}}}}</li></ul><br>where the first factor, namely, \(z /|z|\), is an element of the unit circle.<br>

============================================================

  

    Continuing with our analogy, note that each complex number \(z\) except 0 can be written in the form<br><br>\[<br>z=\left(\frac{z}{|z|}\right)|z|=\left(\frac{z}{|z|}\right) \sqrt{\bar{z} z}<br>\]<br><br>where the first factor, namely, \(z /|z|\), is an element of the unit circle. <br>Our analogy leads us to guess that:<br><ul><li>&nbsp;Each {{c1::operator \(T \in \mathcal{L}(V)\)}} {{c4::can be written as}} {{c2::an isometry}} {{c5::times}} {{c3::\(\sqrt{T^{*} T}\)}}.&nbsp;</li></ul>

============================================================

  

    Notation {{c1::\(\sqrt{T}\)}}<br><br>If \(T\) is a {{c5::positive}} operator, then {{c1::\(\sqrt{T}\)}} denotes the {{c3::unique}} {{c2::positive square root}} {{c4::of \(T\).}}

============================================================

  

    &nbsp;Note that {{c1::\(T^{*}\)}} times {{c2::\(T\)}} is a {{c3::positive operator}} for {{c4::every \(T \in \mathcal{L}(V)\)}}, and thus {{c5::\(\sqrt{T^{*} T}\)}} is {{c5::well defined.}}

============================================================

  

    {{c5::Polar}} Decomposition<br><br>Suppose \(T \in \mathcal{L}(V)\). Then there exists {{c4::an isometry \(S \in \mathcal{L}(V)\)}} such that<br><br><ul><li>{{c3::\(T\)}} = {{c2::\(S\)}} {{c1::\(\sqrt{T^{*} T} .\)}}</li></ul><br>

============================================================

  

    Consider the case \(\mathbf{F}=\mathbf{C}\), and suppose \(T=S \sqrt{T^{*} T}\) is a Polar Decomposition of an operator \(T \in \mathcal{L}(V)\), where \(S\) is an isometry.<br><ul><li>Then there is an {{c5::{{c1::orthonormal}}}} basis of \(V\) with respect to which {{c4::{{c2::\(S\)}}}} has a {{c4::{{c3::diagonal}}}} matrix, and there is an {{c1::orthonormal}} basis of \(V\) with respect to which {{c4::{{c2::\(\sqrt{T^{*} T}\)}}}} has a {{c3::diagonal}} matrix.&nbsp;</li></ul>

============================================================

  

    <ul><li>For {{c1::Polar decomposition}}, there may not exist an {{c2::orthonormal basis}} that {{c3::simultaneously}} puts {{c4::the matrices of}} {{c3::both \(S\) and \(\sqrt{T^* T}\)}} {{c5::</li><li>into diagonal forms }}</li><li>In other words, {{c5::\(S\) may require one orthonormal basis}} and {{c5::\(\sqrt{T^{*} T}\) may require a different orthonormal basis.}}</li></ul>

============================================================

  

    Definition {{c1::singular values}}<br><br>Suppose \(T \in \mathcal{L}(V)\). The {{c1::singular values}} of \(T\) are {{c2::the eigenvalues}} {{c3::of \(\sqrt{T^{*} T}\)}}, with {{c2::each eigenvalue \(\lambda\)}} {{c5::repeated}} {{c4::\(\operatorname{dim} E\left(\lambda, \sqrt{T^{*} T}\right)\)}} {{c5::times}}.

============================================================

  

    The {{c2::singular values}} of \(T\) are all {{c1::nonnegative}}, because they are the {{c5::{{c3::eigenvalues}}}} of {{c5::{{c4::the positive operator \(\sqrt{T^{*} T}\).}}}}

============================================================

  

    \subsection{Example Define \(T \in \mathcal{L}\left(\mathbf{F}^{4}\right)\) by}<br><br>\[<br>T\left(z_{1}, z_{2}, z_{3}, z_{4}\right)=\left(0,3 z_{1}, 2 z_{2},-3 z_{4}\right) .<br>\]<br><br>Find the singular values of \(T\).<br><br>Solution A calculation shows {{c1::\(T^{*} T\)}} \(\left(z_{1}, z_{2}, z_{3}, z_{4}\right)\) = {{c2::\(\left(9 z_{1}, 4 z_{2}, 0,9 z_{4}\right)\)}}, as you should verify. Thus<br><br><ul><li>{{c3::\(\sqrt{T^{*} T}\)}} \(\left(z_{1}, z_{2}, z_{3}, z_{4}\right)\) = {{c4::\(\left(3 z_{1}, 2 z_{2}, 0,3 z_{4}\right),\)}}</li></ul><br>and we see that the eigenvalues of \(\sqrt{T^{*} T}\) are {{c5::3,2,0}}&nbsp;<br>

============================================================

  

    \subsection{Example Define \(T \in \mathcal{L}\left(\mathbf{F}^{4}\right)\) by}<br><br>\[<br>T\left(z_{1}, z_{2}, z_{3}, z_{4}\right)=\left(0,3 z_{1}, 2 z_{2},-3 z_{4}\right) .<br>\]<br><br>Find the singular values of \(T\).<br><br>Solution A calculation shows \(T^{*} T\left(z_{1}, z_{2}, z_{3}, z_{4}\right)=\left(9 z_{1}, 4 z_{2}, 0,9 z_{4}\right)\), as you should verify. Thus<br><br>\[<br>\sqrt{T^{*} T}\left(z_{1}, z_{2}, z_{3}, z_{4}\right)=\left(3 z_{1}, 2 z_{2}, 0,3 z_{4}\right),<br>\]<br><br>and we see that the eigenvalues of \(\sqrt{T^{*} T}\) are 3,2,0 and<br><br><ul><li>{{c1::\(\operatorname{dim} E\)}} {{c2::\(\left(3, \sqrt{T^{*} T}\right)\)}} = {{c5::2}}</li><li>{{c1::\(\operatorname{dim} E\)}} {{c3::\(\left(2, \sqrt{T^{*} T}\right)\)}}</li><li>={{c5::1}}</li><li>{{c1::\(\operatorname{dim} E\)}} {{c4::\(\left(0, \sqrt{T^{*} T}\right)\)}} ={{c5::1}}.</li></ul><br>Hence the singular values of \(T\) are {{c5::3,3,2,0.}}<br><br>

============================================================

  

    \subsection{Example Define \(T \in \mathcal{L}\left(\mathbf{F}^{4}\right)\) by}<br><br>\[<br>T\left(z_{1}, z_{2}, z_{3}, z_{4}\right)=\left(0,3 z_{1}, 2 z_{2},-3 z_{4}\right) .<br>\]<br><br>Find the singular values of \(T\).<br><br>Solution A calculation shows \(T^{*} T\left(z_{1}, z_{2}, z_{3}, z_{4}\right)=\left(9 z_{1}, 4 z_{2}, 0,9 z_{4}\right)\), as you should verify. Thus<br><br>\[<br>\sqrt{T^{*} T}\left(z_{1}, z_{2}, z_{3}, z_{4}\right)=\left(3 z_{1}, 2 z_{2}, 0,3 z_{4}\right),<br>\]<br><br>and we see that the eigenvalues of \(\sqrt{T^{*} T}\) are 3,2,0 and<br><br>\(\operatorname{dim} E\left(3, \sqrt{T^{*} T}\right)=2, \operatorname{dim} E\left(2, \sqrt{T^{*} T}\right)=1, \operatorname{dim} E\left(0, \sqrt{T^{*} T}\right)=1\).<br><br>Hence the singular values of \(T\) are 3,3,2,0.<br><br><ul><li>Note that {{c1::-3}} and {{c1::0}} are {{c2::the only eigenvalues of \(T\).&nbsp;}}</li><li>Thus in this case, {{c3::the collection of eigenvalues did not pick up the number 2}} that a{{c4::ppears in the definition (and hence the behavior) of \(T\)}}, but {{c5::the collection of singular values does include 2}}.</li></ul>

============================================================

  

    Each \(T \in \mathcal{L}(V)\) has {{c1::\(\operatorname{dim} V\)}} {{c2::singular}} {{c3::values}}, as can be seen by applying the {{c4::spectral theorem}} and&nbsp;<br>{{c5::<img src="paste-c034a87a8a749acec04ec38c66ec5cb6d8851843.jpg">}}

============================================================

  

    Singular Value Decomposition<br><br>Suppose \(T \in \mathcal{L}(V)\) has singular values \(s_{1}, \ldots, s_{n}\). Then there exist orthonormal bases \(e_{1}, \ldots, e_{n}\) and \(f_{1}, \ldots, f_{n}\) of \(V\) such that<br><br><ul><li>\(T v\) =&nbsp;{{c1::\(\sum_{i=1}^n\)}} {{c2::\( s_i\)}} {{c5::\(\langle\)}} {{c3::\(v, e_i\)}} {{c5::\(\rangle\)}} {{c4::\(f_i \)}}</li></ul><br>for every \(v \in V\).<br>

============================================================

  

    {{c2::Singular Value}} Decomposition<br><br>Suppose \(T \in \mathcal{L}(V)\) has {{c1::singular values \(s_{1}, \ldots, s_{n}\)}}. Then there exist {{c4::orthonormal bases}} {{c5::\(e_{1}, \ldots, e_{n}\)}} and {{c5::\(f_{1}, \ldots, f_{n}\)}} {{c4::of \(V\)}} such that<br><br><ul><li>{{c3::\(T v\)}} = {{c1::\(\sum_{i=1}^n s_i \langle v, e_i \rangle f_i\)}}</li><li>for {{c4::every \(v \in V\)}}.</li></ul>

============================================================

  

    The {{c5::{{c3::Singular Value}}}} Decomposition allows us a rare opportunity to {{c4::{{c1::make good use of two different bases}}}} for {{c4::{{c2::the matrix of an operator.}}}}

============================================================

  

    Suppose \(T \in \mathcal{L}(V)\). <br><ul><li>Let \(s_{1}, \ldots, s_{n}\) denote the {{c1::singular values}} of \(T\)</li><li>And let \(e_{1}, \ldots, e_{n}\) and \(f_{1}, \ldots, f_{n}\) be {{c2::orthonormal}} bases of \(V\).</li><li>Because {{c5::\(T e_{j}\)}} = {{c3::\(s_{j} f_{j}\)}} for each \(j\)</li><li>we have:</li><li>\(\mathcal{M}\left(T,\left(e_1, \ldots, e_n\right),\left(f_1, \ldots, f_n\right)\right)\) =&nbsp;{{c4::\(\left(\begin{array}{ccc}s_1 &amp; &amp; 0 \\ &amp; \ddots &amp; \\ 0 &amp; &amp; s_n\end{array}\right)\).}}</li></ul><br>

============================================================

  

    Every operator on \(V\) has a {{c1::diagonal}} matrix with respect to {{c2::some orthonormal bases of \(V\)}}, provided that we are permitted to {{c5::{{c3::use two different bases}}}} rather than {{c5::{{c4::a single basis as customary when working with operators.}}}}

============================================================

  

    <ul><li>To compute numeric approximations to the singular values of an operator \(T\), first compute {{c1::\(T^{*} T\)}} and then compute {{c2::approximations to the eigenvalues of \(T^{*} T\)}} (good techniques exist for {{c2::approximating eigenvalues of positive operators}}). </li><li>The {{c3::nonnegative square roots of these (approximate) eigenvalues of \(T^{*} T\)}} will be the {{c4::(approximate) singular values of \(T\).&nbsp;}}</li><li>In other words, {{c5::the singular values of \(T\)}} can be {{c5::approximated}} without {{c5::computing the square root of \(T^{*} T\).&nbsp;}}</li></ul>

============================================================

  

    Suppose \(T \in \mathcal{L}(V)\). Then the {{c1::singular values}} of \(T\) are the {{c2::nonnegative square roots}} of {{c3::the eigenvalues of \(T^{*} T\)}}, with {{c4::each eigenvalue \(\lambda\)}} {{c5::repeated \(\operatorname{dim} E\left(\lambda, T^{*} T\right)\) times.}}

============================================================

  

    1 Fix \(u, x \in V\) with {{c5::\(u \neq 0\)}}. Define \(T \in \mathcal{L}(V)\) by<br><br><ul><li>\(T v\)={{c5::\(\langle v, u\rangle x\)}}</li></ul><br>for every \(v \in V\). Prove that<br><br><ul><li>{{c5::\(\sqrt{T^{*} T}\)}} \(v\) = {{c3::\(\frac{\|x\|}{\|u\|}\)}} {{c2::\(\langle\)}} {{c1::\(v, u\)}}{{c2::\(\rangle\)}} {{c4::\(u\)}}</li></ul><br>for every \(v \in V\).<br>

============================================================

  

    3 Suppose \(T \in \mathcal{L}(V)\). Prove that there exists an {{c5::{{c3::isometry}}}} \(S \in \mathcal{L}(V)\) such that<br><ul><li><br></li><li>\(T\) = {{c4::{{c1::\(\sqrt{T T^{*} }\)}}}} {{c4::{{c2::\(S\)}}}}</li></ul>

============================================================

  

    4 Suppose \(T \in \mathcal{L}(V)\) and \(s\) is a {{c4::singular value}} of \(T\). Prove that there exists:<br><ul><li>A vector \(v \in V\) such that:</li><ul><li>&nbsp;{{c3::\(\|\)}} {{c5::{{c2::\(v\)}}}} {{c3::\(\|\)}} = {{c5::{{c1::\(1\)&nbsp;}}}}</li><li>{{c3::\(\|\)}} {{c5::{{c2::\(T\)}}}} {{c3::\(v\|\)}} = {{c5::{{c1::\(s\).}}}}</li></ul></ul>

============================================================

  

    8 Suppose \(T \in \mathcal{L}(V), S \in \mathcal{L}(V)\) is {{c3::an isometry}}, and \(R \in \mathcal{L}(V)\) is a {{c4::positive operator}} such that {{c5::\(T\)}} = {{c2::\(S R\)}}. Prove that \(R\) ={{c1::\(\sqrt{T^{*} T}\).}}

============================================================

  

    8 Suppose \(T \in \mathcal{L}(V), S \in \mathcal{L}(V)\) is an isometry, and \(R \in \mathcal{L}(V)\) is a positive operator such that \(T=S R\). Prove that \(R=\sqrt{T^{*} T}\).<br><br>[The exercise above shows that if we write \(T\) as the {{c3::product of an isometry and a positive operator}} (as in the {{c2::Polar}} Decomposition, then the {{c5::{{c4::positive operator}}}} equals {{c5::{{c1::\(\sqrt{T^{*} T}\)}}}}.]

============================================================

  

    9 Suppose \(T \in \mathcal{L}(V)\). Prove that \(T\) is {{c3::invertible}} if and only if there exists a {{c4::unique isometry \(S \in \mathcal{L}(V)\)}} such that \(T\) = {{c5::{{c2::\(S\)}}}} {{c5::{{c1::\(\sqrt{T^{*} T}\)}}}}.

============================================================

  

    10 Suppose \(T \in \mathcal{L}(V)\) is self-adjoint. Prove that the {{c5::{{c3::singular values}}}} of \(T\) equal the {{c4::{{c1::absolute values of the eigenvalues of \(T\)}}}}, {{c4::{{c2::repeated appropriately.}}}}

============================================================

  

    11 Suppose \(T \in \mathcal{L}(V)\). Prove that {{c3::\(T\)}} and {{c4::\(T^{*}\)}} have {{c5::{{c2::the same}}}} {{c5::{{c1::singular values.}}}}

============================================================

  

    13 Suppose \(T \in \mathcal{L}(V)\). Prove that \(T\) is {{c1::invertible}} if and only if {{c4::0}} {{c5::{{c3::is not a}}}} {{c5::{{c2::singular value of \(T\).}}}}

============================================================

  

    14 Suppose \(T \in \mathcal{L}(V)\). Prove that {{c4::dim}} {{c3::range \(T\)}} {{c5::equals}} {{c2::the number of}} {{c1::nonzero singular values of \(T\).}}

============================================================

  

    15 Suppose \(S \in \mathcal{L}(V)\). Prove that \(S\) is an {{c5::{{c3::isometry}}}} if and only if all the {{c4::{{c1::singular values}}}} of \(S\) {{c4::{{c2::equal 1}}}} .

============================================================

  

    16 Suppose \(T_{1}, T_{2} \in \mathcal{L}(V)\). Prove that \(T_{1}\) and \(T_{2}\) have the same {{c1::singular values}} if and only if there exist {{c5::isometries \(S_{1}, S_{2} \in \mathcal{L}(V)\)}} such that \(T_{1}\) = {{c4::\(S_{1}\)}} {{c3::\(T_{2}\)}} {{c2::\(S_{2}\).}}

============================================================

  

    17 Suppose \(T \in \mathcal{L}(V)\) has singular value decomposition given by<br><br><ul><li>\(T v\)= \(s_{1}\left\langle v, e_{1}\right\rangle f_{1}+\cdots+s_{n}\left\langle v, e_{n}\right\rangle f_{n}\)</li></ul><br>for every \(v \in V\), where \(s_{1}, \ldots, s_{n}\) are the singular values of \(T\) and \(e_{1}, \ldots, e_{n}\) and \(f_{1}, \ldots, f_{n}\) are orthonormal bases of \(V\).<br><br><ul><li>(b) Prove that if \(v \in V\), then<br></li><ul><li>{{c5::\(T^{*}\)}} {{c4::\(T v\)}} =&nbsp;{{c3::\(\sum_{i=1}^n\)}} {{c2::\(s_i^2\)}} {{c1::\(\langle v, e_i \rangle e_i\)}}</li></ul></ul>

============================================================

  

    17 Suppose \(T \in \mathcal{L}(V)\) has singular value decomposition given by<br><br><ul><li>\(T v\)= \(s_{1}\left\langle v, e_{1}\right\rangle f_{1}+\cdots+s_{n}\left\langle v, e_{n}\right\rangle f_{n}\)</li></ul><br>for every \(v \in V\), where \(s_{1}, \ldots, s_{n}\) are the singular values of \(T\) and \(e_{1}, \ldots, e_{n}\) and \(f_{1}, \ldots, f_{n}\) are orthonormal bases of \(V\).<br><br><ul><li>(d) Suppose \(T\) is {{c5::invertible}}. Prove that if \(v \in V\), then</li><ul><li>{{c5::\(T^{-1} v\)}} =&nbsp;{{c4::\(\sum_{i=1}^n\)}} ( {{c3::\(\langle\)}}&nbsp;{{c2::\(v,f_i\)}} {{c3::\(\rangle\)}}{{c2::&nbsp;\(e_i\)}}&nbsp;\(/\)&nbsp;{{c1::\(s_i\)}} )</li></ul></ul>

============================================================

  

    18 Suppose \(T \in \mathcal{L}(V)\). Let \(\hat{s}\) denote the {{c5::{{c1::smallest}}}} {{c4::{{c2::singular value of \(T\)}}}}, and let \(s\) denote the {{c4::{{c3::largest}}}} {{c4::{{c2::singular value of \(T\).}}}}<br><br>(a) Prove that \(\hat{s}\|v\|\) \(\leq\) \(\|T v\| \leq s\|v\|\) for every \(v \in V\).<br><br>(b) Suppose \(\lambda\) is an eigenvalue of \(T\). Prove that \(\hat{s}\) \(\leq\) \(|\lambda| \leq s\).

============================================================

  

    18 Suppose \(T \in \mathcal{L}(V)\). Let \(\hat{s}\) denote the smallest singular value of \(T\), and let \(s\) denote the largest singular value of \(T\).<br><br><ul><li>(a) Prove that {{c1::\(\hat{s}\) \(\|v\|\)}} {{c4::\(\leq\)}} {{c5::{{c2::\(\|T v\|\)}}}} {{c4::\(\leq \)}} {{c5::{{c3::\(s\|v\|\)}}}} for every \(v \in V\).</li></ul>

============================================================

  

    18 Suppose \(T \in \mathcal{L}(V)\). Let \(\hat{s}\) denote the smallest singular value of \(T\), and let \(s\) denote the largest singular value of \(T\).<br><ul><li>(b) Suppose \(\lambda\) is an {{c1::eigenvalue of \(T\)}}. Prove that {{c2::\(\hat{s}\)}} {{c5::\(\leq\)}} {{c3::\(|\lambda|\)}} {{c5::\(\leq\)}} {{c4::\(s\).}}<br></li></ul>

============================================================

  

    20 Suppose \(S, T \in \mathcal{L}(V)\). Let \(s\) denote the {{c4::largest singular value of \(S\)}}, let \(t\) denote the {{c5::largest singular value of \(T\)}}, and let \(r\) denote the {{c5::largest singular value of \(S+T\)}}. Prove that<br><ul><li>{{c2::&nbsp;\(r\)}} {{c3::\(\leq\)}} {{c1::\(s+t\).}}</li></ul>

============================================================

  

    1. (4 points) Suppose \(V=\mathbb{R}^{2}\). The identity operator \(I\) is a positive selfadjoint operator, so \(I\) has a unique positive square root (namely \(I\) ). Find all self-adjoint square roots of \(I\).<br><br>Any square self-adjoint square root \(S\) has (like any self-adjoint matrix) an orthonormal basis of eigenvectors. The possible eigenvalues are the square roots of 1 , which are \(\pm 1\). So there are three cases.<br><br><ul><li>1. {{c1::both eigenvalues are \(+1\)}} , \(S\)= {{c2::\(I\)}}.</li><li><br></li><li>2. {{c3::both eigenvalues are \(-1\)}} , \(S\) = {{c4::\(-I\)}}.</li><li><br></li><li>3. {{c5::one eigenvalue is +1}} and {{c5::one is -1}} .</li></ul><br>

============================================================

  

    1. (4 points) Suppose \(V=\mathbb{R}^{2}\). The identity operator \(I\) is a positive selfadjoint operator, so \(I\) has a unique positive square root (namely \(I\) ). Find all self-adjoint square roots of \(I\).<br><br>Any square self-adjoint square root \(S\) has (like any self-adjoint matrix) an orthonormal basis of eigenvectors. The possible eigenvalues are the square roots of 1 , which are \(\pm 1\). So there are three cases.<br><br><ul><li>1. both eigenvalues are \(+1\) , \(S\)= \(I\).</li><li><br></li><li>2. both eigenvalues are \(-1\) , \(S\) = \(-I\).</li><li><br></li><li>3. one eigenvalue is +1 and one is -1 .</li></ul><br>To make the third case explicit, we need to write down the possibilities for the +1 eigenvector: it can be any unit vector in \(\mathbb{R}^{2}\), so must have the form<br><br><ul><li>\(f_1\) = {{c1::\( \left(\begin{array}{c} \cos \theta \\ \sin \theta \end{array}\right)\)&nbsp;&nbsp;}}</li><li>\(S f_{1}\) = {{c2::\(f_{1} \)}}</li></ul><br>Then the second eigenvector is determined (up to sign) as the one orthogonal to \(f_{1}\); we can choose<br><br><ul><li>\(f_2\) = {{c5::{{c3::\( \left(\begin{array}{c} \sin \theta \\ -\cos \theta \end{array}\right)\)&nbsp;&nbsp;}}}}</li><li>\(S f_{2}\) = {{c5::{{c4::\(-f_{2}\)}}}}</li></ul><br>

============================================================

  

    4. Suppose \(P \in \mathcal{L}(V)\) is such that \(P^{2}=P\). Prove that \(P\) is an orthogonal projection if and only if \(P\) is self-adjoint.<br><br><ul><li>Suppose that P is an {{c1::orthogonal projection}}</li><li>Thus we can make an {{c2::orthogonal decomposition}} and</li><ul><li>{{c5::\(v_1\)}} =&nbsp;{{c3::\(u_1 + w_1\)}}</li><li>{{c5::\(v_2\)}} =&nbsp;{{c4::\(u_2 + w_2\)}}</li></ul></ul>

============================================================

  

    4. Suppose \(P \in \mathcal{L}(V)\) is such that \(P^{2}=P\). Prove that \(P\) is an orthogonal projection if and only if \(P\) is self-adjoint.<br><br><ul><li>Suppose that P is an orthogonal projection</li><li>Thus we can make an orthogonal decomposition and</li><ul><li>\(v_1\) =&nbsp;\(u_1 + w_1\)</li><li>\(v_2\) =&nbsp;\(u_2 + w_2\)</li></ul><li>Then \(\langle\) \(P v_1,v_2\) \(\rangle\) =&nbsp;</li><ul><li>&nbsp;\(\langle\)&nbsp;{{c1::\(u_1, u_2 + w_2\)}} \(\rangle\) by {{c1::orthogonal projection}}</li><li>&nbsp;\(\langle\)&nbsp;{{c2::\(u_1, u_2\)}} \(\rangle\) +&nbsp; \(\langle\){{c2::&nbsp;\(u_1, w_2\)}} \(\rangle\) by {{c2::linearity}}&nbsp;</li><li>&nbsp;\(\langle\)&nbsp; {{c3::\(u_1,u_2\)}} \(\rangle\) by {{c3::orthogonal complement}}</li><li>&nbsp;\(\langle\)&nbsp; {{c4::\(u_1,u_2\)}} \(\rangle\) +&nbsp;&nbsp;\(\langle\)&nbsp; {{c4::\(w_1,u_2\)}} \(\rangle\) by {{c4::orthogonal complement in the other direction}}<br></li><li>&nbsp;\(\langle\)&nbsp; {{c5::\(u_1 + w_1,u_2\)}} \(\rangle\) by {{c5::linearity}}<br></li><li>&nbsp;\(\langle\)&nbsp; {{c5::\(v_1 P v_2\)}} \(\rangle\) by {{c5::orthognal projection}}<br></li></ul><li>Thus&nbsp;\(P\) =&nbsp;\(P*\) and hence P is self-adjoint</li></ul>

============================================================

  

    4. Suppose \(P \in \mathcal{L}(V)\) is such that \(P^{2}=P\). Prove that \(P\) is an orthogonal projection if and only if \(P\) is self-adjoint.<br><br><ul><li>Suppose that \(P\) is {{c5::self-adjoint}}. Let \(v \in V\).&nbsp;</li><li>Because {{c1::\(P(v-P v)\)}} = {{c2::\(P v-P^{2} v\)}} = {{c2::\(0\)}}, we have</li><ul><li>{{c3::\(v - Pv\)}}&nbsp;\(\in\) {{c3::null \(P\)}} =&nbsp;{{c4::\((range P^*)^\perp\)}} =&nbsp;{{c5::\((range P )^\perp\)}}</li></ul></ul><br>

============================================================

  

    4. Suppose \(P \in \mathcal{L}(V)\) is such that \(P^{2}=P\). Prove that \(P\) is an orthogonal projection if and only if \(P\) is self-adjoint.<br><br><ul><li>Suppose that \(P\) is self-adjoint. Let \(v \in V\).&nbsp;</li><li>Because \(P(v-P v)\) = \(P v-P^{2} v\) = \(0\), we have</li><ul><li>\(v - Pv\)&nbsp;\(\in\) null \(P\) =&nbsp;\((range P^*)^\perp\) =&nbsp;\((range P )^\perp\)<br></li></ul><li>We can then write</li><ul><li>\(v\) =&nbsp;{{c1::\(Pv\)}} +&nbsp;{{c2::\(v - Pv\)}}<br></li></ul><li>Thus&nbsp;\(Pv\) ={{c3::&nbsp;\(P_{rangeP} v\)}} which {{c4::holds for all v}}</li><li>Thus&nbsp;\(Pv\) =&nbsp;{{c5::\(P_{rangeP}\)}} which shows that P is an orthogonal projection</li></ul>

============================================================

  

    13 Prove or give a counterexample: if \(S \in \mathcal{L}(V)\) and there exists an orthonormal basis \(e_1, \ldots, e_n\) of \(V\) such that \(\left\|S e_j\right\|=1\) for each \(e_j\), then \(S\) is an isometry.<br><br><ul><li>Answer: {{c1::false}} since we could {{c2::just send each&nbsp;\(e_j\) to&nbsp;\(e_1\)}} making&nbsp;{{c3::\( ||Se_j||\)}} =&nbsp; {{c3::1}} {{c4::without}} {{c5::being invertible}}</li></ul>

============================================================

  

    4. (4 points) Suppose \(V\) is a finite-dimensional complex inner product space, and \(T \in \mathcal{L}(V)\) is a selfadjoint operator.<br><br><br>a) Give a reasonable definition of the operator \(\cos (T)\), and prove that every selfadjoint operator has a unique cosine (in your definition).<br><br><ul><li>Because T is {{c5::self-adjoint}} we are {{c4::guranteed}} to have {{c4::a basis of eigenvectors no matter what}}</li><li>Thus we can simply define&nbsp;\(\cos T\) based on {{c3::its effects on the eigenbasis}} as {{c2::&nbsp;\((\cos T)v_i\)}} ={{c1::&nbsp;\(\cos (\lambda_i) v_i\)}}</li></ul>

============================================================

  

    4. (4 points) Suppose \(V\) is a finite-dimensional complex inner product space, and \(T \in \mathcal{L}(V)\) is a selfadjoint operator.<br><br><br>a) Give a reasonable definition of the operator \(\cos (T)\), and prove that every selfadjoint operator has a unique cosine (in your definition).<br><br><ul><li>Because T is self-adjoint we are guranteed to have a basis of eigenvectors no matter what</li><li>Thus we can simply define&nbsp;\(\cos T\) based on its effects on the eigenbasis as&nbsp;\((\cos T)v_i\) =&nbsp;\(\cos (\lambda_i) v_i\)</li></ul><div>b) Cosines are harder than square roots. Why is it that all selfadjoint operators have cosines, but only positive self-adjoint operators have square roots?</div><div><ul><li>{{c5::Any real-valued function}} can be defined on a self-adjoint operator by {{c3::applying it to the eigenvalues}} {{c4::corresponding to the eigenbasis}}</li><li>However, for the square root we {{c1::cannot take the square root of a negative number}} so we need to {{c2::restrict it to operators with nonegative eigevanlues}}</li></ul></div>

============================================================

  

    8 Suppose \(T \in \mathcal{L}(V), S \in \mathcal{L}(V)\) is an isometry, and \(R \in \mathcal{L}(V)\) is a positive operator such that \(T=S R\). Prove that \(R=\sqrt{T^* T}\).<br>[The exercise above shows that if we write \(T\) as the product of an isometry and a positive operator (as in the Polar Decomposition 7.45), then the positive operator equals \(\sqrt{T^* T}\).]<br><br>Proof:<br><ul><li>\(T = SR\) implies&nbsp;\(R\) =&nbsp;{{c1::\(S^* T\)}} since {{c2::isometries are their own inverses}}<br></li><li>||&nbsp;\(Rv\) ||\(^2\) =&nbsp; ||&nbsp;{{c1::\(S^* Tv\)}} ||\(^2\) =&nbsp;</li><ul><li>\(\langle\)&nbsp;{{c3::\(S^* Tv, S^* T\)}} \(\rangle\) by definition of {{c3::squared norm}}<br></li><li>\(\langle\)&nbsp;{{c4::\(Tv, Tv\)}} \(\rangle\) since {{c4::isometries and their adjoints preserver inner products}}</li><li>\(\langle\){{c5::&nbsp;\(T^* Tv, v\)}} \(\rangle\) by definition of {{c5::adjoint}}</li></ul></ul>

============================================================

  

    8 Suppose \(T \in \mathcal{L}(V), S \in \mathcal{L}(V)\) is an isometry, and \(R \in \mathcal{L}(V)\) is a positive operator such that \(T=S R\). Prove that \(R=\sqrt{T^* T}\).<br>[The exercise above shows that if we write \(T\) as the product of an isometry and a positive operator (as in the Polar Decomposition 7.45), then the positive operator equals \(\sqrt{T^* T}\).]<br><br>Proof:<br><ul><li>\(T = SR\) implies&nbsp;\(R\) =&nbsp;\(S^* T\) since isometries are their own inverses<br></li><li>||&nbsp;\(Rv\) ||\(^2\) =&nbsp; ||&nbsp;\(S^* Tv\) ||\(^2\) =&nbsp;</li><ul><li>\(\langle\)&nbsp;\(S^* Tv, S^* T\) \(\rangle\) by definition of squared norm<br></li><li>\(\langle\)&nbsp;\(Tv, Tv\) \(\rangle\) since isometries and their adjoints preserver inner products</li><li>\(\langle\)&nbsp;\(T^* Tv, v\) \(\rangle\) by definition of adjoint</li><li>\(\langle\)&nbsp;{{c1::&nbsp;\(\sqrt{T^* Tv} \sqrt{T^* Tv}, v\)}}&nbsp;\(\rangle\) by definition of {{c2::square root}}</li><li>{{c4::||}} {{c3::\(\sqrt{T^* Tv}&nbsp;\)}} {{c4::||\(^2\)}}</li></ul><li>Which implies that&nbsp;\(R\) = {{c5::\(\sqrt{T^* Tv}&nbsp;\)}}</li></ul>

============================================================

  

    14. Suppose \(T \in \mathcal{L}(V)\) is self-adjoint, \(\lambda \in \mathbf{F}\), and \(\epsilon&gt;0\). Prove that if there exists \(v \in V\) such that \(\|v\|=1\) and<br><br>\[<br>\|T v-\lambda v\|&lt;\epsilon<br>\]<br><br>then \(T\) has an eigenvalue \(\lambda^{\prime}\) such that \(\left|\lambda-\lambda^{\prime}\right|&lt;\epsilon\).<br><div><br></div><div>Proof:</div><div><ul><li>Assume we have {{c4::a basis of eigenvectors}} since {{c5::T is self-adjoint}}</li><li>\(\epsilon^2\)&nbsp;\(\geq\) || \(Tv - \lambda v \) ||\(^2\)<br></li><li>=|| {{c3::\( \sum\)}}&nbsp; ({{c1::\(\lambda_i - \lambda\)}}) {{c2::\(\langle v,e_i\rangle e_i \)}}&nbsp; ||\(^2\)&nbsp; by {{c4::orthonormal basis}}</li><li>=&nbsp;{{c3::\(\sum\)}} | {{c1::\(\lambda_i - \lambda\)|\(^2\)}} | {{c2::\(\langle\)&nbsp;\(v,e_i\) \(\rangle\)}}|\(^2\) by {{c5::norm of vector expressed in terms of orthonormal basis&nbsp;}}</li><li>\(\geq\)&nbsp;\(\left(\min \left\{\left|\lambda_1-\lambda\right|^2, \ldots,\left|\lambda_n-\lambda\right|^2\right\}\right)\left(\left|\left\langle v, e_1\right\rangle\right|^2+\cdots+\left|\left\langle v, e_n\right\rangle\right|^2\right)\)<br></li><li>=&nbsp;\(\min \left\{\left|\lambda_1-\lambda\right|^2, \ldots,\left|\lambda_n-\lambda\right|^2\right\}\)</li></ul><div>Thus \(\epsilon&gt;\left|\lambda_j-\lambda\right|\) for some \(j\). In other words, there is an eigenvalue whose distance from \(\lambda\) is less than \(\epsilon\), as desired.</div></div>

============================================================

  

    14. Suppose \(T \in \mathcal{L}(V)\) is self-adjoint, \(\lambda \in \mathbf{F}\), and \(\epsilon&gt;0\). Prove that if there exists \(v \in V\) such that \(\|v\|=1\) and<br><br>\[<br>\|T v-\lambda v\|&lt;\epsilon<br>\]<br><br>then \(T\) has an eigenvalue \(\lambda^{\prime}\) such that \(\left|\lambda-\lambda^{\prime}\right|&lt;\epsilon\).<br><div><br></div><div>Proof:</div><div><ul><li>Assume we have a basis of eigenvectors since T is self-adjoint</li><li>\(\epsilon^2\)&nbsp;\(\geq\) || \(Tv - \lambda v \) ||\(^2\)<br></li><li>=|| \( \sum\)&nbsp; (\(\lambda_i - \lambda\)) \(\langle v,e_i\rangle e_i \)&nbsp; ||\(^2\)</li><li>=&nbsp;\(\sum\) |\(\lambda_i - \lambda\)|\(^2\) | \(\langle\)&nbsp;\(v,e_i\) \(\rangle\)|\(^2\)</li><li>\(\geq\){{c1::&nbsp;\(\left(\min \left\{\left|\lambda_1-\lambda\right|^2, \ldots,\left|\lambda_n-\lambda\right|^2\right\}\right)\left(\left|\left\langle v, e_1\right\rangle\right|^2+\cdots+\left|\left\langle v, e_n\right\rangle\right|^2\right)\)}} because {{c2::we could have picked the minimum as the coefficient}}<br></li><li>=&nbsp;{{c5::{{c3::\(\min \left\{\left|\lambda_1-\lambda\right|^2, \ldots,\left|\lambda_n-\lambda\right|^2\right\}\)}}}} because {{c5::{{c4::the rhs of the paren is constant so the minimum depends only on the lhs}}}}</li></ul><div>Thus \(\epsilon&gt;\left|\lambda_j-\lambda\right|\) for some \(j\). In other words, there is an eigenvalue whose distance from \(\lambda\) is less than \(\epsilon\), as desired.</div></div>

============================================================

  

    1. (6 points) Suppose that \(V\) is a complex inner product space with orthogonal basis \(\left(f_{1}, \ldots, f_{n}\right)\), and \(T \in \mathcal{L}(V)\).<br><br>a) Prove that any vector \(v \in V\) can be written<br><br>\[<br>v=\sum_{i=1}^{n} \frac{\left\langle v, f_{i}\right\rangle}{\left\langle f_{i}, f_{i}\right\rangle} f_{i}<br>\]<br><br>Proof:<br><ul><li>First, {{c1::take inner product with&nbsp;\(f_j\)}}&nbsp;and use the fact that {{c4::\(\langle\)}} {{c2::\(f_i, f_j\)}} {{c4::\(\rangle\)}} = {{c2::\(0\)}} for {{c2::\(i \neq j\).}}</li><ul><li>{{c4::\(\langle\)}} {{c1::\(v, f_j\)}} {{c4::\(\rangle\)}} =&nbsp;{{c3::\(a_j\)}} {{c4::\(\langle\)}}&nbsp;{{c2::\(f_j, f_j\)}} {{c4::\(\rangle\)}}<br></li></ul><li>Now, because {{c4::\(\langle\)}}&nbsp;{{c2::\(f_j, f_j\)}} {{c4::\(\rangle\)}} is {{c2::guaranteed to be nonzero}} due to {{c2::basis vector}} we can rearrange the equation into</li><ul><li>{{c3::\(a_j\)}} =&nbsp;{{c5::\(\frac{\left\langle v, f_j\right\rangle}{\left\langle f_j, f_j\right\rangle}\),}}<br></li></ul></ul>

============================================================

  

    1. (6 points) Suppose that \(V\) is a complex inner product space with orthogonal basis \(\left(f_{1}, \ldots, f_{n}\right)\), and \(T \in \mathcal{L}(V)\).<br><br>a) Prove that any vector \(v \in V\) can be written<br><br>\[<br>v=\sum_{i=1}^{n} \frac{\left\langle v, f_{i}\right\rangle}{\left\langle f_{i}, f_{i}\right\rangle} f_{i}<br>\]<br><br>b) Find a formula involving inner products for the \((i, j)\) entry \(a_{i j}\) of the matrix \(A\) of \(T\) in the basis \(\left(f_{1}, \ldots, f_{n}\right)\).<br><br>Solution:<br>The \(j\) th column of \(A\) is {{c3::\(T f_{j}\)}}, written in the basis \(\left(f_{i}\right)\). That is, the \(i j\) entry is the {{c4::coefficient of \(f_{i}\)}} in {{c5::the expansion of the vector \(T f_{j}\)}}. According to (a), this is<br><ul><li>{{c2::\(a_{i j}\)}}&nbsp;= {{c1::\(\frac{\left\langle T f_{j}, f_{i}\right\rangle}{\left\langle f_{i}, f_{i}\right\rangle}\)}}</li></ul>

============================================================

  

    1. (6 points) Suppose that \(V\) is a complex inner product space with orthogonal basis \(\left(f_{1}, \ldots, f_{n}\right)\), and \(T \in \mathcal{L}(V)\).<br><br>a) Prove that any vector \(v \in V\) can be written<br><br>\[<br>v=\sum_{i=1}^{n} \frac{\left\langle v, f_{i}\right\rangle}{\left\langle f_{i}, f_{i}\right\rangle} f_{i}<br>\]<br>b) Find a formula involving inner products for the \((i, j)\) entry \(a_{i j}\) of the matrix \(A\) of \(T\) in the basis \(\left(f_{1}, \ldots, f_{n}\right)\).<br><br>The \(j\) th column of \(A\) is \(T f_{j}\), written in the basis \(\left(f_{i}\right)\). That is, the \(i j\) entry is the coefficient of \(f_{i}\) in the expansion of the vector \(T f_{j}\). According to (a), this is<br><br>\[<br>a_{i j}=\frac{\left\langle T f_{j}, f_{i}\right\rangle}{\left\langle f_{i}, f_{i}\right\rangle}<br>\]<br><br>c) Find a formula involving inner products for the \((i, j)\) entry \(b_{i j}\) of the matrix \(B\) of \(T^{*}\) in the basis \(\left(f_{1}, \ldots, f_{n}\right)\).<br><br>According to (b), the answer is<br><br><ul><li>\(b_{i j}\) =&nbsp; {{c1::\(\frac{\left\langle T^{*} f_{j}, f_{i}\right\rangle}{\left\langle f_{i}, f_{i}\right\rangle}\)}}</li></ul><div>However we can expand upon it:</div><div><ul><li>{{c1::\(\frac{\left\langle T^* f_j, f_i\right\rangle}{\left\langle f_i, f_i\right\rangle}\)}} =&nbsp;<br></li><li>= {{c2::\(\frac{\left\langle f_j, T f_i\right\rangle}{\left\langle f_i, f_i\right\rangle}\)}} by {{c2::definition of&nbsp;\(T^*\)}}<br></li><li>= \(\frac{\overline{\left\langle T f_i, f_j\right\rangle}}{\left\langle f_i, f_i\right\rangle}\) by {{c3::conjugate symmetry of inner prod}}<br></li><li>=&nbsp;{{c4::\(\overline{a_{j i} }\)}} {{c5::\(\frac{\left\langle f_j, f_j\right\rangle}{\left\langle f_i, f_i\right\rangle}\)}} by {{c5::breaking apart the fraction}}</li></ul></div>

============================================================

  

    19 Suppose \(V\) is a real inner product space. Prove that<br>\[<br>\langle u, v\rangle=\frac{\|u+v\|^2-\|u-v\|^2}{4}<br>\]<br>for all \(u, v \in V\).<br><br>What is the trick?<br><ul><li>Expand until you get ({{c3::\( 2\)}} {{c5::\(\langle\)}} {{c1::u,v}} {{c5::\(\rangle\)}} + {{c3::\(2\)}} {{c5::\(\langle\)}} {{c2::v, u}} {{c5::\(\rangle\)}} )&nbsp;\(/ 4\)&nbsp;</li><li>And then use the fact that {{c4::the vector space is real}} to conclude that&nbsp;{{c5::\(\langle\)}}&nbsp;{{c1::\(v,u\)}} {{c5::\(\rangle\)}}&nbsp; = {{c5::\(\langle\)}}{{c2::&nbsp;\(u,v\)}} {{c5::\(\rangle\)}}&nbsp;</li></ul>

============================================================

  

    10 Suppose \(V\) is a real inner product space and \(v_1, \ldots, v_m\) is a linearly independent list of vectors in \(V\). Prove that there exist exactly \(2^m\) orthonormal lists \(e_1, \ldots, e_m\) of vectors in \(V\) such that<br>\[<br>\operatorname{span}\left(v_1, \ldots, v_j\right)=\operatorname{span}\left(e_1, \ldots, e_j\right)<br>\]<br>for all \(j \in\{1, \ldots, m\}\).<br><br>Motivation:<br><ul><li>Apply the&nbsp;{{c1::Gram-Schmidt Procedure}}</li><li>{{c5::At every step we are free}} to&nbsp;&nbsp;{{c2::choose the sign of the vector}} while maintaining {{c3::orthonormality}} and the property that&nbsp;</li><ul><li>{{c4::\(\operatorname{span}\left(v_1, \ldots, v_j\right)\)}} = {{c4::\(\operatorname{span}\left(e_1, \ldots, e_j\right)\) for all \(j \in\{1, \ldots, m\}\)}}<br></li></ul><li>Because {{c2::we have m such vectors}}, there are&nbsp;\(2^m\) such {{c2::choices}}</li></ul>

============================================================

  

    For a {{c4::selfadjoint}} operator, the {{c2::eigenvectors}} for {{c5::{{c3::distinct eigenvalues}}}} must be {{c5::{{c1::orthogonal}}}}

============================================================

  

    5. (4 points) a) Find a real number \(a\) so that there exists a selfadjoint operator \(T \in \mathcal{L}\left(\mathbb{R}^{3}\right)\) with the properties<br><br>\[<br>T(1,2,3)=(0,0,0), \quad T(2,5, a)=(2,5, a) .<br>\]<br><br>Solution:<br><ul><li>Because self-adjoint operators must&nbsp;have {{c1::orthogonal eigenvectors}}</li><li>{{c3::0}} = \(\langle\) {{c2::(1,2,3), (2,5,a)}} \(\rangle\)&nbsp;<br></li><li>= {{c4::2 +10 + 3a}}</li><li>= {{c4::3(4+a)}}</li></ul><div>Thus a = {{c5::-4}}</div>

============================================================

  

    5. (4 points) a) Find a real number \(a\) so that there exists a selfadjoint operator \(T \in \mathcal{L}\left(\mathbb{R}^{3}\right)\) with the properties<br><br>\[<br>T(1,2,3)=(0,0,0), \quad T(2,5, a)=(2,5, a) .<br>\]<br><br><br>b) Assume \(T\) has an eigenvalue which is neither 0 nor 1 . Then find an eigenvector of \(T\) that is not in \(\operatorname{Span}((1,2,3),(2,5, a))\).<br><br><br>Solution:<br><ul><li>Because {{c5::the two vectors we have}} are {{c5::eigenvectors}}</li><li>The space they span is a {{c4::2-dimensional invariant subspace}}</li><li>We can {{c1::append any vector}} {{c2::not in their subspace}} and {{c3::apply gram schmidt to the list}}</li></ul>

============================================================

  

    30. Suppose \(T \in \mathcal{L}(V, W)\). Prove that<br><br>(a) \(T\) is injective if and only if \(T^{*}\) is surjective;<br><br><ul><li>\(T\) is injective \(\Longleftrightarrow\) {{c1::null \(T\)}}={{c2::\(\{0\}\)}}<br></li><li>\(\Longleftrightarrow\){{c3::&nbsp;\(\left(\text { range } T^*\right)^{\perp}\)}} = {{c2::\(\{0\}\)}}<br></li><li>\(\Longleftrightarrow\)&nbsp;{{c4::\(\operatorname{range} T^*\)}} = {{c5::\(W\)}}<br></li><li>\(\Longleftrightarrow\)&nbsp;\(T^*\) is surjective<br></li></ul>

============================================================

  

    Characterization of positive operators<br><br>Let \(T \in \mathcal{L}(V)\). Then the following are equivalent:<br>(a) \(T\) is {{c1::positive;}}<br>(b) \(T\) is {{c2::self-adjoint}} and {{c2::all eigenvalues of \(T\) are nonnegative;}}<br>(c) \(T\) has a {{c3::positive square root;}}<br>(d) \(T\) has a {{c4::self-adjoint square root;}}<br>(e) there {{c5::exists \(R \in \mathcal{L}(V)\)}} such that \(T\) ={{c5::\(R^* R\).}}

============================================================

  

    <img src="paste-5ec97e519618cd1e846e68362ee90e30ee71e671.jpg"><br>That (e) implies (f) is a special case of the result that if {{c5::\(S, T\)}} \(\in\) {{c1::\(\mathcal{L}(V)\)}} and {{c2::\(T S\)}} = {{c3::\(I\)}} implies {{c4::\(S T\)}} = {{c3::\(I\).}}

============================================================

  

    Polar decomposition of a complex number:<br><ul><li>z = {{c1::|z|}} (&nbsp;{{c2::\(\cos \theta\)}} + {{c3::i&nbsp;\(\sin \theta\)}})</li><li>={{c4::&nbsp;\(\left(\frac{z}{|z|}\right)|z|\)}}</li><li>= {{c5::\(\left(\frac{z}{|z|}\right) \sqrt{\bar{z} z}\)}} by definition of {{c5::norm for complex numbers}}&nbsp;<br></li><li>with&nbsp;{{c5::\(\left|\frac{z}{|z|}\right|\)}} = {{c5::1}}&nbsp;</li></ul>

============================================================

  

    Analogy between Real/complex numbers and operators:<br><ul><li>\(z\)&nbsp;{{c1::\(\in\) C}} = {{c2::T}}<br></li><li>{{c2::\(\bar{z}\)}} =&nbsp;{{c1::\(T^*\)}}<br></li><li>\(z\) {{c5::{{c3::\(\in\) \(R\)}}}} = T is {{c5::{{c4::self-adjoint}}}}<br></li><li>z&nbsp;{{c5::{{c4::\(\geq\) 0}}}} = {{c5::{{c3::T is positive}}}}</li></ul>

============================================================

  

    Analogy between Real/complex numbers and operators:<br><ul><li>\(z\)&nbsp;\(\in\) C = T<br></li><li>\(\bar{z}\) =&nbsp;\(T^*\)<br></li><li>\(z \in R\) = T is self-adjoint<br></li><li>z&nbsp;\(\geq\) 0 = T is positive</li><li>(\(z\) = {{c1::\(\bar{w} w\)}}) = (T =&nbsp;{{c2::\(R^* R\)}})<br></li><li>{{c2::(&nbsp;\(|z|=1(\bar{z} z=1)\))}}&nbsp; = {{c1::T is an isometry (\(T^*T = I\)}}</li><li>{{c5::{{c3::(\(z=\left(\frac{z}{|z|}\right) \sqrt{\bar{z} z}\))}}}} =&nbsp;{{c5::{{c4::\(T=S \sqrt{T^* T}\) for some isometry \(S\)}}}}</li></ul>

============================================================

  

    {{c3::Polar}} Decomposition<br>Suppose \(T \in \mathcal{L}(V)\). Then there exists an {{c2::isometry \(S \in \mathcal{L}(V)\)}} such that<br><ul><li>{{c5::{{c4::\(T\)}}}} = {{c2::\(S\)}} {{c5::{{c1::\(\sqrt{T^* T} .\)}}}}</li></ul>

============================================================

  

    <img src="paste-ddc1d44cb2a70885dac2caf27b8c6caa23806516.jpg"><br>Why do we understand both operators in the polar decomposition pretty well?<br><ul><li>The second operator&nbsp;\(\sqrt{T^* T}\). is a {{c1::positive}} opeartor thus it is {{c2::self-adjoint}} and {{c2::normal}} meaning that it is also {{c3::diagonalizable}} i.e {{c3::a basis of orthonormal eigenvectors exists for it}}</li><li>The first is an isometry, so {{c4::over C}} since all isometries are {{c2::normal}} then S is also {{c3::diagonalizable}} by {{c3::the spectral theorem}}</li><li>Warning: the {{c3::orthonormal eigenbasse}} of the two operators {{c5::are&nbsp; not guaranteed to be the same.}}</li></ul>

============================================================

  

    <img src="paste-04794dcf08d9c1a5d0326368219a8b4c918f42ad.jpg"><br>Proof:<br><ul><li>\(\|T v\|^2\) =&nbsp;\(\langle\)&nbsp;{{c1::\(Tv, Tv\)}}\(\rangle\) by {{c1::norm}} definition<br></li><li>=&nbsp;&nbsp;\(\langle\)&nbsp;{{c2::\(T^*Tv,v\)}}\(\rangle\) by {{c2::adjoint}} definition</li><li>=&nbsp;\(\langle\){{c3::&nbsp;\(\sqrt{T^* T}\sqrt{T^* T}v, v\)}}\(\rangle\) since&nbsp;{{c3::\(T^*T\)}} is {{c4::positive}} and thus has a {{c5::unique positive square root&nbsp;}}</li><li>=&nbsp;&nbsp;\(\langle\)&nbsp;\(\sqrt{T^* T}v, \sqrt{T^* T}v\)\(\rangle\) since positive operators are self-adjoint&nbsp;</li><li>=&nbsp;\(\left\|\sqrt{T^* T} v\right\|^2\) by norm definition</li><li>Thus&nbsp;\(\|T v\|\) = \(\left\|\sqrt{T^* T} v\right\|^2\)</li></ul>

============================================================

  

    <img src="paste-04794dcf08d9c1a5d0326368219a8b4c918f42ad.jpg"><br><span style="color: rgb(0, 0, 0);">Proof:</span><br><ul><li>\(\|T v\|^2\) =&nbsp;\(\langle\)&nbsp;{{c1::\(Tv, Tv\)}}\(\rangle\) by {{c1::norm}} definition<br></li><li>=&nbsp;&nbsp;\(\langle\)&nbsp;{{c2::\(T^*Tv,v\)}}\(\rangle\) by {{c2::adjoint}} definition</li><li>=&nbsp;\(\langle\){{c3::&nbsp;\(\sqrt{T^* T}\sqrt{T^* T}v, v\)}}\(\rangle\) since&nbsp;{{c3::\(T^*T\)}} is {{c4::positive}} and thus has a {{c5::unique positive square root&nbsp;}}</li><li>=&nbsp;&nbsp;\(\langle\)&nbsp;\(\sqrt{T^* T}v, \sqrt{T^* T}v\)\(\rangle\) since positive operators are self-adjoint&nbsp;</li><li>=&nbsp;\(\left\|\sqrt{T^* T} v\right\|^2\) by norm definition</li><li>Thus&nbsp;\(\|T v\|\) = \(\left\|\sqrt{T^* T} v\right\|^2\)</li></ul>

============================================================

  

    <img src="paste-04794dcf08d9c1a5d0326368219a8b4c918f42ad.jpg"><br>Proof:<br><ul><li>\(\|T v\|^2\) =&nbsp;\(\langle\)&nbsp;\(Tv, Tv\)\(\rangle\) by norm definition<br></li><li>=&nbsp;&nbsp;\(\langle\)&nbsp;\(T^*Tv,v\)\(\rangle\) by adjoint definition</li><li>=&nbsp;\(\langle\)&nbsp;\(\sqrt{T^* T}\sqrt{T^* T}v, v\)\(\rangle\) since&nbsp;\(T^*T\) is positive and thus has a unique positive square root&nbsp;</li><li>=&nbsp;&nbsp;\(\langle\){{c1::&nbsp;\(\sqrt{T^* T}v, \sqrt{T^* T}v\)}}\(\rangle\) since positive operators are {{c2::self-adjoint&nbsp;}}</li><li>={{c3::&nbsp;\(\left\|\sqrt{T^* T} v\right\|^2\)}} by {{c2::norm}} definition</li><li>Thus&nbsp;{{c5::\(\|T v\|\)}} = {{c4::\(\left\|\sqrt{T^* T} v\right\|\)}}</li></ul>

============================================================

  

    <br><img src="paste-04794dcf08d9c1a5d0326368219a8b4c918f42ad.jpg"><br>Proof:<br><ul><li>\(\|T v\|^2\) =&nbsp;\(\langle\)&nbsp;\(Tv, Tv\)\(\rangle\) by norm definition<br></li><li>=&nbsp;&nbsp;\(\langle\)&nbsp;\(T^*Tv,v\)\(\rangle\) by adjoint definition</li><li>=&nbsp;\(\langle\)&nbsp;\(\sqrt{T^* T}\sqrt{T^* T}v, v\)\(\rangle\) since&nbsp;\(T^*T\) is positive and thus has a unique positive square root&nbsp;</li><li>=&nbsp;&nbsp;\(\langle\)&nbsp;\(\sqrt{T^* T}v, \sqrt{T^* T}v\)\(\rangle\) since positive operators are self-adjoint&nbsp;</li><li>=&nbsp;\(\left\|\sqrt{T^* T} v\right\|^2\) by norm definition</li><li>Thus&nbsp;\(\|T v\|\) = \(\left\|\sqrt{T^* T} v\right\|^2\)</li></ul><div>Now that we have this we can:</div><div><ul><li>Define a linear map&nbsp;\(S_1:\)&nbsp;\(range \sqrt{T^*T}\)&nbsp;\(\to\)&nbsp;\(range T\) by:</li><ul><li>\(S_1\left(\sqrt{T^* T} v\right)\) = \(T v\)<br></li></ul><li>The equation&nbsp;&nbsp;\(\|T v\|\) = \(\left\|\sqrt{T^* T} v\right\|^2\) shows that:</li><ul><li>\(\left\|S_1 u\right\|\) = \(\|u\|\)<br></li></ul><li>for all u&nbsp;\(\in\)&nbsp;range \(\sqrt{T^* T}\).</li></ul><div>If the entire domain is {{c1::range \(\sqrt{T^* T}\)}} then we are done as we can just {{c2::choose our issometry S =&nbsp;\(S_1\)}} and everything follows by definition</div></div><div>Otherwise:</div><div><ul><li>Extend&nbsp;\(S_1\) to an isometry S&nbsp;\(\in\)&nbsp;\(L(V)\)</li><li>Do this by defining \(S\) to be an isometry from {{c3::(range \(\left.\sqrt{T^* T}\right)^{\perp}\)}} to {{c4::(range \(\left.T\right)^{\perp}\)}}, then {{c5::extend}} by {{c5::linearity}}.</li></ul></div>

============================================================

  

    The {{c4::singular}} values of T are<br>{{c1::nonnegative}} numbers, because<br>they are {{c2::eigenvalues}} of the {{c3::positive}}<br>operator&nbsp;{{c5::\(\sqrt{T^*T}\)}}

============================================================

  

    <img src="paste-6594a741e8f9def1ba601b6671b48342832c0e5e.jpg"><br>Proof:<br><ul><li>By the Spectral Theorem, there is an orthonormal basis \(e_1, \ldots, e_n\) of \(V\) such that:</li><ul><li>{{c1::&nbsp;\(\sqrt{T^* T} e_j\)}} = {{c2::\(s_j e_j\)}} for \(j=1, \ldots, n\).</li><li>the {{c2::associated eigenvalues}} of this {{c2::are precisely the singular values}} by {{c2::definitiojn}}</li></ul><li>Write&nbsp;\(v\) =&nbsp;\(\sum\)&nbsp;\(\langle\)\(v,e_i\)&nbsp;\(\rangle\) \(e_i\)</li><li>Apply {{c1::\(\sqrt{T^* T}\)}} to both sides of this equation, getting<br></li><ul><li>{{c1::\(\sqrt{T^* T}v\)}} = \(\sum\)&nbsp;\(\langle\){{c3::\(v,e_i\)}}&nbsp;\(\rangle\) {{c1::\(\sqrt{T^* T}\)}}{{c3::\(e_i\)}} by {{c4::linearity}}<br></li><li>= \(\sum\){{c5::&nbsp;\(s_i\)\(\langle\)\(v,e_i\)&nbsp;\(\rangle\)&nbsp;\(e_i\)}} by {{c5::definition}}</li></ul><li>By the polar decomposition there exists an isometry S such that&nbsp;\(T\) =&nbsp;\(S \sqrt{T^* T}\).</li><li>Apply S to both sides of the equation above to get</li><ul><li>\(T\) = \(\sum\)&nbsp;\(s_i\)\(\langle\)\(v,e_i\)&nbsp;\(\rangle\)&nbsp;\(S\)\(e_i\)&nbsp;<br></li></ul><li>For each j let&nbsp;\(f_i\) =&nbsp;\(S e_i\), this is an orthonormal basis since S is an isometry</li><li>The equation now becomes</li><ul><li>\(T\) = \(\sum\)&nbsp;\(s_i\)\(\langle\)\(v,e_i\)&nbsp;\(\rangle\)&nbsp;\(f_i\)</li></ul></ul>

============================================================

  

    <img src="paste-6594a741e8f9def1ba601b6671b48342832c0e5e.jpg"><br>Proof:<br><ul><li>By the Spectral Theorem, there is an orthonormal basis \(e_1, \ldots, e_n\) of \(V\) such that:</li><ul><li>&nbsp;\(\sqrt{T^* T} e_j\) = \(s_j e_j\) for \(j=1, \ldots, n\).</li><li>the associated eigenvalues of this are precisely the singular values by definitiojn</li></ul><li>Write&nbsp;\(v\) =&nbsp;\(\sum\)&nbsp;\(\langle\)\(v,e_i\)&nbsp;\(\rangle\) \(e_i\)</li><li>Apply \(\sqrt{T^* T}\) to both sides of this equation, getting<br></li><ul><li>\(\sqrt{T^* T}v\) = \(\sum\)&nbsp;\(\langle\)\(v,e_i\)&nbsp;\(\rangle\) \(\sqrt{T^* T}\)\(e_i\) by linearity<br></li><li>= \(\sum\)&nbsp;\(s_i\)\(\langle\)\(v,e_i\)&nbsp;\(\rangle\)&nbsp;\(e_i\) by definition</li></ul><li>By the {{c5::polar decomposition}} there exists an {{c5::isometry}} S such that&nbsp;{{c5::\(T\)}} =&nbsp;{{c5::\(S \sqrt{T^* T}\)}}.</li><li>Apply S to both sides of the equation above to get</li><ul><li>\(T\) = \(\sum\)&nbsp;{{c1::\(s_i\)\(\langle\)\(v,e_i\)&nbsp;\(\rangle\)&nbsp;\(S\)\(e_i\)&nbsp;}}<br></li></ul><li>For each j let&nbsp;\(f_i\) =&nbsp;{{c2::\(S e_i\)}}, this is an orthonormal basis since {{c3::S is an isometry}}</li><li>The equation now becomes</li><ul><li>\(T\) = {{c4::\(\sum\)&nbsp;\(s_i\)\(\langle\)\(v,e_i\)&nbsp;\(\rangle\)&nbsp;\(f_i\)}}</li></ul></ul>

============================================================

  

    Suppose \(T \in \mathcal{L}(V)\) is {{c4::self-adjoint}}. Prove that {{c5::the singular values of \(T\)}} equal the {{c1::absolute values}} {{c2::of the eigenvalues of \(T\)}}, {{c3::repeated appropriately}}.

============================================================

  

    Suppose \(T \in \mathcal{L}(V)\). Prove that {{c5::{{c1::\(T\)}}}} and {{c4::{{c2::\(T^*\)}}}} have the same {{c4::{{c3::singular}}}} values.

============================================================

  

    Suppose \(T \in \mathcal{L}(V)\). Prove that \(T\) is {{c1::invertible}} if and only if {{c2::0}} is {{c5::{{c3::not}}}} a {{c5::{{c4::singular value}}}} of \(T\).

============================================================

  

    Suppose \(T \in \mathcal{L}(V)\). Prove that {{c5::dim}} {{c1::range \(T\)}} equals the {{c4::number of}} {{c2::nonzero}} {{c3::singular values}} of \(T\).

============================================================

  

    Suppose \(T \in \mathcal{L}(V)\). Prove that dim range \(T\) equals the number of nonzero singular values of \(T\).<br><br>Why is this result meaningful?<br><ul><li>{{c5::Gaussian Elimination}} can effectively find the dim range T if the data is {{c3::not noisy}}, however since it relies on {{c4::deciding if entries are 0 or not}} it has issues with {{c5::imprecise measurement}}</li><li>Thus the best way to find the dimension of the range is to find the singular values of T and see if {{c1::some of them look out of scale with the others}} since they are {{c2::less susceptible}} to {{c2::noise}}</li></ul>

============================================================

  

    <ul><li>If one has a singular value decomposition of a linear map it may make sense to {{c1::throw away}} terms associated with {{c2::very low}} singular values as they may be {{c5::{{c3::errors}}}} or {{c5::{{c4::unimportant}}}}</li><li><br></li></ul>

============================================================

  

    if you need to {{c3::compress}} a matrix the {{c4::SVD}} allows you to {{c5::{{c1::throw away}}}} the {{c5::{{c2::least important}}}} terms when doing the {{c3::compression}}

============================================================

  

    Suppose \(S \in \mathcal{L}(V)\). Prove that \(S\) is an {{c5::{{c1::isometry}}}} if and only if all the {{c4::{{c3::singular values}}}} of \(S\) equal {{c4::{{c2::1}}}} .

============================================================

  

    Applying the {{c5::{{c1::adjoint}}}} to an SVD merely {{c4::{{c2::interchanges the orthonormal bases}}}} since it is equivalent to {{c4::{{c3::taking the transpose}}}}

============================================================

  

    Sequence of {{c4::increasing null spaces}}<br><br>Suppose \(T \in \mathcal{L}(V)\). Then<br><br><ul><li>{{c5::\(\{0\}\)}} = {{c1::\(\operatorname{null} T^{0}\)}} {{c3::\(\subset\)}} {{c1::\(\text { null } T^{1}\)}} {{c3::\(\subset\)}} \(\cdots\) {{c3::\(\subset\)}} {{c2::\(\text { null } T^{k}\)}} {{c3::\(\subset\)}} {{c2::\(\text { null } T^{k+1}\)}} {{c3::\(\subset\)}} \(\cdots .\)</li></ul>

============================================================

  

    <img src="paste-b8c98635f068d0821371de190b579e04dea816cb.jpg"><br>Proof:<br><ul><li>Proof Suppose \(k\) is a nonnegative integer and \(v \in \operatorname{null} T^{k}\).&nbsp;</li><li>Then {{c1::\(T^{k} v\)}} = {{c2::\(0\)}}, and hence&nbsp;</li><ul><li>{{c3::\(T^{k+1} v\)}} = {{c1::\(T\left(T^{k} v\right) \)}}= {{c3::\(T(0)\)}}= {{c2::\(0\)}}.&nbsp;</li></ul><li>Thus \(v \in\) {{c4::\(\operatorname{null} T^{k+1}\).}}&nbsp;</li><li>Hence null \(T^{k} \subset\) {{c5::null \(T^{k+1}\)}}, as desired.<br></li></ul>

============================================================

  

    <img src="paste-b8c98635f068d0821371de190b579e04dea816cb.jpg"><br><span style="color: rgb(0, 0, 0);">Proof:</span><br><ul><li>Proof Suppose \(k\) is a nonnegative integer and \(v \in \operatorname{null} T^{k}\).&nbsp;</li><li>Then {{c1::\(T^{k} v\)}} = {{c2::\(0\)}}, and hence&nbsp;</li><ul><li>{{c3::\(T^{k+1} v\)}} = {{c1::\(T\left(T^{k} v\right) \)}}= {{c3::\(T(0)\)}}= {{c2::\(0\)}}.&nbsp;</li></ul><li>Thus \(v \in\) {{c4::\(\operatorname{null} T^{k+1}\).}}&nbsp;</li><li>Hence null \(T^{k} \subset\) {{c5::null \(T^{k+1}\)}}, as desired.</li></ul>

============================================================

  

    {{c5::Equality}} in the {{c4::sequence of null spaces<br>}}<br>Suppose \(T \in \mathcal{L}(V)\). Suppose \(m\) is a nonnegative integer such that {{c1::null \(T^{m}\)}} {{c5::=}} {{c2::\(\operatorname{null} T^{m+1}\)}}. Then<br><br><ul><li>{{c3::\(\operatorname{null} T^{m}\)}} {{c5::=}} {{c4::\(\operatorname{null} T^{m+1}\)}} {{c5::=}} {{c4::\(\operatorname{null} T^{m+2}\)}} {{c5::=}} {{c4::\(\operatorname{null} T^{m+3}\)}} {{c5::=}} \(\cdots .\)</li></ul>

============================================================

  

    Null spaces {{c1::stop growing}}<br><br>Suppose \(T \in \mathcal{L}(V)\). Let \(n\) = {{c2::\(\operatorname{dim} V\)}}. Then<br><br><ul><li>{{c5::{{c3::\(\operatorname{null} T^{n}\)}}}} {{c1::=}} {{c5::{{c4::\(\operatorname{null} T^{n+1}\)}}}} {{c1::=}} {{c5::{{c4::\(\operatorname{null} T^{n+2}\)}}}} {{c1::=}} \(\cdots .\)</li></ul>

============================================================

  

    Why is this true?<br><img src="paste-40474fac57ce81a59681cd605a5adf81d42817d1.jpg"><br>Because:<br><ul><li>Null spaces of {{c5::increasing powers of an operator}} are {{c1::contained within the null space of the larger power}}</li><li>Null spaces are {{c4::subspaces}}&nbsp;</li><li>A {{c4::subspaces}} cannot have {{c2::dimension larger than the vector space}} so {{c3::the growth must stop}}</li></ul>

============================================================

  

    {{c5::\(V\)}} is {{c2::the direct sum}} {{c3::of null \(T^{\operatorname{dim} V}\)}} {{c4::and \(\operatorname{range} T^{\operatorname{dim} V}\)}}<br><br>Suppose \(T \in \mathcal{L}(V)\). Let \(n\) = {{c1::\(\operatorname{dim} V\)}}. Then<br><br><ul><li>{{c5::\(V\)}} = {{c3::\(\operatorname{null} T^{n}\)}} {{c2::\(\oplus\)}} {{c4::\(\text { range } T^{n} .\)}}</li></ul>

============================================================

  

    Definition {{c1::generalized eigenvector}}<br><br>Suppose \(T \in \mathcal{L}(V)\) and {{c5::\(\lambda\)}} is {{c5::an eigenvalue of \(T\)}}. A vector \(v \in V\) is called a {{c1::generalized eigenvector}} of \(T\) {{c5::corresponding to \(\lambda\)}} if \(v\) {{c1::\(\neq\) \(0\)}}&nbsp;and<br><br><ul><li>{{c2::\((T-\lambda I)^{j}\)}} \(v\) = {{c3::\(0\)}}</li></ul><br>for {{c4::some positive integer \(j\).}}<br>

============================================================

  

    <ul><li>We do not define the concept of a generalized {{c1::eigenvalue}}, because this would {{c5::not lead to anything new.&nbsp;}}</li><li>Reason: if {{c3::\((T-\lambda I)^j\)}} is {{c2::not injective}} for some positive integer \(j\), then {{c4::\(T-\lambda I\)}} is {{c2::not injective}}, and hence \(\lambda\) is an {{c1::eigenvalue}} of \(T\).</li></ul>

============================================================

  

    Definition {{c1::generalized eigenspace}}, {{c2::\(G(\lambda, T)\)}}<br><br>Suppose \(T \in \mathcal{L}(V)\) and {{c5::\(\lambda \in \mathbf{F}\)}}. The {{c1::generalized eigenspace}} of \(T\) {{c5::corresponding to \(\lambda\)}}, denoted {{c2::\(G(\lambda, T)\)}}, is defined to be the {{c3::set of all generalized eigenvectors of \(T\)}} {{c5::corresponding to \(\lambda\)}}, along with {{c4::the 0 vector.}}

============================================================

  

    Because every {{c1::eigenvector}} of \(T\) is a {{c2::generalized eigenvector}} of \(T\) ({{c2::take \(j=1\) in the definition of generalized eigenvector}}), each {{c3::eigenspace}} is {{c4::contained}} in the corresponding {{c5::generalized eigenspace}}. In other words, if \(T \in \mathcal{L}(V)\) and \(\lambda \in \mathbf{F}\), then<br><br><ul><li>{{c3::\(E(\lambda, T)\)}} {{c4::\(\subset\)}} {{c5::\(G(\lambda, T) .\)}}</li></ul>

============================================================

  

    Description of {{c1::generalized eigenspaces}}<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(\lambda \in \mathbf{F}\). Then {{c1::\(G(\lambda, T)\)}} = {{c4::\(\operatorname{null}\)}} {{c5::{{c2::\((T-\lambda I)\)}}}} {{c5::{{c3::\(^{\operatorname{dim} V}\).}}}}

============================================================

  

    {{c1::Linearly independent}} {{c2::generalized eigenvectors}}<br><br>Let \(T \in \mathcal{L}(V)\). Suppose {{c4::\(\lambda_{1}, \ldots, \lambda_{m}\)}} are {{c5::distinct eigenvalues of \(T\)}} and {{c3::\(v_{1}, \ldots, v_{m}\)}} are {{c5::corresponding}} {{c2::generalized eigenvectors}}. Then {{c3::\(v_{1}, \ldots, v_{m}\)}} is {{c1::linearly independent}}.

============================================================

  

    <img src="paste-e9899024693e7c56ae0e87d2ec9f8b6081e05ae0.jpg"><br>Proof Suppose \(a_{1}, \ldots, a_{m}\) are complex numbers such that<br><br>8.14<br><br>\[<br>0=a_{1} v_{1}+\cdots+a_{m} v_{m} .<br>\]<br><br><ul><li>Let \(k\) be the {{c4::largest nonnegative integer}} such that:</li><ul><li>{{c1::&nbsp;\(\left(T-\lambda_{1} I\right)^{k}\)}} \(v_{1}\)&nbsp; {{c3::\(\neq\)}} {{c2::\(0\).&nbsp;}}</li></ul><li>Let \(w\) = {{c1::\(\left(T-\lambda_{1} I\right)^{k}\)}} \(v_{1} .\)</li></ul><br>Thus<br><br><ul><li>{{c5::\(\left(T-\lambda_{1} I\right)\)}} \(w\) = {{c4::\(\left(T-\lambda_{1} I\right)^{k+1}\)}} \(w\) = {{c2::\(0,\)}}</li></ul><br>and hence \(T w\) = \(\lambda_{1} w\). Thus \((T-\lambda I) w=\left(\lambda_{1}-\lambda\right) w\) for every \(\lambda \in \mathbf{F}\) and hence<br><br><ul><li>\((T-\lambda I)^{n} w=\left(\lambda_{1}-\lambda\right)^{n} w\)</li></ul><br>for every \(\lambda \in \mathbf{F}\), where \(n=\operatorname{dim} V\).<br><br>Apply the operator<br><br>\(\left(T-\lambda_{1} I\right)^{k}\) \(\left(T-\lambda_{2} I\right)^{n}\) \(\cdots\) \(\left(T-\lambda_{m} I\right)^{n}\)<br><br>to both sides of 8.14 , getting<br><br><ul><li>0 = \(a_1\left(T-\lambda_1 I\right)^k\left(T-\lambda_2 I\right)^n \cdots\left(T-\lambda_m I\right)^n v_1\)</li><li>=&nbsp;\(a_1\left(T-\lambda_2 I\right)^n \cdots\left(T-\lambda_m I\right)^n w\)</li><li>&nbsp;=&nbsp;\(a_1\left(\lambda_1-\lambda_2\right)^n \cdots\left(\lambda_1-\lambda_m\right)^n w\),</li></ul>where we have used 8.11 to get the first equation above and 8.15 to get the last equation above.<br><br>The equation above implies that \(a_{1}=0\). In a similar fashion, \(a_{j}=0\) for each \(j\), which implies that \(v_{1}, \ldots, v_{m}\) is linearly independent.<br>

============================================================

  

    <img src="paste-e9899024693e7c56ae0e87d2ec9f8b6081e05ae0.jpg"><br>Proof Suppose \(a_{1}, \ldots, a_{m}\) are complex numbers such that<br><br>8.14<br><br>\[<br>0=a_{1} v_{1}+\cdots+a_{m} v_{m} .<br>\]<br><br><ul><li>Let \(k\) be the largest nonnegative integer such that:</li><ul><li>&nbsp;\(\left(T-\lambda_{1} I\right)^{k} v_{1}\)&nbsp; \(\neq\) \(0\).&nbsp;</li></ul><li>Let \(w\) = \(\left(T-\lambda_{1} I\right)^{k} v_{1} .\)</li></ul><br>Thus<br><br><ul><li>\(\left(T-\lambda_{1} I\right) w\) = \(\left(T-\lambda_{1} I\right)^{k+1} w\) = \(0,\)</li></ul><br>Hence:<br><ul><li>\(T w\) = \(\lambda_{1} w\).&nbsp;</li><li>Thus:</li><ul><li>&nbsp;\((T-\lambda I)\) \(w\) = \(\left(\lambda_{1}-\lambda\right) w\) for every \(\lambda \in \mathbf{F}\)&nbsp;</li></ul><li>&nbsp;hence</li><ul><li>\((T-\lambda I)^{n} w=\left(\lambda_{1}-\lambda\right)^{n} w\)</li><li>for every \(\lambda \in \mathbf{F}\), where \(n=\operatorname{dim} V\).</li></ul></ul><br>Apply the operator<br><ul><li>{{c1::\(\left(T-\lambda_{1} I\right)^{k}\)}} {{c1::\(\left(T-\lambda_{2} I\right)^{n}\)}} \(\cdots\) {{c1::\(\left(T-\lambda_{m} I\right)^{n}\)}}</li></ul><br>to both sides of 8.14 , getting<br><br><ul><li>0 = {{c2::\(a_1\)}}{{c1::\(\left(T-\lambda_1 I\right)^k\left(T-\lambda_2 I\right)^n \cdots\left(T-\lambda_m I\right)^n\)}} \(v_1\)</li><li>=&nbsp;{{c2::\(a_1\)}} {{c3::\(\left(T-\lambda_2 I\right)^n \cdots\left(T-\lambda_m I\right)^n\)}} {{c4::\(w\)}}</li><li>&nbsp;=&nbsp;{{c2::\(a_1\)}} {{c5::\(\left(\lambda_1-\lambda_2\right)^n \cdots\left(\lambda_1-\lambda_m\right)^n\)}} {{c4::\(w\),}}</li></ul>where we have used 8.11 to get the first equation above and 8.15 to get the last equation above.<br><br>The equation above implies that {{c2::\(a_{1}\)}} = \(0\). In a similar fashion, {{c2::\(a_{j}\)}} = \(0\) for each \(j\), which implies that \(v_{1}, \ldots, v_{m}\) is linearly independent.<br>

============================================================

  

    (a) The operator \(N \in \mathcal{L}\left(\mathbf{F}^{4}\right)\) defined by<br><br>\[<br>N\left(z_{1}, z_{2}, z_{3}, z_{4}\right)=\left(z_{3}, z_{4}, 0,0\right)<br>\]<br><br>is {{c5::{{c1::nilpotent}}}} because {{c4::{{c2::\(N^{2}\)}}}} = {{c4::{{c3::\(0\).}}}}

============================================================

  

    (b) The operator of differentiation on \(\mathcal{P}_{m}(\mathbf{R})\) is {{c1::nilpotent}} because the {{c2::\((m+1)^{\text {st} } \) }}} derivative of every polynomial of degree {{c5::{{c3::at most \(m\)}}}} equals {{c5::{{c4::0}}}} .&nbsp;

============================================================

  

    {{c5::{{c1::Nilpotent}}}} operator raised {{c4::{{c2::to dimension of domain}}}} is {{c4::{{c3::0}}}}<br><br>Suppose \(N \in \mathcal{L}(V)\) is {{c5::{{c1::nilpotent}}}}. Then {{c4::{{c2::\(N^{\operatorname{dim} V}\)}}}} = {{c4::{{c3::\(0\).}}}}

============================================================

  

    Matrix of a {{c1::nilpotent}} operator<br><br>Suppose \(N\) is a {{c1::nilpotent}} operator on \(V\). Then there is a basis of \(V\) with respect to which the matrix of \(N\) has the form<br><br>{{c2::\[<br>\left(\begin{array}{ccc}<br>0 &amp; &amp; * \\<br>&amp; \ddots &amp; \\<br>0 &amp; &amp; 0<br>\end{array}\right) \text {; }<br>\]}}<br><br>here {{c5::all entries}} {{c4::on}} and {{c4::below the diagonal}} are {{c3::0's.}}

============================================================

  

    <img src="paste-4c50ecf5917708940cd50b108f4f45d3b1fc9680.jpg"><br><br>Proof:<br><ul><li>First choose a basis of {{c1::null \(N\).&nbsp;}}</li><li>Then extend this to a basis of {{c2::null \(N^{2}\)}}, then {{c2::null \(N^{3}\)}}, eventually getting a basis of {{c3::\(V\)&nbsp;}}</li><li>Because 8.18 states that null {{c4::\(N^{\operatorname{dim} V}\)}} = {{c5::\(V\)}} .</li></ul>

============================================================

  

    <img src="paste-4c50ecf5917708940cd50b108f4f45d3b1fc9680.jpg"><br><br>Proof:<br><ul><li>First choose a basis of null \(N\).&nbsp;</li><li>Then extend this to a basis of null \(N^{2}\), then null \(N^{3}\), eventually getting a basis of \(V\)&nbsp;</li><li>Because 8.18 states that null \(N^{\operatorname{dim} V}\) = \(V\) .</li></ul><br>Now let's think about the matrix of \(N\) with respect to this basis. :<br><ul><li>The {{c1::first}} column, and perhaps {{c1::additional columns at the beginning}}, consists of {{c1::all 0 's}}, because the corresponding basis vectors are in {{c2::null \(N\)}}.&nbsp;</li><li>The next set of columns comes from basis vectors in {{c3::null \(N^{2}\).&nbsp;}}</li><li>Applying \(N\) to any such vector, we get a vector in {{c2::null \(N\)}}; in other words, we get a vector that is {{c4::a linear combination of the previous basis vectors.&nbsp;}}</li><li>Thus all {{c5::nonzero entries}} in these columns {{c5::lie above the diagonal.&nbsp;}}</li><li>Repeat like this&nbsp;</li></ul>

============================================================

  

    3 Suppose \(T \in \mathcal{L}(V)\) is {{c5::{{c4::invertible}}}}. Prove that \(G\)( {{c4::{{c1::\(\lambda, T)\)}}}} ) = \(G\) ( {{c4::{{c2::\(\frac{1}{\lambda}, T^{-1}\)}}}} ) for every {{c5::{{c4::\(\lambda \in \mathbf{F}\)}}}} with {{c5::{{c4::\(\lambda \neq 0\).}}}}

============================================================

  

    4 Suppose \(T \in \mathcal{L}(V)\) and \(\alpha, \beta\)&nbsp; \(\in\) \(\mathbf{F}\) with {{c5::\(\alpha \neq \beta\)}}. Prove that<br><br><ul><li>{{c4::G}}( {{c1::\(\alpha, T\)}} ) {{c3::\(\cap\)}} {{c4::G}}( {{c2::\(\beta, T\)}} ) = {{c3::\(\{0\} .\)}}</li></ul>

============================================================

  

    5 Suppose \(T \in \mathcal{L}(V), m\) is a positive integer, and \(v \in V\) is such that {{c4::\(T^{m-1}\)}} \(v\) \(\neq\) {{c5::\(0\)}} but {{c4::\(T^{m}\)}} \(v\)= {{c5::\(0\)}}. Prove that<br><br><ul><li>{{c2::\(v\)}}, {{c3::\(T v\)}}, {{c3::\(T^{2} v\)}}, \(\ldots\), {{c3::\(T^{m-1} v\)}}</li></ul><br>is {{c1::linearly independent}}.<br>

============================================================

  

    7 Suppose \(N \in \mathcal{L}(V)\) is {{c1::nilpotent}}. Prove that {{c3::0}} is the {{c5::{{c2::only}}}} {{c5::{{c4::eigenvalue}}}} of \(N\).

============================================================

  

    9 Suppose \(S, T \in \mathcal{L}(V)\) and {{c4::\(S T\)}} is {{c1::nilpotent}}. Prove that {{c5::{{c3::\(T S\)}}}} is {{c5::{{c2::nilpotent}}}}.

============================================================

  

    10 Suppose that \(T \in \mathcal{L}(V)\) is not {{c1::nilpotent}}. Let {{c5::\(n\)}} = {{c5::\(\operatorname{dim} V\).}} <br><br>Show that:<br><ul><li>&nbsp;\(V\) = {{c2::\(\operatorname{null} T^{n-1}\)}} {{c4::\(\oplus\)}} {{c3::\(\operatorname{range} T^{n-1}\)}}.</li></ul>

============================================================

  

    12 Suppose \(N \in \mathcal{L}(V)\) and there exists a basis of \(V\) with respect to which \(N\) has an {{c5::{{c1::upper-triangular}}}} matrix with {{c4::{{c2::only 0's}}}} {{c5::{{c1::on the diagonal}}}}. Prove that \(N\) is {{c4::{{c3::nilpotent}}}}.

============================================================

  

    Suppose \(V\) is an {{c4::inner product}} space and \(N \in \mathcal{L}(V)\) is {{c3::normal}} and {{c5::{{c2::nilpotent}}}}. Prove that \(N\) = {{c5::{{c1::\(0\).}}}}

============================================================

  

    14 Suppose \(V\) is an inner product space and \(N \in \mathcal{L}(V)\) is {{c2::nilpotent}}. Prove that there exists an {{c3::orthonormal}} basis of \(V\) with respect to which \(N\) has an {{c1::upper-triangular}} matrix.<br><br>[If \(F\) = {{c4::\(\mathbf{C}\)}}, then the result above follows from {{c5::Schur's Theorem (6.38)}} without the hypothesis that \(N\) is {{c2::nilpotent}}. Thus the exercise above needs to be proved only when \(\mathbf{F}\) = {{c4::\(\mathbf{R}\)}}.]

============================================================

  

    15 Suppose \(N \in \mathcal{L}(V)\) is such that:<br><ul><li>&nbsp;{{c5::null}} {{c3::\(N^{\operatorname{dim} V-1}\)}} {{c5::\(\neq\)}} {{c5::\(\operatorname{null}\)}} {{c4::\(N^{\operatorname{dim} V}\).&nbsp;}}</li></ul>Prove that \(N\) is {{c1::nilpotent}} and that<br><ul><li>\(\operatorname{dim}\) {{c1::\(\operatorname{null} N^{j}\)}} = {{c2::\(j\)}}</li></ul><br>for {{c2::every integer \(j\)}} with {{c2::\(0\)}} \(\leq\) {{c2::\(j\)}} \(\leq\) {{c2::\(\operatorname{dim} V\).}}<br>

============================================================

  

    16 Suppose \(T \in \mathcal{L}(V)\). Show that<br><br><ul><li>V = {{c2::\(\operatorname{range} T^{0}\)}} {{c1::\(\supset\)}} {{c3::\(\text { range } T^{1}\)}} {{c1::\(\supset\)}} \(\cdots\) {{c1::\(\supset\)}} {{c4::\(\text { range } T^{k}\)}} {{c1::\(\supset\)}} {{c5::\(\text { range } T^{k+1}\)}} {{c1::\(\supset\)}} \(\cdots\)</li></ul>

============================================================

  

    17 Suppose \(T \in \mathcal{L}(V)\) and \(m\) is a nonnegative integer such that<br><br><ul><li>{{c5::\(\text { range }\)}} {{c1::\(T^{m}\)}} = {{c5::\(\operatorname{range}\)}} {{c4::\(T^{m+1} \text {. }\)}}</li></ul><br>Prove that {{c5::range}} {{c2::\(T^{k}\)}} = {{c5::\(\operatorname{range}\)}} {{c1::\(T^{m}\)}} for {{c3::all \(k&gt;m\).}}<br>

============================================================

  

    18 Suppose \(T \in \mathcal{L}(V)\). Let \(n\) {{c5::=}} {{c1::\(\operatorname{dim} V\)}}. Prove that<br><br><ul><li>{{c4::\(\text { range }\)}} {{c2::\(T^{n}\)}} {{c5::=}} {{c4::\(\operatorname{range}\)}} {{c3::\(T^{n+1}\)}} {{c5::=}} {{c4::\(\operatorname{range}\)}} {{c3::\(T^{n+2}\)}} {{c5::=}} \(\cdots .\)</li></ul>

============================================================

  

    19 Suppose \(T \in \mathcal{L}(V)\) and \(m\) is a {{c5::nonnegative}} integer. <br>Prove that:<br><ul><li>{{c3::\(\operatorname{null}\)}} {{c1::\(T^{m}\)}} = {{c3::\(\operatorname{null}\)}} {{c2::\(T^{m+1}\)}}&nbsp;</li><li>if and only if:</li><ul><li>{{c4::\(\operatorname{range}\)}} {{c1::\(T^{m}\)}} = {{c4::\(\operatorname{range}\)}} {{c2::\( T^{m+1}\).}}</li></ul></ul>

============================================================

  

    <img src="paste-39fb611a8f62440ab86e25bbbd165f3f23d3331f.jpg"><br>Proof:<br><ul><li>Suppose \(k\) is a nonnegative integer and \(v\) \(\in\) \(\operatorname{null} T^k\). Then:</li><ul><li>{{c1::\(T^k\)}} \(v\) = {{c2::\(0\)}}<br></li></ul><li>Hence:</li><ul><li>{{c3::\(T^{k+1}\)}} v =&nbsp;{{c4::\(T(T^k)v\)}}<br></li><li>= {{c5::T(0)}}</li><li>= {{c2::0}}</li></ul><li>Thus&nbsp;\(v\)&nbsp;\(\in\) {{c5::null \(T^{k+1}\)}}</li><li>Showing that&nbsp;\(\operatorname{null} T^k \subset \operatorname{null} T^{k+1}\), as desired</li></ul>

============================================================

  

    <img src="paste-8c6fc87474d7fbea330a1f7f6f6bd1dc03017126.jpg"><br>Proof:<br><ul><li>Let k be a positive integer, we want to prove that</li><ul><li>null&nbsp;\(T^{m+k}\) = null&nbsp;\(T^{m+k+1}\)</li></ul><li>From this:</li><ul><li><img src="paste-c5fa00abe1b56df3a213f4abf509f5b1d86d056f.jpg"></li></ul><li>We already know that null&nbsp;{{c1::\(T^{m+k}\)}} = null&nbsp;{{c2::\(T^{m+k+1}\)}}</li><li>To prove the inclusion in the other direction suppose&nbsp;\(v\)&nbsp;\(\in\) null {{c2::\(T^{m+k+1}\)}} then:</li><ul><li>{{c3::\(T^{m+1} T^k\)}}&nbsp;\(v\) = {{c4::\(T^{m+k+1}\)&nbsp;}}\(v\) = 0<br></li></ul><li>hence&nbsp;&nbsp;{{c3::\(T^k\)}} \(v\) \(\in\) \(\operatorname{null}\) {{c3::\(T^{m+1}\)}} = null&nbsp;{{c4::\(T^m\)}}</li><li>Thus</li><li>{{c1::\(T^{m+k} \)}}&nbsp;\(v\) =&nbsp;{{c5::\(T^m T^k\)}}&nbsp;\(v\) = 0<br></li><li>which means that&nbsp;\(v\)&nbsp;\(\in\)&nbsp;\(null\) {{c1::\(T^{m+k}\)}} and thus that&nbsp;\(\operatorname{null}\) {{c2::\(T^{m+k+1}\)}} \(\subset\) \(\operatorname{null}\) {{c1::\(T^{m+k}\)}}</li></ul><br><br><br>

============================================================

  

    <img src="paste-c1bb62b52dfdaefae3af220a3a58d9d4898659ce.jpg">&nbsp;<br><br>Proof:<br><br><ul><li>We need to prove&nbsp;\(null T^n\) =&nbsp;&nbsp;\(null T^{n+1}\)</li><li>Suppose this is not true, then we only have {{c1::strict inequalities}} of the {{c2::null space}} of {{c2::succesive powers}}.</li><li>However, {{c3::the dimension}} of each succesive null space {{c3::must increase}} for {{c4::the previous one to be contained in the next one}}</li><li>Thus: {{c5::for n+1, we would have a null space of dimension n+1}} which is {{c5::larger than the original vector space}} and thus a contradiction</li></ul>

============================================================

  

    <img src="paste-5b23b582d8d40ab6b0b3b40dd0a5b85fe4075b15.jpg"><br>Proof:<br><ul><li>First we need to show that</li><ul><li>{{c1::\(\left(\right.\) null \(\left.T^n\right)\) \(\cap\) \(\left(\operatorname{range} T^n\right)\)}} = {{c1::\(\{0\}\).}}<br></li></ul><li>Suppose&nbsp;\(v\)&nbsp;\(\in\) {{c1::\(\left(\right.\) null \(\left.T^n\right) \cap\left(\operatorname{range} T^n\right)\).}}</li><li>Then:</li><ul><li>{{c2::\(T^{n}\)}} \(v\) = {{c1::0}} because {{c2::it is in the null}}<br></li></ul><li>and there exists&nbsp;\(u\)&nbsp;\(\in\)&nbsp;\(V\) such that</li><ul><li>{{c3::\(T^{n}\)}} \(u\) = {{c3::\(v\)}} because {{c3::v is in the range}}<br></li></ul><li>we can then transform the last equation:</li><ul><li>{{c4::\(T^{2 n}\)}} \(u\) = {{c4::\(T^n\) \(v\)}} = {{c4::0}}<br></li></ul><li>Since {{c5::the null space of&nbsp;\(T^n\)}} is {{c5::the same as that of&nbsp;\(T^{2n}\)}} given that n is {{c5::the dimension of V}}, then&nbsp;{{c3::\(T^{n}\)}} u = {{c1::0&nbsp;}}</li><li>Finally this shows that {{c5::v is 0}} meaning the intersection is empty</li></ul>

============================================================

  

    <img src="paste-879c0ba241c27c4d1408ce2eea78c594cd86771a.jpg"><br>Proof:<br><ul><li>Assume the intersection is empty and show that the sum covers the whole of V</li><li>\(\operatorname{dim}\left(\operatorname{null} T^n \oplus \operatorname{range} T^n\right)\)<br></li><ul><li>=&nbsp;{{c1::\(\operatorname{dim} \operatorname{null} T^n\)}} {{c4::+}} {{c5::{{c2::\(\operatorname{dim} \operatorname{range} T^n\)}}}}&nbsp; by {{c5::{{c2::direct sum defiinition}}}}</li><li>= {{c5::{{c3::dim V}}}}&nbsp; by {{c5::{{c3::fundamental theorem of algebra}}}}</li></ul></ul>

============================================================

  

    Thus {{c4::\(E(\lambda, T)\)}} is the {{c1::set of eigenvectors of \(T\)}} {{c5::{{c2::corresponding to \(\lambda\)}}}}, {{c5::{{c3::along with the 0 vector}}}}.

============================================================

  

    What are the two implications of this?<br><img src="paste-4c4e609216e831238121df0cc440e878a4bd4ec6.jpg"><br>Implications:<br><ul><li>The generalized eigenspace is a {{c5::{{c1::subsbspace}}}}</li><li>We only need to consider {{c4::{{c2::the power dim V}}}} rather than {{c4::{{c3::all possible powers}}}}</li></ul>

============================================================

  

    The {{c1::null space}} and {{c2::range}} of {{c5::{{c4::\(p(T)\)}}}} are {{c5::{{c3::invariant under \(T\)}}}}<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(p \in \mathcal{P}(\mathbf{F})\). Then {{c1::null \(p(T)\)}} and {{c2::range \(p(T)\)}} are {{c5::{{c3::invariant under \(T\).}}}}

============================================================

  

    <img src="paste-e9b9347966e1654ef8d7ac56f5de473d66822c8d.jpg"><br><br>Proof Suppose \(v \in\) null \(p(T)\). Then \(p(T) v=0\). Thus<br><br><ul><li>\(((p(T))\) \((T v)\) ={{c1::\(T\)}} {{c2::\(p(T)\)}} \(v\)={{c3::\(T(0)\)}}= {{c3::\(0 .\)}}</li></ul><br>Hence \(T v \in\) null \(p(T)\). Thus null \(p(T)\) is invariant under \(T\), as desired.<br><br>Suppose \(v \in\) range \(p(T)\). Then there exists \(u \in V\) such that \(v=p(T) u\). Thus<br><br><ul><li>\(T v\) = {{c4::\(T(p(T) u)\)}} ={{c4::\( p(T)\)}} {{c5::\((T u) .\)}}</li></ul><br>Hence \(T v \in\) range \(p(T)\). Thus range \(p(T)\) is invariant under \(T\), as desired.<br>

============================================================

  

    Suppose \(V\) is a complex vector space and \(T \in \mathcal{L}(V)\). Let \(\lambda_{1}, \ldots, \lambda_{m}\) be the distinct eigenvalues of \(T\). Then<br><ul><li>(a) \(V\) = {{c1::\(\bigoplus_{j}^{m}&nbsp;\)}} {{c2::\(G\)( \(\lambda_j,T\) )}}</li><li>(b) each {{c2::\(G\left(\lambda_{j}, T\right)\)}} is {{c3::invariant under \(T\);}}</li><li>(c) each {{c4::\(\left.\left(T-\lambda_{j} I\right)\right|_{G\left(\lambda_{j}, T\right)}\)}} is {{c5::nilpotent}}.</li></ul>

============================================================

  

    A {{c1::basis}} of {{c2::generalized}} {{c5::{{c3::eigenvectors}}}}<br><br>Suppose \(V\) is a {{c5::{{c4::complex}}}} vector space and \(T \in \mathcal{L}(V)\). Then there is a {{c1::basis of \(V\)}} consisting of {{c2::generalized}} {{c5::{{c3::eigenvectors of \(T\).}}}}

============================================================

  

    Definition {{c1::multiplicity}}<br><br><ul><li>Suppose \(T \in \mathcal{L}(V)\). The {{c1::multiplicity}} of an {{c4::eigenvalue \(\lambda\) of \(T\)}} is defined to be the {{c5::{{c2::dimension}}}} of the {{c5::{{c2::corresponding generalized eigenspace \(G(\lambda, T)\)}}}}.</li><li>In other words, the {{c1::multiplicity}} of an {{c4::eigenvalue \(\lambda\) of \(T\)}} equals {{c5::{{c3::\(\operatorname{dim} \operatorname{null}(T-\lambda I)^{\operatorname{dim} V}\)}}}}.</li></ul>

============================================================

  

    {{c3::Sum}} of the {{c2::multiplicities}} equals {{c1::\(\operatorname{dim} V\)}}<br><br>Suppose \(V\) is a {{c5::complex}} vector space and \(T \in \mathcal{L}(V)\). Then the {{c3::sum}} of the {{c2::multiplicities}} of {{c4::all the eigenvalues of \(T\)}} equals {{c1::\(\operatorname{dim} V\).}}

============================================================

  

    If \(T \in \mathcal{L}(V)\) and \(\lambda\) is an eigenvalue of \(T\), then<br><br><ul><li>{{c3::algebraic}} {{c4::multiplicity}} of \(\lambda\) = \(\operatorname{dim} \) \(\operatorname{null}\) {{c5::{{c1::\((T-\lambda I)^{\operatorname{dim} V} \)}}}} = \(\operatorname{dim}\) {{c5::{{c2::\(G(\lambda, T)\)}}}}</li><li>{{c3::geometric}} {{c4::multiplicity}} of \(\lambda\)= \(\operatorname{dim}\) \(\operatorname{null}\) {{c5::{{c1::\((T-\lambda I)\)}}}}=\(\operatorname{dim}\) {{c5::{{c2::\(E(\lambda, T)\).}}}}</li></ul>

============================================================

  

    Definition {{c1::block diagonal}} matrix<br><br>A {{c1::block diagonal}} matrix is a square matrix of the form<br><br>{{c2::\[<br>\left(\begin{array}{ccc}<br>A_{1} &amp; &amp; 0 \\<br>&amp; \ddots &amp; \\<br>0 &amp; &amp; A_{m}<br>\end{array}\right),<br>\]}}<br><br>where \(A_{1}, \ldots, A_{m}\) are {{c2::square}} matrices {{c4::lying along the diagonal}} and {{c5::all the other entries of the matrix}} equal {{c3::0}} .

============================================================

  

    {{c5::Block diagonal}} matrix with {{c1::upper-triangular}} blocks<br><br>Suppose \(V\) is a {{c2::complex}} vector space and \(T \in \mathcal{L}(V)\). Let \(\lambda_{1}, \ldots, \lambda_{m}\) be the distinct eigenvalues of \(T\), with multiplicities \(d_{1}, \ldots, d_{m}\). Then there is a basis of \(V\) with respect to which \(T\) has a {{c5::block diagonal}} matrix of the form<br><br>\[<br>\left(\begin{array}{ccc}<br>A_{1} &amp; &amp; 0 \\<br>&amp; \ddots &amp; \\<br>0 &amp; &amp; A_{m}<br>\end{array}\right)<br>\]<br><br>where each \(A_{j}\) is a {{c4::\(d_{j}\)}}-by- {{c4::\(d_{j}\)}} {{c1::upper-triangular}} matrix of the form<br><br>{{c3::\[<br>A_{j}=\left(\begin{array}{ccc}<br>\lambda_{j} &amp; &amp; * \\<br>&amp; \ddots &amp; \\<br>0 &amp; &amp; \lambda_{j}<br>\end{array}\right)<br>\]}}<br>

============================================================

  

    {{c5::{{c3::Block diagonal}}}} matrices are liable to have {{c4::{{c1::many more zeroes}}}} than {{c4::{{c2::upper-traingular}}}} matrices

============================================================

  

    {{c2::Identity}} {{c4::plus}} {{c5::{{c3::nilpotent}}}} has {{c5::{{c1::a square root}}}}<br><br>Suppose \(N \in \mathcal{L}(V)\) is {{c3::nilpotent}}. Then {{c2::\(I\)}} {{c4::+}} {{c5::{{c3::\(N\)}}}} has {{c5::{{c1::a square root.}}}}

============================================================

  

    Over {{c5::{{c3::\(\mathbf{C}\)}}}}, {{c4::{{c1::invertible}}}} operators have {{c4::{{c2::square roots}}}}<br><br>Suppose \(V\) is a {{c5::{{c3::complex}}}} vector space and \(T \in \mathcal{L}(V)\) is {{c1::invertible}}. Then \(T\) has {{c4::{{c2::a square root.}}}}

============================================================

  

    <img src="paste-46ccebafecf30978eabbab44754a32e9565c80da.jpg"><br>Proof <br><br>Let \(\lambda_{1}, \ldots, \lambda_{m}\) be the distinct eigenvalues of \(T\). For each \(j\), there exists a nilpotent operator \(N_{j} \in \mathcal{L}\left(G\left(\lambda_{j}, T\right)\right)\) such that \(\left.T\right|_{G\left(\lambda_{j}, T\right)}\) = {{c1::\(\lambda_{j} I+N_{j}\)}} [see 8.21(c)]. Because \(T\) is invertible, {{c2::none of the \(\lambda_{j}\) 's equals 0}} , so we can write<br><br><ul><li>\(\left.T\right|_{G\left(\lambda_{j}, T\right)}\) = {{c3::\(\lambda_{j}\)}} {{c4::\(\left(I+\frac{N_{j} }{\lambda_{j} }\right)\)}}</li></ul><br>for each \(j\). Clearly {{c4::\(N_{j} / \lambda_{j}\)}} is {{c5::nilpotent}}, and so {{c5::\(I+N_{j} / \lambda_{j}\)}} has {{c5::a square root (by 8.31)}}. Multiplying a square root of the complex number \(\lambda_{j}\) by a square root of \(I+N_{j} / \lambda_{j}\), we obtain a square root \(R_{j}\) of \(\left.T\right|_{G\left(\lambda_{j}, T\right)}\).<br><br>A typical vector \(v \in V\) can be written uniquely in the form<br><br>\[<br>v=u_{1}+\cdots+u_{m}<br>\]<br><br>where each \(u_{j}\) is in \(G\left(\lambda_{j}, T\right)\) (see 8.21). Using this decomposition, define an operator \(R \in \mathcal{L}(V)\) by<br><br>\[<br>R v=R_{1} u_{1}+\cdots+R_{m} u_{m} .<br>\]<br><br>You should verify that this operator \(R\) is a square root of \(T\), completing the proof.<br>

============================================================

  

    <img src="paste-46ccebafecf30978eabbab44754a32e9565c80da.jpg"><br><span style="color: rgb(0, 0, 0);">Proof</span><br><br><span style="color: rgb(0, 0, 0);">Let \(\lambda_{1}, \ldots, \lambda_{m}\) be the distinct eigenvalues of \(T\). For each \(j\), there exists a nilpotent operator \(N_{j} \in \mathcal{L}\left(G\left(\lambda_{j}, T\right)\right)\) such that \(\left.T\right|_{G\left(\lambda_{j}, T\right)}\) = {{c1::\(\lambda_{j} I+N_{j}\)}} [see 8.21(c)]. Because \(T\) is invertible, {{c2::none of the \(\lambda_{j}\) 's equals 0}} , so we can write</span><br><br><ul><li>\(\left.T\right|_{G\left(\lambda_{j}, T\right)}\) = {{c3::\(\lambda_{j}\)}} {{c4::\(\left(I+\frac{N_{j} }{\lambda_{j} }\right)\)}}</li></ul><br><span style="color: rgb(0, 0, 0);">for each \(j\). Clearly {{c4::\(N_{j} / \lambda_{j}\)}} is {{c5::nilpotent}}, and so {{c5::\(I+N_{j} / \lambda_{j}\)}} has {{c5::a square root (by 8.31)}}. Multiplying a square root of the complex number \(\lambda_{j}\) by a square root of \(I+N_{j} / \lambda_{j}\), we obtain a square root \(R_{j}\) of \(\left.T\right|_{G\left(\lambda_{j}, T\right)}\).</span><br><br><span style="color: rgb(0, 0, 0);">A typical vector \(v \in V\) can be written uniquely in the form</span><br><br><span style="color: rgb(0, 0, 0);">\[</span><br><span style="color: rgb(0, 0, 0);">v=u_{1}+\cdots+u_{m}</span><br><span style="color: rgb(0, 0, 0);">\]</span><br><br><span style="color: rgb(0, 0, 0);">where each \(u_{j}\) is in \(G\left(\lambda_{j}, T\right)\) (see 8.21). Using this decomposition, define an operator \(R \in \mathcal{L}(V)\) by</span><br><br><span style="color: rgb(0, 0, 0);">\[</span><br><span style="color: rgb(0, 0, 0);">R v=R_{1} u_{1}+\cdots+R_{m} u_{m} .</span><br><span style="color: rgb(0, 0, 0);">\]</span><br><br><span style="color: rgb(0, 0, 0);">You should verify that this operator \(R\) is a square root of \(T\), completing the proof.</span><br>

============================================================

  

    <img src="paste-2796e54f31fed3b00816f8632242366536386ab3.jpg"><br><br>Proof Let \(\lambda_{1}, \ldots, \lambda_{m}\) be the distinct eigenvalues of \(T\). For each \(j\), there exists a nilpotent operator \(N_{j} \in \mathcal{L}\left(G\left(\lambda_{j}, T\right)\right)\) such that \(\left.T\right|_{G\left(\lambda_{j}, T\right)}=\lambda_{j} I+N_{j}\) [see 8.21(c)]. Because \(T\) is invertible, none of the \(\lambda_{j}\) 's equals 0 , so we can write<br><br>\[<br>\left.T\right|_{G\left(\lambda_{j}, T\right)}=\lambda_{j}\left(I+\frac{N_{j}}{\lambda_{j}}\right)<br>\]<br><br>for each \(j\). Clearly \(N_{j} / \lambda_{j}\) is nilpotent, and so \(I+N_{j} / \lambda_{j}\) has a square root (by 8.31). <br><br>Multiplying a {{c2::square root}} of {{c3::the complex number \(\lambda_{j}\)}} by a {{c2::square root}} of {{c1::\(I+N_{j} / \lambda_{j}\)}}, we obtain a {{c2::square root}} \(R_{j}\) of {{c4::\(\left.T\right|_{G\left(\lambda_{j}, T\right)}\).}}<br><br>A typical vector \(v \in V\) can be written uniquely in the form<br><br>\[<br>v=u_{1}+\cdots+u_{m}<br>\]<br><br>where each \(u_{j}\) is in \(G\left(\lambda_{j}, T\right)\) (see 8.21). Using this decomposition, define an operator \(R \in \mathcal{L}(V)\) by<br><ul><li>\(R\) v= {{c5::\(R_{1} u_{1}+\cdots+R_{m} u_{m} .\)}}<br></li></ul><br>You should verify that this operator \(R\) is a square root of \(T\), completing the proof.<br>

============================================================

  

    1. Suppose \(V\) is a {{c4::complex}} vector space, \(N \in \mathcal{L}(V)\), and {{c2::0}} is the {{c5::{{c3::only eigenvalue of \(N\)}}}}. Prove that \(N\) is {{c5::{{c1::nilpotent}}}}.

============================================================

  

    2 Give an example of an operator \(T\) on a {{c5::finite}}-dimensional {{c5::real}} vector space such that {{c3::0}} is {{c4::the only eigenvalue of \(T\)}} but \(T\) is {{c2::not}} {{c1::nilpotent}}.

============================================================

  

    3 Suppose \(T \in \mathcal{L}(V)\). Suppose \(S \in \mathcal{L}(V)\) is {{c5::invertible}}. Prove that \(T\) and {{c4::\(S^{-1} T S\)}} have {{c3::the same}} {{c1::eigenvalues}} with {{c3::the same}} {{c2::multiplicities}}.

============================================================

  

    4 Suppose \(V\) is an \(n\)-dimensional complex vector space and \(T\) is an operator on \(V\) such that null {{c2::\(T^{n-2}\)}} {{c4::\(\neq\)}} \(\operatorname{null}\) {{c3::\(T^{n-1}\)}}. Prove that \(T\) has {{c5::at most}} {{c1::two}} {{c5::distinct}} eigenvalues.

============================================================

  

    5 Suppose \(V\) is a {{c5::complex}} vector space and \(T \in \mathcal{L}(V)\). Prove that \(V\) has {{c4::a basis consisting of eigenvectors of \(T\)}} if and only if {{c3::every}} {{c2::generalized eigenvector of \(T\)}} is {{c1::an eigenvector of \(T\)}}.

============================================================

  

    7 Suppose \(V\) is a {{c4::complex}} vector space. Prove that every {{c3::invertible}} operator on \(V\) has a {{c5::{{c1::cube}}}} {{c5::{{c2::root}}}}.

============================================================

  

    8 Suppose \(T \in \mathcal{L}(V)\) and 3 and 8 are {{c5::eigenvalues}} of \(T\). Let \(n\) = {{c4::\(\operatorname{dim} V\).}} <br><br>Prove that:<br><ul><li>&nbsp;\(V\)={{c2::\(\left(\right.\) null \(\left.T^{n-2}\right)\)}} {{c3::\(\oplus\)}} {{c1::\(\left(\operatorname{range} T^{n-2}\right)\)}}.</li></ul>

============================================================

  

    9 Suppose \(A\) and \(B\) are block diagonal matrices of the form<br><br>\[<br>A=\left(\begin{array}{ccc}<br>A_{1} &amp; &amp; 0 \\<br>&amp; \ddots &amp; \\<br>0 &amp; &amp; A_{m}<br>\end{array}\right), \quad B=\left(\begin{array}{ccc}<br>B_{1} &amp; &amp; 0 \\<br>&amp; \ddots &amp; \\<br>0 &amp; &amp; B_{m}<br>\end{array}\right),<br>\]<br><br>where {{c3::\(A_{j}\)}} has {{c2::the same size}} as {{c3::\(B_{j}\)}} for {{c3::\(j=1, \ldots, m\)}}. Show that {{c4::\(A B\)}} is a {{c5::block diagonal}} matrix of the form<br>{{c1::<br>\[<br>A B=\left(\begin{array}{ccc}<br>A_{1} B_{1} &amp; &amp; 0 \\<br>&amp; \ddots &amp; \\<br>0 &amp; &amp; A_{m} B_{m}<br>\end{array}\right) .<br>\]}}

============================================================

  

    10 Suppose \(\mathbf{F}\) = {{c5::\(\mathbf{C}\)}} and \(T \in \mathcal{L}(V)\). Prove that there exist \(D, N \in \mathcal{L}(V)\) such that \(T\) = {{c3::\(D+N\)}}, the operator \(D\) is {{c4::diagonalizable}}, \(N\) is {{c5::nilpotent}}, and {{c1::\(D N\)}} = {{c2::\(N D\)}}.

============================================================

  

    11 Suppose \(T \in \mathcal{L}(V)\) and \(\lambda \in \mathbf{F}\). Prove that for every basis of \(V\) with respect to which \(T\) has an {{c5::upper-triangular}} matrix, the {{c3::number of times}} that \(\lambda\) {{c4::appears on the diagonal of the matrix of \(T\)}} equals the {{c1::multiplicity of \(\lambda\)}} as {{c2::an eigenvalue of \(T\).}}

============================================================

  

    <img src="paste-411a6ff3cdbd82d616a739f3e82f4f844941e175.jpg"><br>Proof:<br><ul><li>Since {{c1::multiplying p(T)v by T is commutative}}, if p(T)v = 0 then {{c2::T(v)}} {{c3::must also equal 0}} and thus null P(T) is invariant under T</li><li>Similarly, suppose a v = {{c4::p(T)u}} then Tv = {{c5::T( p(T) u)}} = {{c5::P(T)(Tu)}} and hence Tv is in the range of p(T)</li></ul>

============================================================

  

    <img src="paste-5f41922b5ce852e7690462f04da210736d43d1f9.jpg"><br>Proof of (b):<br><ul><li>&nbsp;Recall that<br></li><ul><li>\(G\) (\(\lambda_j, T\)) ={{c4::<ul><li> \(\operatorname{null}\left(T-\lambda_j I\right)^{\operatorname{dim} V}\)</li></ul>}}</li></ul><li>Then use the result that {{c1::null \(p(T)\)}} is {{c5::{{c2::invariant under \(T\)}}}} for {{c5::{{c3::every polynomial \(p\).}}}}</li></ul>

============================================================

  

    <img src="paste-88bf2c093d571b8e32a5a2cf3648786538171955.jpg"><br>&nbsp;Recall that<br><ul><li>\(G\left(\lambda_j, T\right)\) = {{c1::\(\operatorname{null}\left(T-\lambda_j I\right)^{\operatorname{dim} V}\)}}</li></ul>Proof of (c): {{c3::Clear from definition of}} {{c2::\(\left.\left(T-\lambda_j I\right)\right|_{G\left(\lambda_j, T\right)}\)}} since \(G\left(\lambda_j, T\right)\) is defined to be {{c4::exactly the null space}} of {{c5::that operator}} when {{c5::raised to the dim V power}}<br>

============================================================

  

    <img src="paste-8ec1567cba8d3348149d8ae1a6354fc2dbb4ba52.jpg"><br>Proof of a)<br><ul><li>Since we already know that {{c2::generalized eigenvectors}} {{c5::corresponding to distinct eigenvalues}} are {{c4::linearly independent.}}</li><li>Thus to show a) we just need {{c3::a proof by induction}} for {{c1::the dimensions adding up to dim V}}</li></ul>

============================================================

  

    <img src="paste-ea621df9d6819a2a60364d3ddbd7c5ef37c63458.jpg"><br>Proove the basis part:<br><ul><li>{{c5::Choose}} {{c1::a basis of each \(G\left(\lambda_j, T\right)\)}}. {{c2::Put all these bases together}} to {{c3::form a basis of \(V\)}} consisting of {{c4::generalized eigenvectors of \(T\).}}</li></ul>

============================================================

  

    <img src="paste-21d245c37058afdab6a6d7e2eaeb09c3bd8befbb.jpg"><br>Proof:<br><ul><li>\(V\) =&nbsp;{{c1::\(G\left(\lambda_1, T\right) \oplus \cdots \oplus G\left(\lambda_m, T\right)\)}}<br></li><li>Thus {{c5::the dimension of V}} is equal to the {{c2::sum of the dimensions}} of {{c3::the generalized eigenspaces}}</li><li>However, those dimensions are {{c4::precisely the multiplicities of eigenvalues}}</li></ul>

============================================================

  

    Define \(T \in \mathcal{L}\left(\mathbf{C}^3\right)\) by<br>\[<br>\begin{aligned}<br>T\left(z_1, z_2, z_3\right) &amp; =\left(3 z_1+4 z_2, 3 z_2, 8 z_3\right) . \\<br>\mathcal{M}(T) &amp; =\left(\begin{array}{lll}<br>3 &amp; 4 &amp; 0 \\<br>0 &amp; 3 &amp; 0 \\<br>0 &amp; 0 &amp; 8<br>\end{array}\right)<br>\end{aligned}<br>\]<br><br>The eigenvalues of \(T\) are 3 and 8 .<br>The eigenspaces of \(T\) are<br>\[<br>\begin{aligned}<br>&amp; E(3, T)=\left\{\left(z_1, 0,0\right): z_1 \in \mathbf{C}\right\}, \\<br>&amp; E(8, T)=\left\{\left(0,0, z_3\right): z_3 \in \mathbf{C}\right\} .<br>\end{aligned}<br>\]<br><br>Thus the eigenvalue 3 has {{c1::geometric}} {{c2::multiplicity}} {{c3::1}} and the eigenvalue 8 has {{c1::geometric}} {{c2::multiplicity}} {{c3::1}} .<br><br>The generalized eigenspaces of \(T\) are<br>\[<br>\begin{aligned}<br>&amp; G(3, T)=\left\{\left(z_1, z_2, 0\right): z_1, z_2 \in \mathbf{C}\right\}, \\<br>&amp; G(8, T)=\left\{\left(0,0, z_3\right): z_3 \in \mathbf{C}\right\} .<br>\end{aligned}<br>\]<br><br>Thus the eigenvalue 3 has {{c4::algebraic}} {{c2::multiplicity}} {{c5::2}} and the eigenvalue 8 has {{c4::algebraic}} {{c2::multiplicity}} {{c3::1}} .<br><br>We have<br>\[<br>\mathbf{C}^3=G(3, T) \oplus G(8, T),<br>\]<br>as expected by the Decomposition Theorem.

============================================================

  

    The {{c4::multiplicity}} of an eigevanlue is always {{c5::the number of times}} it {{c3::appears on}} the {{c1::diagonal}} of an {{c2::upper-triangualr}} matrix

============================================================

  

    A {{c5::{{c1::block diagonal}}}} matrix looks just like a {{c5::{{c1::diagonal}}}} matrix execpt for maybe having {{c4::{{c2::matrices}}}} rather than {{c4::{{c3::numbers}}}} on the {{c5::{{c1::daigonal}}}}

============================================================

  

    <img src="paste-75f6c3469ae19f1b4a1e96e0b6c97e01294cbfd6.jpg"><br>What two theorems are needed for this?<br><ul><li>{{c1::Generalized eigenspace decomposition}}</li><li>{{c2::Nillpotent operators matrices}}: {{c3::\(( T + \lambda I)\)\(^{dimV}\)}} when {{c4::restricted to a generalized eigenspace}} can be decomposed with the T part being {{c5::nillpotent}} and thus having a matrix of {{c5::only 0s at and bellow the diagonal.}}</li><ul><li>Thus all the values on the diagonal must be&nbsp;\(\lambda\)</li></ul></ul>

============================================================

  

    Despite every {{c1::complex number}} having {{c2::a square root}}, {{c3::operators on complex vector spaces}} are {{c4::not guaranteed}} to have {{c5::square roots&nbsp;}}

============================================================

  

    <img src="paste-29973234f071bb3b2b46155472c2e72f49068335.jpg"><br>Proof Consider the Taylor series for the function \(\sqrt{1+x}\) :<br><ul><li>\(\sqrt{1+x}\) ={{c3::\(1 + \lim_{m \to \infty}\sum_{i=1}^{m} a_i x^i\)}}</li></ul><br>Because \(N\) is nilpotent, \(N^m=0\) for some positive integer \(m\). Thus we guess that there is a square root of \(I+N\) of the form:<br><ul><li>&nbsp;\(I+N\) = {{c4::\(I +\sum_{i=1}^{m-1} a_i N^i\)}}<br></li></ul><br>Now<br>{{c5::\[<br>\begin{aligned}<br>&amp; \left(I+a_1 N+a_2 N^2+a_3 N^3+\cdots+a_{m-1} N^{m-1}\right)^2 \\<br>&amp; =I+2 a_1 N+\left(2 a_2+a_1^2\right) N^2+\left(2 a_3+2 a_1 a_2\right) N^3+\cdots \\<br>&amp; \quad+\left(2 a_{m-1}+\text { terms involving } a_1, \ldots, a_{m-2}\right) N^{m-1}<br>\end{aligned}<br>\]}}<br><br>We want this to equal I + N, thus we can choose the coefficients to make all other terms 0:<br><ul><li>if \(2 a_1=1\) (thus \(a_1=1 / 2\) ) and \(2 a_2+a_1^2=0\) (thus \(a_2=-1 / 8\) ) and&nbsp;\(a_3\) =&nbsp;\(1/16\)<br></li><li>Continue in this fashion, which works because every expression involves {{c1::just the current coefficient}} + {{c2::lower-order terms we have already solved for}}</li><li>Which proves that I+N must have a square root since the coefficients do exist</li></ul>

============================================================

  

    Over {{c1::C}}, {{c2::invertible}} operators {{c5::{{c4::have}}}} {{c5::{{c3::square roots}}}}<br><br>Suppose \(V\) is a {{c1::complex}} vector space and \(T \in \mathcal{L}(V)\) is {{c2::invertible}}. Then \(T\) {{c5::{{c4::has}}}} {{c5::{{c3::a square root.}}}}

============================================================

  

    <img src="paste-5423a84c5fb4f78ff3c0be8eebc6d33e230a9f71.jpg"><br>Proof:<br><ul><li>Let \(\lambda_1, \ldots, \lambda_m\) be the distinct eigenvalues of \(T\).&nbsp;</li><li>For each \(j\), there exists a {{c4::nilpotent}} operator \(N_j \in \mathcal{L}\left(G\left(\lambda_j, T\right)\right)\) such that:</li><ul><li>{{c2::&nbsp;\(\left.T\right|_{G\left(\lambda_j, T\right)}\)}} = {{c1::\(\lambda_j I+N_j\).&nbsp;}}</li></ul><li>Because \(T\) is {{c5::invertible}}, none of the \(\lambda_j\) 's equals 0 , so we can write</li><ul><li>{{c2::\(\left.T\right|_{G\left(\lambda_j, T\right)}\)}} = {{c3::\(\lambda_j\left(I+\frac{N_j}{\lambda_j}\right) .\)}}</li></ul><li>Because a {{c4::nillpotent}} operator divided by a scalar {{c4::remains}} {{c4::nillpotent}} then:</li><li>Clearly \(N_j / \lambda_j\) is {{c4::nilpotent}}, and so \(I+N_j / \lambda_j\) has {{c5::a square root}} because {{c5::all such sums have square roots}}.</li></ul>

============================================================

  

    <img src="paste-5423a84c5fb4f78ff3c0be8eebc6d33e230a9f71.jpg"><br>Proof:<br><ul><li>Let \(\lambda_1, \ldots, \lambda_m\) be the distinct eigenvalues of \(T\).&nbsp;</li><li>For each \(j\), there exists a nilpotent operator \(N_j \in \mathcal{L}\left(G\left(\lambda_j, T\right)\right)\) such that:</li><ul><li>&nbsp;\(\left.T\right|_{G\left(\lambda_j, T\right)}=\lambda_j I+N_j\).&nbsp;</li></ul><li>Because \(T\) is invertible, none of the \(\lambda_j\) 's equals 0 , so we can write</li><ul><li>\(\left.T\right|_{G\left(\lambda_j, T\right)}\) = \(\lambda_j\left(I+\frac{N_j}{\lambda_j}\right) .\)</li></ul><li>Because a nillpotent operator divided by a scalar remains nillpotent then:</li><li>Clearly \(N_j / \lambda_j\) is nilpotent, and so \(I+N_j / \lambda_j\) has a square root because all such sums have suare roots.<br></li><li>Then because every {{c2::complex}} number has a {{c2::complex}} {{c1::square root}}:</li><li>Multiplying a {{c1::square root}} of {{c2::the number \(\lambda_j\)}} by a {{c1::square root}} of {{c3::\(I+N_j / \lambda_j\)}} gives a {{c1::square root}} \(R_j\) of {{c4::\(\left.T\right|_{G\left(\lambda_j, T\right)}\).}}<br></li><li>Then we can define an operator R by {{c5::applying each&nbsp;\(R_j\)}} to {{c5::the corresponding eigenvector}} from {{c5::the generalized eigensapce decomposition}}</li><li>This operator is a {{c1::square root}} of T.</li></ul>

============================================================

  

    Definition {{c3::characteristic polynomial}}<br><br>Suppose \(V\) is a complex vector space and \(T \in \mathcal{L}(V)\). Let \(\lambda_{1}, \ldots, \lambda_{m}\) denote the {{c4::distinct eigenvalues of \(T\)}}, with {{c5::multiplicities}} \(d_{1}, \ldots, d_{m}\). The {{c3::polynomial}}<br><br><ul><li>\(\prod_{i=1}^m\){{c1::\((z-\lambda_i)\)}}{{c2::\(^{d_i}\)}}</li></ul><br>is called the {{c3::characteristic polynomial of \(T\).}}<br>

============================================================

  

    Suppose \(V\) is a complex vector space and \(T \in \mathcal{L}(V)\). Then<br><br><ul><li>(a) the characteristic polynomial of \(T\) has {{c3::degree}} {{c1::\(\operatorname{dim} V\)}};</li><li>(b) the {{c5::{{c4::zeros}}}} of the characteristic polynomial of \(T\) {{c5::{{c2::are the eigenvalues of \(T\).}}}}</li></ul>

============================================================

  

    Cayley-Hamilton Theorem<br><br>Suppose \(V\) is a complex vector space and \(T \in \mathcal{L}(V)\). Let \(q\) denote the {{c4::characteristic polynomial}} of \(T\). Then:<br><ul><li>{{c2::&nbsp;\(q(T)\)}} {{c5::{{c3::=}}}} {{c5::{{c1::\(0\).}}}}</li></ul>

============================================================

  

    <img src="paste-e1fc3bcece69a82f3536472bbc5ebb238a8984e0.jpg"><br>Proof:<br><ol><li>Let \(\lambda_{1}, \ldots, \lambda_{m}\) be the distinct eigenvalues of the operator \(T\).</li><li>Let \(d_{1}, \ldots, d_{m}\) be the dimensions of the corresponding generalized eigenspaces \(G\left(\lambda_{1}, T\right), \ldots, G\left(\lambda_{m}, T\right)\).&nbsp;<br></li><li>For each j,&nbsp;{{c1::\(\left.\left(T-\lambda_j I\right)\right|_{G\left(\lambda_j, T\right)}\)}} is {{c2::nilpotent}}</li><li>Thus we have:</li><ol><li>{{c3::&nbsp;\(\left.\left(T-\lambda_{j} I\right)^{d_{j} }\right|_{G\left(\lambda_{j}, T\right)}\) }}= {{c4::\(0\)&nbsp;}}</li><li>Because {{c2::nillpotent}} operators {{c5::to the power of the dimension}} are {{c4::0}}</li></ol></ol>

============================================================

  

    <img src="paste-e1fc3bcece69a82f3536472bbc5ebb238a8984e0.jpg"><br>Proof:<br><ol><li>Let \(\lambda_{1}, \ldots, \lambda_{m}\) be the distinct eigenvalues of the operator \(T\).</li><li>Let \(d_{1}, \ldots, d_{m}\) be the dimensions of the corresponding generalized eigenspaces \(G\left(\lambda_{1}, T\right), \ldots, G\left(\lambda_{m}, T\right)\).&nbsp;<br></li><li>For each j,&nbsp;\(\left.\left(T-\lambda_j I\right)\right|_{G\left(\lambda_j, T\right)}\) is nilpotent</li><li>Thus we have:</li><ol><li>&nbsp;\(\left.\left(T-\lambda_{j} I\right)^{d_{j}}\right|_{G\left(\lambda_{j}, T\right)}\) = \(0\)&nbsp;</li><li>Because nill potent operators to the power of the dimension are 0</li></ol><li>Every vector v can be written as the sum of vectors in {{c1::generalized eigenspaces}}, thus to prove that&nbsp;\(q(T)\) = 0 we need to show that&nbsp;{{c2::\(\left.q(T)\right|_{G\left(\lambda_j, T\right)}\)}} = {{c3::\(0\)}} for each j</li><li>Since the characteristic polynomial is defined as&nbsp;</li><ol><li>\(q(T)\) = {{c4::\(\left(T-\lambda_{1} I\right)^{d_{1} } \cdots\left(T-\lambda_{m} I\right)^{d_{m} } .\)}}<br></li></ol><li>We can {{c5::match the component corresponding to&nbsp;\(\lambda_j\)}} to vectors from&nbsp;{{c1::\(G(\lambda_j,T)\).}}</li><li>Because {{c2::\(\left.\left(T-\lambda_{j} I\right)^{d_{j} }\right|_{G\left(\lambda_{j}, T\right) }\)}} = {{c3::\(0\)}}, we conclude that {{c5::\(\left.q(T)\right|_{G\left(\lambda_{j}, T\right)}\)}} = {{c3::\(0\)}}, as desired.</li></ol>

============================================================

  

    <img src="paste-e1fc3bcece69a82f3536472bbc5ebb238a8984e0.jpg"><br><span style="color: rgb(0, 0, 0);">Proof:</span><br><ol><li>Let \(\lambda_{1}, \ldots, \lambda_{m}\) be the distinct eigenvalues of the operator \(T\).</li><li>Let \(d_{1}, \ldots, d_{m}\) be the dimensions of the corresponding generalized eigenspaces \(G\left(\lambda_{1}, T\right), \ldots, G\left(\lambda_{m}, T\right)\).&nbsp;<br></li><li>For each j,&nbsp;\(\left.\left(T-\lambda_j I\right)\right|_{G\left(\lambda_j, T\right)}\) is nilpotent</li><li>Thus we have:</li><ol><li>&nbsp;\(\left.\left(T-\lambda_{j} I\right)^{d_{j}}\right|_{G\left(\lambda_{j}, T\right)}\) = \(0\)&nbsp;</li><li>Because nill potent operators to the power of the dimension are 0</li></ol><li>Every vector v can be written as the sum of vectors in {{c1::generalized eigenspaces}}, thus to prove that&nbsp;\(q(T)\) = 0 we need to show that&nbsp;{{c2::\(\left.q(T)\right|_{G\left(\lambda_j, T\right)}\)}} = {{c3::\(0\)}} for each j</li><li>Since the characteristic polynomial is defined as&nbsp;</li><ol><li>\(q(T)\) = {{c4::\(\left(T-\lambda_{1} I\right)^{d_{1} } \cdots\left(T-\lambda_{m} I\right)^{d_{m} } .\)}}<br></li></ol><li>We can {{c5::match the component corresponding to&nbsp;\(\lambda_j\)}} to vectors from&nbsp;{{c1::\(G(\lambda_j,T)\).}}</li><li>Because {{c2::\(\left.\left(T-\lambda_{j} I\right)^{d_{j} }\right|_{G\left(\lambda_{j}, T\right) }\)}} = {{c3::\(0\)}}, we conclude that {{c5::\(\left.q(T)\right|_{G\left(\lambda_{j}, T\right)}\)}} = {{c3::\(0\)}}, as desired.</li></ol>

============================================================

  

    Definition {{c1::monic}} {{c3::polynomial}}<br><br>A {{c1::monic}} {{c3::polynomial}} is a {{c3::polynomial}} whose {{c5::{{c4::highest-degree coefficient}}}} equals {{c5::{{c2::1}}}} .

============================================================

  

    {{c1::Minimal}} polynomial<br><br>Suppose \(T \in \mathcal{L}(V)\). Then there is a unique {{c3::monic}} polynomial \(p\) {{c5::{{c4::of smallest degree}}}} such that \(p(T)\) = {{c5::{{c2::\(0\).}}}}

============================================================

  

    <img src="paste-622c8f0ee70dfe028246bca066e500d732fed2c0.jpg"><br>Proof broad steps:<br><ol><li>Construct a list in L(V,V), since dim L(V,V) = {{c1::\(n^2\)}} we can construct a list which is {{c2::linearly dependent}} as {{c3::\(<br>I, T, T^{2}, \ldots, T^{n^{2} } <br>\)}}&nbsp;because {{c1::the list is longer than the dimension the of the space}}</li><li>Let m be the {{c4::smallest positive integer}} such that {{c5::the list&nbsp;\(I, T, T^2, \ldots, T^m\)}} is {{c2::linearly dependent}}</li></ol>

============================================================

  

    <img src="paste-622c8f0ee70dfe028246bca066e500d732fed2c0.jpg"><br>Proof broad steps:<br><ol><li>Construct a list in L(V,V), since dim L(V,V) = \(n^2\) we can construct a list which is linearly dependent as \(<br>I, T, T^{2}, \ldots, T^{n^{2}}<br>\) because the list is longer than the dimension the of the space</li><li>Let m be the smallest positive integer such that the list&nbsp;\(I, T, T^2, \ldots, T^m\) is linearly dependent</li><li>Thus {{c1::one of the powers of T}} in the list is a {{c2::linear combination of the other ones}} by the {{c1::linear dependence lema}}</li><li>Because {{c3::m was chosen as the smallest positive integer}} such that {{c4::the list is lin dep}}, we can conclude&nbsp;{{c5::\(T^m\)}} is {{c2::a linear combination of the rest of the list}}</li></ol>

============================================================

  

    <img src="paste-622c8f0ee70dfe028246bca066e500d732fed2c0.jpg"><br>Proof broad steps:<br><ol><li>Construct a list in L(V,V), since dim L(V,V) = \(n^2\) we can construct a list which is linearly dependent as \(<br>I, T, T^{2}, \ldots, T^{n^{2}}<br>\) because the list is longer than the dimension the of the space</li><li>Let m be the smallest positive integer such that the list&nbsp;\(I, T, T^2, \ldots, T^m\) is linearly dependent</li><li>Thus one of the powers of T in the list is a linear combination of the other ones by the linear dependence lema</li><li>Because m was chosen as the smallest positive integer such that the list is lin dep, we can conclude&nbsp;\(T^m\) is a linear combination of the rest of the list</li><li>Thus there exist scalars such tthat</li><ol><li>{{c1::\(\sum_{i=0}^{m-1}\)}} {{c2::\(a_i T^{i}\)}} +&nbsp;{{c3::\(T^m\)}} = {{c4::0}}<br></li></ol><li>Define a monic polynomial p by:</li><ol><li>\(p(z)\) = {{c1::\(\sum_{i=0}^{m-1}\)}}&nbsp;{{c2::\(a_i z^{i}\)}} +&nbsp;{{c3::\(z^m\)}}<br></li><li>p(T) = {{c4::0}} by {{c4::the above}}</li></ol><li>Since {{c5::m is the smallest degree}}, the polynomial is {{c5::unique}}</li></ol>

============================================================

  

    Definition {{c1::minimal}} polynomial<br><br>Suppose \(T \in \mathcal{L}(V)\). Then the {{c1::minimal}} polynomial of \(T\) is the {{c3::unique}} {{c4::monic}} polynomial \(p\) {{c5::of smallest degree}} such that \(p(T)\) = {{c2::\(0\).}}

============================================================

  

    &nbsp;The Cayley-Hamilton Theorem (8.37) tells us that the {{c4::minimal}} polynomial of each operator on \(V\) has {{c2::degree}} {{c5::{{c3::at most}}}} {{c5::{{c1::\(\operatorname{dim} V\).}}}}&nbsp;

============================================================

  

    Suppose you are given the matrix (with respect to some basis) of an operator \(T \in \mathcal{L}(V)\). You could program a computer to find the {{c1::minimal polynomial}} of \(T\) as follows: Consider the system of linear equations<br><br><ol><li>\( \sum_{i=0}^{m-1}\){{c3::&nbsp;\(a_i M(T^i)\)}} = {{c4::\(-\mathcal{M}(T)^{m} \)}}</li><li>This system has a solution in the form of the {{c5::values of the coefficients.}}</li><li>The {{c5::coefficient}} will then be {{c5::the coefficients of the}} {{c1::minimal polynomial of T}}</li><li>This can be computed using a process like {{c2::Gaussian Elimination}}</li></ol>

============================================================

  

    {{c1::\(q(T)\)}} = {{c2::\(0\)}} implies {{c1::\(q\)}} is a {{c5::{{c4::multiple}}}} of {{c5::{{c3::the minimal polynomial}}}}<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(q \in \mathcal{P}(\mathbf{F})\). Then {{c1::\(q(T)\)}} = {{c2::\(0\)}} if and only if \(q\) is a {{c5::{{c4::polynomial multiple}}}} of {{c5::{{c3::the minimal polynomial of \(T\).}}}}

============================================================

  

    <img src="paste-87a16b8c0a0a94ae4ff1a161bd2d53bfb9533191.jpg"><br>Proof:<br><ol><li>First direction is easy, if q is a multiple then q(T) = {{c1::p(T) s(T)}} = {{c1::0 s(T)}} = {{c1::0}}</li><li>The other direction:</li><ol><li>Decompose</li><ol><li>\(q\) = {{c2::\(ps\)}} + {{c2::\(r\)&nbsp;}}</li><li>and {{c2::\(deg r\)}} &lt; {{c2::\(deg p\)}} by {{c2::division algorithm}}</li></ol><li>{{c1::\(0\)}} = \(q(T)\) = {{c3::\(p(T) s(T)\)}}+{{c4::\(r(T)\)}}={{c4::\(r(T) .\)}}<br></li><li>Since {{c5::dividing r}} by {{c5::its highest-degree coefficient}} would produce a {{c5::smaller minimal polynomial}}, r must be {{c1::0}}</li><li>Thus \(q\) = \(ps\) and q is a polynomial multiple of p</li></ol></ol>

============================================================

  

    {{c1::Characteristic}} {{c4::polynomial}} is a {{c5::{{c3::multiple}}}} of {{c5::{{c2::minimal}}}} {{c4::polynomial}}<br><br>Suppose \(\mathbf{F}=\mathbf{C}\) and \(T \in \mathcal{L}(V)\). Then the {{c1::characteristic}} {{c4::polynomial}} of \(T\) is a {{c4::polynomial}} {{c3::multiple}} of the {{c2::minimal}} {{c4::polynomial}} of \(T\).

============================================================

  

    {{c1::Eigenvalues}} are the {{c2::zeros}} of the {{c5::{{c3::minimal}}}} {{c5::{{c4::polynomial}}}}<br><br>Let \(T \in \mathcal{L}(V)\). Then the {{c2::zeros}} of the {{c3::minimal}} {{c4::polynomial}} of \(T\) are precisely the {{c1::eigenvalues of \(T\).}}

============================================================

  

    Although the {{c2::minimal}} and {{c3::characteristic}} polynomials both have the {{c4::eigenvalues}} as their {{c5::zeroes}}, their {{c1::multiplicities}} may differ

============================================================

  

    <img src="paste-ffefb39e579ba840ec32457ebf2aa829f125a9a8.jpg"><br>Proof:<br><ul><li>Let p be the minimal polynomial of T</li><ul><li>First suppose \(\lambda \in \mathbf{F}\) is a {{c1::zero of \(p\)}}. Then \(p\) can be written in the form</li><ul><li>\(p(z)\) = {{c2::\((z-\lambda)\)}} {{c3::\(q(z)\)}}</li><li>where {{c3::\(q\)}} is a {{c4::monic}} polynomial with coefficients in \(\mathbf{F}\) (see 4.11).&nbsp;</li></ul><li>Because \(p(T)=0\), we have</li><ul><li>\(0\) ={{c2::\((T-\lambda I)\)}} {{c3::\((q(T) v)\)}} for al v</li></ul><li>Because the {{c5::degree}} of q is {{c5::less than the minimal polynomial}}, there must exist a vector such that&nbsp;\(q(T)\)&nbsp;\(v\)&nbsp;\{{c5::(\neq\)&nbsp;\(0\)&nbsp;}}</li><ul><li>This implies that&nbsp;\(\lambda \) is an {{c5::eigenvalue of T}}</li></ul></ul></ul><br>

============================================================

  

    <img src="paste-ffefb39e579ba840ec32457ebf2aa829f125a9a8.jpg"><br>Proof:<br><ul><li>Let p be the minimal polynomial of T</li><li>First suppose \(\lambda \in \mathbf{F}\) is a zero of \(p\). Then \(p\) can be written in the form</li><ul><li>\(p(z)\) = \((z-\lambda)\) \(q(z)\)</li><li>where \(q\) is a monic polynomial with coefficients in \(\mathbf{F}\) (see 4.11).&nbsp;</li></ul><li>Because \(p(T)=0\), we have</li><ul><li>\(0\) =\((T-\lambda I)\) \((q(T) v)\) for al v</li></ul><li>Because the degree of q is less than the minimal polynomial, there must exist a vector such that&nbsp;\(q(T)\)&nbsp;\(v\)&nbsp;\(\neq\)&nbsp;\(0\)&nbsp;</li><ul><li>This implies that&nbsp;\(\lambda \) is an eigenvalue of T</li></ul><li>Conversely, if&nbsp;\(\lambda\) is an eigenvalue of T with&nbsp;\(v\)&nbsp; \(\neq 0\) then{{c2::&nbsp;\(T^{j}\)}}&nbsp;\(v\) =&nbsp;{{c3::\(\lambda^j\)}}&nbsp;\(v\)</li><ul><li>Thus:</li><ul><li>0 =&nbsp;</li><li>= {{c4::\(p(T)\)}}&nbsp;\(v\)&nbsp;&nbsp;</li><li>= {{c5::(\(\sum_{i=0}^{m-1}\)&nbsp;\(a_i T^{i}\) +&nbsp;\(T^m\))}}&nbsp;\(v\)&nbsp;</li><li>= {{c5::(\(\sum_{i=0}^{m-1}\)&nbsp;\(a_i \lambda^{i}\) +&nbsp;\(\lambda^m\))}}&nbsp;\(v\)</li><li>=&nbsp;{{c1::\(p(\lambda)\)}} \(v\).</li></ul><li>Because \(v \neq 0\), the equation above implies that {{c1::\(p(\lambda)\)}} = \(0\), as desired.</li></ul><li></li></ul>

============================================================

  

    1 Suppose \(T \in \mathcal{L}\left(\mathbf{C}^{4}\right)\) is such that the eigenvalues of \(T\) are {{c5::3}}, {{c5::5}}, {{c5::8}}. Prove that {{c1::\((T-3 I)^{2}\)}} {{c2::\((T-5 I)^{2}\)}} {{c3::\((T-8 I)^{2}\)}} = {{c4::\(0\).}}

============================================================

  

    Suppose \(V\) is a complex vector space. Suppose \(T \in \mathcal{L}(V)\) is such that {{c3::\(P^{2}\)}} = {{c3::\(P\)}}. <br><br>Prove that the characteristic polynomial of \(P\) is {{c1::\(z^{m}\)}}{{c2::\((z-1)^{n}\)}}, where {{c5::\(m\)}}=\(\operatorname{dim}\) {{c4::null \(P\)}} and {{c5::\(n\)}}=\(\operatorname{dim}\) {{c4::range \(P\)}}.

============================================================

  

    8. Suppose \(T \in \mathcal{L}(V)\). Prove that \(T\) is {{c2::invertible}} if and only if {{c3::the constant term}} in the {{c5::{{c4::minimal}}}} polynomial of \(T\) is {{c5::{{c1::nonzero}}}}.

============================================================

  

    10 Suppose \(V\) is a complex vector space and \(T \in \mathcal{L}(V)\) is invertible. Let \(p\) denote the characteristic polynomial of \(T\) and let \(q\) denote the characteristic polynomial of {{c4::\(T^{-1}\)}}. Prove that<br><br><ul><li>\(q(z)\)= {{c1::\(\frac{1}{p(0)}\)}} {{c5::{{c3::z}}}}&nbsp;{{c5::{{c2::\(^{\operatorname{dim} V}\)::the power?}}::the power?}} {{c5::{{c3::\(p\left(\frac{1}{z}\right)\)}}}}</li></ul><br>for all {{c4::nonzero}} \(z \in \mathbf{C}\).<br>

============================================================

  

    11 Suppose \(T \in \mathcal{L}(V)\) is {{c5::{{c3::invertible}}}}. Prove that there exists a polynomial \(p \in \mathcal{P}(\mathbf{F})\) such that {{c4::{{c2::\(T^{-1}\)}}}}= {{c4::{{c1::\(p(T)\)}}}}.

============================================================

  

    12 Suppose \(V\) is a {{c4::complex}} vector space and \(T \in \mathcal{L}(V)\). Prove that \(V\) has {{c2::a basis}} consisting of {{c2::eigenvectors of \(T\)}} if and only if the {{c5::{{c3::minimal}}}} polynomial of \(T\) has {{c5::{{c1::no repeated zeros::property}}::property}}.

============================================================

  

    Suppose \(V\) is an inner product space and \(T \in \mathcal{L}(V)\) is {{c5::{{c2::normal}}}}. Prove that the {{c4::{{c3::minimal}}}} polynomial of \(T\) has no {{c4::{{c1::repeated zeros.}}}}

============================================================

  

    14 Suppose \(V\) is a complex inner product space and \(S \in \mathcal{L}(V)\) is an {{c2::isometry}}. Prove that the {{c4::constant term}} in the {{c5::characteristic}} polynomial of \(S\) has {{c3::absolute value}} {{c1::1}} .

============================================================

  

    15 Suppose \(T \in \mathcal{L}(V)\) and \(v \in V\).<ul><li>(a) Prove that there exists a {{c4::unique}} {{c3::monic}} polynomial \(p\) of smallest degree such that \(p(T)\) \(v\) = {{c5::{{c1::\(0\)}}}}.</li><li>(b) Prove that \(p\) {{c5::{{c2::divides}}}} the {{c3::minimal}}&nbsp;polynomial of \(T\).</li></ul>

============================================================

  

    16 Suppose \(V\) is an inner product space and \(T \in \mathcal{L}(V)\). Suppose<br><br><ul><li>\(\sum_{i=0}^{m-1}\)&nbsp;{{c2::\(a_i\)}} \(z^i\) +&nbsp;{{c4::\(z^m\)}}<br></li><li>is the {{c5::minimal}} polynomial of \(T\).&nbsp;</li></ul>Prove that<br><br><ul><li>\(\sum_{i=0}^{m-1}\)&nbsp;{{c3::\(\overline{a_i}\)}} \(z^i\) +&nbsp;{{c4::\(z^m\)}}</li><li>is the {{c5::minimal}} polynomial of {{c1::\(T^{*}\)}}.</li></ul>

============================================================

  

    Suppose \(\mathbf{F}\) = {{c5::\(\mathbf{C}\)}} and \(T \in \mathcal{L}(V)\). <br><ul><li>Suppose the {{c1::minimal}} polynomial of \(T\) has degree {{c3::\(\operatorname{dim} V\).&nbsp;}}</li><li>Prove that the {{c2::characteristic}} polynomial of \(T\) {{c4::equals}} the {{c1::minimal}} polynomial of \(T\).</li></ul><br>

============================================================

  

    19 Suppose \(V\) is a {{c4::complex}} vector space and \(T \in \mathcal{L}(V)\). <br><br><ul><li>Suppose that with respect to some basis of \(V\) the matrix of \(T\) is at-least {{c3::upper triangular}}, with {{c1::\(\lambda_{1}, \ldots, \lambda_{n}\)}} {{c5::on the diagonal of this matrix.}}</li><li>Prove that the {{c2::characteristic}} polynomial of \(T\) is {{c1::\(\left(z-\lambda_{1}\right) \cdots\left(z-\lambda_{n}\right)\).}}</li></ul>

============================================================

  

    20 <br><br><ul><li>Suppose \(V\) is a complex vector space and \(V_{1}, \ldots, V_{m}\) are nonzero subspaces of \(V\) such that:</li><ul><li>&nbsp;\(V\)= {{c4::\(V_{1} \oplus \cdots \oplus V_{m}\).&nbsp;}}</li></ul><li>Suppose \(T \in \mathcal{L}(V)\) and each {{c4::\(V_{j}\)}} is {{c5::invariant under \(T\).}}&nbsp;</li><li>For each \(j\), let \(p_{j}\) denote the {{c2::characteristic}} polynomial of {{c3::\(\left.T\right|_{V_{j} }\).}}&nbsp;</li><li>Prove that the {{c2::characteristic}} polynomial of \(T\) equals {{c1::\(p_{1} \cdots p_{m}\).}}</li></ul>

============================================================

  

    Why is avoiding determinants when defining eigenvalues not a problem from a computational perspective?<br><ul><li>Determinants are a {{c5::{{c2::highly}}}} {{c4::{{c1::inefficient}}}} way to {{c4::{{c3::compute}}}} eigenvalues</li></ul>

============================================================

  

    <img src="paste-cf0cefa25f25833e1845a70954b0a91d7fded35f.jpg"><br><br>Proof:<br><ul><li>Let&nbsp;\(\lambda_i\) be the distinct eigenvalues of T</li><li>Let&nbsp;\(d_i\) be the dimensions of their corresponding generalized eigenspaces&nbsp;\(G(\lambda_i, T)\)</li><li>Each {{c1::\((T-\lambda_j I)\)}}{{c3::&nbsp;\(|_{G\left(\lambda_j, T\right)}\)}} is {{c2::nilpotent}} and:<br></li><ul><li>{{c1::\( (T-\lambda_j I)\)}} {{c3::\(|_{G\left(\lambda_j, T\right)}\)}} = {{c4::0}}<br></li></ul><li>V =&nbsp;{{c5::\(\bigoplus\)}} {{c3::\(G(\lambda_i,T)\)}}</li></ul>

============================================================

  

    <img src="paste-cf0cefa25f25833e1845a70954b0a91d7fded35f.jpg" style="width: 793.987px;"><br><br>Proof:<br><ul><li>Let&nbsp;\(\lambda_i\) be the distinct eigenvalues of T</li><li>Let&nbsp;\(d_i\) be the dimensions of their corresponding generalized eigenspaces&nbsp;\(G(\lambda_i, T)\)</li><li>Each \(\left.\left(T-\lambda_j I\right)\right|_{G\left(\lambda_j, T\right)}\) is nilpotent and:<br></li><ul><li>\(\left.\left(T-\lambda_j I\right)\right|_{G\left(\lambda_j, T\right)}^{d_j}\) = 0<br></li></ul><li>V =&nbsp;\(\bigoplus G(\lambda_i,T)\)</li><li>Thus we only need to show that&nbsp;\(q(T)\){{c1::\(|_{G\left(\lambda_j, T\right)}\)}} = {{c2::\(0\)}} for each \(j\).</li><li>We have</li><ul><li>\(q(T)\) =&nbsp;\(\prod_i\) {{c5::{{c3::\((T-\lambda_i)^{d_i}\)}}}}<br></li></ul><li>If we fix a j, we can comute the equation above so{{c5::{{c4::&nbsp;\(\left(T-\lambda_j I\right)^{d_j}\)}}}} is the last term on the RHS</li><li>Because :</li><ul><li>{{c5::{{c4::\((T-\lambda_j I)^{d_j}\)}}}} {{c1::\(|_{G\left(\lambda_j, T\right)}\)}} = {{c2::\(0\)}}</li></ul><li>We conclude that&nbsp;</li><ul><li>\(q(T)\) {{c1::\(|_{G\left(\lambda_j, T\right)}\)}} = {{c2::\(0\)}}</li></ul><li>As desired.</li></ul>

============================================================

  

    <img src="paste-64cacea41c038d23efa347a13437b5661ba6ef19.jpg"><br>Proof <br>Let \(n=\operatorname{dim} V\). The list<br>\[<br>I, T, T^2, \ldots, T^{n^2}<br>\]<br>is not linearly independent in \(\mathcal{L}(V)\), because \(\mathcal{L}(V)\) has dimension \(n^2\) and the list has length \(n^2+1\). Let \(m\) be the smallest positive integer such that<br>\[<br>I, T, T^2, \ldots, T^m<br>\]<br>is linearly dependent.<br><ul><li>The {{c2::Linear Dependence}} Lemma implies that \(T^m\) is a {{c1::linear combination of \(I, T, T^2, \ldots, T^{m-1}\).&nbsp;}}<br></li><li>Thus there exist {{c3::scalars \(a_0, a_1, a_2, \ldots, a_{m-1} \in \mathbf{F}\)}} such that {{c3::\(a_0 I+a_1 T+a_2 T^2+\cdots+a_{m-1} T^{m-1}+T^m\)}} = {{c4::\(0\).&nbsp;}}</li><li>Define a monic polynomial \(p \in \mathcal{P}(\mathbf{F})\) by:</li><ul><li>&nbsp;\(p(z)\) = {{c3::\(a_0+a_1 z+a_2 z^2+\cdots+a_{m-1} z^{m-1}+z^m\).&nbsp;}}</li></ul><li>Then \(p(T)\) = {{c4::\(0\).&nbsp;}}<br></li><li>No monic polynomial \(q \in \mathcal{P}(\mathbf{F})\) with {{c5::degree smaller than \(m\)}} can satisfy \(q(T)\) = {{c4::\(0\).&nbsp;}} because {{c5::we choose m to be the smallest degree}}</li></ul>

============================================================

  

    <img src="paste-3b865119ee888f885cd647f0b11e3ac260ea92cc.jpg"><br>Proof Let \(p\) denote the minimal polynomial of \(T\).<br><br>Suppose \(q\) is a polynomial multiple of \(p\). Thus there exists a polynomial \(s \in \mathcal{P}(\mathbf{F})\) such that \(q\) = {{c4::\(p s\).}} We have<br><ul><li>\(q(T)\) = {{c1::\(p(T)\)}} {{c5::{{c2::\(s(T)\)}}}}={{c5::{{c3::\(0,\)}}}}</li></ul>as desired.<br>

============================================================

  

    <img src="paste-0d7e8c1725ccdad8afdeb991e0554f8aa428adfd.jpg"><br>To prove the other direction, now suppose \(q(T)\) = {{c1::\(0\).}} By the Division Algorithm for Polynomials, there exist polynomials \(s, r \in \mathcal{P}(\mathbf{F})\) such that<br><ul><li>\(q\) = {{c2::\(p s\)}} +{{c5::{{c3::\(r\)}}}}</li></ul>and \(\operatorname{deg} r&lt;\operatorname{deg} p\). We have<br><br><ul><li>{{c1::0}} =&nbsp;\(q(T)\)</li><li>={{c2::&nbsp;\(p(T) s(T)\)}} +{{c5::{{c3::&nbsp;\(r(T)\)}}}}</li><li>= {{c5::{{c4::\(r(T)\)}}}}</li></ul><br>The equation above implies that {{c5::{{c4::\(r\)}}}} = {{c1::\(0\).}}<br>

============================================================

  

    <ul><li>If you choose an operator {{c1::at random}} it is extremely likely the {{c2::characteristic}} polynomial and {{c5::{{c3::minimal}}}} polynomial are the same</li><li>Thus the easiest way to find the {{c2::characteristic}} polynomial is usually to find the {{c3::minimal}} polynomial</li><li>If the {{c3::minimal}} polynomial has {{c5::{{c4::degree dim V}}}} then we are done, otherwise search by other means</li></ul>

============================================================

  

    <img src="paste-f9fa7e823504027c64a83fb58982269824641558.jpg"><br>Proof:<br><ul><li>Let p be the minimal polynomial of T<br></li><li>First: suppose&nbsp;\(\lambda \in F\) is a {{c1::zero of p}}</li><ul><li>Then we can write p in the form</li><ul><li>p(z) =&nbsp;{{c2::\((z-\lambda)\)}}&nbsp;{{c3::\(q(z)\)}}</li><li>where q is a {{c4::monic}} polynomial</li></ul><li>For every v</li><ul><li>0 = p(T) v</li><li>=&nbsp;{{c2::\((T-\lambda I)\)}} {{c3::\(q(T)\)}} \(v\)</li></ul><li>Because the degree of \(q\) is less than the degree of the minimal polynomial \(p\), there exists at least one vector \(v \in V\) such that \(q(T) v\) {{c5::\(\neq\)}} \(0\) but {{c5::</li><li>&nbsp;\(T-\lambda I\) applied to v is 0}}</li><li>Thus&nbsp;\(\lambda\) is {{c5::an eigenvalue of T}}</li><li>Thus every {{c1::0 of p}} is {{c5::an eigenvalue of T}}</li></ul></ul>

============================================================

  

    <img src="paste-779c6b5fbf7a19d21c5ba2a62669e4885ece5984.jpg"><br>Proof:<br><ul><li>Let p be the minimal polynomial</li><li>Suppose&nbsp;\(\lambda \in F\) is an {{c1::eigenvalue of T}}</li><li>Thus there exists&nbsp;\(v\in V\) with&nbsp;\(v\) {{c1::\(\neq 0\)}} such that</li><ul><li>\(T\)&nbsp;\(v\) =&nbsp;{{c2::\(\lambda\)}}&nbsp;\(v\)<br></li></ul><li>Repeated applications of T to both sides show that</li><ul><li>\(T^j\)&nbsp;\(v\) =&nbsp;{{c3::\(\lambda^j\)}}&nbsp;\(v\)</li><li>for every nonnegative integer j</li></ul><li>Since p(T) is just taking linear combinations of that we can conclude that</li><ul><li>\(p(T)\)&nbsp;\(v\) =&nbsp;{{c4::\(p(\lambda)\)&nbsp;}}\(v\)<br></li></ul><li>Then:</li><ul><li>\(0\) =&nbsp;{{c5::\(p(T)\)}}&nbsp;\(v\) because {{c5::p is the minimal polynomial}}<br></li><li>=&nbsp;{{c4::\(p(\lambda)\)}}&nbsp;\(v\)</li></ul><li>Because&nbsp;\(v\)&nbsp;{{c1::\(\neq\)&nbsp;\(0\)}}, the equation above implies that&nbsp;\(p(\lambda)\) ={{c5::&nbsp;\(0\)}}</li><li>Thus we have shown that {{c1::every eigenvalue of T}} is a {{c5::zero of p}}</li></ul>

============================================================

  

    Basis corresponding to a {{c1::nilpotent}} operator<br><br>Suppose \(N \in \mathcal{L}(V)\) is {{c1::nilpotent}}. Then there exist vectors \(v_{1}, \ldots, v_{n} \in V\) and nonnegative integers \(m_{1}, \ldots, m_{n}\) such that<br><br><ul><li>(a) {{c2::\(N^{m_{1} } v_{1}, \ldots, N v_{1}, v_{1}, \ldots, N^{m_{n} } v_{n}, \ldots, N v_{n}, v_{n} \)}} is a basis of \(V\);</li><li>(b) {{c4::\(N^{m_{1}+1} v_{1}\)}} = {{c5::\(\cdots=N^{m_{n}+1} v_{n}\)}} = {{c3::\(0\).}}</li></ul>

============================================================

  

    <img src="paste-e6c465f8f4dddf39cd587c1c082ae6c53e083170.jpg"><br>Proof by induction:<br><ul><li>Obviously true for dim V = 1</li><li>Inductive case:</li><li>Because N is {{c1::nilpotent}} , N is not {{c2::injective}} and thus not {{c2::surjective}}, thus range N is a {{c3::strict subspace}} of V</li><li>Thus we can apply I.H to the {{c4::restriction operatotr&nbsp;\(\left.N\right|_{\text {range } N} \in \mathcal{L}(\) range \(N)\)}}.</li><li>Thus the following is a basis of range N:</li><ul><li>{{c5::\(N^{m_{1} } v_{1}, \ldots,\) \(N v_{1}, v_{1}, \ldots, \) \( N^{m_{n} } v_{n}, \ldots,\) \(N v_{n}, v_{n}\)}}<br></li></ul><li>And&nbsp;</li><ul><li>{{c5::\(N^{m_{1}+1} v_{1}\)}} = {{c5::\(\cdots\)}} ={{c5::\(N^{m_{n}+1} v_{n}\)}} = {{c5::\(0 .\)}}</li></ul></ul>

============================================================

  

    <img src="paste-e6c465f8f4dddf39cd587c1c082ae6c53e083170.jpg"><br>Proof by induction:<br><ul><li>Obviously true for dim V = 1</li><li>Inductive case:</li><li>Because N is nilpotent , N is not injective and thus not surjective, thus range N is a strict subspace of V</li><li>Thus we can apply I.H to the restriction operatotr&nbsp;\(\left.N\right|_{\text {range } N} \in \mathcal{L}(\) range \(N)\).</li><li>Thus the following is a basis of range N:</li><ul><li>\(N^{m_{1}} v_{1}, \ldots,\) \(N v_{1}, v_{1}, \ldots, \) \( N^{m_{n}} v_{n}, \ldots,\) \(N v_{n}, v_{n}\)<br></li></ul><li>And&nbsp;</li><ul><li>\(N^{m_{1}+1} v_{1}\) = \(\cdots\) =\(N^{m_{n}+1} v_{n}\) = \(0 .\)<br></li></ul><li>Because each&nbsp;\(v_j\) is in range N, there exists&nbsp;\(u_j\) such that&nbsp;\(v_j\)&nbsp; =&nbsp;&nbsp;\(N\)&nbsp;\(u_j\) which means we can claim the following basis to be linearly independent in V:</li><ul><li>{{c1::\(N^{m_{1}+1} u_{1}, \ldots,\) \(N u_{1}, u_{1}, \ldots,\)\( N^{m_{n}+1} u_{n}, \ldots,\)\( N u_{n}, u_{n}\)}}</li></ul><li>Which can be proven by contradiction, since if it were to equal 0 and we applied N to it we would get {{c2::the original list}} which we know is {{c2::linearly independent}}</li><ul><li>Thus the only vectors which could have non-zero coefficients are:</li><ul><li>{{c3::\[<br>N^{m_{1}+1} u_{1}, \ldots, N^{m_{n}+1} u_{n},<br>\]}}<br></li></ul><li>Which equal:</li><ul><li>{{c4::\[<br>N^{m_{1} } v_{1}, \ldots, N^{m_{n} } v_{n}<br>\]}}<br></li></ul><li>Again, since the {{c2::original list}} was {{c2::linearly indepedent}}, these must have coefficients {{c5::0}}</li></ul><li>We can extend the u-list to a basis by concatenating basis vectors</li><ul><li>\(N^{m_{1}+1} u_{1}, \ldots,\) \(N u_{1}, u_{1}, \ldots,\)\( N^{m_{n}+1} u_{n}, \ldots,\) \(N u_{n}, u_{n}, \)\(w_{1}, \ldots, w_{p}\)</li></ul><li>Applying N to any of the new vectors would result in a vector in range N and thus in the span of the riginal list.&nbsp;</li><li>Since each vector in the original list can be written based on u's, there exists&nbsp;\(x_j\) in the span of the u-list such that&nbsp;\(N w_j\) =&nbsp;\(N x_j\).</li><li>Let: \(u_{n+j}\) = \(w_{j}-x_{j} .\)</li><li>Then&nbsp;\(N u_{n+j}\) = \(0\), furthermore the list:</li><ul><li>\[<br>N^{m_{1}+1} u_{1}, \ldots, N u_{1}, u_{1}, \ldots, N^{m_{n}+1} u_{n}, \ldots, N u_{n}, u_{n}, u_{n+1}, \ldots, u_{n+p}<br>\]&nbsp;</li><li>spans V becuase its span contains each&nbsp;\(x_j\), and each&nbsp;\(u_{n+j}\) and hence each&nbsp;\(w_j\)</li></ul><li>This is the basis of the entire space we were looking for</li></ul>

============================================================

  

    <img src="paste-e6c465f8f4dddf39cd587c1c082ae6c53e083170.jpg"><br>Proof by induction:<br><ul><li>Obviously true for dim V = 1</li><li>Inductive case:</li><li>Because N is nilpotent , N is not injective and thus not surjective, thus range N is a strict subspace of V</li><li>Thus we can apply I.H to the restriction operatotr&nbsp;\(\left.N\right|_{\text {range } N} \in \mathcal{L}(\) range \(N)\).</li><li>Thus the following is a basis of range N:</li><ul><li>\(N^{m_{1}} v_{1}, \ldots,\) \(N v_{1}, v_{1}, \ldots, \) \( N^{m_{n}} v_{n}, \ldots,\) \(N v_{n}, v_{n}\)<br></li></ul><li>And&nbsp;</li><ul><li>\(N^{m_{1}+1} v_{1}\) = \(\cdots\) =\(N^{m_{n}+1} v_{n}\) = \(0 .\)<br></li></ul><li>Because each&nbsp;\(v_j\) is in range N, there exists&nbsp;\(u_j\) such that&nbsp;\(v_j\)&nbsp; =&nbsp;&nbsp;\(N\)&nbsp;\(u_j\) which means we can claim the following basis to be linearly independent in V:</li><ul><li>\(N^{m_{1}+1} u_{1}, \ldots,\) \(N u_{1}, u_{1}, \ldots,\)\( N^{m_{n}+1} u_{n}, \ldots,\)\( N u_{n}, u_{n}\)</li></ul><li>Which can be proven by contradiction, since if it were to equal 0 and we applied N to it we would get the original list which we know is linearly independent</li><ul><li>Thus the only vectors which could have non-zero coefficients are:</li><ul><li>\[<br>N^{m_{1}+1} u_{1}, \ldots, N^{m_{n}+1} u_{n},<br>\]</li></ul><li>Which equal:</li><ul><li>\[<br>N^{m_{1}} v_{1}, \ldots, N^{m_{n}} v_{n}<br>\]<br></li></ul><li>Again, since the original list was linearly indepedent, these must have coefficients 0</li></ul><li>We can extend the u-list to a basis by concatenating basis vectors</li><ul><li>{{c1::\(N^{m_{1}+1} u_{1}, \ldots,\) \(N u_{1}, u_{1}, \ldots,\)\( N^{m_{n}+1} u_{n}, \ldots,\) \(N u_{n}, u_{n}, \)\(w_{1}, \ldots, w_{p}\)}}</li></ul><li>Applying N to any of the new vectors would result in a vector in {{c2::range N}} and thus in {{c3::the span of the riginal list.&nbsp;}}</li><li>Since each vector in the original list can be written based on u's, there exists&nbsp;\(x_j\) in the span of the u-list such that&nbsp;\(N w_j\) =&nbsp;{{c4::\(N x_j\).}}</li><li>Let: \(u_{n+j}\) = {{c5::\(w_{j}-x_{j} .\)}}</li></ul>

============================================================

  

    <img src="paste-e6c465f8f4dddf39cd587c1c082ae6c53e083170.jpg"><br>Proof by induction:<br><ul><li>Obviously true for dim V = 1</li><li>Inductive case:</li><li>Because N is nilpotent , N is not injective and thus not surjective, thus range N is a strict subspace of V</li><li>Thus we can apply I.H to the restriction operatotr&nbsp;\(\left.N\right|_{\text {range } N} \in \mathcal{L}(\) range \(N)\).</li><li>Thus the following is a basis of range N:</li><ul><li>\(N^{m_{1}} v_{1}, \ldots,\) \(N v_{1}, v_{1}, \ldots, \) \( N^{m_{n}} v_{n}, \ldots,\) \(N v_{n}, v_{n}\)<br></li></ul><li>And&nbsp;</li><ul><li>\(N^{m_{1}+1} v_{1}\) = \(\cdots\) =\(N^{m_{n}+1} v_{n}\) = \(0 .\)<br></li></ul><li>Because each&nbsp;\(v_j\) is in range N, there exists&nbsp;\(u_j\) such that&nbsp;\(v_j\)&nbsp; =&nbsp;&nbsp;\(N\)&nbsp;\(u_j\) which means we can claim the following basis to be linearly independent in V:</li><ul><li>\(N^{m_{1}+1} u_{1}, \ldots,\) \(N u_{1}, u_{1}, \ldots,\)\( N^{m_{n}+1} u_{n}, \ldots,\)\( N u_{n}, u_{n}\)</li></ul><li>Which can be proven by contradiction, since if it were to equal 0 and we applied N to it we would get the original list which we know is linearly independent</li><ul><li>Thus the only vectors which could have non-zero coefficients are:</li><ul><li>\[<br>N^{m_{1}+1} u_{1}, \ldots, N^{m_{n}+1} u_{n},<br>\]</li></ul><li>Which equal:</li><ul><li>\[<br>N^{m_{1}} v_{1}, \ldots, N^{m_{n}} v_{n}<br>\]<br></li></ul><li>Again, since the original list was linearly indepedent, these must have coefficients 0</li></ul><li>We can extend the u-list to a basis by concatenating basis vectors</li><ul><li>\(N^{m_{1}+1} u_{1}, \ldots,\) \(N u_{1}, u_{1}, \ldots,\)\( N^{m_{n}+1} u_{n}, \ldots,\) \(N u_{n}, u_{n}, \)\(w_{1}, \ldots, w_{p}\)</li></ul><li>Applying N to any of the new vectors would result in a vector in range N and thus in the span of the riginal list.&nbsp;</li><li>Since each vector in the original list can be written based on u's, there exists&nbsp;\(x_j\) in the span of the u-list such that&nbsp;\(N w_j\) =&nbsp;\(N x_j\).</li><li>Let: \(u_{n+j}\) = {{c1::\(w_{j}-x_{j} .\)}}</li><li>Then&nbsp;{{c2::\(N u_{n+j}\)}} = {{c5::\(0\)}}, furthermore the list:</li><ul><li>{{c3::\[<br>N^{m_{1}+1} u_{1}, \ldots, N u_{1}, u_{1}, \ldots, N^{m_{n}+1} u_{n}, \ldots, N u_{n}, u_{n}, u_{n+1}, \ldots, u_{n+p}<br>\]&nbsp;}}<br></li><li>spans V becuase {{c4::its span contains each&nbsp;\(x_j\),}} and {{c4::each&nbsp;\(u_{n+j}\)}} and hence each{{c4::&nbsp;\(w_j\)}}</li></ul><li>This is the basis of the entire space we were looking for</li></ul>

============================================================

  

    Definition {{c1::Jordan}} basis<br><br>Suppose \(T \in \mathcal{L}(V)\). A basis of \(V\) is called a {{c1::Jordan}} basis for \(T\) if with respect to this basis \(T\) has a {{c2::block diagonal}} matrix<br><br>{{c5::\[<br>\left(\begin{array}{ccc}<br>A_{1} &amp; &amp; 0 \\<br>&amp; \ddots &amp; \\<br>0 &amp; &amp; A_{p}<br>\end{array}\right),<br>\]}}<br><br>where each \(A_{j}\) is an {{c3::upper-triangular}} matrix of the form<br><br>{{c4::\[<br>A_{j}=\left(\begin{array}{cccc}<br>\lambda_{j} &amp; 1 &amp; &amp; 0 \\<br>&amp; \ddots &amp; \ddots &amp; \\<br>&amp; &amp; \ddots &amp; 1 \\<br>0 &amp; &amp; &amp; \lambda_{j}<br>\end{array}\right) .<br>\]}}

============================================================

  

    {{c5::{{c1::Jordan}}}} Form<br><br>Suppose \(V\) is a {{c4::{{c3::complex}}}} vector space. If \(T \in \mathcal{L}(V)\), then there is a {{c4::{{c2::basis}}}} of \(V\) that is a {{c1::Jordan}} {{c2::basis}} for \(T\).

============================================================

  

    <img src="paste-38f854533fe9c300346a9fd950b3594d316f8b04.jpg"><br>ProofL<br><br><ul><li>First consider a {{c1::nilpotent}} operator \(N \in \mathcal{L}(V)\) and the vectors \(v_{1}, \ldots, v_{n} \in V\) given by 8.55 .&nbsp;</li><ul><li>8.55: {{c2::<img src="paste-469e4737e5f98e9ac1a08cabf2103e7ef4c2a7e2.jpg">}}</li></ul><li>For each \(j\), note that \(N\) sends the first vector in the list \(N^{m_{j}} v_{j}, \ldots, N v_{j}, v_{j}\) to {{c3::0}} and that \(N\) sends {{c3::each vector in this list other than the first vector}} to {{c4::the previous vector.&nbsp;}}</li><li>In other words, 8.55 gives a basis of \(V\) with respect to which \(N\) has a block diagonal matrix, where each matrix on the diagonal has the form</li><ul><li>{{c5::\[\left(\begin{array}{cccc}0 &amp; 1 &amp; &amp; 0 \\&amp; \ddots &amp; \ddots &amp; \\&amp;&amp; \ddots &amp; 1 \\0 &amp; &amp; &amp; 0\end{array}\right) .\]}}</li></ul></ul>Thus the desired result holds for {{c1::nilpotent}} operators.

============================================================

  

    <img src="paste-38f854533fe9c300346a9fd950b3594d316f8b04.jpg"><br>ProofL<br><br><ul><li>First consider a nilpotent operator \(N \in \mathcal{L}(V)\) and the vectors \(v_{1}, \ldots, v_{n} \in V\) given by 8.55 .&nbsp;</li><ul><li>8.55:&nbsp;<img src="paste-469e4737e5f98e9ac1a08cabf2103e7ef4c2a7e2.jpg"></li></ul><li>For each \(j\), note that \(N\) sends the first vector in the list \(N^{m_{j}} v_{j}, \ldots, N v_{j}, v_{j}\) to 0 and that \(N\) sends each vector in this list other than the first vector to the previous vector.&nbsp;</li><li>In other words, 8.55 gives a basis of \(V\) with respect to which \(N\) has a block diagonal matrix, where each matrix on the diagonal has the form</li><ul><li>\[\left(\begin{array}{cccc}0 &amp; 1 &amp; &amp; 0 \\&amp; \ddots &amp; \ddots &amp; \\&amp;&amp; \ddots &amp; 1 \\0 &amp; &amp; &amp; 0\end{array}\right) .\]</li></ul></ul>Thus the desired result holds for nilpotent operators.<br><br><ul><li>Now suppose \(T \in \mathcal{L}(V)\).&nbsp;</li><li>Let \(\lambda_{1}, \ldots, \lambda_{m}\) be the distinct eigenvalues of \(T\). We have the {{c1::generalized eigenspace}} decomposition</li><ul><li>\(V\) = {{c1::\(G\left(\lambda_{1}, T\right) \oplus \cdots \oplus G\left(\lambda_{m}, T\right),\)}}</li></ul><li>where each {{c2::\(\left.\left(T-\lambda_{j} I\right)\right|_{G\left(\lambda_{j}, T\right)}\)}} is {{c3::nilpotent}} (see 8.21).&nbsp;</li><li>Thus some basis of each {{c1::\(G\left(\lambda_{j}, T\right)\)}} is a Jordan basis for {{c4::\(\left.\left(T-\lambda_{j} I\right)\right|_{G\left(\lambda_{m}, T\right)}\)}} (see previous paragraph).&nbsp;</li><li>{{c5::Put these bases together}} to get a basis of \(V\) that is a Jordan basis for \(T\).</li></ul>

============================================================

  

    3 Suppose \(N \in \mathcal{L}(V)\) is {{c5::nilpotent}}. Prove that the minimal polynomial of \(N\) is {{c1::\(z^{m+1}\)}}, where \(m\) is the length of the {{c2::longest consecutive string of 1 's}} that appears {{c3::on the line directly above the diagonal}} in the matrix of \(N\) with respect to any {{c4::Jordan}} basis for \(N\).

============================================================

  

    6 Suppose \(N \in \mathcal{L}(V)\) is {{c1::nilpotent}} and \(v_{1}, \ldots, v_{n}\) and \(m_{1}, \ldots, m_{n}\) are as in 8.55. Prove that \(N^{m_{1}} v_{1}, \ldots, N^{m_{n}} v_{n}\) is a {{c3::basis}} of {{c5::{{c2::null \(N\).}}}}<br><br>8.55: {{c5::{{c4::<img src="paste-7f1ce38852cbf39bd13eb87d8029d391f3b82597.jpg">}}}}

============================================================

  

    7 Suppose \(p, q \in \mathcal{P}(\mathbf{C})\) are {{c3::monic}} polynomials with {{c4::the same zeros}} and \(q\) is {{c5::a polynomial multiple of \(p\)}}. Prove that there exists \(T \in \mathcal{L}\) {{c5::\(\left(C^{\operatorname{deg} q}\right)\)}} such that the {{c1::characteristic}} polynomial of \(T\) is \(q\) and the {{c2::minimal}} polynomial of \(T\) is \(p\).

============================================================

  

    Suppose \(V\) is a {{c5::complex}} vector space and \(T \in \mathcal{L}(V)\). Prove that there does not exist a {{c4::direct sum}} decomposition of \(V\) into {{c5::two proper subspaces invariant under \(T\)}} if and only if the {{c1::minimal}} polynomial of \(T\) is of the form {{c2::\((z-\lambda)^{\operatorname{dim} V}\)}} for {{c3::some \(\lambda \in \mathbf{C}\).}}

============================================================

  

    <img src="paste-16ac95ba2f11b60a610de9c7d53062babf1be5a6.jpg"><br><img src="paste-f102893410e4bd563bc3aff300b2318a2a446864.jpg"><br>Proof:<br><ul><li>Let&nbsp;{{c3::\(\lambda_i\)}} be {{c2::the eigenvalues of T}}</li><li>Decompose V into {{c1::generalized eigenspaces&nbsp;}}</li><ul><li>\(V\) ={{c4::&nbsp;\(\bigoplus\)&nbsp;\(G(\lambda_i,T)\)}} because the vector space is {{c5::complex}}</li></ul></ul>

============================================================

  

    <img src="paste-16ac95ba2f11b60a610de9c7d53062babf1be5a6.jpg"><br><img src="paste-f102893410e4bd563bc3aff300b2318a2a446864.jpg"><br>Proof:<br><ul><li>Let&nbsp;\(\lambda_i\) be the eigenvalues of T</li><li>Decompose V into generalized eigenspaces&nbsp;</li><ul><li>\(V\) =&nbsp;\(\bigoplus\)&nbsp;\(G(\lambda_i,T)\) because the vector space is complex<br></li></ul><li>Where each&nbsp;{{c1::\((T-\lambda_j I)\)}} {{c3::\(|_{G(\lambda_j, T)}\)}} is {{c2::nilpotent}}</li><li>Thus some basis of each {{c4::generalized eigenspace}} is a Jordan basis for&nbsp;{{c1::&nbsp;\((T-\lambda_j I)\)}} {{c3::\(|_{G(\lambda_j, T)}\).}}</li><li>Thus {{c1::&nbsp;\((T-\lambda_j I)\)}} {{c3::\(|_{G(\lambda_j, T)}\).}}. has the form {{c5::<img src="paste-501e90f5a0cb38845eb8352dfe16597aeb9c46da.jpg">}}</li><li>Put the bases together to get a jordan basis for T</li></ul>

============================================================

  

    <ul><li>As we will soon see, a {{c3::real}} vector space \(V\) can be {{c1::embedded}}, in a natural way, in a {{c3::complex}} vector space called the {{c5::{{c2::complexification}}}} of \(V\).&nbsp;</li><li>Each operator on \(V\) can be {{c5::{{c4::extended}}}} to an {{c5::{{c4::operator}}}} on the {{c2::complexification}} of \(V\).&nbsp;</li></ul>

============================================================

  

    Definition {{c1::complexification}} of \(V\), {{c2::\(V_{\mathbf{C} }\)}}<br><br>Suppose \(V\) is a real vector space.<br><br><ul><li>The {{c1::complexification}} of \(V\), denoted {{c2::\(V_{\mathbf{C} }\), equals \(V \times V\)}}. An element of {{c5::\(V_{\mathbf{C}}}}\) is an {{c3::ordered pair \((u, v)\)}}, where {{c3::\(u, v \in V\)}}, but we will write this as {{c4::\(u+i v\).}}</li></ul>

============================================================

  

    Definition complexification of \(V\), \(V_{\mathbf{C}}\)<br><br>Suppose \(V\) is a real vector space.<br><br><ul><li>The complexification of \(V\), denoted \(V_{\mathbf{C}}\), equals \(V \times V\)}}. An element of \(V_{\mathbf{C}}\) is an ordered pair \((u, v)\), where \(u, v \in V\), but we will write this as \(u+i v\).</li><li>Addition on \(V_{\mathbf{C}}\) is defined by</li><ul><li>{{c3::\(\left(u_{1}+i v_{1}\right)+\left(u_{2}+i v_{2}\right)\)}} = {{c1::\(\left(u_{1}+u_{2}\right)+i\left(v_{1}+v_{2}\right)\)}}</li></ul><ul><li>for {{c5::\(u_{1}, v_{1}, u_{2}, v_{2} \in V\).}}</li></ul><li>Complex scalar multiplication on \(V_{\mathbf{C}}\) is defined by<br></li><ul><li>{{c4::\((a+b i)(u+i v)\)}} = {{c2::\((a u-b v)+i(a v+b u)\)}}</li><li>for {{c5::\(a, b \in \mathbf{R}\) and \(u, v \in V\).}}</li></ul></ul>

============================================================

  

    We think of \(V\) as a subset of {{c1::\(V_{\mathbf{C} }\)}} by identifying {{c3::\(u \in V\)}} with {{c5::{{c2::\(u+i 0\)}}}}. The construction of {{c1::\(V_{\mathbf{C} }\)}} from \(V\) can then be thought of as generalizing the construction of {{c5::{{c4::\(\mathbf{C}^{n}\)}}}} from {{c5::{{c4::\(\mathbf{R}^{n}\).}}}}

============================================================

  

    {{c2::&nbsp;\(V_{\mathbf{C} }\)}} is a {{c1::complex}} {{c4::vector space.}}<br><br>Suppose \(V\) is a {{c3::real}} vector space. Then with the definitions of {{c5::addition}} and {{c5::scalar multiplication}} as above, {{c2::\(V_{\mathbf{C} }\)}} is a {{c1::complex}} {{c2::vector space.}}

============================================================

  

    {{c1::Basis}} of \(V\) is {{c1::basis}} of {{c2::\(V_{\mathbf{C} }\)}}<br><br>Suppose \(V\) is a real vector space.<br><ul><li>(a) If \(v_{1}, \ldots, v_{n}\) is a {{c1::basis}} of \(V\) (as a {{c5::real}} vector space), then \(v_{1}, \ldots, v_{n}\) is a {{c1::basis}} of {{c2::\(V_{\mathbf{C} }\)}} (as a {{c5::complex}} vector space).<br></li><li>(b) The {{c4::dimension of}} {{c3::\(V_{\mathbf{C} }\)}} (as a {{c5::complex}} vector space) {{c4::equals the dimension of}} {{c3::\(V\)}} (as a {{c5::real}} vector space).</li></ul>

============================================================

  

    <img src="paste-42b412ec0898c668ed9c21738bc25da5b12648cb.jpg"><br>Proof:<br><ul><li>To prove (a), suppose \(v_{1}, \ldots, v_{n}\) is a basis of the real vector space \(V\).&nbsp;</li><li>Then {{c1::\(\operatorname{span}\left(v_{1}, \ldots, v_{n}\right)\)}} in the complex vector space \(V_{\mathbf{C}}\) contains {{c2::all the vectors \(v_{1}, \ldots, v_{n}, i v_{1}, \ldots, i v_{n}\)}}. Thus {{c1::\(v_{1}, \ldots, v_{n}\)}} {{c5::{{c3::spans}}}} {{c5::{{c4::the complex vector space \(V_{\mathbf{C} }\).}}}}</li></ul>

============================================================

  

    <img src="paste-2d07474ae11ae00257e7560eef98d3a972a46a78.jpg"><br>Proof:<br><ul><li>To show that \(v_{1}, \ldots, v_{n}\) is {{c5::linearly independent}} in the complex vector space \(V_{\mathbf{C}}\), suppose \(\lambda_{1}, \ldots, \lambda_{n} \in \mathbf{C}\) and<br></li><ul><li>{{c5::\(\sum\)}} {{c4::\(\lambda_i\)}} {{c5::\(v_i \)}} = {{c3::\( 0 \)}}</li></ul><li>Then the equation above and our definitions imply that:</li><ul><li>&nbsp;{{c5::\(\sum\)}} {{c1::\((\operatorname{Re}\lambda_i)\)}}&nbsp;{{c5::\(v_i\)}} = {{c3::\(0\)}}&nbsp;</li><li>and&nbsp;</li><li>{{c5::\(\sum\)}} {{c2::\((\operatorname{Im}\lambda_i)&nbsp;\)}}&nbsp;{{c5::\(v_i\)}}&nbsp;= {{c3::\(0\).}}</li></ul><li>Because \(v_{1}, \ldots, v_{n}\) is {{c5::linearly independent}} in \(V\), the equations above imply:</li><ul><li>{{c1::&nbsp;\(\operatorname{Re}\lambda_i\)}}&nbsp;=&nbsp;{{c3::\(0\)}} for {{c5::all i}}</li><li>and&nbsp;</li><li>{{c2::\(\operatorname{Im}\lambda_i\)}} =&nbsp;{{c3::\(0\)}} for all i</li></ul><li>Thus we have {{c4::\(\lambda_{1}=\cdots=\lambda_{n}\)}} = {{c3::\(0\)}}.&nbsp;</li><li>Hence \(v_{1}, \ldots, v_{n}\) is linearly independent in \(V_{\mathbf{C}}\), completing the proof of (a).<br></li></ul>

============================================================

  

    Definition {{c1::complexification}} of \(T\), {{c2::\(T_{\mathbf{C} }\)}}<br><br>Suppose \(V\) is a {{c5::real}} vector space and \(T \in \mathcal{L}(V)\). <br><br>The {{c1::complexification}} of \(T\), denoted {{c2::\(T_{\mathbf{C} }\)}}, is the operator {{c2::\(T_{\mathbf{C} }\)}} \(\in\) {{c5::\(\mathcal{L}\left(V_{\mathbf{C} }\right)\)}} defined by<br><br><ul><li>{{c2::\(T_{\mathbf{C} }\)}} \((\) {{c4::\(u+i v\)}} \()\)= {{c3::\(T u+i T v\)}}</li><li>for {{c4::\(u, v \in V\).}}<br></li></ul>

============================================================

  

    {{c3::Matrix}} of {{c1::\(T_{\mathbf{C} }\)}} equals {{c3::matrix}} of {{c5::{{c2::\(T\)}}}}<br><br>Suppose \(V\) is a {{c5::{{c4::real}}}} vector space with basis \(v_{1}, \ldots, v_{n}\) and \(T \in \mathcal{L}(V)\). <br><br>Then:<br><ul><li>{{c5::{{c2::&nbsp;\(\mathcal{M}(T)\)}}}} ={{c1::\(\mathcal{M}\left(T_{\mathbf{C} }\right)\)}},&nbsp;</li><li>where both {{c3::matrices}} are with respect to the basis \(v_{1}, \ldots, v_{n}\).</li></ul>

============================================================

  

    Every operator has an {{c4::invariant}} {{c5::subspace}} of {{c1::dimension}} {{c2::1}} or {{c3::2}}<br><br>Every operator on a {{c5::nonzero}} {{c5::finite}}-dimensional vector space has an {{c4::invariant}} {{c5::subspace}} of {{c1::dimension}} {{c2::1}} or {{c3::2}} .

============================================================

  

    <img src="paste-9eb0c523b8c723672c35b0444259780a6274209c.jpg"><br>Proof:<br>For real vector spaces (since we know the complex case is guaranteed to have an eigenvalue and a 1-dim invariant subspace):<br><ul><li>Hence assume \(V\) is a real vector space and \(T \in \mathcal{L}(V)\).&nbsp;</li><li>The {{c1::complexification}} {{c2::\(T_{\mathbf{C} }\)}} has an {{c5::{{c3::eigenvalue}}}} {{c5::{{c4::\(a+b i\)}}}} , where {{c5::{{c4::\(a, b \in \mathbf{R}\).&nbsp;}}}}</li></ul>

============================================================

  

    <img src="paste-9eb0c523b8c723672c35b0444259780a6274209c.jpg"><br>Proof:<br>For real vector spaces (since we know the complex case is guaranteed to have an eigenvalue and a 1-dim invariant subspace):<br><ul><li>Hence assume \(V\) is a real vector space and \(T \in \mathcal{L}(V)\).&nbsp;</li><li>The complexification \(T_{\mathbf{C}}\) has an eigenvalue \(a+b i\) (by 5.21), where \(a, b \in \mathbf{R}\).&nbsp;</li><li>Thus there exist \(u, v \in V\), not both 0 , such that:</li><ul><li>&nbsp;\(T_{\mathbf{C}}\) ( {{c2::\(u+i v\)}} )= {{c1::\((a+b i)\)}} {{c2::\((u+i v)\)}}</li></ul><li>Using the definition of \(T_{\mathbf{C}}\), the last equation can be rewritten as</li><ul><li>{{c4::\(T u\)}} + {{c5::\(i T v\)}}={{c3::\((a u-b v)\)}} + {{c3::\((a v+b u) i \)}}</li></ul><li>Thus</li><ul><li>{{c4::\(T u\)}} = {{c3::\(a u-b v\)&nbsp;}}</li><li>and&nbsp;</li><li>{{c5::\(T v\)}} = {{c3::\(a v+b u .\)}}</li></ul><li>Let \(U\) equal the span in \(V\) of the list {{c2::\(u, v\).}}&nbsp;</li><li>Then \(U\) is a subspace of \(V\) with dimension 1 or 2 .&nbsp;</li><li>The equations above show that \(U\) is {{c5::invariant under \(T\)}}, completing the proof.</li></ul>

============================================================

  

    Suppose \(V\) is a real vector space and \(T \in \mathcal{L}(V)\). Repeated application of the definition of \(T_{\mathbf{C}}\) shows that<br><br><ul><li><br></li><li>{{c1::\(\left(T_{\mathbf{C} }\right)^{n}\)}} \((\) {{c4::\(u+i v\)}} \()\) ={{c5::{{c2::\(T^{n} u\)}}}} + {{c5::{{c3::\(i T^{n} v\)}}}}</li></ul><br>for every positive integer \(n\) and all \(u, v \in V\).<br>

============================================================

  

    {{c3::Minimal}} polynomial of {{c1::\(T_{\mathbf{C} }\)}} {{c5::equals}} {{c3::minimal}} polynomial of {{c2::\(T\)}}<br><br>Suppose \(V\) is a {{c4::real}} vector space and \(T \in \mathcal{L}(V)\). Then the {{c3::minimal}} polynomial of {{c1::\(T_{\mathbf{C} }\)}} {{c5::equals}} the {{c3::minimal}} polynomial of {{c2::\(T\).}}

============================================================

  

    <img src="paste-db663e2e48fddd04cd496ff7918e29dd3828c303.jpg"><br>Proof:<br>Let \(p \in \mathcal{P}(\mathbf{R})\) denote the minimal polynomial of \(T\). From 9.9 it is easy to see that:<br><ul><ul><li>{{c1::&nbsp;\(p\left(T_{\mathbf{C} }\right)\)}} = {{c2::\((p(T))_{\mathbf{C} }\),&nbsp;}}</li><li>and thus {{c1::\(p\left(T_{\mathbf{C} }\right)\)}} = {{c3::\(0\).}}</li></ul></ul><ul><li>Suppose \(q \in \mathcal{P}(\mathbf{C})\) is a {{c4::monic}} polynomial such that \(q\left(T_{\mathbf{C}}\right)\) = {{c3::\(0\).}} </li><li>Then \(\left(q\left(T_{\mathbf{C}}\right)\right)(u)\) = {{c3::\(0\)}} for every \(u \in V\).&nbsp;</li><li>Letting \(r\) denote the polynomial whose {{c5::\(j^{\text {th}&nbsp; &nbsp;}\) coefficient}} is {{c5::the real part of the \(j^{\text {th } }\) coefficient of \(q\)}}, we see that \(r\) is a {{c4::monic}} polynomial and \(r(T)\) = {{c3::\(0\).&nbsp;}}</li><li>Thus \(\operatorname{deg} q\) = {{c5::\(\operatorname{deg} r\)}} \(\geq\) {{c5::\(\operatorname{deg} p\).}}</li><li>The conclusions of the two previous paragraphs imply that \(p\) is the minimal polynomial of \(T_{\mathbf{C}}\), as desired.<br></li></ul>

============================================================

  

    {{c1::Real}} {{c2::eigenvalues}} of {{c3::\(T_{\mathbf{C} }\)}}<br><br>Suppose \(V\) is a {{c1::real}} vector space, \(T \in \mathcal{L}(V)\), and {{c5::\(\lambda\)}} \(\in\) {{c1::\(\mathbf{R}\)}}. Then {{c5::\(\lambda\)}} is an {{c2::eigenvalue}} of {{c3::\(T_{\mathbf{C} }\)}} if and only if {{c5::\(\lambda\)}} is an {{c2::eigenvalue}} of {{c4::\(T\).}}

============================================================

  

    <img src="paste-2ed8a0710d360c78a55de02592b68673ce530e6b.jpg"><br>Proof 1 First suppose \(\lambda\) is an {{c1::eigenvalue}} of \(T\). Then there exists {{c5::\(v \in V\)}} with {{c5::\(v \neq 0\)}} such that {{c3::\(T v\)}} = {{c2::\(\lambda v\)}}. Thus {{c4::\(T_{\mathbf{C} } v\)}} = {{c2::\(\lambda v\)}}, which shows that \(\lambda\) is an {{c1::eigenvalue}} of \(T_{\mathbf{C}}\), completing one direction of the proof.

============================================================

  

    <img src="paste-e111a18693d7de267dd56c7e3d0d8d6eabe3a3a2.jpg"><br>To prove the other direction, suppose now that \(\lambda\) is an eigenvalue of {{c2::\(T_{\mathbf{C} }\)}}. Then there exist \(u, v \in V\) with \(u+i v \neq 0\) such that<br><br><ul><li>{{c2::\(T_{\mathbf{C} }\)}} \((\) {{c3::\(u+i v\)}} \()\)={{c1::\( \lambda(u+i v) .\)}}</li></ul><br>The equation above implies that:<br><ul><li>{{c4::&nbsp;\(T u\)}} = {{c5::\(\lambda u\)&nbsp;}}</li><li>and&nbsp;</li><li>{{c5::\(T v\)}} = {{c4::\(\lambda v\).&nbsp;}}</li></ul>Because \(u \neq 0\) or \(v \neq 0\), this implies that \(\lambda\) is an eigenvalue of \(T\), completing the proof.<br>

============================================================

  

    <img src="paste-ae66688317a74162cc0c0752164d11d60ca8e611.jpg"><br>Proof 2 <br><ul><li>The ({{c1::real}}) eigenvalues of \(T\) are the ({{c1::real}}) {{c4::zeros}} of the {{c5::{{c2::minimal}}}} polynomial of \(T\)&nbsp;</li><li>The {{c1::real}} eigenvalues of \(T_{\mathbf{C}}\) are the {{c1::real}} {{c4::zeros}} of the {{c2::minimal}} polynomial of {{c5::{{c3::\(T_{\mathbf{C} }\)}}}} .&nbsp;</li><li>These two {{c2::minimal}} polynomials are the same.&nbsp;</li><li>Thus the eigenvalues of \(T\) are precisely the {{c1::real}} eigenvalues of {{c3::\(T_{\mathbf{C} }\)}}, as desired.</li></ul><br>

============================================================

  

    Suppose \(V\) is a {{c5::real}} vector space, \(T \in \mathcal{L}(V), \lambda \in \mathbf{C}, j\) is a nonnegative integer, and \(u, v \in V\). Then<br><br><ul><li>{{c1::\(\left(T_{\mathbf{C} }-\lambda I\right)^j\)}} \((\) {{c3::\(u+i v\)}} \()\) = {{c5::\(0\)}}<br></li><li>if and only if</li><li>{{c2::\(\left(T_{\mathbf{C} }-\bar{\lambda} I\right)^j\)}} \((\) {{c4::\(u-i v\)}} \()\) = {{c5::\(0\).}}<br></li></ul>

============================================================

  

    Suppose \(V\) is a {{c5::real}} vector space, \(T \in \mathcal{L}(V)\), and {{c4::\(\lambda\)}}&nbsp; \(\in \mathbf{C}\). Then {{c4::\(\lambda\)}} is an {{c2::eigenvalue}} of {{c3::\(T_{\mathbf{C} }\)}} if and only if {{c1::\(\bar{\lambda}\)}} is an {{c2::eigenvalue}} of {{c3::\(T_{\mathbf{C} }\).}}

============================================================

  

    By definition, the eigenvalues of an operator on a real vector space are {{c5::{{c1::real}}}} numbers. Thus when mathematicians sometimes informally mention the {{c4::{{c2::complex}}}} eigenvalues of an operator on a real vector space, what they have in mind is the eigenvalues of the {{c4::{{c3::complexification}}}} of the operator.

============================================================

  

    {{c1::Nonreal}} {{c4::eigenvalues}} of {{c5::{{c3::\(T_{\mathrm{C} }\)}}}} {{c5::{{c2::come in pairs}}}}

============================================================

  

    {{c3::Multiplicity}} of {{c2::\(\lambda\)}} {{c5::{{c4::equals}}}} {{c3::multiplicity}} of {{c5::{{c1::\(\bar{\lambda}\)}}}}

============================================================

  

    Suppose \(V\) is a {{c4::real}} vector space, \(T \in \mathcal{L}(V)\), and \(\lambda \in \mathbf{C}\) is an eigenvalue of \(T_{\mathbf{C}}\). Then the {{c3::multiplicity}} of \(\lambda\) as an eigenvalue of \(T_{\mathbf{C}}\) equals the {{c3::multiplicity}} of {{c5::{{c1::\(\bar{\lambda}\)}}}} as an eigenvalue of {{c5::{{c2::\(T_{\mathbf{C} }\).}}}}

============================================================

  

    <img src="paste-3255c16f31f45233f520791cd0e3848db05b0a3b.jpg"><br>Proof:<br><ul><li>Suppose {{c1::\(u_{1}+i v_{1}, \ldots, u_{m}+i v_{m}\)}} is a basis of the {{c2::generalized eigenspace \(G\left(\lambda, T_{\mathbf{C} }\right)\)}}, where {{c1::\(u_{1}, \ldots, u_{m}, v_{1}, \ldots, v_{m}\)}}&nbsp; \(\in\) \(V\).&nbsp;<br></li><li>Then using&nbsp; routine arguments based on 9.12:</li><ul><li>{{c5::<img src="paste-e3b533652c97c5eb65835e6bce751df4ff3adf2a.jpg">}}<br></li></ul><li>We can show that {{c3::\(u_{1}-i v_{1}, \ldots, u_{m}-i v_{m}\)}} is a basis of the {{c2::generalized eigenspace}} {{c4::\(G\left(\bar{\lambda}, T_{\mathbf{C} }\right)\).&nbsp;}}</li><li>Thus both \(\lambda\) and \(\bar{\lambda}\) have multiplicity \(m\) as eigenvalues of {{c4::\(T_{\mathbf{C} }\).}}<br></li></ul>

============================================================

  

    Every operator on an {{c5::{{c1::odd}}}}-dimensional {{c4::{{c2::real}}}} vector space has an {{c4::{{c3::eigenvalue}}}}.

============================================================

  

    Suppose \(V\) is a {{c1::real}} vector space and \(T \in \mathcal{L}(V)\). Then the {{c4::coefficients}} of the {{c5::{{c3::characteristic}}}} {{c4::polynomial}} of {{c5::{{c2::\(T_{\mathbf{C} }\)}}}} are all {{c1::real}}.

============================================================

  

    <img src="paste-29f0dab45ab6209f206d88b81643b2c70028a006.jpg"><br>Proof Suppose \(\lambda\) is a {{c1::nonreal}} eigenvalue of \(T_{\mathbf{C}}\) with multiplicity {{c3::\(m\)}}. Then {{c5::{{c2::\(\bar{\lambda}\)}}}} is {{c3::also}} an eigenvalue of \(T_{\mathbf{C} }\) with multiplicity {{c3::\(m\)}} (by 9.17). Thus the characteristic polynomial of \(T_{\mathbf{C}}\) includes factors of {{c1::\((z-\lambda)^{m}\)}} and {{c5::{{c2::\((z-\bar{\lambda})^{m}\)}}}}. Multiplying together these two factors, we have<br><br><ul><li>{{c1::\( (z-\lambda)^{m}\)}} {{c2::\((z-\bar{\lambda})^{m}\)}}= {{c5::{{c4::\( \left(z^{2}-2(\operatorname{Re} \lambda) z+|\lambda|^{2}\right)^{m} \)}}}}</li></ul><br>The polynomial above on the right has real coefficients.<br>

============================================================

  

    <img src="paste-25c7b9a17c4c2c681382076d9c04417d90f0257c.jpg"><br>Proof Suppose \(\lambda\) is a nonreal eigenvalue of \(T_{\mathbf{C}}\) with multiplicity \(m\). Then \(\bar{\lambda}\) is also an eigenvalue of \(T_{\mathbf{C}}\) with multiplicity \(m\) (by 9.17). Thus the characteristic polynomial of \(T_{\mathbf{C}}\) includes factors of \((z-\lambda)^{m}\) and \((z-\bar{\lambda})^{m}\). Multiplying together these two factors, we have<br><br>\[<br>(z-\lambda)^{m}(z-\bar{\lambda})^{m}=\left(z^{2}-2(\operatorname{Re} \lambda) z+|\lambda|^{2}\right)^{m} .<br>\]<br><br>The polynomial above on the right has real coefficients.<br><br>The characteristic polynomial of \(T_{\mathbf{C}}\) is the product of terms of the form above and terms of the form {{c1::\((z-t)^{d}\)}}, where \(t\) is a {{c2::real eigenvalue}} of {{c5::{{c3::\(T_{\mathbf{C} }\)}}}} with {{c5::{{c4::multiplicity \(d\)}}}}. Thus the coefficients of the characteristic polynomial of \(T_{\mathbf{C}}\) are all real.

============================================================

  

    Suppose \(V\) is a {{c1::real}} vector space and \(T \in \mathcal{L}(V)\). Then<br><br><ul><li>(a) the {{c4::coefficients}} of the {{c2::characteristic}} polynomial of \(T\) are all {{c1::real}};</li><li>(b) the {{c2::characteristic}} polynomial of \(T\) has {{c5::degree}} {{c3::\(\operatorname{dim} V\)}};</li><li>(c) the {{c4::eigenvalues}} of \(T\) are precisely the {{c1::real}} {{c4::zeros}} of the {{c2::characteristic}} polynomial of \(T\).</li></ul>

============================================================

  

    {{c5::Cayley-Hamilton}} Theorem<br><br>Suppose \(T \in \mathcal{L}(V)\). Let {{c4::\(q\)}} denote the {{c3::characteristic}} {{c5::polynomial}} of \(T\). Then {{c2::\(q(T)\)}} = {{c1::\(0\).}}

============================================================

  

    Suppose \(T\) \(\in\) \(\mathcal{L}(V)\). Then<br><br><ul><li>(a) the {{c3::degree}} of the {{c1::minimal}} polynomial of \(T\) is {{c3::at most}} {{c5::{{c2::\(\operatorname{dim} V\)}}}};</li><li>(b) the {{c5::{{c2::characteristic}}}} polynomial of \(T\) is a {{c5::{{c4::polynomial multiple}}}} of the {{c1::minimal}} polynomial of \(T\).</li></ul>

============================================================

  

    2 Verify that if \(V\) is a {{c5::real}} vector space and {{c4::\(T\)}} \(\in\) {{c1::\(\mathcal{L}(V)\)}}, then {{c3::\(T_{\mathbf{C} }\)}} \(\in\) {{c2::\(\mathcal{L}\left(V_{\mathbf{C} }\right)\).}}

============================================================

  

    3 Suppose \(V\) is a real vector space and \(v_{1}, \ldots, v_{m} \in V\). Prove that {{c3::\(v_{1}, \ldots, v_{m}\)}} is {{c4::linearly independent}} in {{c5::{{c1::\(V_{\mathbf{C} }\)}}}} if and only if {{c3::\(v_{1}, \ldots, v_{m}\)}} is {{c4::linearly independent}} in {{c5::{{c2::\(V\).}}}}

============================================================

  

    4 Suppose \(V\) is a {{c5::real}} vector space and {{c1::\(v_{1}, \ldots, v_{m} \in V\)}}. Prove that {{c1::\(v_{1}, \ldots, v_{m}\)}} {{c4::spans}} {{c2::\(V_{\mathbf{C} }\)}} if and only if {{c1::\(v_{1}, \ldots, v_{m}\)}} {{c4::spans}} {{c3::\(V\)}}.

============================================================

  

    5 Suppose that \(V\) is a {{c5::real}} vector space and \(S, T \in \mathcal{L}(V)\). <br><br>Show that for the complexification \(T_{\mathbf{C}} \) :<br><ul><li>{{c4::&nbsp;\((S+T)_{\mathbf{C} }\)}} = {{c1::\(S_{\mathbf{C} }+T_{\mathbf{C} }\)}}</li><li>&nbsp;and that&nbsp;</li><li>{{c3::\((\lambda T)_{\mathbf{C} }\)}} = {{c2::\(\lambda T_{\mathbf{C} }\)}} for {{c5::every \(\lambda \in \mathbf{R}\).}}</li></ul>

============================================================

  

    6 Suppose \(V\) is a {{c4::real}} vector space and \(T \in \mathcal{L}(V)\). Prove that {{c2::\(T_{\mathbf{C} }\)}} is {{c5::{{c1::invertible}}}} if and only if {{c5::{{c3::\(T\)}}}} is {{c1::invertible}}.

============================================================

  

    7 Suppose \(V\) is a {{c4::real}} vector space and \(N \in \mathcal{L}(V)\). Prove that {{c2::\(N_{\mathbf{C} }\)}} is {{c5::{{c1::nilpotent}}}} if and only if {{c5::{{c3::\(N\)}}}} is {{c1::nilpotent}}.

============================================================

  

    11 Suppose \(V\) is a {{c5::real}} vector space and \(T \in \mathcal{L}(V)\). Suppose there exist \(b, c \in \mathbf{R}\) such that {{c2::\(T^{2}+b T+c I\)}} = {{c3::\(0\)}}. Prove that \(T\) has an {{c5::eigenvalue}} if and only if {{c1::\(b^{2}\)}} {{c4::\(\geq\)}} {{c1::\(4 c\)}}.

============================================================

  

    12 Suppose \(V\) is a {{c5::real}} vector space and \(T \in \mathcal{L}(V)\). Suppose there exist \(b, c \in \mathbf{R}\) such that {{c2::\(b^{2}\)}} {{c1::&lt;}} {{c2::\(4 c\)}} and {{c4::\(T^{2}+b T+c I\)}} is {{c3::nilpotent}}. Prove that \(T\) has {{c1::no eigenvalues.}}

============================================================

  

    13 Suppose \(V\) is a {{c5::real}} vector space, \(T \in \mathcal{L}(V)\), and \(b, c \in \mathbf{R}\) are such that {{c1::\(b^{2}\)}} {{c3::&lt;}} {{c2::\(4 c\)}}. Prove that {{c4::null}} {{c5::\(\left(T^{2}+b T+c I\right)^{j}\)}} has {{c3::even}} dimension for {{c3::every positive integer \(j\).}}

============================================================

  

    14 Suppose \(V\) is a {{c4::real}} vector space with \(\operatorname{dim} V\) = {{c3::\(8\)}}. Suppose \(T \in \mathcal{L}(V)\) is such that \(T^{2}+T+I\) is {{c5::{{c1::nilpotent}}}}. Prove that \(\left(T^{2}+T+I\right)^{4}\) = {{c5::{{c2::\(0\).}}}}

============================================================

  

    15 Suppose \(V\) is a {{c4::real}} vector space and \(T \in \mathcal{L}(V)\) has {{c3::no eigenvalues}}. Prove that every subspace of \(V\) {{c5::{{c2::invariant under \(T\)}}}} has {{c5::{{c1::even}}}} dimension.

============================================================

  

    16 Suppose \(V\) is a {{c4::real}} vector space. Prove that there exists \(T \in \mathcal{L}(V)\) such that {{c3::\(T^{2}\)}} = {{c5::{{c1::\(-I\)}}}} if and only if \(V\) has {{c5::{{c2::even}}}} dimension.

============================================================

  

    17 Suppose \(V\) is a {{c3::real}} vector space and \(T \in \mathcal{L}(V)\) satisfies {{c2::\(T^{2}\)}} = {{c1::\(-I\).}} Define complex scalar multiplication on \(V\) as follows: if \(a, b \in \mathbf{R}\), then<br><ul><ul><li>{{c4::\((a+b i)\)}} \(v\) = {{c5::\(a v+b T v \)}}</li></ul><li>(a) Show that the complex scalar multiplication on \(V\) defined above and the addition on \(V\) makes \(V\) into a {{c3::complex}} vector space.</li><li>(b) Show that the dimension of \(V\) as a {{c3::complex}} vector space is {{c5::half}} the dimension of \(V\) as a {{c3::real}} vector space.</li></ul>

============================================================

  

    18 Suppose \(V\) is a {{c1::real}} vector space and \(T \in \mathcal{L}(V)\). Prove that the following are equivalent:<br><br><ul><li>(a) {{c4::All the eigenvalues}} of {{c5::\(T_{\mathbf{C} }\)}} are {{c1::real}}.</li><li>(b) There exists a basis of \(V\) with respect to which \(T\) has an {{c2::uppertriangular}} matrix.</li><li>(c) There exists a basis of \(V\) consisting of {{c3::generalized eigenvectors of \(T\)}}.</li></ul>

============================================================

  

    19 Suppose \(V\) is a real vector space with \(\operatorname{dim} V\) = {{c1::\(n\)}} and \(T \in \mathcal{L}(V)\) is such that {{c1::null \(T^{n-2}\)}} {{c2::\(\neq\)}} {{c5::\(\operatorname{null} T^{n-1}\)}}. Prove that \(T\) has at most {{c2::two distinct}} {{c4::eigenvalues}} and that \(T_{\mathbf{C}}\) has no {{c3::nonreal}} {{c4::eigenvalues}}.

============================================================

  

    Suppose \(V\) is a {{c5::2}}-dimensional {{c5::real}} inner product space and \(T \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(T\) is {{c1::normal}} but not {{c1::self-adjoint}}.</li><li>(b) The matrix of \(T\) with respect to every {{c2::orthonormal}} basis of \(V\) has the form</li><ul><li>{{c2::\[\left(\begin{array}{cc}a &amp; -b \\b &amp; a\end{array}\right)\]}}</li><li>with {{c2::\(b\)}} {{c4::\(\neq\)}} {{c5::\(0\)}}.</li></ul><li>(c) The matrix of \(T\) with respect to some {{c2::orthonormal}} basis of \(V\) has the form</li><ul><li>{{c3::\[\left(\begin{array}{cc}a &amp; -b \\b &amp; a\end{array}\right)\]}}</li><li>with {{c2::\(b\)}} {{c4::\(&gt;\)}} {{c5::\(0\)}}.</li></ul></ul>

============================================================

  

    A {{c5::{{c2::normal}}}} operator restricted to an {{c4::{{c1::invariant}}}} {{c4::{{c3::subspace}}}} is {{c2::normal}}

============================================================

  

    Suppose \(V\) is an inner product space, \(T \in \mathcal{L}(V)\) is {{c4::normal}}, and \(U\) is a subspace of \(V\) that is {{c1::invariant}} under \(T\). Then<br><br><ul><li>(a) {{c5::{{c2::\(U^{\perp}\)}}}} is {{c1::invariant}} under {{c5::{{c2::\(T\)}}}};</li><li>(b) {{c5::{{c3::\(U\)}}}} is {{c1::invariant}} under {{c5::{{c3::\(T^{*}\)}}}};</li></ul>

============================================================

  

    Suppose \(V\) is an inner product space, \(T \in \mathcal{L}(V)\) is normal, and \(U\) is a subspace of \(V\) that is invariant under \(T\). Then<br><br><ul><li>(a) \(U^{\perp}\) is invariant under \(T\);</li><li>(b) \(U\) is invariant under \(T^{*}\);</li><li>(c) {{c1::\((T\)\(|_{U})^{*}\)}} = {{c2::\((T^{*})\)\(|_{U}\);}}</li><li>(d) \(T\){{c1::\(|_{U}\)}} \(\in\) {{c4::\(\mathcal{L}(U)\)}} and \(T\){{c5::\(|_{U^{\perp} } \)}} \(\in\) {{c4::\(\mathcal{L}\left(U^{\perp}\right)\)}} are {{c3::normal}} operators.</li></ul>

============================================================

  

    Note that if an operator \(T\) has a {{c5::{{c3::block diagonal}}}} matrix with respect to some basis, then the entry in each {{c4::{{c1::1-by-1 block on the diagonal}}}} of this matrix is an {{c4::{{c2::eigenvalue of \(T\).}}}}

============================================================

  

    <br>Our next result shows that normal operators on real inner product spaces come close to having {{c1::diagonal}} matrices. Specifically, we get {{c2::block diagonal}} matrices, with {{c5::{{c3::each block}}}} having {{c5::{{c4::size at most 2-by-2.}}}}

============================================================

  

    Suppose \(V\) is a {{c3::real}} inner product space and \(T \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(T\) is {{c1::normal}}.</li><li>(b) There is an orthonormal basis of \(V\) with respect to which \(T\) has a {{c4::block diagonal}} matrix such that {{c4::each block}} is a {{c5::1-by-1 matrix}} or a {{c5::2-by-2 matrix}} of the form</li><ul><li>{{c2::\[\left(\begin{array}{cc}a &amp; -b \\b &amp; a\end{array}\right)\]}}</li><li>with {{c3::\(b&gt;0\)}}.</li></ul></ul>

============================================================

  

    The next result shows that every isometry on a real inner product space is composed of pieces that are {{c1::rotations}} on {{c2::2}}-dimensional {{c2::subspaces}}, pieces that equal the {{c5::{{c3::identity}}}} operator, and pieces that equal {{c5::{{c4::multiplication by -1}}}} .

============================================================

  

    Suppose \(V\) is a {{c3::real}} inner product space and \(S \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(S\) is an {{c1::isometry}}.</li><li>(b) There is an orthonormal basis of \(V\) with respect to which \(S\) has a {{c4::block diagonal}} matrix such that {{c4::each block on the diagonal}} is a {{c5::1-by-1 matrix containing 1}} or {{c5::-1}} or is a {{c5::2-by-2 matrix}} of the form</li><ul><li>{{c2::\[\left(\begin{array}{cc}\cos \theta &amp; -\sin \theta \\\sin \theta &amp; \cos \theta\end{array}\right),\]}}</li><li><br></li><li>with {{c3::\(\theta \in(0, \pi)\)}}.</li></ul></ul>

============================================================

  

    1 Suppose \(S \in \mathcal{L}\left(\mathbf{R}^{3}\right)\) is an {{c3::isometry}}. Prove that there exists a {{c4::nonzero}} vector \(x \in \mathbf{R}^{3}\) such that {{c5::{{c2::\(S^{2} x\)}}}} = {{c5::{{c1::\(x\).}}}}

============================================================

  

    2 Prove that every isometry on an {{c4::odd}}-dimensional {{c4::real}} inner product space has {{c1::1}} or {{c5::{{c2::-1}}}} as an {{c5::{{c3::eigenvalue}}}}.

============================================================

  

    3 Suppose \(V\) is a {{c5::real}} inner product space. Show that<br><br><ul><li>\(\langle\) {{c1::\(u+i v, x+i y\)}}&nbsp;\(\rangle\) = {{c2::\(\langle u, x\rangle\)}} {{c5::+}} {{c2::\(\langle v, y\rangle\)}} {{c5::+}} {{c3::\((\langle v, x\rangle-\langle u, y\rangle)\)}} {{c4::\(i\)}}</li></ul><ul><li>for \(u, v, x, y \in V\) defines a {{c1::complex inner product}} on \(V_{\mathbf{C}}\).</li></ul>

============================================================

  

    4 Suppose \(V\) is a {{c5::{{c2::real}}}} inner product space and \(T \in \mathcal{L}(V)\) is {{c4::{{c1::self-adjoint}}}}. Show that \(T_{\mathbf{C}}\) is a {{c4::{{c3::self-adjoint}}}} operator on the inner product space \(V_{\mathbf{C}}\) defined by the previous exercise.<br><br>\[<br>\langle u+i v, x+i y\rangle=\langle u, x\rangle+\langle v, y\rangle+(\langle v, x\rangle-\langle u, y\rangle) i<br>\]

============================================================

  

    6 Give an example of an operator \(T\) on an inner product space such that \(T\) has an {{c5::{{c2::invariant}}}} subspace whose {{c4::{{c3::orthogonal complement}}}} is not {{c4::{{c3::invariant}}}} under \(T\).<br><br>[The exercise above shows that 9.30 can fail without the hypothesis that \(T\) is {{c4::{{c1::normal}}}}.]

============================================================

  

    7 Suppose \(T \in \mathcal{L}(V)\) and \(T\) has a block diagonal matrix<br><br><ul><li>\[\left(\begin{array}{ccc}A_{1} &amp; &amp; 0 \\&amp; \ddots &amp; \\0 &amp; &amp; A_{m}\end{array}\right)\]</li></ul><br>with respect to some basis of \(V\). <br><br>For \(j=1, \ldots, m\), let \(T_{j}\) be the operator on \(V\) whose matrix with respect to the same basis is a block diagonal matrix with {{c3::blocks the same size as in the matrix above}}, with {{c4::\(A_{j}\)}} in the {{c4::\(j^{\text {th } }\)}} block, and with all the other blocks on the diagonal equal to {{c5::identity}} matrices (of the appropriate size). Prove that:<br><ul><li>{{c2::&nbsp;\(T\)}} = {{c1::\(T_{1} \cdots T_{m}\).}}</li></ul>

============================================================

  

    <img src="paste-55ed5aa1cea157e96fe87d9c3640ee1d70d8715e.jpg"><br>Proof for a:<br><ul><li>Choose an orthonormal basis&nbsp;\(\vec{e}\) of U and extend to an orthonormal basis of v&nbsp;\(\vec{e},\vec{f}\)</li><li>Which gives us a matrix of the form {{c1::<img src="paste-d48189cafada581d7f40e05e5a01b1f67d8f4600.jpg">}}&nbsp;because {{c2::U is invariant under T}} and thus we get {{c3::all 0s in the lower left corner}}</li><li>Because T is normal {{c4::\(\sum_{j=1}^m\left\|T e_j\right\|^2\)}} = {{c5::\(\sum_{j=1}^m\left\|T^* e_j\right\|^2\)}}, the lhs equals the {{c5::sum of the squares of all entries of A}} since each component is {{c5::the norm of a column}} while the rhs equals the {{c4::sum of the squares of all entries in both A and B}} since each componennt is {{c4::the norm of a row.}}</li><li>Thus the matrix B must be composed of {{c5::all 0's}} thus proving a)</li></ul>

============================================================

  

    Every {{c2::isometry}} is {{c1::normal}}<br><ul><li>Suppose \(S \in \mathcal{L}(V)\). Then the following are equivalent:</li><li>- \(S\) is an {{c2::isometry}}.</li><li>- {{c3::\(S^* S\)}} = {{c4::\(I\)}}.</li><li>- {{c5::\(S S^*\)}} = {{c4::\(I\)}}.</li></ul>

============================================================

  

    <img src="paste-b640ed7d09b8eea0b2f067bceb71c9a39c266f85.jpg"><br>Proof:<br><ul><li>Suppose a holds</li><li>Since S is a normal operator there is an orthonormal basis such that S has a block diagonal matrix such that each block is a {{c1::1-by-1}} matrix or a {{c2::2-by-2}} matrix of the form<br>{{c2::\[<br>\left(\begin{array}{cc}<br>a &amp; -b \\<br>b &amp; a<br>\end{array}\right),<br>\]}}<br>with \(b&gt;0\).</li><li>If&nbsp;\(\lambda\) is an entry in a 1-by-1 matrix, because S is an isometry&nbsp;\(\lambda\) must have {{c3::absolute value 1}} and thus be {{c3::-1}} or {{c3::1}}</li><li>For {{c2::2-by-2}} matrices, since S is an isometry it must map {{c4::every orthonormal vector}} to {{c4::a vector with norm 1 as well}}, thus&nbsp;{{c5::\(a^2+b^2\)}} =&nbsp; {{c4::1&nbsp;}} as they are the {{c5::coefficients of an orthonormal vector}} and must thus have {{c4::norm equal 1}}</li><li>Since&nbsp;\(b&gt;0\) we can by definition pick&nbsp;\(\theta \in (0,\pi)\) such that&nbsp;\(a\)&nbsp;\(= \)&nbsp;{{c5::\(\cos \theta\)}} and&nbsp;\(b\) =&nbsp;{{c5::\(\sin \theta\)}}</li></ul>

============================================================

  

    Definition {{c1::invertible}}, {{c2::inverse}}, {{c3::\(A^{-1}\)}}<br><br>A {{c5::square}} matrix \(A\) is called {{c1::invertible}} if there is a {{c5::square matrix}} \(B\) {{c5::of the same size}} such that {{c4::\(A B\)}} = {{c4::\(B A\)}} = {{c4::\(I\)}}; we call \(B\) the {{c2::inverse}} of \(A\) and denote it by {{c3::\(A^{-1}\).}}

============================================================

  

    Some mathematicians use the terms {{c1::nonsingular}}, which means the same as {{c2::invertible}}, and {{c5::{{c3::singular}}}}, which means the same as {{c5::{{c4::noninvertible}}}}.

============================================================

  

    Suppose \(u_{1}, \ldots, u_{n}\) and \(v_{1}, \ldots, v_{n}\) and \(w_{1}, \ldots, w_{n}\) are all {{c5::{{c3::bases of \(V\).}}}} Suppose \(S, T \in \mathcal{L}(V)\). Then<br><ul><li><br></li><li>{{c4::{{c2::\(\mathcal{M}\left(S T,\left(u_{1}, \ldots, u_{n}\right),\left(w_{1}, \ldots, w_{n}\right)\right) \)}}}}= {{c4::{{c1::\(\mathcal{M}\left(S,\left(v_{1}, \ldots, v_{n}\right),\left(w_{1}, \ldots, w_{n}\right)\right) \mathcal{M}\left(T,\left(u_{1}, \ldots, u_{n}\right),\left(v_{1}, \ldots, v_{n}\right)\right) \)}}}}</li></ul>

============================================================

  

    <br>Suppose \(u_{1}, \ldots, u_{n}\) and \(v_{1}, \ldots, v_{n}\) are bases of \(V\). Then the matrices {{c1::\(\mathcal{M}\left(I,\left(u_{1}, \ldots, u_{n}\right),\left(v_{1}, \ldots, v_{n}\right)\right)\)}} and {{c2::\(\mathcal{M}\left(I,\left(v_{1}, \ldots, v_{n}\right),\left(u_{1}, \ldots, u_{n}\right)\right)\)}} are {{c5::{{c3::invertible}}}}, and {{c5::{{c4::each is the inverse of the other.}}}}

============================================================

  

    Suppose \(T \in \mathcal{L}(V)\). <br><br>Let \(u_{1}, \ldots, u_{n}\) and \(v_{1}, \ldots, v_{n}\) be bases of \(V\). Let \(A=\mathcal{M}\left(I,\left(u_{1}, \ldots, u_{n}\right),\left(v_{1}, \ldots, v_{n}\right)\right)\). Then<br><br><ul><li>{{c4::\(\mathcal{M}\left(T,\left(u_{1}, \ldots, u_{n}\right)\right)\)}} = {{c2::\(A^{-1}\)}} {{c5::{{c1::\(\mathcal{M}\left(T,\left(v_{1}, \ldots, v_{n}\right)\right)\)&nbsp;}}}} {{c5::{{c3::\(A .\)}}}}</li></ul>

============================================================

  

    In the definition below, the sum of the eigenvalues "with each eigenvalue repeated according to its multiplicity" means that if \(\lambda_{1}, \ldots, \lambda_{m}\) are the distinct eigenvalues of \(T\) (or of \(T_{\mathbf{C}}\) if \(V\) is a real vector space) with multiplicities \(d_{1}, \ldots, d_{m}\), then the sum is<br><br>{{c1::\[<br>d_{1} \lambda_{1}+\cdots+d_{m} \lambda_{m}<br>\]<br>}}<br>Or if you prefer to list the eigenvalues with each r{{c2::epeated according to its multiplicity,}} then the eigenvalues could be denoted {{c5::{{c4::\(\lambda_{1}, \ldots, \lambda_{n}\)}}}} (where the index \(n\) equals {{c5::{{c3::\(\operatorname{dim} V\)}}}} ) and the sum is<br><br>{{c5::{{c4::\[<br>\lambda_{1}+\cdots+\lambda_{n}<br>\]}}}}<br><br><img src="paste-5016d7e33cb06793a9b75b2e067cb462c4fa8f9b.jpg">

============================================================

  

    Definition {{c1::trace}} of an operator<br><br>Suppose \(T \in \mathcal{L}(V)\).<br><br>- If \(\mathbf{F}=\mathbf{C}\), then the {{c1::trace}} of \(T\) is the {{c2::sum of the eigenvalues}} of {{c3::\(T\)}}, with {{c5::each eigenvalue repeated according to its multiplicity.}}<br><br>- If \(\mathbf{F}=\mathbf{R}\), then the {{c1::trace}} of \(T\) is the {{c2::sum of the eigenvalues}} of {{c4::\(T_{\mathbf{C} }\)}}, with {{c5::each eigenvalue repeated according to its multiplicity.}}<br><br>The {{c1::trace}} of \(T\) is denoted by {{c1::trace \(T\).}}

============================================================

  

    The trace has a close connection with the characteristic polynomial. Suppose \(\lambda_{1}, \ldots, \lambda_{n}\) are the eigenvalues of \(T\) (or of \(T_{\mathbf{C}}\) if \(V\) is a real vector space) with each eigenvalue repeated according to its multiplicity. Then by definition (see 8.34 and 9.21), the characteristic polynomial of \(T\) equals<br><br>{{c5::{{c1::\[<br>\left(z-\lambda_{1}\right) \cdots\left(z-\lambda_{n}\right)<br>\]<br>}}}}<br>Expanding the polynomial above, we can write the characteristic polynomial of \(T\) in the form<br><br>{{c4::{{c2::\[<br>10.11 \quad z^{n}-\left(\lambda_{1}+\cdots+\lambda_{n}\right) z^{n-1}+\cdots+(-1)^{n}\left(\lambda_{1} \cdots \lambda_{n}\right) .<br>\]}}}}<br><br>The expression above immediately leads to the following result.<br><br>\subsection{Trace and characteristic polynomial}<br><br>Suppose \(T \in \mathcal{L}(V)\). Let \(n=\operatorname{dim} V\). Then trace \(T\) equals the {{c4::{{c3::negative of the coefficient of \(z^{n-1}\)}}}} in the characteristic polynomial of \(T\).

============================================================

  

    {{c1::Trace}} and {{c2::characteristic polynomial}}<br><br>Suppose \(T \in \mathcal{L}(V)\). Let \(n=\operatorname{dim} V\). Then {{c1::trace \(T\)}} equals the {{c5::negative of}} {{c4::the coefficient of \(z^{n-1}\)}} in the {{c3::characteristic polynomial of \(T\).}}

============================================================

  

    Definition {{c1::trace}} of a matrix<br><br>The {{c1::trace}} of a {{c5::square}} matrix \(A\), denoted {{c2::trace \(A\)}}, is defined to be the {{c3::sum}} of {{c4::the diagonal entries of \(A\)}}.

============================================================

  

    {{c3::Trace}} of {{c1::\(A B\)}} equals {{c5::{{c4::trace}}}} of {{c5::{{c2::\(B A\)}}}}

============================================================

  

    {{c2::Trace}} of matrix of operator {{c3::does not}} {{c5::{{c4::depend on}}}} {{c5::{{c1::basis}}}}

============================================================

  

    Let \(T \in \mathcal{L}(V)\). Suppose \(u_{1}, \ldots, u_{n}\) and \(v_{1}, \ldots, v_{n}\) are bases of \(V\). Then<br><br><ul><li>{{c5::{{c3::\(\operatorname{trace}\)}}}} {{c4::{{c1::\(\mathcal{M}\left(T,\left(u_{1}, \ldots, u_{n}\right)\right)\)}}}} = {{c3::\(\operatorname{trace}\)}} {{c4::{{c2::\( \mathcal{M}\left(T,\left(v_{1}, \ldots, v_{n}\right)\right) \text {. }\)}}}}</li></ul>

============================================================

  

    {{c5::{{c3::Trace}}}} of an {{c4::{{c1::operator}}}} equals {{c5::{{c3::trace}}}} of {{c4::{{c2::its matrix}}}}

============================================================

  

    <img src="paste-40e7b1948e17f3282baf2f58cc1ddbebbd743c55.jpg"><br>If we know the matrix of an operator on a complex vector space, the result above allows us to find the {{c5::{{c1::sum of all the eigenvalues}}}} {{c4::{{c3::without finding}}}} {{c4::{{c2::any of the eigenvalues}}}},

============================================================

  

    {{c4::Trace}} is {{c1::additive}}<br><br>Suppose \(S, T \in \mathcal{L}(V)\). Then:<br><ul><li>{{c4::\(\operatorname{trace}\)}} {{c5::{{c3::\((S+T)\)}}}} = {{c5::{{c2::\(\operatorname{trace}\) \(S+\operatorname{trace} T\).}}}}</li></ul>

============================================================

  

    The {{c4::identity}} is not the {{c1::difference}} of {{c5::{{c2::\(S T\)}}}} and {{c5::{{c3::\(T S\)}}}}

============================================================

  

    The {{c1::identity}} is {{c4::not}} the {{c5::{{c2::difference}}}} of {{c5::{{c3::\(S T\)}}}} and {{c5::{{c3::\(T S\)}}}}<br><br>There {{c4::do not exist}} operators \(S, T \in \mathcal{L}(V)\) such that {{c3::\(S T\)}} {{c5::{{c2::-}}}} {{c3::\(T S\)}} = {{c1::\(I\).}}

============================================================

  

    Suppose \(T \in \mathcal{L}(V)\) and \(v_{1}, \ldots, v_{n}\) is a basis of \(V\). Prove that the matrix {{c5::{{c2::\(\mathcal{M}\left(T,\left(v_{1}, \ldots, v_{n}\right)\right)\)}}}} is {{c4::{{c1::invertible}}}} if and only if {{c4::{{c3::\(T\)}}}} is {{c1::invertible}}.

============================================================

  

    2 Suppose \(A\) and \(B\) are {{c4::square}} matrices {{c5::of the same size}} and {{c3::\(A B\)}} = {{c1::\(I\)}}. Prove that {{c2::\(B A\)}} = {{c1::\(I\).}}

============================================================

  

    3 Suppose \(T \in \mathcal{L}(V)\) has {{c1::the same}} matrix {{c4::with respect to every basis of \(V\)}}. Prove that \(T\) is a {{c5::{{c2::scalar multiple}}}} of {{c5::{{c3::the identity operator.}}}}

============================================================

  

    uppose \(u_{1}, \ldots, u_{n}\) and \(v_{1}, \ldots, v_{n}\) are bases of \(V\). <br><br>Let \(T \in \mathcal{L}(V)\) be the operator such that {{c4::\(T v_{k}\)}} = {{c3::\(u_{k}\)}} for {{c4::\(k=1, \ldots, n\}}). Prove that<br><br><ul><li>\(\mathcal{M}\) {{c5::{{c2::\(\left(T,\left(v_{1}, \ldots, v_{n}\right)\right)\)}}}} = \(\mathcal{M}\) {{c5::{{c1::\(\left(I,\left(u_{1}, \ldots, u_{n}\right),\left(v_{1}, \ldots, v_{n}\right)\right) .\)}}}}</li></ul>

============================================================

  

    5 Suppose \(B\) is a {{c5::square}} matrix with {{c5::complex}} entries. Prove that there exists an {{c1::invertible}} {{c5::square}} matrix \(A\) with {{c5::complex}} entries such that {{c3::\(A^{-1}\)}} {{c4::\(B\)}}&nbsp;{{c3::\(A\)}} is an {{c2::upper-triangular}} matrix.

============================================================

  

    7 Suppose \(V\) is a {{c5::real}} vector space, \(T \in \mathcal{L}(V)\), and \(V\) has a basis consisting of {{c5::eigenvectors of \(T\).}}<br><br>&nbsp;Prove that {{c3::\(\operatorname{trace}\)}} {{c2::\(\left(T^{2}\right)\)}} {{c4::\(\geq\)}} {{c1::\(0\).}}

============================================================

  

    9 Suppose \(P \in \mathcal{L}(V)\) satisfies {{c4::\(P^{2}\)}} = {{c3::\(P\)}}. Prove that<br><br><ul><li>\(\text { trace }\) {{c5::{{c1::\(P\)}}}} = {{c5::{{c2::\(\text { dim range } P \text {. }\)}}}}</li></ul>

============================================================

  

    10 Suppose \(V\) is an {{c4::inner product}} space and \(T \in \mathcal{L}(V)\). Prove that<br><br><ul><li>{{c3::\(\operatorname{trace}\)}} {{c5::{{c1::\(T^{*}\)}}}} = {{c5::{{c2::\(\overline{\operatorname{trace} T} \text {. }\)}}}}</li></ul>

============================================================

  

    11 Suppose \(V\) is an inner product space. <br><br>Suppose \(T \in \mathcal{L}(V)\) is a {{c5::positive}} operator and {{c4::trace \(T\)}} ={{c1::\(0\)}}. Prove that {{c3::\(T\)}} = {{c2::\(0\).}}

============================================================

  

    12 Suppose \(V\) is an inner product space and \(P, Q \in \mathcal{L}(V)\) are {{c5::orthogonal projections}}. Prove that {{c4::\(\operatorname{trace}\)}} {{c2::\((P Q)\)}} {{c3::\(\geq\)}} {{c1::\(0\).}}

============================================================

  

    14 Suppose \(T \in \mathcal{L}(V)\) and \(c \in \mathbf{F}\). Prove that {{c4::\(\operatorname{trace}\)}} {{c1::\((c T)\)}} ={{c5::{{c2::\(c\)}}}} {{c5::{{c3::trace}}}} {{c5::{{c2::\(T\).}}}}

============================================================

  

    15 Suppose \(S, T \in \mathcal{L}(V)\). Prove that {{c4::\(\operatorname{trace}\)}} {{c2::\((S T)\)}} = {{c5::{{c3::\(\operatorname{trace}\)}}}} {{c5::{{c1::\((T S)\).}}}}

============================================================

  

    17 Suppose \(T \in \mathcal{L}(V)\) is such that {{c4::\(\operatorname{trace}\)}} {{c2::\((S T)\)}} = {{c5::{{c1::\(0\)}}}} for all \(S \in \mathcal{L}(V)\). Prove that {{c5::{{c3::\(T\)}}}} = {{c2::\(0\).}}

============================================================

  

    18 Suppose \(V\) is an inner product space with orthonormal basis \(e_{1}, \ldots, e_{n}\) and \(T \in \mathcal{L}(V)\). Prove that<br><br><ul><li>{{c4::\(\operatorname{trace}\)}} {{c3::\(\left(T^{*} T\right)\)}} = {{c1::\(\left\|T e_{1}\right\|^{2}\)}} {{c2::+}} \(\cdots\) {{c2::+}} {{c1::\(\left\|T e_{n}\right\|^{2} .\)}}</li></ul><br>Conclude that the right side of the equation above is independent of {{c5::which orthonormal basis \(e_{1}, \ldots, e_{n}\) is chosen for \(V\).}}<br>

============================================================

  

    19 Suppose \(V\) is an inner product space. Prove that<br><br><ul><li>\(\langle S, T\rangle\) = {{c5::{{c2::\(\operatorname{trace}\)}}}}&nbsp; {{c4::{{c1::\(\left(S T^{*}\right)\)}}}}</li></ul><br>defines {{c4::{{c3::an inner product on \(\mathcal{L}(V)\).}}}}<br>

============================================================

  

    20 Suppose \(V\) is a complex inner product space and \(T \in \mathcal{L}(V)\). Let \(\lambda_{1}, \ldots, \lambda_{n}\) be the eigenvalues of \(T\), repeated according to multiplicity. Suppose<br><br>\[<br>\left(\begin{array}{ccc}<br>A_{1,1} &amp; \ldots &amp; A_{1, n} \\<br>\vdots &amp; &amp; \vdots \\<br>A_{n, 1} &amp; \ldots &amp; A_{n, n}<br>\end{array}\right)<br>\]<br><br>is the matrix of \(T\) with respect to some orthonormal basis of \(V\). Prove that<br><br><ul><li>{{c3::\(\sum_{i=1}^n\)}} {{c2::&nbsp;\(| \lambda_i |^2\)}} &nbsp;{{c5::\(\leq\)}} {{c4::\(\sum_{k=1}^{n}\)}} {{c3::\(\sum_{j=1}^{n}\)}} {{c1::\(\left|A_{j, k}\right|^{2} .\)}}</li></ul>

============================================================

  

    21 Suppose \(V\) is an inner product space. Suppose \(T \in \mathcal{L}(V)\) and<br><ul><li>{{c4::\(\left\|T^{*} v\right\|\)}}&nbsp; {{c3::\(\leq\)}} {{c2::\(\|T v\|\)}}</li></ul><br>for every \(v \in V\). Prove that \(T\) is {{c1::normal}}.<br><br>[The exercise above fails on {{c5::infinite-dimensional}} inner product spaces, leading to what are called {{c5::hyponormal}} operators, which have a welldeveloped theory.]<br>

============================================================

  

    <img src="paste-b2a541c93ad583029ae4d518dc3ac540e3ddb13b.jpg"><br>Proof Let \(A=\mathcal{M}\left(I,\left(u_1, \ldots, u_n\right),\left(v_1, \ldots, v_n\right)\right)\).<br>Then<br>:<br><ul><li>\(\operatorname{trace} \mathcal{M}\left(T,\left(u_1, \ldots, u_n\right)\right)\)&nbsp;</li><li>= \(\operatorname{trace}\) ({{c5::{{c1::\(A^{-1} \mathcal{M}\left(T,\left(v_1, \ldots, v_n\right)\right) A\)}}}} ) by {{c5::{{c1::change of basis}}}}<br></li><li>=&nbsp;\(\operatorname{trace}\) {{c4::{{c2::\(\mathcal{M}\left(T,\left(v_1, \ldots, v_n\right)\right) A A^{-1}\)}}}}&nbsp; ) by {{c4::{{c2::comutativity of matrix mult w.r.t trace&nbsp;}}}}</li><li>=&nbsp;\(\operatorname{trace}\) ( {{c4::{{c3::\(\mathcal{M}\left(T,\left(v_1, \ldots, v_n\right)\right)\)}}}} ) by {{c4::{{c3::inverses cancelling out}}}}</li></ul><br>The last equality completes the proof.<br>

============================================================

  

    <img src="paste-c402c4405866395f27cddfec9ccdbd672db54cee.jpg"><br>Proof Choose a basis of \(V\). Then:<br><ul><li>&nbsp;\(\operatorname{trace}(S+T)\)&nbsp;</li><li>= {{c1::\(\operatorname{trace} \mathcal{M}(S+T)\)}}</li><li>=&nbsp;{{c2::\(\operatorname{trace}(\mathcal{M}(S)+\mathcal{M}(T))\)}}</li><li>=&nbsp;{{c5::{{c3::\(\operatorname{trace} \mathcal{M}(S)+\operatorname{trace} \mathcal{M}(T)\)}}}}</li><li>=&nbsp;{{c5::{{c4::\(\operatorname{trace} S+\operatorname{trace} T\)}}}}</li></ul>

============================================================

  

    - Suppose \(P \in \mathcal{L}(V)\) satisfies {{c2::\(P^2\)}} = {{c3::\(P\)}}. <br>Prove that trace \(P\) = {{c5::{{c1::\(\operatorname{dim}\) range \(P\).}}}}<br><br>This implies that if {{c2::\(P^2\)}} = {{c3::\(P\)}} then the trace is a {{c5::{{c4::nonnegative integer}}}}

============================================================

  

    - Suppose \(V\) is an inner product space with {{c4::orthonormal}} basis \(e_1, \ldots, e_n\) and \(T \in \mathcal{L}(V)\). Prove that :<br><ul><li>\(\operatorname{trace}\) {{c2::\(\left(T^* T\right)\)}} = {{c5::{{c1::\(\left\|T e_1\right\|^2+\cdots+\left\|T e_n\right\|^2\).}}}}</li></ul>Conclude that the right side of the equation above is {{c5::{{c3::independent of the orthonormal basis \(e_1, \ldots, e_n\).}}}}<br>

============================================================

  

    - Suppose \(V\) is an inner product space. Prove that<br><ul><li>\(\langle S, T\rangle\) ={{c5::{{c3::\(\operatorname{trace}\)}}}} {{c4::{{c2::\(\left(S T^*\right)\)}}}}</li></ul>defines an {{c4::{{c1::inner product on \(\mathcal{L}(V)\).}}}}<br>

============================================================

  

    Suppose \(V\) is a complex inner product space and \(T \in \mathcal{L}(V)\). Let \(\lambda_1, \ldots, \lambda_n\) be the eigenvalues of \(T\), repeated according to multiplicity. Suppose<br>\[<br>\left(\begin{array}{ccc}<br>A_{1,1} &amp; \ldots &amp; A_{1, n} \\<br>\vdots &amp; &amp; \vdots \\<br>A_{n, 1} &amp; \ldots &amp; A_{n, n}<br>\end{array}\right)<br>\]<br>is the matrix of \(T\) with respect to some {{c4::orthonormal}} basis of \(V\). Prove that<br><ul><li>{{c2::\(\left|\lambda_1\right|^2+\cdots+\left|\lambda_n\right|^2\)}} {{c5::{{c3::\(\leq\)}}}} {{c5::{{c1::\(\sum_{k=1}^n \sum_{j=1}^n\left|A_{j, k}\right|^2 .\)}}}}</li></ul>

============================================================

  

    Definition {{c1::determinant}} of an operator<br><br>Suppose \(T \in \mathcal{L}(V)\).<br><br>- If \(\mathbf{F}=\mathbf{C}\), then the {{c1::determinant}} of \(T\) is {{c2::the product of the eigenvalues}} of {{c3::\(T\)}}, with {{c2::each eigenvalue}}&nbsp;&nbsp;{{c5::repeated according to its multiplicity.}}<br><br>- If \(\mathbf{F}=\mathbf{R}\), then the {{c1::determinant}} of \(T\) is the {{c2::product of the eigenvalues}} of {{c4::\(T_{\mathbf{C} }\)}}, with {{c2::each eigenvalue}} {{c5::repeated according to its multiplicity.}}<br>

============================================================

  

    Suppose \(T \in \mathcal{L}(V)\). Let \(n\) = \(\operatorname{dim} V\). Then {{c1::\(\operatorname{det} T\)}} equals {{c3::\((-1)^{n}\) times}} {{c5::{{c4::the constant term}}}} of the {{c5::{{c2::characteristic polynomial}}}} of \(T\).

============================================================

  

    Suppose \(T \in \mathcal{L}(V)\). Then the {{c4::characteristic}} polynomial of \(T\) can be written as<br><br><ul><li>{{c5::\(z^{n}\)}} - {{c1::\((\operatorname{trace} T)\)}} {{c5::\(z^{n-1}\)}} +\(\cdots\)+{{c3::\((-1)^{n}\)}} {{c2::\((\operatorname{det} T)\)}}&nbsp;{{c5::}}</li></ul>

============================================================

  

    {{c5::{{c1::Invertible}}}} is equivalent to {{c4::{{c2::nonzero}}}} {{c4::{{c3::determinant}}}}<br><br>An operator on \(V\) is {{c5::{{c1::invertible}}}} if and only if its {{c3::determinant}} is {{c2::nonzero}}.

============================================================

  

    {{c5::{{c1::Characteristic polynomial}}}} of \(T\) equals {{c4::{{c3::\(\operatorname{det}\)}}}} {{c4::{{c2::\((z I-T)\)}}}}<br><br>Suppose \(T \in \mathcal{L}(V)\). Then the {{c5::{{c1::characteristic polynomial}}}} of \(T\) equals {{c3::\(\operatorname{det}\)}} {{c4::{{c2::\((z I-T)\).}}}}

============================================================

  

    10.26 Example Suppose \(a_{1}, \ldots, a_{n} \in \mathbf{F}\). Let<br><br>\[<br>A=\left(\begin{array}{ccccc}<br>0 &amp; &amp; &amp; &amp; a_{n} \\<br>a_{1} &amp; 0 &amp; &amp; &amp; \\<br>&amp; a_{2} &amp; 0 &amp; &amp; \\<br>&amp; &amp; \ddots &amp; \ddots &amp; \\<br>&amp; &amp; &amp; a_{n-1} &amp; 0<br>\end{array}\right) ;<br>\]<br><br>here all entries of the matrix are 0 except for the upper-right corner and along the line just below the diagonal. Suppose \(v_{1}, \ldots, v_{n}\) is a basis of \(V\) and \(T \in \mathcal{L}(V)\) is such that \(\mathcal{M}\left(T,\left(v_{1}, \ldots, v_{n}\right)\right)=A\). Find the determinant of \(T\).<br><br>Solution First assume \(a_{j} \neq 0\) for each \(j=1, \ldots, n-1\). Note that the list {{c5::\(v_{1}, T v_{1}, T^{2} v_{1}, \ldots, T^{n-1} v_{1}\)}} equals {{c4::\(v_{1}, a_{1} v_{2}, a_{1} a_{2} v_{3}, \ldots, a_{1} \cdots a_{n-1} v_{n}\)}}.<br><br>Thus {{c5::\(v_{1}, T v_{1}, \ldots, T^{n-1} v_{1}\)}} is {{c5::linearly independent}} (because {{c5::the \(a\) 's are all nonzero}}). Hence if \(p\) is a monic polynomial with degree at most \(n-1\), then \(p(T) v_{1}\) {{c5::\(\neq 0\)}}. Thus the minimal polynomial of \(T\) cannot have degree {{c5::less than \(n\)}}.<br><br>As you should verify, {{c3::\(T^{n} v_{j}\)}} = {{c2::\(a_{1} \cdots a_{n} v_{j}\)}} for each \(j\). Thus we have {{c2::\(T^{n}\)}} = {{c3::\(a_{1} \cdots a_{n} I\).}} Hence {{c4::\(z^{n}-a_{1} \cdots a_{n}\)}} is the minimal polynomial of \(T\). Because \(n=\operatorname{dim} V\) and the characteristic polynomial is a polynomial multiple of the minimal polynomial (9.26), this implies that {{c4::\(z^{n}-a_{1} \cdots a_{n}\)}} is also the characteristic polynomial of \(T\).<br><br>Thus 10.22 implies that<br><br><ul><li>\(\operatorname{det} T\) = {{c1::\((-1)^{n-1} a_{1} \cdots a_{n} .\)}}</li></ul><br>If some \(a_{j}\) equals 0 , then \(T v_{j}=0\) for some \(j\), which implies that 0 is an eigenvalue of \(T\) and hence \(\operatorname{det} T=0\). In other words, the formula above also holds if some \(a_{j}\) equals 0 .<br><br>Thus in order to have \(\operatorname{det} T=\operatorname{det} \mathcal{M}(T)\), we will have to make the determinant of the matrix in Example 10.26 equal to {{c1::\((-1)^{n-1} a_{1} \cdots a_{n}\)}}. However, we do not yet have enough evidence to make a reasonable guess about the proper definition of the determinant of an arbitrary square matrix.<br>

============================================================

  

    Definition {{c1::permutation}}, {{c2::perm \(n\)}}<br><br>- A {{c1::permutation}} of {{c5::\((1, \ldots, n)\)}} is a {{c3::list \(\left(m_{1}, \ldots, m_{n}\right)\)}} that contains {{c5::each of the numbers \(1, \ldots, n\)}} {{c4::exactly once.}}<br><br>- The set of all {{c1::permutations}} of {{c5::\((1, \ldots, n)\)}} is denoted {{c2::perm \(n\).}}

============================================================

  

    - The sign of a permutation \(\left(m_{1}, \ldots, m_{n}\right)\) is defined to be {{c1::1}} if the number of pairs of integers \((j, k)\) with {{c3::\(1 \leq j&lt;k \leq n\)}} such that {{c4::\(j\) appears after \(k\)}} in the list \(\left(m_{1}, \ldots, m_{n}\right)\) is {{c2::even}} and {{c1::-1}} if the number of such pairs is {{c2::odd}}.<br><br>- In other words, the sign of a permutation equals {{c2::1}} if the natural order has been changed {{c5::an even number of times}} and equals {{c2::-1}} if the natural order has been changed {{c5::an odd number of times.}}

============================================================

  

    {{c1::Interchanging}} two entries in a permutation {{c4::multiplies}} {{c5::{{c3::the sign of the permutation}}}} by {{c5::{{c2::-1 .}}}}

============================================================

  

    Suppose \(A\) is an \(n\)-by- \(n\) matrix<br><br>\[<br>A=\left(\begin{array}{ccc}<br>A_{1,1} &amp; \ldots &amp; A_{1, n} \\<br>\vdots &amp; &amp; \vdots \\<br>A_{n, 1} &amp; \ldots &amp; A_{n, n}<br>\end{array}\right) .<br>\]<br><br>The determinant of \(A\), denoted \(\operatorname{det} A\), is defined by<br><br><ul><li>\(\operatorname{det} A\) = {{c5::{{c3::\(\sum_{\left(m_{1}, \ldots, m_{n}\right) \in \operatorname{perm} n}\)}}}} {{c4::{{c2::\( \left(\operatorname{sign}\left(m_{1}, \ldots, m_{n}\right)\right)\)&nbsp;}}}} {{c4::{{c1::\(A_{m_{1}, 1} \cdots A_{m_{n}, n} .\)}}}}</li></ul>

============================================================

  

    The {{c2::number of permutations}} that one has to check to compute the {{c3::determinant}} of an {{c5::{{c4::nXn}}}} matrix explicitly is {{c5::{{c1::n!}}}}

============================================================

  

    In other words, the determinant of an {{c5::{{c1::uppertriangular}}}} matrix equals the {{c4::{{c2::product}}}} of {{c4::{{c3::the diagonal entries.}}}}

============================================================

  

    Suppose \(A\) is a square matrix and \(B\) is the matrix obtained from \(A\) by i{{c4::nterchanging two columns}}. Then<br><br><ul><li>{{c3::\(\operatorname{det} A\)}} = {{c5::{{c2::\(-\)}}}}{{c5::{{c1::\(\operatorname{det} B \text {. }\)}}}}</li></ul>

============================================================

  

    <br>If \(A\) is a square matrix that has {{c2::two}} {{c1::equal}} {{c3::columns}}, then {{c5::\(\operatorname{det} A\)}} = {{c4::\(0\).}}

============================================================

  

    <img src="paste-ae3bffff9d0b6980a22d6fc0e9982226ddd4a37e.jpg"><br>Proof Suppose \(A\) is a square matrix that has two equal columns. Interchanging the two equal columns of \(A\) gives the original matrix \(A\). Thus from 10.36 (with \(B=A\) ), we have<br><br><ul><li>{{c5::{{c2::\(\operatorname{det} A\)}}}} = {{c4::{{c1::\(-\operatorname{det} A \text {, }\)}}}}</li></ul><br>which implies that \(\operatorname{det} A\) = {{c4::{{c3::\(0\).}}}}<br>

============================================================

  

    Some books define the determinant to be the function defined on the square matrices that is {{c1::linear}} as a function of {{c2::each column}} {{c3::separately::how?}} and that satisfies 10.38 and {{c5::\(\operatorname{det} I\)}} = {{c4::\(1\).}} To prove that such a function exists and that it is unique takes a nontrivial amount of work.

============================================================

  

    A {{c2::permutation}} of the {{c3::columns}} of a matrix changes the {{c5::{{c4::determinant}}}} by a factor of {{c5::{{c1::the sign}}}} of {{c2::the permutation.}}

============================================================

  

    Suppose \(A=\left(\begin{array}{ccc}A \cdot, 1 &amp; \ldots &amp; A \cdot, n\end{array}\right)\) is an \(n\)-by- \(n\) matrix and \(\left(m_{1}, \ldots, m_{n}\right)\) is a permutation. Then<br><ul><li>{{c4::\(\operatorname{det}\)}} {{c3::\(\left(\begin{array}{lll}A \cdot, m_1 &amp; \ldots &amp; A \cdot, m_n\end{array}\right)\)}} = {{c5::{{c2::\(\left(\operatorname{sign}\left(m_1, \ldots, m_n\right)\right)\)}}}} {{c5::{{c1::\( \operatorname{det} A\)}}}}<br></li></ul>

============================================================

  

    {{c3::Determinant}} is a {{c1::linear}} {{c2::function}} of {{c5::each}} {{c4::column}}

============================================================

  

    Suppose \(k, n\) are positive integers with \(1 \leq k \leq n\). Fix \(n\)-by-1 matrices \(A \cdot, 1, \ldots, A_{\cdot, n}\) except \(A_{\cdot, k}\). Then the function that takes an {{c2::\(n\)-by-1 column vector}} {{c3::\(A \cdot, k\)}} to<br><br>{{c1::\[<br>\operatorname{det}\left(\begin{array}{lllll}<br>A \cdot, 1 &amp; \ldots &amp; A_{\cdot, k} &amp; \ldots &amp; A_{\cdot, n}<br>\end{array}\right)<br>\]}}<br><br>is a {{c4::linear map}} from the vector space of {{c5::\(n\)-by-1 matrices}} with entries in \(\mathbf{F}\) to {{c5::\(\mathbf{F}\).}}

============================================================

  

    Suppose \(A\) and \(B\) are square matrices of the same size. Then<br><br><ul><li>{{c5::{{c3::\(\operatorname{det}(A B)\)}}}} = {{c4::{{c2::\(\operatorname{det}(B A)\)}}}} = {{c4::{{c1::\((\operatorname{det} A)(\operatorname{det} B) .\)}}}}</li></ul>

============================================================

  

    {{c5::{{c3::Determinant}}}} of an {{c4::{{c1::operator}}}} equals {{c5::{{c3::determinant}}}} of its {{c4::{{c2::matrix}}}}<br><br>Suppose \(T \in \mathcal{L}(V)\). Then {{c5::{{c3::\(\operatorname{det}\)}}}} {{c4::{{c1::\(T\)}}}} = {{c3::\(\operatorname{det}\)}} {{c4::{{c2::\(\mathcal{M}(T)\).}}}}

============================================================

  

    Suppose \(S, T \in \mathcal{L}(V)\). Then<br><br><ul><li>{{c5::{{c3::\(\operatorname{det}(S T)\)}}}} = {{c4::{{c2::\(\operatorname{det}(T S)\)}}}} = {{c4::{{c1::\((\operatorname{det} S)(\operatorname{det} T) .\)}}}}<br></li></ul>

============================================================

  

    {{c1::Isometries}} have {{c3::determinant}} with {{c5::{{c4::absolute value}}}} {{c5::{{c2::1}}}}<br><br>Suppose \(V\) is an inner product space and \(S \in \mathcal{L}(V)\) is an {{c1::isometry}}. Then {{c5::{{c4::\(|\)}}}} {{c3::\(\operatorname{det} S\)}} {{c4::\(|\)}} = {{c5::{{c2::\(1\).}}}}

============================================================

  

    Recall that if \(V\) is an inner product space and \(T \in \mathcal{L}(V)\), then \(T^{*} T\) is a {{c1::positive operator}} and hence has a unique {{c2::positive square root}}, denoted {{c5::\(\sqrt{T^{*} T}\)}} . Because {{c5::\(\sqrt{T^{*} T}\)}} is {{c2::positive}}, all its eigenvalues are {{c3::nonnegative}}, and hence det {{c5::\(\sqrt{T^{*} T}\)}} {{c4::\(\geq\) \( 0\)}}.

============================================================

  

    10.46 Example Suppose \(V\) is a real inner product space and \(T \in \mathcal{L}(V)\) is invertible (and thus det \(T\) is either positive or negative). Attach a geometric meaning to the sign of \(\operatorname{det} T\).<br><br>Solution First we consider an isometry \(S \in \mathcal{L}(V)\). By 10.45, the determinant of \(S\) equals {{c1::1}} or {{c1::-1}} . Note that<br><br>{{c3::\[<br>\{v \in V: S v=-v\}<br>\]}}<br><br>is the {{c2::eigenspace \(E(-1, S)\)}}. Thinking geometrically, we could say that this is the subspace on which \(S\) {{c4::reverses direction}}. An examination of proof 2 of 10.45 shows that \(\operatorname{det} S\) = {{c1::\(1\)}} if this subspace has {{c5::even}} dimension and \(\operatorname{det} S\) = {{c1::\(-1\)}} if this subspace has {{c5::odd}} dimension.<br><br>Returning to our arbitrary invertible operator \(T \in \mathcal{L}(V)\), by the Polar Decomposition (7.45) there is an isometry \(S \in \mathcal{L}(V)\) such that<br><br>\[<br>T=S \sqrt{T^{*} T}<br>\]<br><br>Now 10.44 tells us that<br><br>\[<br>\operatorname{det} T=(\operatorname{det} S)\left(\operatorname{det} \sqrt{T^{*} T}\right) .<br>\]<br><br>The remarks just before this example pointed out that det \(\sqrt{T^{*} T} \geq 0\). Thus whether \(\operatorname{det} T\) is positive or negative depends on whether \(\operatorname{det} S\) is {{c1::positive}} or {{c1::negative}}. As we saw in the paragraph above, this depends on whether the subspace on which \(S\) reverses direction has {{c5::even}} or {{c5::odd}} dimension.<br><br>Because \(T\) is the product of \(S\) and an operator that never {{c4::reverses direction}} (namely, \(\sqrt{T^{*} T}\) ), we can reasonably say that whether \(\operatorname{det} T\) is positive or negative depends on whether \(T\) {{c4::reverses vectors}} an {{c5::even}} or an {{c5::odd}} number of times.

============================================================

  

    Suppose \(V\) is an inner product space and \(T \in \mathcal{L}(V)\). Then<br><br><ul><li>{{c5::{{c2::\(|\)}}}} {{c4::{{c3::\(\operatorname{det} T\)}}}} {{c2::\(|\)}} = {{c4::{{c1::\(\operatorname{det} \sqrt{T^{*} T} .\)}}}}</li></ul>

============================================================

  

    A {{c1::box}} in \(\mathbf{R}^{n}\) is a set of the form<br><br>{{c2::\[<br>\left\{\left(y_{1}, \ldots, y_{n}\right) \in \mathbf{R}^{n}: x_{j}&lt;y_{j}&lt;x_{j}+r_{j} \text { for } j=1, \ldots, n\right\},<br>\]}}<br><br>where \(r_{1}, \ldots, r_{n}\) are {{c5::{{c4::positive}}}} numbers and \(\left(x_{1}, \ldots, x_{n}\right) \in \mathbf{R}^{n}\). The numbers \(r_{1}, \ldots, r_{n}\) are called the {{c5::{{c3::side lengths}}}} of the {{c1::box}}.

============================================================

  

    Definition {{c1::volume}} of a {{c2::box}}<br><br>The {{c1::volume}} of a {{c2::box}} \(B\) in \(\mathbf{R}^{n}\) with {{c5::{{c4::side lengths \(r_{1}, \ldots, r_{n}\)}}}} is defined to be {{c5::{{c3::\(r_{1} \cdots r_{n}\)}}}} and is denoted by {{c1::volume}} \(B\).

============================================================

  

    Definition {{c1::volume}}<br><br>Suppose \(\Omega \subset \mathbf{R}^{n}\). Then the {{c1::volume}} of \(\Omega\), denoted {{c1::volume}} \(\Omega\), is defined to be the {{c3::infimum}} of<br><br><ul><li>{{c2::\(\text { volume } B_{1}+\text { volume } B_{2}+\cdots \text {, }\)}}</li></ul><br>where the {{c3::infimum}} is taken over {{c4::all sequences \(B_{1}, B_{2}, \ldots\)}} of boxes in {{c5::\(\mathbf{R}^{n}\)}} whose {{c4::union}} {{c5::contains \(\Omega\).}}<br>

============================================================

  

    For \(T\) a function defined on a {{c5::{{c2::set}}}} \(\Omega\), define {{c4::{{c3::\(T(\Omega)\)}}}} by<br><br><ul><li>{{c4::{{c3::\(T(\Omega) \)}}}} = {{c4::{{c1::\(\{T x: x \in \Omega\} .\)}}}}</li></ul>

============================================================

  

    {{c5::{{c1::Positive}}}} operators change {{c4::{{c2::volume}}}} by factor of {{c4::{{c3::determinant}}}}<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;

============================================================

  

    Suppose \(T \in \mathcal{L}\left(\mathbf{R}^{n}\right)\) is a {{c1::positive}} operator and \(\Omega \subset \mathbf{R}^{n}\). Then<br><br><ul><li>{{c3::\(\text { volume } T(\Omega)\)}} = {{c5::{{c2::\((\operatorname{det} T)\)}}}} {{c5::{{c4::\((\text { volume } \Omega) .\)}}}}</li></ul>

============================================================

  

    Suppose \(S \in \mathcal{L}\left(\mathbf{R}^{n}\right)\) is an {{c1::isometry}} and \(\Omega \subset \mathbf{R}^{n}\). Then<br><br><ul><li>{{c5::\(\text { volume }\)}}{{c2::\(S(\Omega)\)}} = {{c4::\(\text { volume }\)}}{{c3::\( \Omega \text {. }\)}}</li></ul><br>

============================================================

  

    Suppose \(T \in \mathcal{L}\left(\mathbf{R}^{n}\right)\) and \(\Omega \subset \mathbf{R}^{n}\). Then<br><ul><li>{{c4::\(\text { volume }\)}} \(T(\Omega)\) = {{c2::\(|\)}} {{c1::\(\operatorname{det} T\)}} {{c3::\(|\)}} {{c5::\((\text { volume } \Omega) .\)}}<br></li></ul>

============================================================

  

    \subsection{Definition differentiable, derivative, \(\sigma^{\prime}(x)\)}<br><br>Suppose \(\Omega\) is an {{c5::open}} subset of \(\mathbf{R}^{n}\) and \(\sigma\) is a function from \(\Omega\) to \(\mathbf{R}^{n}\). For \(x \in \Omega\), the function \(\sigma\) is called differentiable at \(x\) if there exists an operator \(T \in \mathcal{L}\left(\mathbf{R}^{n}\right)\) such that<br><br><ul><li>{{c2::\(\lim_{y \rightarrow 0}\)}} {{c1::\(\frac{\|\sigma(x+y)-\sigma(x)-T y\|}{\|y\|} \)}} = {{c3::\(0 .\)}}</li></ul><br>If \(\sigma\) is differentiable at \(x\), then the {{c5::unique}} operator \(T \in \mathcal{L}\left(\mathbf{R}^{n}\right)\) satisfying the equation above is called the {{c4::derivative of \(\sigma\) at \(x\)}} and is denoted by {{c5::\(\sigma^{\prime}(x)\).}}<br>

============================================================

  

    <img src="paste-da2f4d3e563bb2e989468abf7ce73c1092a0da0a.jpg"><br><br>If {{c1::\(n=1\)}}, then the derivative in the sense of the definition above is the operator on {{c4::\(\mathbf{R}\)}} of {{c5::{{c3::multiplication}}}} by the derivative in the usual sense of {{c5::{{c2::one-variable calculus.}}}}

============================================================

  

    <br>The idea of the derivative is that for \(x\) {{c5::fixed}} and \(\|y\|\) {{c5::small}},<br><br><ul><li>{{c4::\(\sigma(x+y)\)}}&nbsp; \(\approx\) {{c1::\( \sigma(x)\)}} {{c3::+}} {{c2::\(\left(\sigma^{\prime}(x)\right)(y)\)}}</li></ul><br>because \(\sigma^{\prime}(x) \in \mathcal{L}\left(\mathbf{R}^{n}\right)\), this makes sense.<br>

============================================================

  

    Suppose \(\Omega\) is an open subset of \(\mathbf{R}^{n}\) and \(\sigma\) is a function from \(\Omega\) to \(\mathbf{R}^{n}\). We can write<br><br><ul><li>\(\sigma(x)\) = {{c2::\(\left(\sigma_{1}(x), \ldots, \sigma_{n}(x)\right),\)}}</li></ul><br>where each {{c2::\(\sigma_{j}\)}} is a {{c3::function from \(\Omega\) to \(\mathbf{R}\)}}. The {{c4::partial derivative}} of {{c2::\(\sigma_{j}\)}} with respect to {{c5::the \(k^{\text {th } } \) coordinate}} is denoted {{c4::\(D_{k} \sigma_{j}\)}}. Evaluating this {{c4::partial derivative}} at a point \(x \in \Omega\) gives {{c5::\(D_{k} \sigma_{j}(x)\)}}. If \(\sigma\) is differentiable at \(x\), then the matrix of \(\sigma^{\prime}(x)\) with respect to the standard basis of \(\mathbf{R}^{n}\) contains {{c5::\(D_{k} \sigma_{j}(x)\)}} in row \(j\), column \(k\) (this is left as an exercise). In other words,<br><br>\(\mathcal{M}\left(\sigma^{\prime}(x)\right)\) = {{c1::\(\left(\begin{array}{ccc}D_{1} \sigma_{1}(x) &amp; \ldots &amp; D_{n} \sigma_{1}(x) \\ \vdots &amp; &amp; \vdots \\ D_{1} \sigma_{n}(x) &amp; \ldots &amp; D_{n} \sigma_{n}(x)\end{array}\right)\).}}<br>

============================================================

  

    <b>Change of variables in an integral</b><br><br>Suppose \(\Omega\) is an {{c5::open}} subset of \(\mathbf{R}^{n}\) and \(\sigma: \Omega \rightarrow \mathbf{R}^{n}\) is differentiable at every point of \(\Omega\). If \(f\) is a real-valued function defined on \(\sigma(\Omega)\), then<br><ul><li>\(\int_{\sigma(\Omega)} f(y)\) \(d y\) = {{c5::\(\int_{\Omega}\)}} {{c3::\(f(\sigma(x))\)}} {{c4::\(|\)}} {{c1::\(\operatorname{det}\)}} {{c2::\(\sigma^{\prime}(x)\)}} {{c4::\(|\)}} {{c5::\(d x\)}}<br></li></ul>

============================================================

  

    Suppose \(V\) is a {{c5::real}} vector space. Suppose \(T \in \mathcal{L}(V)\) has {{c5::no}} {{c1::eigenvalues}}. Prove that {{c3::\(\operatorname{det} T\)}} {{c4::\(&gt;\)}} {{c2::\(0\).}}

============================================================

  

    2 Suppose \(V\) is a real vector space with even dimension and \(T \in \mathcal{L}(V)\). Suppose {{c3::\(\operatorname{det} T\)}} \(&lt;\) {{c1::\(0\)}}. Prove that \(T\) has at least {{c5::{{c2::two}}}} {{c5::{{c4::distinct eigenvalues.}}}}

============================================================

  

    4 Suppose \(T \in \mathcal{L}(V)\) and \(c \in \mathbf{F}\). Prove that {{c4::\(\operatorname{det}\)}}( {{c3::\(c T\)}}) = {{c5::{{c1::\(c^{\operatorname{dim} V}\)}}}} {{c5::{{c2::\(\operatorname{det} T\).}}}}

============================================================

  

    6 Suppose \(A\) is a {{c5::block upper-triangular}} matrix<br><br>{{c5::\[<br>A=\left(\begin{array}{ccc}<br>A_{1} &amp; &amp; * \\<br>&amp; \ddots &amp; \\<br>0 &amp; &amp; A_{m}<br>\end{array}\right),<br>\]}}<br><br>where {{c5::each \(A_{j}\) along the diagonal}} is a {{c4::square}} matrix. Prove that<br><br><ul><li>{{c3::\(\operatorname{det} A\)}} = {{c1::\(\left(\operatorname{det} A_{1}\right)\)}}{{c4::\( \cdots\)}}{{c2::\(\left(\operatorname{det} A_{m}\right) \text {. }\)}}</li></ul>

============================================================

  

    7 Suppose \(A\) is an \(n\)-by- \(n\) matrix with {{c3::real}} entries. Let \(S \in \mathcal{L}\left(\mathbf{C}^{n}\right)\) denote the operator on \(\mathbf{C}^{n}\) whose matrix {{c4::equals \(A\)}}, and let \(T \in \mathcal{L}\left(\mathbf{R}^{n}\right)\) denote the operator on \(\mathbf{R}^{n}\) whose matrix {{c4::equals \(A\).}} <br><br>Prove that:<br><ul><li>{{c5::{{c1::&nbsp;\(\operatorname{trace}\)}}}} \(S\) = {{c5::{{c2::\(\operatorname{trace}\)}}}} \(T\)</li><li>{{c5::{{c2::\(\operatorname{det}\)}}}} \(S\)= {{c5::{{c1::\(\operatorname{det}\)}}}} \(T\).</li></ul>

============================================================

  

    8 Suppose \(V\) is an inner product space and \(T \in \mathcal{L}(V)\). Prove that<br><br><ul><li>{{c2::\(\operatorname{det} T^{*}\)}} = {{c1::\(\overline{\operatorname{det} T} .\)}}</li></ul><br>Use this to prove that {{c5::{{c3::\(|\operatorname{det} T|\)}}}} = {{c5::{{c4::\(\operatorname{det} \sqrt{T^{*} T}\)}}}}, giving a different proof than was given in 10.47 .<br>

============================================================

  

    9 Suppose \(\Omega\) is an open subset of \(\mathbf{R}^{n}\) and \(\sigma\) is a function from \(\Omega\) to \(\mathbf{R}^{n}\). Suppose \(x \in \Omega\) and \(\sigma\) is differentiable at \(x\). Prove that the operator \(T \in \mathcal{L}\left(\mathbf{R}^{n}\right)\) satisfying the equation in 10.56 is {{c5::{{c1::unique}}}}.<br><br>[This exercise shows that the notation {{c4::{{c3::\(\sigma^{\prime}(x)\)}}}} is {{c4::{{c2::justified}}}}.]<br><br><img src="paste-012df6b74f775b6d8e929c08c404c10f0d20565c.jpg">

============================================================

  

    Why is this true?<br><img src="paste-3d2461b21ad50d724d078c7393ed9e12af132832.jpg"><br><ul><li>{{c1::Polar decomposition}} + {{c2::isometries}} having {{c2::absolute value 1}} + {{c5::{{c3::positive}}}} operators having {{c5::{{c3::positive determinants}}}}<br></li><li>{{c5::{{c4::<img src="paste-a92dd181146cebb2249484b2786f1b8eeafd7e9b.jpg">}}}}<br></li></ul>

============================================================

  

    Why is the following true?<br><img src="paste-e6a4bcd16ea508f4b4869f36795caff771609e76.jpg"><br><ul><li>Since T is positive it is {{c1::self-adjoint}} and applying it to the proper {{c2::orthonormal eigenbasis}} just {{c5::{{c3::stretches each basis vectord}}}} by the {{c5::{{c3::corresponding eigenvalues}}}}</li><li>The volume is then {{c2::stretched by each side of a box}} {{c5::{{c3::getting multiplied by an eigenvalue}}}} and thus by a factor equal to the determinant</li><li>Full proof:</li><li>{{c5::{{c4::<img src="paste-ba3d87c4ac1fe692de9bca7996fa5a6cbfeaa616.jpg">}}}}<br></li></ul>

============================================================

  

    Why&nbsp; is this true in the simplest way?<br><img src="paste-7d190438a79d987f41a903a7a7b1b120b5b15f9d.jpg"><br><ul><li>Because S {{c5::{{c1::maintains distances}}}} {{c4::{{c2::between points}}}} and thus the volume</li><li>Full proof</li><li>{{c4::{{c3::<img src="paste-b29b5de9aec9122677716f7d29cba2f01896caab.jpg">}}}}<br></li></ul>

============================================================

  

    <img src="paste-6205300301b9d195f570d2fb798e0962be9e228b.jpg"><br>Proof 2 Consider our complete description of isometries on real inner product spaces.<br>The isometry \(S\) can be decomposed into pieces, each of which is the {{c5::{{c1::identity}}}} on some subspace (which clearly does not change volume) or multiplication by {{c4::{{c2::-1}}}} on some subspace (which again clearly does not change volume) or a {{c4::{{c3::rotation on a 2-dimensional subspace}}}} (which again does not change volume).

============================================================

  

    <img src="paste-3ed5a2b589f17f44d21bebc2b03de1c4c22870d6.jpg"><br>Proof By the Polar Decomposition, there is an isometry \(S \in \mathcal{L}(V)\) such that<br>\[<br>T=S \sqrt{T^* T} .<br>\]<br><br>Thus \(T(\Omega)\) = {{c5::\(S\left(\sqrt{T^* T}(\Omega)\right)\)}}. Hence<br><ul><li>\(\operatorname{volume} T(\Omega) \)&nbsp;</li><li>= {{c1::\(\operatorname{volume} S\left(\sqrt{T^* T}(\Omega)\right) \)}} by {{c2::polar decomp}}</li><li>= {{c2::\(\text { volume } \sqrt{T^* T}(\Omega) \)}} by {{c1::isometries not changing volume}}</li><li>= {{c3::\(\left(\operatorname{det} \sqrt{T^* T}\right)(\operatorname{volume} \Omega) \)}} by {{c4::volume of positive operator}}</li><li>= {{c4::\(|\operatorname{det} T|(\operatorname{volume} \Omega),\)}}&nbsp; by {{c3::det \(\sqrt{T T^*}\) = abs(det T)}}</li></ul>as desired.<br>

============================================================

  

    Definition: integral, \(\int_{\Omega} f\)<br>Suppose \(\Omega \subset \mathbf{R}^n\) and \(f: \Omega \rightarrow \mathbf{R}\). The integral of \(f\) over \(\Omega\), denoted<br>\[<br>\int_{\Omega} f \text { or } \int_{\Omega} f(x) d x<br>\]<br>is defined by breaking \(\Omega\) into pieces small enough that \(f\) is {{c1::almost constant on each piece}}. On each piece, multiply the ({{c1::almost constant}}) value of \(f\) by the {{c2::volume of the piece}}, then {{c3::add up these numbers for all the pieces}}, getting an {{c5::approximation to the integral}} that becomes {{c5::more accurate}} as {{c4::\(\Omega\) is divided into finer pieces.}}

============================================================

  

    For an integral to make sense:<br>Actually, \(\Omega\) in the definition needs to be a reasonable set (for example, {{c5::{{c1::open}}}} or {{c4::{{c2::measurable}}}}) and \(f\) needs to be a reasonable function (for example, {{c4::{{c3::continuous}}}} or {{c2::measurable}}).

============================================================

  

    How does this relate to the standard definition of the derivative on 1-d calculus?<br><img src="paste-e4b721614d2b4a25575452377cd9270b6818a4a9.jpg"><br>When n={{c5::{{c1::1}}}}:<br><ul><li>This derivative is defined as the operator of {{c4::{{c2::multiplication}}}} by {{c4::{{c3::the classical standard derivative}}}}</li></ul>

============================================================

  

    <img src="paste-d657e51b811f7c0a0fbcc922da5c0c64f1aae4cd.jpg"><br>Proof Let \(x \in \Omega\) and let \(\Gamma\) be a small subset of \(\Omega\) containing \(x\) such that \(f\) is approximately equal to the constant \(f(\sigma(x))\) on the set \(\sigma(\Gamma)\).<br>The approximation<br><ul><li>\(\sigma(x+y) \approx\) {{c1::\(\sigma(x)+\left(\sigma^{\prime}(x)\right) y\)}}</li></ul>shows that<br><ul><li>\(\text { volume } \sigma(\Gamma) \approx\) {{c2::\( \operatorname{volume}\left[\left(\sigma^{\prime}(x)\right)(\Gamma)\right] \text {. }\)}}</li></ul><br>Hence<br><ul><li>\(\text { volume } \sigma(\Gamma) \approx\) {{c3::\(\left|\operatorname{det} \sigma^{\prime}(x)\right|(\text { volume } \Gamma) \text {. }\)}}</li></ul><br>Let \(w=\sigma(x)\). Multiply the left side of the equation above by \(f(w)\) and the right side by \(f(\sigma(x))\), getting<br><ul><li>\(f(w) \text { volume } \sigma(\Gamma) \approx\) {{c4::\( f(\sigma(x))\left|\operatorname{det} \sigma^{\prime}(x)\right|(\text { volume } \Gamma) \text {. }\)}}</li></ul><br>Now {{c5::break \(\Omega\) into many small pieces}} and add the corresponding versions of the equation above, getting the desired result.<br>

============================================================

