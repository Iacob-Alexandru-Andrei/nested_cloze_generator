=== Pair #1 ===
Content differs beyond just cloze markup!
Differences (normalized text):
--- before (normalized)
+++ after (normalized)
@@ -1,1809 +1,8 @@
-      Why is clients starting from different initialisations bad?<br><ul><li>Causes weight divergence even if the data is IID</li></ul>
-
-    After:
-      Why is clients starting from different initialisations bad?<br><ul><li>Causes weight divergence even if the data is IID</li></ul>
+      Keeping only the optimiser state in fp32 matches the performance of fp32 training and only slightly increases memory compared to bf16
 
 ============================================================
 
-Note ID: 1693264851824
+Note ID: 1701947741469
   Field: Text
     Before:
-      5 (a) Show that if we think of \(\mathbf{C}\) as a vector space over \(\mathbf{R}\), then the list \((1+i, 1-i)\) is linearly independent.<br><br>(b) Show that if we think of \(\mathbf{C}\) as a vector space over \(\mathbf{C}\), then the list \((1+i, 1-i)\) is linearly dependent.
-
-    After:
-      5 (a) Show that if we think of \(\mathbf{C}\) as a vector space over \(\mathbf{R}\), then the list \((1+i, 1-i)\) is linearly independent.<br><br>(b) Show that if we think of \(\mathbf{C}\) as a vector space over \(\mathbf{C}\), then the list \((1+i, 1-i)\) is linearly dependent.
-
-============================================================
-
-Note ID: 1693264905846
-  Field: Text
-    Before:
-      1 Suppose \(v_{1}, v_{2}, v_{3}, v_{4}\) spans \(V\). Prove that the list<br><br>\[<br>v_{1}-v_{2}, v_{2}-v_{3}, v_{3}-v_{4}, v_{4}<br>\]<br><br>also spans \(V\).
-
-    After:
-      1 Suppose \(v_{1}, v_{2}, v_{3}, v_{4}\) spans \(V\). Prove that the list<br><br>\[<br>v_{1}-v_{2}, v_{2}-v_{3}, v_{3}-v_{4}, v_{4}<br>\]<br><br>also spans \(V\).
-
-============================================================
-
-Note ID: 1693265369941
-  Field: Text
-    Before:
-      Suppose \(v_{1}, \ldots, v_{m}\) is linearly independent in \(V\) and \(w \in V\). <br><ul><li>Show that \(v_{1}, \ldots, v_{m}, w\) is linearly independent if and only if</li></ul><br>\[<br>w \notin \operatorname{span}\left(v_{1}, \ldots, v_{m}\right) .<br>\]<br>
-
-    After:
-      Suppose \(v_{1}, \ldots, v_{m}\) is linearly independent in \(V\) and \(w \in V\). <br><ul><li>Show that \(v_{1}, \ldots, v_{m}, w\) is linearly independent if and only if</li></ul><br>\[<br>w \notin \operatorname{span}\left(v_{1}, \ldots, v_{m}\right) .<br>\]<br>
-
-============================================================
-
-Note ID: 1693304890148
-  Field: Text
-    Before:
-      <ul><li>Suppose \(T \in \mathcal{L}(V, W)\) is injective and \(v_{1}, \ldots, v_{n}\) is linearly independent in \(V\).&nbsp;</li><li>Prove that \(T v_{1}, \ldots, T v_{n}\) is linearly independent in \(W\).</li></ul>
-
-    After:
-      <ul><li>Suppose \(T \in \mathcal{L}(V, W)\) is injective and \(v_{1}, \ldots, v_{n}\) is linearly independent in \(V\).&nbsp;</li><li>Prove that \(T v_{1}, \ldots, T v_{n}\) is linearly independent in \(W\).</li></ul>
-
-============================================================
-
-Note ID: 1693305017320
-  Field: Text
-    Before:
-      10 <br><br><ul><li>Suppose \(v_{1}, \ldots, v_{n}\) spans \(V\) and \(T \in \mathcal{L}(V, W)\).&nbsp;</li><li>Prove that the list \(T v_{1}, \ldots, T v_{n}\) spans range \(T\).</li></ul>
-
-    After:
-      10 <br><br><ul><li>Suppose \(v_{1}, \ldots, v_{n}\) spans \(V\) and \(T \in \mathcal{L}(V, W)\).&nbsp;</li><li>Prove that the list \(T v_{1}, \ldots, T v_{n}\) spans range \(T\).</li></ul>
-
-============================================================
-
-Note ID: 1693305064083
-  Field: Text
-    Before:
-      <ul><li>Suppose \(S_{1}, \ldots, S_{n}\) are injective linear maps such that \(S_{1} S_{2} \cdots S_{n}\) makes sense.&nbsp;</li><li>Prove that \(S_{1} S_{2} \cdots S_{n}\) is injective.</li></ul>
-
-    After:
-      <ul><li>Suppose \(S_{1}, \ldots, S_{n}\) are injective linear maps such that \(S_{1} S_{2} \cdots S_{n}\) makes sense.&nbsp;</li><li>Prove that \(S_{1} S_{2} \cdots S_{n}\) is injective.</li></ul>
-
-============================================================
-
-Note ID: 1693306042543
-  Field: Text
-    Before:
-      Suppose there exists a linear map on \(V\) whose null space and range are both finite-dimensional. Prove that \(V\) is finite-dimensional.
-
-    After:
-      Suppose there exists a linear map on \(V\) whose null space and range are both finite-dimensional. Prove that \(V\) is finite-dimensional.
-
-============================================================
-
-Note ID: 1693307869548
-  Field: Text
-    Before:
-      11 <br>Suppose \(a=\left(\begin{array}{lll}a_{1} &amp; \cdots &amp; a_{n}\end{array}\right)\) is a 1-by- \(n\) matrix and \(C\) is an \(n\)-by- \(p\) matrix. <br><ul><li>Prove that</li><li>\(a C\)= \(a_{1} C_{1, \cdot}+\cdots+a_{n} C_{n, \cdots}\)</li><li>In other words, show that \(a C\) is a linear combination of the rows of \(C\), with the scalars that multiply the rows coming from \(a\).</li></ul>
-
-    After:
-      11 <br>Suppose \(a=\left(\begin{array}{lll}a_{1} &amp; \cdots &amp; a_{n}\end{array}\right)\) is a 1-by- \(n\) matrix and \(C\) is an \(n\)-by- \(p\) matrix. <br><ul><li>Prove that</li><li>\(a C\)= \(a_{1} C_{1, \cdot}+\cdots+a_{n} C_{n, \cdots}\)</li><li>In other words, show that \(a C\) is a linear combination of the rows of \(C\), with the scalars that multiply the rows coming from \(a\).</li></ul>
-
-============================================================
-
-Note ID: 1694674357653
-  Field: Text
-    Before:
-      1 <br><br>Suppose \(T \in \mathcal{L}(U, V)\) and \(S \in \mathcal{L}(V, W)\) are both invertible linear maps. <br><ul><li>Prove that \(S T \in \mathcal{L}(U, W)\) is invertible&nbsp;</li><li>And that \((S T)^{-1}\) = \(T^{-1} S^{-1}\).</li></ul>
-
-    After:
-      1 <br><br>Suppose \(T \in \mathcal{L}(U, V)\) and \(S \in \mathcal{L}(V, W)\) are both invertible linear maps. <br><ul><li>Prove that \(S T \in \mathcal{L}(U, W)\) is invertible&nbsp;</li><li>And that \((S T)^{-1}\) = \(T^{-1} S^{-1}\).</li></ul>
-
-============================================================
-
-Note ID: 1694675926329
-  Field: Text
-    Before:
-      10 <br>Suppose \(V\) is finite-dimensional and \(S, T \in \mathcal{L}(V)\). <br><ul><li>Prove that \(S T\) = \(I\) if and only if \(T S\) = \(I\).</li></ul>
-
-    After:
-      10 <br>Suppose \(V\) is finite-dimensional and \(S, T \in \mathcal{L}(V)\). <br><ul><li>Prove that \(S T\) = \(I\) if and only if \(T S\) = \(I\).</li></ul>
-
-============================================================
-
-Note ID: 1694774847509
-  Field: Text
-    Before:
-      4. (3 points) The second edition Axler, page 12 , exercise 5 . <br><br>For each of the following subsets of \(\mathbf{F}^{3}\), determine whether it is a subspace of \(F^3\)<ul><li>(a) \(\left\{\left(x_{1}, x_{2}, x_{3}\right) \in \mathbf{F}^{3}: x_{1}+2 x_{2}+3 x_{3}=0\right\}\).&nbsp;</li><ul><li>Addition \( (x_1+y_1) + 2(x_2+y_2) + 3(x_3+y_3) \) = 0</li><li>Mult:&nbsp;\(\lambda x_1 + 2 (\lambda x_2) + 3(\lambda x_3)\) = 0</li></ul><li>(b) \(\left\{\left(x_{1}, x_{2}, x_{3}\right) \in \mathbf{F}^{3}: x_{1}+2 x_{2}+3 x_{3}=4\right\}\). </li><ul><li>No origin</li></ul><li>(c) \(\left\{\left(x_{1}, x_{2}, x_{3}\right) \in \mathbf{F}^{3}: x_{1} x_{2} x_{3}=0\right\}\).</li><ul><li>&nbsp;Addition:&nbsp; &nbsp;\( \prod x_i = 0 \land \prod y_i = 0\) \(\not \implies\) \(&nbsp; \prod (x_i+y_i) = 0 \)&nbsp;</li></ul><li>(d) \(\left\{\left(x_{1}, x_{2}, x_{3}\right) \in \mathbf{F}^{3}: x_{1}=5 x_{3}\right\}\).</li><ul><li>Addition:&nbsp;\(x_1 + y_1\) = \(5x_3 + 5 y_3\) =&nbsp;\(5 (x_3+y_3)\)</li><li>Mult:&nbsp;\(&nbsp; \lambda x_1\) =&nbsp;\(5 \lambda x_3\)</li></ul></ul>
-
-    After:
-      4. (3 points) The second edition Axler, page 12 , exercise 5 . <br><br>For each of the following subsets of \(\mathbf{F}^{3}\), determine whether it is a subspace of \(F^3\)<ul><li>(a) \(\left\{\left(x_{1}, x_{2}, x_{3}\right) \in \mathbf{F}^{3}: x_{1}+2 x_{2}+3 x_{3}=0\right\}\).&nbsp;</li><ul><li>Addition \( (x_1+y_1) + 2(x_2+y_2) + 3(x_3+y_3) \) = 0</li><li>Mult:&nbsp;\(\lambda x_1 + 2 (\lambda x_2) + 3(\lambda x_3)\) = 0</li></ul><li>(b) \(\left\{\left(x_{1}, x_{2}, x_{3}\right) \in \mathbf{F}^{3}: x_{1}+2 x_{2}+3 x_{3}=4\right\}\). </li><ul><li>No origin</li></ul><li>(c) \(\left\{\left(x_{1}, x_{2}, x_{3}\right) \in \mathbf{F}^{3}: x_{1} x_{2} x_{3}=0\right\}\).</li><ul><li>&nbsp;Addition:&nbsp; &nbsp;\( \prod x_i = 0 \land \prod y_i = 0\) \(\not \implies\) \(&nbsp; \prod (x_i+y_i) = 0 \)&nbsp;</li></ul><li>(d) \(\left\{\left(x_{1}, x_{2}, x_{3}\right) \in \mathbf{F}^{3}: x_{1}=5 x_{3}\right\}\).</li><ul><li>Addition:&nbsp;\(x_1 + y_1\) = \(5x_3 + 5 y_3\) =&nbsp;\(5 (x_3+y_3)\)</li><li>Mult:&nbsp;\(&nbsp; \lambda x_1\) =&nbsp;\(5 \lambda x_3\)</li></ul></ul>
-
-============================================================
-
-Note ID: 1694777496098
-  Field: Text
-    Before:
-      (3 points) The second edition Axler, page 35, exercise 3.<br><br>Suppose \(\left(v_{1}, \ldots, v_{n}\right)\) is linearly independent in \(V\) and \(w \in V\). <br><br>Prove that if \(\left(v_{1}+\right.\) \(\left.w, \ldots, v_{n}+w\right)\) is linearly dependent, then \(w \in \operatorname{span}\left(v_{1}, \ldots, v_{n}\right)\).<br><br>Solution:<br><ul><li>\(\vec{c} \cdot \vec{v} = 0\)&nbsp;\(\implies\)&nbsp;\( \vec{c} = \vec{0}\)<br></li><li>Not linearly indp implies&nbsp;\(\vec{c} \neq \vec{0}\)<br></li><ul><li>\(\vec{c} \cdot (\vec{v} + w \cdot \vec{1})\)<br></li><li>=&nbsp;\(\vec{c} \cdot \vec{v}\) + \(w \times \vec{c} \cdot \vec{1}\)</li><li>=&nbsp;\(\vec{c} \cdot \vec{v}\)&nbsp; + \(w \sum c_i\)</li></ul><li>Which implies:</li><ul><li>w =&nbsp;\(\frac{\vec{c} \cdot \vec{v} }{\sum c_i}\)</li><li>w&nbsp;\(\in\)&nbsp;\(\operatorname{span} \vec{v}\)</li></ul></ul><br>
-
-    After:
-      (3 points) The second edition Axler, page 35, exercise 3.<br><br>Suppose \(\left(v_{1}, \ldots, v_{n}\right)\) is linearly independent in \(V\) and \(w \in V\). <br><br>Prove that if \(\left(v_{1}+\right.\) \(\left.w, \ldots, v_{n}+w\right)\) is linearly dependent, then \(w \in \operatorname{span}\left(v_{1}, \ldots, v_{n}\right)\).<br><br>Solution:<br><ul><li>\(\vec{c} \cdot \vec{v} = 0\)&nbsp;\(\implies\)&nbsp;\( \vec{c} = \vec{0}\)<br></li><li>Not linearly indp implies&nbsp;\(\vec{c} \neq \vec{0}\)<br></li><ul><li>\(\vec{c} \cdot (\vec{v} + w \cdot \vec{1})\)<br></li><li>=&nbsp;\(\vec{c} \cdot \vec{v}\) + \(w \times \vec{c} \cdot \vec{1}\)</li><li>=&nbsp;\(\vec{c} \cdot \vec{v}\)&nbsp; + \(w \sum c_i\)</li></ul><li>Which implies:</li><ul><li>w =&nbsp;\(\frac{\vec{c} \cdot \vec{v} }{\sum c_i}\)</li><li>w&nbsp;\(\in\)&nbsp;\(\operatorname{span} \vec{v}\)</li></ul></ul><br>
-
-============================================================
-
-Note ID: 1694789939193
-  Field: Text
-    Before:
-      2. (3 points) Let \(V\) be the vector space of polynomials of degree at most 999 with real coefficients. Define a linear map<br><br>\[<br>T: V \rightarrow \mathbb{R}^{100}, \quad T(p)=(p(1), p(2), \ldots, p(100)) .<br>\]<br><br><ul><li>a) Find the dimension of the null space of \(T\)</li><ul><li>Since p must be divisible by the polynomial \(z(x)\) =&nbsp;\(\prod_{i=1}^{100} (x-i)\) with degree 100&nbsp;</li><li>A basis of null space consists of the polynomials&nbsp;</li><ul><li>\( (z(x), x z(x),....,x^{899} z(x) )\)<br></li></ul><li>Meaning that the null space has dimension 900&nbsp;</li></ul></ul>
-
-    After:
-      2. (3 points) Let \(V\) be the vector space of polynomials of degree at most 999 with real coefficients. Define a linear map<br><br>\[<br>T: V \rightarrow \mathbb{R}^{100}, \quad T(p)=(p(1), p(2), \ldots, p(100)) .<br>\]<br><br><ul><li>a) Find the dimension of the null space of \(T\)</li><ul><li>Since p must be divisible by the polynomial \(z(x)\) =&nbsp;\(\prod_{i=1}^{100} (x-i)\) with degree 100&nbsp;</li><li>A basis of null space consists of the polynomials&nbsp;</li><ul><li>\( (z(x), x z(x),....,x^{899} z(x) )\)<br></li></ul><li>Meaning that the null space has dimension 900&nbsp;</li></ul></ul>
-
-============================================================
-
-Note ID: 1694790432109
-  Field: Text
-    Before:
-      3. (6 points) Let \(V\) be the vector space of polynomials of degree at most 99 with real coefficients. Define a linear map<br><br>\[<br>T: V \rightarrow \mathbb{R}^{1000}, \quad T(p)=(p(1), p(2), \ldots, p(1000)) .<br>\]<br><br><ul><li>a) Find the dimension of the null space:</li><ul><li>Since for a polynomial to be in the null space it must vanish for 1,...,1000 then it must be of at degree 1000 at least</li><li>Since the polynomials of V have degree at most 99, the null space must be have dimension 0</li></ul></ul>
-
-    After:
-      3. (6 points) Let \(V\) be the vector space of polynomials of degree at most 99 with real coefficients. Define a linear map<br><br>\[<br>T: V \rightarrow \mathbb{R}^{1000}, \quad T(p)=(p(1), p(2), \ldots, p(1000)) .<br>\]<br><br><ul><li>a) Find the dimension of the null space:</li><ul><li>Since for a polynomial to be in the null space it must vanish for 1,...,1000 then it must be of at degree 1000 at least</li><li>Since the polynomials of V have degree at most 99, the null space must be have dimension 0</li></ul></ul>
-
-============================================================
-
-Note ID: 1695047324471
-  Field: Text
-    Before:
-      4. (3 points)<br><br>Suppose \(V\) and \(W\) are vector spaces over \(F\); that \(T \in\) \(\mathcal{L}(V, W)\); that \(\left(v_{1}, \ldots, v_{n}\right)\) is any list of vectors in \(V\); and that \(\left(T v_{1}, \ldots, T v_{n}\right)\) is linearly independent in \(W\).<ol><li>Prove that \(\left(v_{1}, \ldots, v_{n}\right)\) is linearly independent in \(V\).</li></ol><div>Solution:</div><div><ol><li>\(\forall x \in V\)&nbsp;\(x = \vec{v} \cdot \vec{v}\)<br></li><li>\(T(x)\) =&nbsp;\(T(\vec{c} \cdot \vec{v}) \)<br></li><li>Assume that&nbsp;\(\vec{v}\) is not lin indp:</li><ol><li>Meaning \(\exists \vec{b}\) such that :</li><ol><li>\(\vec{b} \cdot \vec{v} = 0\)</li><li>And&nbsp;\(\vec{b} \neq \vec{0}\)</li></ol><li>Which implies&nbsp;</li><ol><li>\(T(\vec{b}\cdot \vec{v}) \) = \( 0\)</li><li>&nbsp;Thus \(\sum b_i Tv_i = 0\) and&nbsp;\(\vec{b} \neq \vec{0}\)</li><li>Which would imply that&nbsp;\(\vec{Tv}\) was not linearly independent, which would be a contradiction</li></ol></ol></ol></div>
-
-    After:
-      4. (3 points)<br><br>Suppose \(V\) and \(W\) are vector spaces over \(F\); that \(T \in\) \(\mathcal{L}(V, W)\); that \(\left(v_{1}, \ldots, v_{n}\right)\) is any list of vectors in \(V\); and that \(\left(T v_{1}, \ldots, T v_{n}\right)\) is linearly independent in \(W\).<ol><li>Prove that \(\left(v_{1}, \ldots, v_{n}\right)\) is linearly independent in \(V\).</li></ol><div>Solution:</div><div><ol><li>\(\forall x \in V\)&nbsp;\(x = \vec{v} \cdot \vec{v}\)<br></li><li>\(T(x)\) =&nbsp;\(T(\vec{c} \cdot \vec{v}) \)<br></li><li>Assume that&nbsp;\(\vec{v}\) is not lin indp:</li><ol><li>Meaning \(\exists \vec{b}\) such that :</li><ol><li>\(\vec{b} \cdot \vec{v} = 0\)</li><li>And&nbsp;\(\vec{b} \neq \vec{0}\)</li></ol><li>Which implies&nbsp;</li><ol><li>\(T(\vec{b}\cdot \vec{v}) \) = \( 0\)</li><li>&nbsp;Thus \(\sum b_i Tv_i = 0\) and&nbsp;\(\vec{b} \neq \vec{0}\)</li><li>Which would imply that&nbsp;\(\vec{Tv}\) was not linearly independent, which would be a contradiction</li></ol></ol></ol></div>
-
-============================================================
-
-Note ID: 1695200542904
-  Field: Text
-    Before:
-      6 <br>For \(n\) a positive integer, define \(V^{n}\) by<br><br><ul><li>\(V^{n}\) = \(\underbrace{V \times \cdots \times V}_{n \text { times } } .\)</li></ul><br>Prove that \(V^{n}\) and \(\mathcal{L}\)\(\left(\mathbf{F}^{n}, V\right)\) are isomorphic vector spaces.<br>
-
-    After:
-      6 <br>For \(n\) a positive integer, define \(V^{n}\) by<br><br><ul><li>\(V^{n}\) = \(\underbrace{V \times \cdots \times V}_{n \text { times } } .\)</li></ul><br>Prove that \(V^{n}\) and \(\mathcal{L}\)\(\left(\mathbf{F}^{n}, V\right)\) are isomorphic vector spaces.<br>
-
-============================================================
-
-Note ID: 1695202241388
-  Field: Text
-    Before:
-      11 Suppose \(v_{1}, \ldots, v_{m} \in V\). Let<br><br>\(A=\left\{\lambda_{1} v_{1}+\cdots+\lambda_{m} v_{m}: \lambda_{1}, \ldots, \lambda_{m} \in \mathbf{F}\right.\) and \(\left.\lambda_{1}+\cdots+\lambda_{m}=1\right\}\).<br><br><ul><li>(a) Prove that \(A\) is an affine subset of \(V\).</li><li><br></li><li>(b) Prove that every affine subset of \(V\) that contains \(v_{1}, \ldots, v_{m}\) also contains \(A\).</li><li><br></li><li>(c) Prove that \(A\) = \(v+U\) for some \(v \in V\) and some subspace \(U\) of \(V\) with \(\operatorname{dim} U \leq m-1\).</li></ul>
-
-    After:
-      11 Suppose \(v_{1}, \ldots, v_{m} \in V\). Let<br><br>\(A=\left\{\lambda_{1} v_{1}+\cdots+\lambda_{m} v_{m}: \lambda_{1}, \ldots, \lambda_{m} \in \mathbf{F}\right.\) and \(\left.\lambda_{1}+\cdots+\lambda_{m}=1\right\}\).<br><br><ul><li>(a) Prove that \(A\) is an affine subset of \(V\).</li><li><br></li><li>(b) Prove that every affine subset of \(V\) that contains \(v_{1}, \ldots, v_{m}\) also contains \(A\).</li><li><br></li><li>(c) Prove that \(A\) = \(v+U\) for some \(v \in V\) and some subspace \(U\) of \(V\) with \(\operatorname{dim} U \leq m-1\).</li></ul>
-
-============================================================
-
-Note ID: 1695204113618
-  Field: Text
-    Before:
-      9 <br><ul><li>Suppose \(v_{1}, \ldots, v_{n}\) is a basis of \(V\) and \(\varphi_{1}, \ldots, \varphi_{n}\) is the corresponding dual basis of \(V^{\prime}\).</li><li>Suppose \(\psi\) \(\in\) \(V^{\prime}\). </li><li><b>Prove that</b></li><ul><li>\(\psi\) = \(\psi\left(v_{1}\right) \varphi_{1}\) + \(\cdots\) + \(\psi\left(v_{n}\right) \varphi_{n} .\)</li></ul></ul>
-
-    After:
-      9 <br><ul><li>Suppose \(v_{1}, \ldots, v_{n}\) is a basis of \(V\) and \(\varphi_{1}, \ldots, \varphi_{n}\) is the corresponding dual basis of \(V^{\prime}\).</li><li>Suppose \(\psi\) \(\in\) \(V^{\prime}\). </li><li><b>Prove that</b></li><ul><li>\(\psi\) = \(\psi\left(v_{1}\right) \varphi_{1}\) + \(\cdots\) + \(\psi\left(v_{n}\right) \varphi_{n} .\)</li></ul></ul>
-
-============================================================
-
-Note ID: 1695204272427
-  Field: Text
-    Before:
-      12 Show that the dual map of the identity map on \(V\) is the identity map on \(V^{\prime}\).
-
-    After:
-      12 Show that the dual map of the identity map on \(V\) is the identity map on \(V^{\prime}\).
-
-============================================================
-
-Note ID: 1695204319746
-  Field: Text
-    Before:
-      15<br><br>&nbsp;Suppose \(W\) is finite-dimensional and \(T \in \mathcal{L}(V, W)\). <br><ul><li>Prove that \(T^{\prime}\) = \(0\) if and only if \(T\) =\(0\).</li></ul>
-
-    After:
-      15<br><br>&nbsp;Suppose \(W\) is finite-dimensional and \(T \in \mathcal{L}(V, W)\). <br><ul><li>Prove that \(T^{\prime}\) = \(0\) if and only if \(T\) =\(0\).</li></ul>
-
-============================================================
-
-Note ID: 1695234295671
-  Field: Text
-    Before:
-      <b>Definition&nbsp;</b>\(\operatorname{Re} z\)<b>&nbsp;,&nbsp;</b>\(\operatorname{Im} z\)<br><br>Suppose \(z\) = \(a+b i\), where \(a\) and \(b\) are real numbers.<br><ul><li>- The real part of \(z\), denoted \(\operatorname{Re} z\), is defined by \(\operatorname{Re} z\) = \(a\).<br></li><li><br></li><li>- The imaginary part of \(z\), denoted \(\operatorname{Im} z\), is defined by \(\operatorname{Im} z\) = \(b\).</li></ul>
-
-    After:
-      <b>Definition&nbsp;</b>\(\operatorname{Re} z\)<b>&nbsp;,&nbsp;</b>\(\operatorname{Im} z\)<br><br>Suppose \(z\) = \(a+b i\), where \(a\) and \(b\) are real numbers.<br><ul><li>- The real part of \(z\), denoted \(\operatorname{Re} z\), is defined by \(\operatorname{Re} z\) = \(a\).<br></li><li><br></li><li>- The imaginary part of \(z\), denoted \(\operatorname{Im} z\), is defined by \(\operatorname{Im} z\) = \(b\).</li></ul>
-
-============================================================
-
-Note ID: 1695234542611
-  Field: Text
-    Before:
-      <b>Definition complex conjugate, absolute value</b><br><br>Suppose \(z \in \mathbf{C}\).<br><br>- The complex conjugate of \(z \in \mathbf{C}\), denoted \(\bar{z}\), is defined by<br><br><ul><li>\(\bar{z}\) = \(\operatorname{Re} z-(\operatorname{Im} z) i .\)</li></ul><br>- The absolute value of a complex number \(z\), denoted \(|z|\), is defined by<br><br><ul><li>\(|z|\) = \(\sqrt{ (\operatorname{Re} z)^{2}+(\operatorname{Im} z)^{2} }&nbsp;.\)  </li></ul>
-
-    After:
-      <b>Definition complex conjugate, absolute value</b><br><br>Suppose \(z \in \mathbf{C}\).<br><br>- The complex conjugate of \(z \in \mathbf{C}\), denoted \(\bar{z}\), is defined by<br><br><ul><li>\(\bar{z}\) = \(\operatorname{Re} z-(\operatorname{Im} z) i .\)</li></ul><br>- The absolute value of a complex number \(z\), denoted \(|z|\), is defined by<br><br><ul><li>\(|z|\) = \(\sqrt{ (\operatorname{Re} z)^{2}+(\operatorname{Im} z)^{2} }&nbsp;.\)  </li></ul>
-
-============================================================
-
-Note ID: 1695236931274
-  Field: Text
-    Before:
-      <b>Definition&nbsp;</b>factor<br><br>A polynomial \(s \in \mathcal{P}(\mathbf{F})\) is called a factor of \(p \in \mathcal{P}(\mathbf{F})\) if:<br><ul><li>There exists a polynomial \(q \in \mathcal{P}(\mathbf{F})\) such that \(p=s q\).</li></ul>
-
-    After:
-      <b>Definition&nbsp;</b>factor<br><br>A polynomial \(s \in \mathcal{P}(\mathbf{F})\) is called a factor of \(p \in \mathcal{P}(\mathbf{F})\) if:<br><ul><li>There exists a polynomial \(q \in \mathcal{P}(\mathbf{F})\) such that \(p=s q\).</li></ul>
-
-============================================================
-
-Note ID: 1695237528795
-  Field: Text
-    Before:
-      Polynomials with real coefficients have zeros in pairs<br><ul><li>Suppose \(p \in \mathcal{P}(\mathbf{C})\) is a polynomial with real coefficients. If \(\lambda \in \mathbf{C}\) is a zero of \(p\), then so is \(\bar{\lambda}\).<br></li></ul>
-
-    After:
-      Polynomials with real coefficients have zeros in pairs<br><ul><li>Suppose \(p \in \mathcal{P}(\mathbf{C})\) is a polynomial with real coefficients. If \(\lambda \in \mathbf{C}\) is a zero of \(p\), then so is \(\bar{\lambda}\).<br></li></ul>
-
-============================================================
-
-Note ID: 1695237971317
-  Field: Text
-    Before:
-      To show why we can factorise real-coefficient polynomials as quadratic + linear polynomials:<br><ul><li>Because:&nbsp;\( (x-\lambda) (x-\bar{\lambda})\) =<br>\( \left(x^{2}-2(\operatorname{Re} \lambda) x+|\lambda|^{2}\right)<br>\)<br></li><li>Gives us a quadratic term of the required form</li></ul>
-
-    After:
-      To show why we can factorise real-coefficient polynomials as quadratic + linear polynomials:<br><ul><li>Because:&nbsp;\( (x-\lambda) (x-\bar{\lambda})\) =<br>\( \left(x^{2}-2(\operatorname{Re} \lambda) x+|\lambda|^{2}\right)<br>\)<br></li><li>Gives us a quadratic term of the required form</li></ul>
-
-============================================================
-
-Note ID: 1695238227321
-  Field: Text
-    Before:
-      <b>Factorization of a polynomial over \(\mathbf{R}\)</b><br><br>Suppose \(p \in \mathcal{P}(\mathbf{R})\) is a nonconstant polynomial. Then \(p\) has a unique factorization (except for the order of the factors) of the form<br><ul><li>\(p(x)\) = \(c\) \((x-\lambda_{1})\) \(\cdots\) \((x-\lambda_{m})\) \((x^{2}+b_{1} x+c_{1})\) \(\cdots\) \((x^{2}+b_{M} x+c_{M}),\)</li><li>where \(c, \lambda_{1}, \ldots, \lambda_{m}, b_{1}, \ldots, b_{M}, c_{1}, \ldots, c_{M} \in \mathbf{R}\), with \(b_{j}{ }^{2}&lt;4 c_{j}\) for each \(j\).</li></ul>
-
-    After:
-      <b>Factorization of a polynomial over \(\mathbf{R}\)</b><br><br>Suppose \(p \in \mathcal{P}(\mathbf{R})\) is a nonconstant polynomial. Then \(p\) has a unique factorization (except for the order of the factors) of the form<br><ul><li>\(p(x)\) = \(c\) \((x-\lambda_{1})\) \(\cdots\) \((x-\lambda_{m})\) \((x^{2}+b_{1} x+c_{1})\) \(\cdots\) \((x^{2}+b_{M} x+c_{M}),\)</li><li>where \(c, \lambda_{1}, \ldots, \lambda_{m}, b_{1}, \ldots, b_{M}, c_{1}, \ldots, c_{M} \in \mathbf{R}\), with \(b_{j}{ }^{2}&lt;4 c_{j}\) for each \(j\).</li></ul>
-
-============================================================
-
-Note ID: 1695307250073
-  Field: Text
-    Before:
-      Definition eigenvalue<br><br>Suppose \(T \in \mathcal{L}(V)\). A number \(\lambda \in \mathbf{F}\) is called an eigenvalue of \(T\) if there exists \(v \in V\) such that \(v \neq 0\) and \(T v=\lambda v\).
-
-    After:
-      Definition eigenvalue<br><br>Suppose \(T \in \mathcal{L}(V)\). A number \(\lambda \in \mathbf{F}\) is called an eigenvalue of \(T\) if there exists \(v \in V\) such that \(v \neq 0\) and \(T v=\lambda v\).
-
-============================================================
-
-Note ID: 1695307399510
-  Field: Text
-    Before:
-      <b>Equivalent conditions to be an eigenvalue</b><br><br>Suppose \(V\) is finite-dimensional, \(T \in \mathcal{L}(V)\), and \(\lambda \in F\). Then the following are equivalent:<br><br><ul><li>(a) \(\lambda\) is an eigenvalue of \(T\);</li><li>(b) \(T-\lambda I\) is not injective;</li><li>(c) \(T-\lambda I\) is not surjective;</li><li>(d) \(T-\lambda I\) is not invertible.</li></ul>
-
-    After:
-      <b>Equivalent conditions to be an eigenvalue</b><br><br>Suppose \(V\) is finite-dimensional, \(T \in \mathcal{L}(V)\), and \(\lambda \in F\). Then the following are equivalent:<br><br><ul><li>(a) \(\lambda\) is an eigenvalue of \(T\);</li><li>(b) \(T-\lambda I\) is not injective;</li><li>(c) \(T-\lambda I\) is not surjective;</li><li>(d) \(T-\lambda I\) is not invertible.</li></ul>
-
-============================================================
-
-Note ID: 1695308032274
-  Field: Text
-    Before:
-      <b>Definition</b> eigenvector<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(\lambda \in \mathbf{F}\) is an eigenvalue of \(T\). A vector \(v \in V\) is called an eigenvector of \(T\) corresponding to \(\lambda\) if \(v \neq 0\) and \(T v=\lambda v\).
-
-    After:
-      <b>Definition</b> eigenvector<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(\lambda \in \mathbf{F}\) is an eigenvalue of \(T\). A vector \(v \in V\) is called an eigenvector of \(T\) corresponding to \(\lambda\) if \(v \neq 0\) and \(T v=\lambda v\).
-
-============================================================
-
-Note ID: 1695308642410
-  Field: Text
-    Before:
-      Proof that a vector space V has at most dim V distinct eigenvalues<br><ul><li>Because eigenvectors form a linearly independent list</li><li>The maximum length of an eigenvectors list is dim V</li></ul>
-
-    After:
-      Proof that a vector space V has at most dim V distinct eigenvalues<br><ul><li>Because eigenvectors form a linearly independent list</li><li>The maximum length of an eigenvectors list is dim V</li></ul>
-
-============================================================
-
-Note ID: 1695308945769
-  Field: Text
-    Before:
-      Definition \(\left.T\right|_{U}\) and \(T / U\)<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(U\) is a subspace of \(V\) invariant under \(T\).<br><br><ul><li>The restriction operator \(\left.T\right|_{U}\)&nbsp; \(\in\) \( \mathcal{L}(U)\) is defined by</li><ul><li>\(\left.T\right|_{U}\) (u)= \(T u\)</li><li>for \(u \in U\).</li></ul></ul><ul><li>The quotient operator \(T / U\)&nbsp; \(\in\) \( \mathcal{L}(V / U)\) is defined by</li><ul><li>\((T / U)\) (v+U)= \(T v+U\)</li><li>for \(v \in V\).</li></ul></ul><br>
-
-    After:
-      Definition \(\left.T\right|_{U}\) and \(T / U\)<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(U\) is a subspace of \(V\) invariant under \(T\).<br><br><ul><li>The restriction operator \(\left.T\right|_{U}\)&nbsp; \(\in\) \( \mathcal{L}(U)\) is defined by</li><ul><li>\(\left.T\right|_{U}\) (u)= \(T u\)</li><li>for \(u \in U\).</li></ul></ul><ul><li>The quotient operator \(T / U\)&nbsp; \(\in\) \( \mathcal{L}(V / U)\) is defined by</li><ul><li>\((T / U)\) (v+U)= \(T v+U\)</li><li>for \(v \in V\).</li></ul></ul><br>
-
-============================================================
-
-Note ID: 1695309770149
-  Field: Text
-    Before:
-      &nbsp;1 Suppose \(T \in \mathcal{L}(V)\) and \(U\) is a subspace of \(V\).<br><br><ul><li>(a) Prove that if \(U\) \(\subset\) null \(T\), then \(U\) is invariant under \(T\).</li><li><br></li><li>(b) Prove that if range \(T\)&nbsp; \(\subset\)&nbsp; \(U\), then \(U\) is invariant under \(T\).</li></ul>
-
-    After:
-      &nbsp;1 Suppose \(T \in \mathcal{L}(V)\) and \(U\) is a subspace of \(V\).<br><br><ul><li>(a) Prove that if \(U\) \(\subset\) null \(T\), then \(U\) is invariant under \(T\).</li><li><br></li><li>(b) Prove that if range \(T\)&nbsp; \(\subset\)&nbsp; \(U\), then \(U\) is invariant under \(T\).</li></ul>
-
-============================================================
-
-Note ID: 1695309958682
-  Field: Text
-    Before:
-      4 Suppose that \(T \in \mathcal{L}(V)\) and \(U_{1}, \ldots, U_{m}\) are subspaces of \(V\) invariant under \(T\). Prove that \(U_{1}+\cdots+U_{m}\) is invariant under \(T\).
-
-    After:
-      4 Suppose that \(T \in \mathcal{L}(V)\) and \(U_{1}, \ldots, U_{m}\) are subspaces of \(V\) invariant under \(T\). Prove that \(U_{1}+\cdots+U_{m}\) is invariant under \(T\).
-
-============================================================
-
-Note ID: 1695310009569
-  Field: Text
-    Before:
-      5 Suppose \(T \in \mathcal{L}(V)\). Prove that the intersection of every collection of subspaces of \(V\) invariant under \(T\) is invariant under \(T\).
-
-    After:
-      5 Suppose \(T \in \mathcal{L}(V)\). Prove that the intersection of every collection of subspaces of \(V\) invariant under \(T\) is invariant under \(T\).
-
-============================================================
-
-Note ID: 1695310357147
-  Field: Text
-    Before:
-      21 Suppose \(T \in \mathcal{L}(V)\) is invertible.<br><br><ul><li>(a) Suppose \(\lambda \in \mathbf{F}\) with \(\lambda \neq 0\).&nbsp;</li><ul><li>Prove that \(\lambda\) is an eigenvalue of \(T\) if and only if \(\frac{1}{\lambda}\) is an eigenvalue of \(T^{-1}\).</li></ul><li>(b) Prove that \(T\) and \(T^{-1}\) have the same eigenvectors.</li></ul>
-
-    After:
-      21 Suppose \(T \in \mathcal{L}(V)\) is invertible.<br><br><ul><li>(a) Suppose \(\lambda \in \mathbf{F}\) with \(\lambda \neq 0\).&nbsp;</li><ul><li>Prove that \(\lambda\) is an eigenvalue of \(T\) if and only if \(\frac{1}{\lambda}\) is an eigenvalue of \(T^{-1}\).</li></ul><li>(b) Prove that \(T\) and \(T^{-1}\) have the same eigenvectors.</li></ul>
-
-============================================================
-
-Note ID: 1695310449694
-  Field: Text
-    Before:
-      24 <br><br>Suppose \(A\) is an \(n\)-by- \(n\) matrix with entries in \(\mathbf{F}\). Define \(T \in \mathcal{L}\left(\mathbf{F}^{n}\right)\) by \(T x=A x\), where elements of \(\mathbf{F}^{n}\) are thought of as \(n\)-by-1 column vectors.<br><br><ol><li>(a) Suppose the sum of the entries in each row of \(A\) equals 1 . Prove that 1 is an eigenvalue of \(T\).</li><li>(b) Suppose the sum of the entries in each column of \(A\) equals 1 . Prove that 1 is an eigenvalue of \(T\).</li></ol>
-
-    After:
-      24 <br><br>Suppose \(A\) is an \(n\)-by- \(n\) matrix with entries in \(\mathbf{F}\). Define \(T \in \mathcal{L}\left(\mathbf{F}^{n}\right)\) by \(T x=A x\), where elements of \(\mathbf{F}^{n}\) are thought of as \(n\)-by-1 column vectors.<br><br><ol><li>(a) Suppose the sum of the entries in each row of \(A\) equals 1 . Prove that 1 is an eigenvalue of \(T\).</li><li>(b) Suppose the sum of the entries in each column of \(A\) equals 1 . Prove that 1 is an eigenvalue of \(T\).</li></ol>
-
-============================================================
-
-Note ID: 1695396306003
-  Field: Text
-    Before:
-      <b>Definition</b> \(T^{m}\)<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(m\) is a positive integer.<ul><li>- \(T^{m}\) is defined by</li><ul><li>\(T^{m}\)  = \(\underbrace{T \cdots T}_{m \text { times } } \text {. }\)</li></ul><li>- \(T^{0}\) is defined to be the identity operator \(I\) on \(V\).</li><li>- If \(T\) is invertible with inverse \(T^{-1}\), then \(T^{-m}\) is defined by</li><ul><li>\(T^{-m}\) = \(\left(T^{-1}\right)^{m} \text {. }\)</li></ul></ul>
-
-    After:
-      <b>Definition</b> \(T^{m}\)<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(m\) is a positive integer.<ul><li>- \(T^{m}\) is defined by</li><ul><li>\(T^{m}\)  = \(\underbrace{T \cdots T}_{m \text { times } } \text {. }\)</li></ul><li>- \(T^{0}\) is defined to be the identity operator \(I\) on \(V\).</li><li>- If \(T\) is invertible with inverse \(T^{-1}\), then \(T^{-m}\) is defined by</li><ul><li>\(T^{-m}\) = \(\left(T^{-1}\right)^{m} \text {. }\)</li></ul></ul>
-
-============================================================
-
-Note ID: 1695396526873
-  Field: Text
-    Before:
-      <b>Definition</b> \(p(T)\)<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(p \in \mathcal{P}(\mathbf{F})\) is a polynomial given by<br><br><ul><li>\(p(z)\) = \(a_{0}+a_{1} z+a_{2} z^{2}+\cdots+a_{m} z^{m}\)</li><li>for \(z \in \mathbf{F}\).&nbsp;</li><li>Then \(p(T)\) is the operator defined by<br></li><ul><li>\(p(T)\) = \(a_{0} I+a_{1} T+a_{2} T^{2}+\cdots+a_{m} T^{m} .\)</li></ul></ul>
-
-    After:
-      <b>Definition</b> \(p(T)\)<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(p \in \mathcal{P}(\mathbf{F})\) is a polynomial given by<br><br><ul><li>\(p(z)\) = \(a_{0}+a_{1} z+a_{2} z^{2}+\cdots+a_{m} z^{m}\)</li><li>for \(z \in \mathbf{F}\).&nbsp;</li><li>Then \(p(T)\) is the operator defined by<br></li><ul><li>\(p(T)\) = \(a_{0} I+a_{1} T+a_{2} T^{2}+\cdots+a_{m} T^{m} .\)</li></ul></ul>
-
-============================================================
-
-Note ID: 1695396685144
-  Field: Text
-    Before:
-      Multiplicative properties of polynomials of operartors<br><br>Suppose \(p, q \in \mathcal{P}(\mathbf{F})\) and \(T \in \mathcal{L}(V)\). Then<br><ul><li>(a) \((p q)(T)\) = \(p(T) q(T)\);</li><li>(b) \(p(T) q(T)\) = \(q(T) p(T)\).</li></ul>
-
-    After:
-      Multiplicative properties of polynomials of operartors<br><br>Suppose \(p, q \in \mathcal{P}(\mathbf{F})\) and \(T \in \mathcal{L}(V)\). Then<br><ul><li>(a) \((p q)(T)\) = \(p(T) q(T)\);</li><li>(b) \(p(T) q(T)\) = \(q(T) p(T)\).</li></ul>
-
-============================================================
-
-Note ID: 1695396788479
-  Field: Text
-    Before:
-      Operators on complex vector spaces have an eigenvalue<br><br>Every operator on a finite-dimensional, nonzero, complex vector space has an eigenvalue.
-
-    After:
-      Operators on complex vector spaces have an eigenvalue<br><br>Every operator on a finite-dimensional, nonzero, complex vector space has an eigenvalue.
-
-============================================================
-
-Note ID: 1695397195775
-  Field: Text
-    Before:
-      Definition matrix of an operator, \(\mathcal{M}(T)\)<br><br>Suppose \(T\) \(\in\) \(\mathcal{L}(V)\) and \(v_{1}, \ldots, v_{n}\) is a basis of \(V\). The matrix of \(T\) with respect to this basis is the \(n\)-by- \(n\) matrix<br><br>\[<br>\mathcal{M}(T)=\left(\begin{array}{ccc}<br>A_{1,1} &amp; \ldots &amp; A_{1, n} \\<br>\vdots &amp; &amp; \vdots \\<br>A_{n, 1} &amp; \ldots &amp; A_{n, n}<br>\end{array}\right)<br>\]<br><br>whose entries \(A_{j, k}\) are defined by<br><br><ul><li>\(T v_{k}\) = \(A_{1, k} v_{1}+\cdots+A_{n, k} v_{n} .\)</li></ul><br>
-
-    After:
-      Definition matrix of an operator, \(\mathcal{M}(T)\)<br><br>Suppose \(T\) \(\in\) \(\mathcal{L}(V)\) and \(v_{1}, \ldots, v_{n}\) is a basis of \(V\). The matrix of \(T\) with respect to this basis is the \(n\)-by- \(n\) matrix<br><br>\[<br>\mathcal{M}(T)=\left(\begin{array}{ccc}<br>A_{1,1} &amp; \ldots &amp; A_{1, n} \\<br>\vdots &amp; &amp; \vdots \\<br>A_{n, 1} &amp; \ldots &amp; A_{n, n}<br>\end{array}\right)<br>\]<br><br>whose entries \(A_{j, k}\) are defined by<br><br><ul><li>\(T v_{k}\) = \(A_{1, k} v_{1}+\cdots+A_{n, k} v_{n} .\)</li></ul><br>
-
-============================================================
-
-Note ID: 1695648094768
-  Field: Text
-    Before:
-      Definition diagonal of a matrix<br><br>The diagonal of a square matrix consists of the entries along the line from the upper left corner to the bottom right corner.
-
-    After:
-      Definition diagonal of a matrix<br><br>The diagonal of a square matrix consists of the entries along the line from the upper left corner to the bottom right corner.
-
-============================================================
-
-Note ID: 1695648515294
-  Field: Text
-    Before:
-      Over \(\mathbf{C}\), every operator has an upper-triangular matrix<br><br>Suppose \(V\) is a finite-dimensional complex vector space and \(T \in \mathcal{L}(V)\). Then \(T\) has an upper-triangular matrix with respect to some basis of \(V\).
-
-    After:
-      Over \(\mathbf{C}\), every operator has an upper-triangular matrix<br><br>Suppose \(V\) is a finite-dimensional complex vector space and \(T \in \mathcal{L}(V)\). Then \(T\) has an upper-triangular matrix with respect to some basis of \(V\).
-
-============================================================
-
-Note ID: 1695649167203
-  Field: Text
-    Before:
-      Determination of invertibility from upper-triangular matrix<br><br>Suppose \(T \in \mathcal{L}(V)\) has an upper-triangular matrix with respect to some basis of \(V\). Then \(T\) is invertible if and only if all the entries on the diagonal of that upper-triangular matrix are nonzero.
-
-    After:
-      Determination of invertibility from upper-triangular matrix<br><br>Suppose \(T \in \mathcal{L}(V)\) has an upper-triangular matrix with respect to some basis of \(V\). Then \(T\) is invertible if and only if all the entries on the diagonal of that upper-triangular matrix are nonzero.
-
-============================================================
-
-Note ID: 1695649225306
-  Field: Text
-    Before:
-      Determination of eigenvalues from upper-triangular matrix<br><br>Suppose \(T \in \mathcal{L}(V)\) has an upper-triangular matrix with respect to some basis of \(V\). Then the eigenvalues of \(T\) are precisely the entries on the diagonal of that upper-triangular matrix.
-
-    After:
-      Determination of eigenvalues from upper-triangular matrix<br><br>Suppose \(T \in \mathcal{L}(V)\) has an upper-triangular matrix with respect to some basis of \(V\). Then the eigenvalues of \(T\) are precisely the entries on the diagonal of that upper-triangular matrix.
-
-============================================================
-
-Note ID: 1695649821494
-  Field: Text
-    Before:
-      7 Suppose \(T \in \mathcal{L}(V)\). Prove that 9 is an eigenvalue of \(T^{2}\) if and only if 3 or -3 is an eigenvalue of \(T\).
-
-    After:
-      7 Suppose \(T \in \mathcal{L}(V)\). Prove that 9 is an eigenvalue of \(T^{2}\) if and only if 3 or -3 is an eigenvalue of \(T\).
-
-============================================================
-
-Note ID: 1695650125485
-  Field: Text
-    Before:
-      <b>Definition</b> diagonal <b>matrix</b><br><br>A diagonal matrix is a square matrix that is 0 everywhere except possibly along the diagonal.
-
-    After:
-      <b>Definition</b> diagonal <b>matrix</b><br><br>A diagonal matrix is a square matrix that is 0 everywhere except possibly along the diagonal.
-
-============================================================
-
-Note ID: 1695650171001
-  Field: Text
-    Before:
-      If an operator has a diagonal or upper-triangular matrix with respect to some basis, then the entries along the diagonal are precisely the eigenvalues of the operator
-
-    After:
-      If an operator has a diagonal or upper-triangular matrix with respect to some basis, then the entries along the diagonal are precisely the eigenvalues of the operator
-
-============================================================
-
-Note ID: 1695650277938
-  Field: Text
-    Before:
-      Definition eigenspace, \(E(\lambda, T)\)<br><br><ul><li>Suppose \(T \in \mathcal{L}(V)\) and \(\lambda \in \mathbf{F}\). The eigenspace of \(T\) corresponding to \(\lambda\), denoted \(E(\lambda, T)\), is defined by&nbsp;</li><ul><li>\(E(\lambda, T)\) = \(\operatorname{null}(T-\lambda I) .\)</li></ul><li>In other words, \(E(\lambda, T)\) is the set of all eigenvectors of \(T\) corresponding to \(\lambda\), along with the 0 vector.<br></li></ul>
-
-    After:
-      Definition eigenspace, \(E(\lambda, T)\)<br><br><ul><li>Suppose \(T \in \mathcal{L}(V)\) and \(\lambda \in \mathbf{F}\). The eigenspace of \(T\) corresponding to \(\lambda\), denoted \(E(\lambda, T)\), is defined by&nbsp;</li><ul><li>\(E(\lambda, T)\) = \(\operatorname{null}(T-\lambda I) .\)</li></ul><li>In other words, \(E(\lambda, T)\) is the set of all eigenvectors of \(T\) corresponding to \(\lambda\), along with the 0 vector.<br></li></ul>
-
-============================================================
-
-Note ID: 1695651052561
-  Field: Text
-    Before:
-      Sum of eigenspaces is a direct sum<br><br>Suppose \(V\) is finite-dimensional and \(T \in \mathcal{L}(V)\). Suppose also that \(\lambda_{1}, \ldots, \lambda_{m}\) are distinct eigenvalues of \(T\). Then<br><br><ul><li>\(E\left(\lambda_{1}, T\right)+\cdots+E\left(\lambda_{m}, T\right)\)</li><ul><li>is a direct sum,</li></ul><li>Furthermore</li><li>\(\operatorname{dim} E\left(\lambda_{1}, T\right)+\cdots+\operatorname{dim} E\left(\lambda_{m}, T\right)\)&nbsp; \(\leq\) \( \operatorname{dim} V\)</li></ul>
-
-    After:
-      Sum of eigenspaces is a direct sum<br><br>Suppose \(V\) is finite-dimensional and \(T \in \mathcal{L}(V)\). Suppose also that \(\lambda_{1}, \ldots, \lambda_{m}\) are distinct eigenvalues of \(T\). Then<br><br><ul><li>\(E\left(\lambda_{1}, T\right)+\cdots+E\left(\lambda_{m}, T\right)\)</li><ul><li>is a direct sum,</li></ul><li>Furthermore</li><li>\(\operatorname{dim} E\left(\lambda_{1}, T\right)+\cdots+\operatorname{dim} E\left(\lambda_{m}, T\right)\)&nbsp; \(\leq\) \( \operatorname{dim} V\)</li></ul>
-
-============================================================
-
-Note ID: 1695651156316
-  Field: Text
-    Before:
-      <b>Definition</b> diagonalizable<br><br>An operator \(T \in \mathcal{L}(V)\) is called diagonalizable if it has a diagonal matrix with respect to some basis of \(V\).
-
-    After:
-      <b>Definition</b> diagonalizable<br><br>An operator \(T \in \mathcal{L}(V)\) is called diagonalizable if it has a diagonal matrix with respect to some basis of \(V\).
-
-============================================================
-
-Note ID: 1695652010098
-  Field: Text
-    Before:
-      <img src="paste-56c6ac480fcee3254347c9d29a6a8837480fd05f.jpg"><br>Proof Suppose \(T \in \mathcal{L}(V)\) has \(\operatorname{dim} V\) distinct eigenvalues \(\lambda_{1}, \ldots, \lambda_{\operatorname{dim} V}\). <br><ul><li>For each \(j\), let \(v_{j} \in V\) be an eigenvector corresponding to the eigenvalue \(\lambda_{j}\).&nbsp;</li><li>Because eigenvectors corresponding to distinct eigenvalues are linearly independent \(v_{1}, \ldots, v_{\operatorname{dim} V}\) is linearly independent</li><li>. A linearly independent list of \(\operatorname{dim} V\) vectors in \(V\) is a basis of \(V\)&nbsp; thus \(v_{1}, \ldots, v_{\operatorname{dim}} V\) is a basis of \(V\). With respect to this basis consisting of eigenvectors, \(T\) has a diagonal matrix.</li></ul>
-
-    After:
-      <img src="paste-56c6ac480fcee3254347c9d29a6a8837480fd05f.jpg"><br>Proof Suppose \(T \in \mathcal{L}(V)\) has \(\operatorname{dim} V\) distinct eigenvalues \(\lambda_{1}, \ldots, \lambda_{\operatorname{dim} V}\). <br><ul><li>For each \(j\), let \(v_{j} \in V\) be an eigenvector corresponding to the eigenvalue \(\lambda_{j}\).&nbsp;</li><li>Because eigenvectors corresponding to distinct eigenvalues are linearly independent \(v_{1}, \ldots, v_{\operatorname{dim} V}\) is linearly independent</li><li>. A linearly independent list of \(\operatorname{dim} V\) vectors in \(V\) is a basis of \(V\)&nbsp; thus \(v_{1}, \ldots, v_{\operatorname{dim}} V\) is a basis of \(V\). With respect to this basis consisting of eigenvectors, \(T\) has a diagonal matrix.</li></ul>
-
-============================================================
-
-Note ID: 1695654120373
-  Field: Text
-    Before:
-      16 The Fibonacci sequence \(F_{1}, F_{2}, \ldots\) is defined by<br><br>\[<br>F_{1}=1, F_{2}=1, \quad \text { and } F_{n}=F_{n-2}+F_{n-1} \text { for } n \geq 3 .<br>\]<br><br>Define \(T \in \mathcal{L}\left(\mathbf{R}^{2}\right)\) by \(T(x, y)=(y, x+y)\).<ul><li>(a) Show that \(T^{n}(0,1)\) = \(\left(F_{n}, F_{n+1}\right)\) for each positive integer \(n\).</li><li>(b) Find the eigenvalues of \(T\).</li><li>(c) Find a basis of \(\mathbf{R}^{2}\) consisting of eigenvectors of \(T\).</li><li>(d) Use the solution to part (c) to compute \(T^{n}(0,1)\). Conclude that</li><ul><li>\(F_{n}\) = \(\frac{1}{\sqrt{5} }\) \(\left[\left(\frac{1+\sqrt{5} }{2}\right)^{n}-\left(\frac{1-\sqrt{5} }{2}\right)^{n}\right]\)</li><li>for each positive integer \(n\).</li></ul><li>(e) Use part (d) to conclude that for each positive integer \(n\), the Fibonacci number \(F_{n}\) is the integer that is closest to</li><ul><li>\(\frac{1}{\sqrt{5} }\) \(\left(\frac{1+\sqrt{5} }{2}\right)^{n} \text {. }\)</li></ul></ul><br>
-
-    After:
-      16 The Fibonacci sequence \(F_{1}, F_{2}, \ldots\) is defined by<br><br>\[<br>F_{1}=1, F_{2}=1, \quad \text { and } F_{n}=F_{n-2}+F_{n-1} \text { for } n \geq 3 .<br>\]<br><br>Define \(T \in \mathcal{L}\left(\mathbf{R}^{2}\right)\) by \(T(x, y)=(y, x+y)\).<ul><li>(a) Show that \(T^{n}(0,1)\) = \(\left(F_{n}, F_{n+1}\right)\) for each positive integer \(n\).</li><li>(b) Find the eigenvalues of \(T\).</li><li>(c) Find a basis of \(\mathbf{R}^{2}\) consisting of eigenvectors of \(T\).</li><li>(d) Use the solution to part (c) to compute \(T^{n}(0,1)\). Conclude that</li><ul><li>\(F_{n}\) = \(\frac{1}{\sqrt{5} }\) \(\left[\left(\frac{1+\sqrt{5} }{2}\right)^{n}-\left(\frac{1-\sqrt{5} }{2}\right)^{n}\right]\)</li><li>for each positive integer \(n\).</li></ul><li>(e) Use part (d) to conclude that for each positive integer \(n\), the Fibonacci number \(F_{n}\) is the integer that is closest to</li><ul><li>\(\frac{1}{\sqrt{5} }\) \(\left(\frac{1+\sqrt{5} }{2}\right)^{n} \text {. }\)</li></ul></ul><br>
-
-============================================================
-
-Note ID: 1695739236158
-  Field: Text
-    Before:
-      A polynomial must be a finite sum meaning that its degree must be finite&nbsp;
-
-    After:
-      A polynomial must be a finite sum meaning that its degree must be finite&nbsp;
-
-============================================================
-
-Note ID: 1695831249270
-  Field: Text
-    Before:
-      1. (15 points) Let \(E\) consist of all real numbers of the form \(a+b \sqrt{2}\), with \(a\) and \(b\) rational numbers.<br><br><ul><li>a) Is \(E\) closed under addition (of real numbers)?&nbsp;</li><ul><li>Yes</li></ul><li>b) Is \(E\) closed under multiplication (of real numbers)? Yes</li><li>c) Does \((5+4 \sqrt{2})^{-1}\) belong to \(E\) ?</li><ul><li>Yes, it's equal to \(\underline{\frac{1}{7}(4 \sqrt{2}-5)}\).</li></ul><li>d) Is \(E\) a field?</li><ul><li>Yes, we can check the multiplication and the addition is easier.</li><ul><li>\((a+b \sqrt{2})(c+d \sqrt{2}) \) = \((a c+2 b d)+(a d+b c) \sqrt{2},\)</li></ul><li>and</li><ul><li>\(1 /(a+b \sqrt{2})\) = \(\frac{a-b \sqrt{2} }{a^{2}-2 b^{2} } .\)</li></ul></ul><ul><li>All expressions are rational numbers as long as \(a, b, c, d\) are rational numbers.</li></ul></ul>
-
-    After:
-      1. (15 points) Let \(E\) consist of all real numbers of the form \(a+b \sqrt{2}\), with \(a\) and \(b\) rational numbers.<br><br><ul><li>a) Is \(E\) closed under addition (of real numbers)?&nbsp;</li><ul><li>Yes</li></ul><li>b) Is \(E\) closed under multiplication (of real numbers)? Yes</li><li>c) Does \((5+4 \sqrt{2})^{-1}\) belong to \(E\) ?</li><ul><li>Yes, it's equal to \(\underline{\frac{1}{7}(4 \sqrt{2}-5)}\).</li></ul><li>d) Is \(E\) a field?</li><ul><li>Yes, we can check the multiplication and the addition is easier.</li><ul><li>\((a+b \sqrt{2})(c+d \sqrt{2}) \) = \((a c+2 b d)+(a d+b c) \sqrt{2},\)</li></ul><li>and</li><ul><li>\(1 /(a+b \sqrt{2})\) = \(\frac{a-b \sqrt{2} }{a^{2}-2 b^{2} } .\)</li></ul></ul><ul><li>All expressions are rational numbers as long as \(a, b, c, d\) are rational numbers.</li></ul></ul>
-
-============================================================
-
-Note ID: 1695905516351
-  Field: Text
-    Before:
-      Proposition 2.7. Suppose \(A\) is an \(m \times n\) matrix.<br><br><ul><li>1. The row rank and the column rank of \(A\) are equal, and equal to the dimension of the range of \(A\) :</li><ul><li>\(\mathrm{r}-\operatorname{rank}(A)\) = \(\operatorname{c}-\operatorname{rank}(A)=\operatorname{dim} \operatorname{Range}(A) .\)</li><li>Their common value is called the rank of \(A\), and written \(\operatorname{rank}(A)\).</li></ul></ul>
-
-    After:
-      Proposition 2.7. Suppose \(A\) is an \(m \times n\) matrix.<br><br><ul><li>1. The row rank and the column rank of \(A\) are equal, and equal to the dimension of the range of \(A\) :</li><ul><li>\(\mathrm{r}-\operatorname{rank}(A)\) = \(\operatorname{c}-\operatorname{rank}(A)=\operatorname{dim} \operatorname{Range}(A) .\)</li><li>Their common value is called the rank of \(A\), and written \(\operatorname{rank}(A)\).</li></ul></ul>
-
-============================================================
-
-Note ID: 1695910925645
-  Field: Text
-    Before:
-      The row-echelon matrix \(A\) is said to be in reduced row-echelon form if in addition to the row-echelon&nbsp;form:<br><br>1. each pivot entry is equal to 1 , and<br><br>2. all the other entries in the column of a pivot are equal to zero.
-
-    After:
-      The row-echelon matrix \(A\) is said to be in reduced row-echelon form if in addition to the row-echelon&nbsp;form:<br><br>1. each pivot entry is equal to 1 , and<br><br>2. all the other entries in the column of a pivot are equal to zero.
-
-============================================================
-
-Note ID: 1695911155167
-  Field: Text
-    Before:
-      Suppose that the row-echelon matrix \(A\) has pivots in the first \(r\) rows, in columns<br><br>\[<br>1 \leq j(1)&lt;j(2)&lt;\cdots&lt;j(r) \leq n .<br>\]<br><br>We call \(x_{1}, x_{2}, \ldots, x_{n}\) the variables, . The \(r\) variables \(x_{j(i)}\) corresponding to the pivot columns are called pivot variables. The remaining \(n-r\) variables are called free variables.
-
-    After:
-      Suppose that the row-echelon matrix \(A\) has pivots in the first \(r\) rows, in columns<br><br>\[<br>1 \leq j(1)&lt;j(2)&lt;\cdots&lt;j(r) \leq n .<br>\]<br><br>We call \(x_{1}, x_{2}, \ldots, x_{n}\) the variables, . The \(r\) variables \(x_{j(i)}\) corresponding to the pivot columns are called pivot variables. The remaining \(n-r\) variables are called free variables.
-
-============================================================
-
-Note ID: 1695912073329
-  Field: Text
-    Before:
-      Proposition 2.11. Suppose that \(A\) is in reduced row-echelon form, with \(r\) pivots in the entries \(\{(i, j(i)) \mid 1 \leq i \leq r\}\).<br><br><ul><li>The equation \(A x=b\) (see (2.5)) has a solution if and only if \(b_{i}=0\) for all \(i&gt;r\). In that case, one solution is</li><li>\(x_{j(i)}\) = \(b_{i} \quad(1 \leq i \leq r)\)&nbsp;</li><li>\( \quad x_{j}\) = 0 \(\quad\left(x_{j} \text { free variable }\right) .\)</li></ul><br>5. Still assuming that \(b_{i}=0\) for all \(i&gt;r\), the most general solution of \(A x=b\) has arbitrary values \(x_{j}\) for the \(n-r\) free variables, and<br><br><ul><li>\(x_{j(i)}\) = \(b_{i}-\sum_{j \text { free } } a_{i j} x_{j}\) for \(\quad(1 \leq i \leq r) .\)</li></ul><br>That is, we choose the \(n-r\) free variables, and then define the \(r\) pivot variables by the equation above.<br><br>
-
-    After:
-      Proposition 2.11. Suppose that \(A\) is in reduced row-echelon form, with \(r\) pivots in the entries \(\{(i, j(i)) \mid 1 \leq i \leq r\}\).<br><br><ul><li>The equation \(A x=b\) (see (2.5)) has a solution if and only if \(b_{i}=0\) for all \(i&gt;r\). In that case, one solution is</li><li>\(x_{j(i)}\) = \(b_{i} \quad(1 \leq i \leq r)\)&nbsp;</li><li>\( \quad x_{j}\) = 0 \(\quad\left(x_{j} \text { free variable }\right) .\)</li></ul><br>5. Still assuming that \(b_{i}=0\) for all \(i&gt;r\), the most general solution of \(A x=b\) has arbitrary values \(x_{j}\) for the \(n-r\) free variables, and<br><br><ul><li>\(x_{j(i)}\) = \(b_{i}-\sum_{j \text { free } } a_{i j} x_{j}\) for \(\quad(1 \leq i \leq r) .\)</li></ul><br>That is, we choose the \(n-r\) free variables, and then define the \(r\) pivot variables by the equation above.<br><br>
-
-============================================================
-
-Note ID: 1695994677630
-  Field: Text
-    Before:
-      The first part of Gaussian elimination finds (in succession) \(r\) special entries<br><br><ul><li>\( (i(1), j(1)),(i(2), j(2)), \ldots,(i(r), j(r)), \)</li><ul><li>such that</li><li>&nbsp;\(1 \leq j(1)&lt;j(2)&lt;\cdots&lt;j(r) \leq n, \)</li><li>\( 1 \leq i(p) \leq m \text { all distinct }\)</li></ul></ul>These entries will become the pivots in the row-echelon form&nbsp;<br>
-
-    After:
-      The first part of Gaussian elimination finds (in succession) \(r\) special entries<br><br><ul><li>\( (i(1), j(1)),(i(2), j(2)), \ldots,(i(r), j(r)), \)</li><ul><li>such that</li><li>&nbsp;\(1 \leq j(1)&lt;j(2)&lt;\cdots&lt;j(r) \leq n, \)</li><li>\( 1 \leq i(p) \leq m \text { all distinct }\)</li></ul></ul>These entries will become the pivots in the row-echelon form&nbsp;<br>
-
-============================================================
-
-Note ID: 1696002130682
-  Field: Text
-    Before:
-      Theorem 4.8. Suppose \(A^{\prime}\) is an \(m \times n\) matrix with entries in a field \(F\). Then we can perform a finite sequence of elementary row operations on \(A^{\prime}\) to obtain a new \(m \times n\) matrix \(A^{\prime}\) in reduced row-echelon form. More precisely, we perform<br><br><ol><li>at most \(m\) row operations of type \(M\) (multiply a row by a nonzero scalar)  interspersed with at most
-
-&nbsp;&nbsp;&nbsp;\(m(m-1) / 2\) operations of type \(L\) (add a multiple of a row to a later row); then</li><li>at most \(m(m-1) / 2\) operations of type \(E\) (exchange two rows); then</li><li>at most \(m(m-1) / 2\) operations of type \(U\) (add a multiple of a row to an earlier row).</li></ol>
-
-    After:
-      Theorem 4.8. Suppose \(A^{\prime}\) is an \(m \times n\) matrix with entries in a field \(F\). Then we can perform a finite sequence of elementary row operations on \(A^{\prime}\) to obtain a new \(m \times n\) matrix \(A^{\prime}\) in reduced row-echelon form. More precisely, we perform<br><br><ol><li>at most \(m\) row operations of type \(M\) (multiply a row by a nonzero scalar)  interspersed with at most
-
-&nbsp;&nbsp;&nbsp;\(m(m-1) / 2\) operations of type \(L\) (add a multiple of a row to a later row); then</li><li>at most \(m(m-1) / 2\) operations of type \(E\) (exchange two rows); then</li><li>at most \(m(m-1) / 2\) operations of type \(U\) (add a multiple of a row to an earlier row).</li></ol>
-
-============================================================
-
-Note ID: 1696002567366
-  Field: Text
-    Before:
-      <ul><li>We can write Gaussian Elimination as&nbsp;</li><ul><li>\(A^{\prime}=U E L A\) where \(A^{\prime}\) is A but&nbsp; in reduced row-echalon form</li><li>\(\quad A=L^{-1} E^{-1} U^{-1} A^{\prime} .\)</li></ul><li>Here:</li><ul><li>&nbsp;\(L\) and \(L^{-1}\) are \(m \times m\) invertible lower-triangular::shape?::shape?::shape?::shape? matrices;&nbsp;</li><li>\(E\) and \(E^{-1}\) are invertible \(m \times m\) permutation matrices;&nbsp;</li><li>and \(U\) and \(U^{-1}\) are invertible \(m \times m\) upper-triangular matrices with ones on the diagonal.&nbsp;</li></ul><li>The reduced row echelon matrix \(A\) is unique (independent of how the row reduction is performed).<br></li></ul>
-
-    After:
-      <ul><li>We can write Gaussian Elimination as&nbsp;</li><ul><li>\(A^{\prime}=U E L A\) where \(A^{\prime}\) is A but&nbsp; in reduced row-echalon form</li><li>\(\quad A=L^{-1} E^{-1} U^{-1} A^{\prime} .\)</li></ul><li>Here:</li><ul><li>&nbsp;\(L\) and \(L^{-1}\) are \(m \times m\) invertible lower-triangular::shape?::shape?::shape?::shape? matrices;&nbsp;</li><li>\(E\) and \(E^{-1}\) are invertible \(m \times m\) permutation matrices;&nbsp;</li><li>and \(U\) and \(U^{-1}\) are invertible \(m \times m\) upper-triangular matrices with ones on the diagonal.&nbsp;</li></ul><li>The reduced row echelon matrix \(A\) is unique (independent of how the row reduction is performed).<br></li></ul>
-
-============================================================
-
-Note ID: 1696240176900
-  Field: Text
-    Before:
-      What are the three primary benefits
-     of previous DAG dataflow systems that the authors enumerate and wish to
-     ensure for CIEL?<ul><li>Task-level parallelism</li><li>Transparent fault-tolerance</li><li>Transparent scaling</li></ul>
-
-    After:
-      What are the three primary benefits
-     of previous DAG dataflow systems that the authors enumerate and wish to
-     ensure for CIEL?<ul><li>Task-level parallelism</li><li>Transparent fault-tolerance</li><li>Transparent scaling</li></ul>
-
-============================================================
-
-Note ID: 1696240435973
-  Field: Text
-    Before:
-      Definition 1: Original Curriculum Learning [6]. A curriculum is a sequence of training criteria over \(T\) training steps: \(\mathcal{C}=\left\langle Q_1, \ldots, Q_t, \ldots, Q_T\right\rangle\). Each criterion \(Q_t\) is a reweighting of the target training distribution \(P(z)\) :<br>\[<br>Q_t(z) \propto W_t(z) P(z) \quad \forall \text { example } z \in \text { training set } D,<br>\]<br>such that the following three conditions are satisfied:<br>- 1) The entropy of distributions gradually increases, i.e., \(H\left(Q_t\right)&lt;H\left(Q_{t+1}\right)\).<br>- 2) The weight for any example increases, i.e., \(W_t(z) \leq W_{t+1}(z) \quad \forall z \in D\).<br>- 3) \(Q_T(z)=P(z)\).
-
-    After:
-      Definition 1: Original Curriculum Learning [6]. A curriculum is a sequence of training criteria over \(T\) training steps: \(\mathcal{C}=\left\langle Q_1, \ldots, Q_t, \ldots, Q_T\right\rangle\). Each criterion \(Q_t\) is a reweighting of the target training distribution \(P(z)\) :<br>\[<br>Q_t(z) \propto W_t(z) P(z) \quad \forall \text { example } z \in \text { training set } D,<br>\]<br>such that the following three conditions are satisfied:<br>- 1) The entropy of distributions gradually increases, i.e., \(H\left(Q_t\right)&lt;H\left(Q_{t+1}\right)\).<br>- 2) The weight for any example increases, i.e., \(W_t(z) \leq W_{t+1}(z) \quad \forall z \in D\).<br>- 3) \(Q_T(z)=P(z)\).
-
-============================================================
-
-Note ID: 1696244727942
-  Field: Text
-    Before:
-      Theorem 5.2. Suppose \(n\) and \(r\) are nonnegative integers. There is a oneto-one correspondence between \(r\)-dimensional subspaces \(U \subset F^{n}\), and \(r \times n\) matrices \(A\) in reduced row-echelon form, with one pivot in each row; that is, with no rows equal to zero. The correspondence sends the matrix \(A\) to the span \(\operatorname{Row}(A)\) of the rows of \(A\). To go in the other direction, suppose \(U\) is an \(r\)-dimensional subspace of \(F_{n}\). Choose a basis \(\left(u_{1}, \ldots, u_{r}\right)\) of \(U\), and let \(A^{\prime}\) be the \(r \times n\) matrix with rows \(u_{i}\). Perform Gaussian elimination on \(A^{\prime}\), getting an \(r \times n\) matrix \(A\) in reduced row echelon form; this is the matrix corresponding to the subspace \(U\).<br><br>Sketch of proof. A matrix \(A\) of the desired form clearly has \(r\) pivots, and so has rank \(r\) (Proposition 2.7). Therefore the row space \(\operatorname{Row}(A)\) is indeed an \(r\)-dimensional subspace of \(F^{n}\). <br><ul><li>Conversely, given an \(r\)-dimensional \(U\), the construction in the theorem produces an \(r \times n\)::size? matrix \(A^{\prime}\) with \(\operatorname{Row}\left(A^{\prime}\right)\) = \(U\). Now perform Gaussian elimination on \(A^{\prime}\) (Theorem 4.8), obtaining a reduced row echelon matrix \(A\) with \(\operatorname{Row}(A)=\operatorname{Row}\left(A^{\prime}\right)=U\), as desired.</li></ul>
-
-    After:
-      Theorem 5.2. Suppose \(n\) and \(r\) are nonnegative integers. There is a oneto-one correspondence between \(r\)-dimensional subspaces \(U \subset F^{n}\), and \(r \times n\) matrices \(A\) in reduced row-echelon form, with one pivot in each row; that is, with no rows equal to zero. The correspondence sends the matrix \(A\) to the span \(\operatorname{Row}(A)\) of the rows of \(A\). To go in the other direction, suppose \(U\) is an \(r\)-dimensional subspace of \(F_{n}\). Choose a basis \(\left(u_{1}, \ldots, u_{r}\right)\) of \(U\), and let \(A^{\prime}\) be the \(r \times n\) matrix with rows \(u_{i}\). Perform Gaussian elimination on \(A^{\prime}\), getting an \(r \times n\) matrix \(A\) in reduced row echelon form; this is the matrix corresponding to the subspace \(U\).<br><br>Sketch of proof. A matrix \(A\) of the desired form clearly has \(r\) pivots, and so has rank \(r\) (Proposition 2.7). Therefore the row space \(\operatorname{Row}(A)\) is indeed an \(r\)-dimensional subspace of \(F^{n}\). <br><ul><li>Conversely, given an \(r\)-dimensional \(U\), the construction in the theorem produces an \(r \times n\)::size? matrix \(A^{\prime}\) with \(\operatorname{Row}\left(A^{\prime}\right)\) = \(U\). Now perform Gaussian elimination on \(A^{\prime}\) (Theorem 4.8), obtaining a reduced row echelon matrix \(A\) with \(\operatorname{Row}(A)=\operatorname{Row}\left(A^{\prime}\right)=U\), as desired.</li></ul>
-
-============================================================
-
-Note ID: 1696319964400
-  Field: Text
-    Before:
-      2. (3 points) Suppose \(A\) is an \(m \times n\) matrix, and that \(m&lt;n\). For each statement below, tell whether it is true or false; give a counterexample if it is false; and explain why if it is true.<br><br>b) There is a constant \(C\) in \(\mathbb{R}^{m}\) so that the equation \(A X=C\) has no solutions.<br><br><ul><li>This depepends on weather the linear map is surjective</li><li>It the linear map is surjective there is always a solution</li></ul>
-
-    After:
-      2. (3 points) Suppose \(A\) is an \(m \times n\) matrix, and that \(m&lt;n\). For each statement below, tell whether it is true or false; give a counterexample if it is false; and explain why if it is true.<br><br>b) There is a constant \(C\) in \(\mathbb{R}^{m}\) so that the equation \(A X=C\) has no solutions.<br><br><ul><li>This depepends on weather the linear map is surjective</li><li>It the linear map is surjective there is always a solution</li></ul>
-
-============================================================
-
-Note ID: 1696321573025
-  Field: Text
-    Before:
-      4. (3 points) (Based on Axler, 2nd edition page 60, exercise 16 or 3rd edition page 69 , exercise 22). Suppose \(U\) is a finite-dimensional vector spaces, that \(S \in \mathcal{L}(V, W)\), and that \(T \in \mathcal{L}(U, V)\). Prove that<br><br>\[<br>\operatorname{dim} \operatorname{Null}(S T)=\operatorname{dim} \operatorname{Null}(T)+\operatorname{dim}(\operatorname{Range}(T) \cap \operatorname{Null}(S)) .<br>\]<br><br>Original proof:<br><ul><li>E = ST, E in L(U,W)</li><li>dim null E = dim U - dim range E</li><li>dim U = dim range T + dim null T</li><li>Thus dim null E = dim range T + dim&nbsp; null T - dim range E</li><li>As such, the question comes down to dim range E</li></ul>
-
-    After:
-      4. (3 points) (Based on Axler, 2nd edition page 60, exercise 16 or 3rd edition page 69 , exercise 22). Suppose \(U\) is a finite-dimensional vector spaces, that \(S \in \mathcal{L}(V, W)\), and that \(T \in \mathcal{L}(U, V)\). Prove that<br><br>\[<br>\operatorname{dim} \operatorname{Null}(S T)=\operatorname{dim} \operatorname{Null}(T)+\operatorname{dim}(\operatorname{Range}(T) \cap \operatorname{Null}(S)) .<br>\]<br><br>Original proof:<br><ul><li>E = ST, E in L(U,W)</li><li>dim null E = dim U - dim range E</li><li>dim U = dim range T + dim null T</li><li>Thus dim null E = dim range T + dim&nbsp; null T - dim range E</li><li>As such, the question comes down to dim range E</li></ul>
-
-============================================================
-
-Note ID: 1696322644237
-  Field: Text
-    Before:
-      6. (3 points) Suppose \(T \in \mathcal{L}(V, W)\), and that \(V\) is finite-dimensional.<br><br>a) Prove that \(\operatorname{Null}(T)=\{0\}\) if and only if for every linearly independent list \(\left(v_{1}, v_{2}, \ldots, v_{p}\right)\) in \(V,\left(T v_{1}, \ldots, T v_{p}\right)\) is linearly independent in \(W\).<br><br><ul><li>Two steps:</li><ul><li>If null T = {0} then mapping a lin indp list in V via T can only result in a lin indp list in W since no&nbsp;\(Tv_i\) can be 0&nbsp;</li><li>\(T(\vec{c} \cdot \vec{v})\) = 0&nbsp;</li><li>\(\vec{c} \cdot T(\vec{v})\) = 0 implies&nbsp;\(\vec{c} = \vec{0}\)<br></li></ul></ul>
-
-    After:
-      6. (3 points) Suppose \(T \in \mathcal{L}(V, W)\), and that \(V\) is finite-dimensional.<br><br>a) Prove that \(\operatorname{Null}(T)=\{0\}\) if and only if for every linearly independent list \(\left(v_{1}, v_{2}, \ldots, v_{p}\right)\) in \(V,\left(T v_{1}, \ldots, T v_{p}\right)\) is linearly independent in \(W\).<br><br><ul><li>Two steps:</li><ul><li>If null T = {0} then mapping a lin indp list in V via T can only result in a lin indp list in W since no&nbsp;\(Tv_i\) can be 0&nbsp;</li><li>\(T(\vec{c} \cdot \vec{v})\) = 0&nbsp;</li><li>\(\vec{c} \cdot T(\vec{v})\) = 0 implies&nbsp;\(\vec{c} = \vec{0}\)<br></li></ul></ul>
-
-============================================================
-
-Note ID: 1696323204708
-  Field: Text
-    Before:
-      6. (3 points) Suppose \(T \in \mathcal{L}(V, W)\), and that \(V\) is finite-dimensional.<br>Background:<br><ul><li>a) Prove that \(\operatorname{Null}(T)=\{0\}\) if and only if for every linearly independent list \(\left(v_{1}, v_{2}, \ldots, v_{p}\right)\) in \(V,\left(T v_{1}, \ldots, T v_{p}\right)\) is linearly independent in \(W\).</li><li>b) Prove that Range \((T)=W\) if and only if for every spanning list \(\left(v_{1}, v_{2}, \ldots, v_{q}\right)\) in \(V,\left(T v_{1}, \ldots, T v_{q}\right)\) is a spanning list in \(W\).</li></ul><b>c) Prove \(T\) is invertible if and only if \(T\) takes each basis of \(V\) to a basis of \(W\).</b><br><br>Proof: suppose it is invertible then:<ul><li>if T takes each basis of V to a basis of W:</li><ul><li>Suppose&nbsp;\(\vec{v}_{1\to p}\) is lin indp, then we can extend it to a basis&nbsp;\(\vec{v}\)</li><li>Since any sublist of a basis must be linearly independent and&nbsp;\(T(\vec{v})\) is lin indp, then&nbsp;\(T(\vec{v}_{1\to p})\) must also be lin indp so null T = 0</li></ul><li>The same method can be applied for spanning lists to prove surjectivity</li></ul>
-
-    After:
-      6. (3 points) Suppose \(T \in \mathcal{L}(V, W)\), and that \(V\) is finite-dimensional.<br>Background:<br><ul><li>a) Prove that \(\operatorname{Null}(T)=\{0\}\) if and only if for every linearly independent list \(\left(v_{1}, v_{2}, \ldots, v_{p}\right)\) in \(V,\left(T v_{1}, \ldots, T v_{p}\right)\) is linearly independent in \(W\).</li><li>b) Prove that Range \((T)=W\) if and only if for every spanning list \(\left(v_{1}, v_{2}, \ldots, v_{q}\right)\) in \(V,\left(T v_{1}, \ldots, T v_{q}\right)\) is a spanning list in \(W\).</li></ul><b>c) Prove \(T\) is invertible if and only if \(T\) takes each basis of \(V\) to a basis of \(W\).</b><br><br>Proof: suppose it is invertible then:<ul><li>if T takes each basis of V to a basis of W:</li><ul><li>Suppose&nbsp;\(\vec{v}_{1\to p}\) is lin indp, then we can extend it to a basis&nbsp;\(\vec{v}\)</li><li>Since any sublist of a basis must be linearly independent and&nbsp;\(T(\vec{v})\) is lin indp, then&nbsp;\(T(\vec{v}_{1\to p})\) must also be lin indp so null T = 0</li></ul><li>The same method can be applied for spanning lists to prove surjectivity</li></ul>
-
-============================================================
-
-Note ID: 1696328093395
-  Field: Text
-    Before:
-      2. (20 points) List all \(3 \times 2\) matrices over \(\mathbb{R}\) which are in reduced row-echelon form. Point out the rank of each one.<br><br>Solution:<br><br><ul><li>\(\left(\begin{array}{ll}1 &amp; 0 \\ 0 &amp; 1 \\ 0 &amp; 0\end{array}\right)\)</li><ul><li>with rank 2</li></ul><li>\(\left(\begin{array}{ll}1 &amp; a \\ 0 &amp; 0 \\ 0 &amp; 0\end{array}\right)\)<br></li><ul><li>with rank 1</li></ul><li>\(\left(\begin{array}{ll}0 &amp; 1 \\ 0 &amp; 0 \\ 0 &amp; 0\end{array}\right)\)<br></li><ul><li>with rank 1</li></ul><li>\(\left(\begin{array}{ll}0 &amp; 0 \\ 0 &amp; 0 \\ 0 &amp; 0\end{array}\right)\)</li><ul><li>with rank 0</li></ul></ul>
-
-    After:
-      2. (20 points) List all \(3 \times 2\) matrices over \(\mathbb{R}\) which are in reduced row-echelon form. Point out the rank of each one.<br><br>Solution:<br><br><ul><li>\(\left(\begin{array}{ll}1 &amp; 0 \\ 0 &amp; 1 \\ 0 &amp; 0\end{array}\right)\)</li><ul><li>with rank 2</li></ul><li>\(\left(\begin{array}{ll}1 &amp; a \\ 0 &amp; 0 \\ 0 &amp; 0\end{array}\right)\)<br></li><ul><li>with rank 1</li></ul><li>\(\left(\begin{array}{ll}0 &amp; 1 \\ 0 &amp; 0 \\ 0 &amp; 0\end{array}\right)\)<br></li><ul><li>with rank 1</li></ul><li>\(\left(\begin{array}{ll}0 &amp; 0 \\ 0 &amp; 0 \\ 0 &amp; 0\end{array}\right)\)</li><ul><li>with rank 0</li></ul></ul>
-
-============================================================
-
-Note ID: 1696329111006
-  Field: Text
-    Before:
-      5.(20 points) Let \(T(p)=\left(\left(x^{2}+1\right) p\right)^{\prime \prime}\) be a map on \(\mathcal{P}(z)\) (the polynomials with complex coefficients of any degree) to itself. Prove \(T\) is surjective.<br><ul><li>T sends all polynomials to polynomials of degree equal to themselves<br></li><li>T sends only 0 to 0 so it is injective</li><li>Meaning it is surjective for&nbsp;\(P_m\)&nbsp;\(\forall m\), meaning it is surjective over&nbsp;\(P(z)\)</li></ul>
-
-    After:
-      5.(20 points) Let \(T(p)=\left(\left(x^{2}+1\right) p\right)^{\prime \prime}\) be a map on \(\mathcal{P}(z)\) (the polynomials with complex coefficients of any degree) to itself. Prove \(T\) is surjective.<br><ul><li>T sends all polynomials to polynomials of degree equal to themselves<br></li><li>T sends only 0 to 0 so it is injective</li><li>Meaning it is surjective for&nbsp;\(P_m\)&nbsp;\(\forall m\), meaning it is surjective over&nbsp;\(P(z)\)</li></ul>
-
-============================================================
-
-Note ID: 1696345244130
-  Field: Text
-    Before:
-      &nbsp;If \(T\) has both a left and a right inverse, then the left and right inverses are unique and equal to each other.&nbsp;
-
-    After:
-      &nbsp;If \(T\) has both a left and a right inverse, then the left and right inverses are unique and equal to each other.&nbsp;
-
-============================================================
-
-Note ID: 1696404819154
-  Field: Text
-    Before:
-      Example:<br>If \(T \in \mathcal{L}\left(\mathbf{C}^2\right)\) is defined by \(T(w, z)=(-z, w)\), then \((1,-i)\) is an eigenvector corresponding to the eigenvalue \(i\) because<br><ul><li>\(T \) \((1,-i)\) = \(i\) \((1,-i)\)</li></ul>
-
-    After:
-      Example:<br>If \(T \in \mathcal{L}\left(\mathbf{C}^2\right)\) is defined by \(T(w, z)=(-z, w)\), then \((1,-i)\) is an eigenvector corresponding to the eigenvalue \(i\) because<br><ul><li>\(T \) \((1,-i)\) = \(i\) \((1,-i)\)</li></ul>
-
-============================================================
-
-Note ID: 1696404886391
-  Field: Text
-    Before:
-      Any nonzero scalar multiple of an eigenvector is an eigenvector
-
-    After:
-      Any nonzero scalar multiple of an eigenvector is an eigenvector
-
-============================================================
-
-Note ID: 1696408424794
-  Field: Text
-    Before:
-      How do the usual proofs of the existance of an eigenvalue go?<br><ul><li>Build&nbsp;\(det (\lambda I -&nbsp; T)\) which is called the characteristic polynomial of T</li><li>This polynomial is only equal to 0 if&nbsp;\(\lambda\) is an eigenvalue</li></ul>
-
-    After:
-      How do the usual proofs of the existance of an eigenvalue go?<br><ul><li>Build&nbsp;\(det (\lambda I -&nbsp; T)\) which is called the characteristic polynomial of T</li><li>This polynomial is only equal to 0 if&nbsp;\(\lambda\) is an eigenvalue</li></ul>
-
-============================================================
-
-Note ID: 1696491872632
-  Field: Text
-    Before:
-      Composition of the same operator is obviously comutative, which is part of the reason why multiplying polynomials of the same operator is comutative
-
-    After:
-      Composition of the same operator is obviously comutative, which is part of the reason why multiplying polynomials of the same operator is comutative
-
-============================================================
-
-Note ID: 1696494685416
-  Field: Text
-    Before:
-      The spans of column vectors in upper triangular matrices can be said to be nested, with a total of dim V such nested subspaces
-
-    After:
-      The spans of column vectors in upper triangular matrices can be said to be nested, with a total of dim V such nested subspaces
-
-============================================================
-
-Note ID: 1696495948966
-  Field: Text
-    Before:
-      Definition: trace<br><br>Let&nbsp;\(T \in L(V)\) be an operator and&nbsp;\(B\) be a basis making the matrix \(M_B(T)\) upper triangular.<br><ul><li>The trace, written \(tr(T)\), is the sum of the diagonal entries of the upper-triangular matrix</li></ul>
-
-    After:
-      Definition: trace<br><br>Let&nbsp;\(T \in L(V)\) be an operator and&nbsp;\(B\) be a basis making the matrix \(M_B(T)\) upper triangular.<br><ul><li>The trace, written \(tr(T)\), is the sum of the diagonal entries of the upper-triangular matrix</li></ul>
-
-============================================================
-
-Note ID: 1696496137052
-  Field: Text
-    Before:
-      <b>Definition</b>: trace, determinant<br><br>Let&nbsp;\(T \in L(V)\) be an operator and&nbsp;\(B\) a basis w.r.t which the matrix&nbsp;\(M_B(T)\) is upper-triangular:<br><ul><li>The trace,written \(tr(T)\) is the sum of the diagonal entries of the matrix&nbsp;\(M_B(T)\)</li><li>The determinant, written&nbsp;\(det(T)\) is the product ot the diagonal entries of the matrix&nbsp;\(M_B(T)\)</li></ul>
-
-    After:
-      <b>Definition</b>: trace, determinant<br><br>Let&nbsp;\(T \in L(V)\) be an operator and&nbsp;\(B\) a basis w.r.t which the matrix&nbsp;\(M_B(T)\) is upper-triangular:<br><ul><li>The trace,written \(tr(T)\) is the sum of the diagonal entries of the matrix&nbsp;\(M_B(T)\)</li><li>The determinant, written&nbsp;\(det(T)\) is the product ot the diagonal entries of the matrix&nbsp;\(M_B(T)\)</li></ul>
-
-============================================================
-
-Note ID: 1696496843722
-  Field: Text
-    Before:
-      Suppose an operator T has an upper-triangular matrix w.r.t some basis&nbsp;\(B\).<br><br><ul><li>Then T is invertible iff all diagonal of&nbsp;\(M_B(T)\) entries are nonzero</li><li>Furthermore, since a list of field elements is nonzero iff their product is nonzero this implies that T is invertible iiff its determinant&nbsp;\(det(T)\)&nbsp; \( \neq\)&nbsp; \(0\)&nbsp;</li></ul>
-
-    After:
-      Suppose an operator T has an upper-triangular matrix w.r.t some basis&nbsp;\(B\).<br><br><ul><li>Then T is invertible iff all diagonal of&nbsp;\(M_B(T)\) entries are nonzero</li><li>Furthermore, since a list of field elements is nonzero iff their product is nonzero this implies that T is invertible iiff its determinant&nbsp;\(det(T)\)&nbsp; \( \neq\)&nbsp; \(0\)&nbsp;</li></ul>
-
-============================================================
-
-Note ID: 1696497883811
-  Field: Text
-    Before:
-      Since the elements on the diagonal of an upper-triangular matrix are eigenvalues, the trace is the sum of eigenvalues while the determinant is the product of eigenvalues
-
-    After:
-      Since the elements on the diagonal of an upper-triangular matrix are eigenvalues, the trace is the sum of eigenvalues while the determinant is the product of eigenvalues
-
-============================================================
-
-Note ID: 1696499593298
-  Field: Text
-    Before:
-      Proof that the elements of the diagonal of an upper-triangular matrix of T, w.r.t base B, are eigenvalues:<br><ul><li>Consider&nbsp;\(\lambda \in F\)</li><li>To test weather it is an eigenvalue we can test whether \(T&nbsp; - \lambda I\) is not injective</li><li>Put in matrix form,&nbsp;\ test \(M_B(T - \lambda I)\) is&nbsp; not invertible</li><li>Since \(M_B(T)\) is upper-triangular, \(M_B(T - \lambda I)\) has the same shape with&nbsp;\(\lambda\) subtracted from the diagonal<br></li><li>Since \(M_B(T - \lambda I)\) is not invertible iff one of the elements is 0, then the elements of the diagonal of&nbsp; \(M_B(T - \lambda I)\) must be eigenvalues since&nbsp;\(\lambda_k - \lambda = 0\) iff&nbsp;\(\lambda = lambda_k\)</li></ul>
-
-    After:
-      Proof that the elements of the diagonal of an upper-triangular matrix of T, w.r.t base B, are eigenvalues:<br><ul><li>Consider&nbsp;\(\lambda \in F\)</li><li>To test weather it is an eigenvalue we can test whether \(T&nbsp; - \lambda I\) is not injective</li><li>Put in matrix form,&nbsp;\ test \(M_B(T - \lambda I)\) is&nbsp; not invertible</li><li>Since \(M_B(T)\) is upper-triangular, \(M_B(T - \lambda I)\) has the same shape with&nbsp;\(\lambda\) subtracted from the diagonal<br></li><li>Since \(M_B(T - \lambda I)\) is not invertible iff one of the elements is 0, then the elements of the diagonal of&nbsp; \(M_B(T - \lambda I)\) must be eigenvalues since&nbsp;\(\lambda_k - \lambda = 0\) iff&nbsp;\(\lambda = lambda_k\)</li></ul>
-
-============================================================
-
-Note ID: 1696499724493
-  Field: Text
-    Before:
-      A diagonal matrix has nonzero elements only on the diagonal, however, it can still have zeros on the diagonal
-
-    After:
-      A diagonal matrix has nonzero elements only on the diagonal, however, it can still have zeros on the diagonal
-
-============================================================
-
-Note ID: 1696501568253
-  Field: Text
-    Before:
-      We can use the idea of an evaluate map between dual vectors and normal vectors:<br><img src="paste-b21f2ee395dd92d1c84acbacfe432b9b730010fe.jpg"><br>to diagrammatically define the dual dual as follows:<br><img src="paste-77c82a1ec0e5648ed15767befa65c99caa70538f.jpg"><br>Which is why the dual dual is naturally isomorphic to V
-
-    After:
-      We can use the idea of an evaluate map between dual vectors and normal vectors:<br><img src="paste-b21f2ee395dd92d1c84acbacfe432b9b730010fe.jpg"><br>to diagrammatically define the dual dual as follows:<br><img src="paste-77c82a1ec0e5648ed15767befa65c99caa70538f.jpg"><br>Which is why the dual dual is naturally isomorphic to V
-
-============================================================
-
-Note ID: 1696511620822
-  Field: Text
-    Before:
-      Direct sum projection definition:<br><br><ul><li>Suppose V =&nbsp;\(U \oplus W\)</li><li>This means that&nbsp;\(\forall v \in V, \exists u \in U, w \in W \) such that&nbsp;\(v = u+w\)</li><li>We can then define a projection:</li><ul><li>\(P_{u,w}(v)\)&nbsp; = \( u \) which is a function beacuse u is unique and can be thought of as a map from V onto U or onto V</li></ul><li>And thus the direct sum decomposition can be rewritten:</li><li>V =&nbsp;\(P_{u,w}(v) \) +&nbsp;\(P_{w,u}(v)\)</li></ul><br>
-
-    After:
-      Direct sum projection definition:<br><br><ul><li>Suppose V =&nbsp;\(U \oplus W\)</li><li>This means that&nbsp;\(\forall v \in V, \exists u \in U, w \in W \) such that&nbsp;\(v = u+w\)</li><li>We can then define a projection:</li><ul><li>\(P_{u,w}(v)\)&nbsp; = \( u \) which is a function beacuse u is unique and can be thought of as a map from V onto U or onto V</li></ul><li>And thus the direct sum decomposition can be rewritten:</li><li>V =&nbsp;\(P_{u,w}(v) \) +&nbsp;\(P_{w,u}(v)\)</li></ul><br>
-
-============================================================
-
-Note ID: 1696511707034
-  Field: Text
-    Before:
-      A projection onto a subspace is surjective::surj/inj? as a map on that subspace
-
-    After:
-      A projection onto a subspace is surjective::surj/inj? as a map on that subspace
-
-============================================================
-
-Note ID: 1696514323722
-  Field: Text
-    Before:
-      <b>Definition</b> dot product<br><br>For \(x, y \in \mathbf{R}^{n}\), the dot product of \(x\) and \(y\), denoted \(x \cdot y\), is defined by<br><br><ul><li>\(x \cdot y\) = \(x_{1} y_{1}+\cdots+x_{n} y_{n},\)</li></ul><br>where \(x=\left(x_{1}, \ldots, x_{n}\right)\) and \(y=\left(y_{1}, \ldots, y_{n}\right)\).<br>
-
-    After:
-      <b>Definition</b> dot product<br><br>For \(x, y \in \mathbf{R}^{n}\), the dot product of \(x\) and \(y\), denoted \(x \cdot y\), is defined by<br><br><ul><li>\(x \cdot y\) = \(x_{1} y_{1}+\cdots+x_{n} y_{n},\)</li></ul><br>where \(x=\left(x_{1}, \ldots, x_{n}\right)\) and \(y=\left(y_{1}, \ldots, y_{n}\right)\).<br>
-
-============================================================
-
-Note ID: 1696514525565
-  Field: Text
-    Before:
-      The dot product on \(\mathbf{R}^{n}\) has the following properties:<br><br><ul><li>- \(x \cdot x \geq 0\) for all \(x \in \mathbf{R}^{n}\);</li><li>- \(x \cdot x=0\) if and only if \(x=0\);</li><li>- for \(y \in \mathbf{R}^{n}\) fixed, the map from \(\mathbf{R}^{n}\) to \(\mathbf{R}\) that sends \(x \in \mathbf{R}^{n}\) to \(x \cdot y\) is linear;</li><li>- \(x \cdot y\) = \(y \cdot x\) for all \(x, y \in \mathbf{R}^{n}\).</li></ul>
-
-    After:
-      The dot product on \(\mathbf{R}^{n}\) has the following properties:<br><br><ul><li>- \(x \cdot x \geq 0\) for all \(x \in \mathbf{R}^{n}\);</li><li>- \(x \cdot x=0\) if and only if \(x=0\);</li><li>- for \(y \in \mathbf{R}^{n}\) fixed, the map from \(\mathbf{R}^{n}\) to \(\mathbf{R}\) that sends \(x \in \mathbf{R}^{n}\) to \(x \cdot y\) is linear;</li><li>- \(x \cdot y\) = \(y \cdot x\) for all \(x, y \in \mathbf{R}^{n}\).</li></ul>
-
-============================================================
-
-Note ID: 1696518025193
-  Field: Text
-    Before:
-      <b>Definition</b> inner product space<br><br>An inner product space is a vector space \(V\) along with an inner product on \(V\).
-
-    After:
-      <b>Definition</b> inner product space<br><br>An inner product space is a vector space \(V\) along with an inner product on \(V\).
-
-============================================================
-
-Note ID: 1696522867339
-  Field: Text
-    Before:
-      <b>Basic properties of an inner product</b><br><br><ul><li>(a) For each fixed \(u \in V\)&nbsp;the function that takes \(v\) to \(\langle v, u\rangle\)  is a linear map from \(V\) to \(\mathbf{F}\).</li><li>(b) \(\langle 0, u\rangle\) = \(0\) for every \(u \in V\).</li><li>(c) \(\langle u, 0\rangle\) = \(0\) for every \(u \in V\).</li><li>(d) \(\langle u, v+w\rangle\) = \(\langle u, v\rangle+\langle u, w\rangle\) for all \(u, v, w \in V\).</li><li>(e) \(\langle u, \lambda v\rangle\) = \(\bar{\lambda}\langle u, v\rangle\) for all \(\lambda \in \mathbf{F}\) and \(u, v \in V\).</li></ul>
-
-    After:
-      <b>Basic properties of an inner product</b><br><br><ul><li>(a) For each fixed \(u \in V\)&nbsp;the function that takes \(v\) to \(\langle v, u\rangle\)  is a linear map from \(V\) to \(\mathbf{F}\).</li><li>(b) \(\langle 0, u\rangle\) = \(0\) for every \(u \in V\).</li><li>(c) \(\langle u, 0\rangle\) = \(0\) for every \(u \in V\).</li><li>(d) \(\langle u, v+w\rangle\) = \(\langle u, v\rangle+\langle u, w\rangle\) for all \(u, v, w \in V\).</li><li>(e) \(\langle u, \lambda v\rangle\) = \(\bar{\lambda}\langle u, v\rangle\) for all \(\lambda \in \mathbf{F}\) and \(u, v \in V\).</li></ul>
-
-============================================================
-
-Note ID: 1696523273833
-  Field: Text
-    Before:
-      Definition norm, \(\|v\|\)<br><br>For \(v \in V\), the norm of \(v\), denoted \(\|v\|\), is defined by<br><br><ul><li>\(\|v\|\) = \(\sqrt{\langle v, v\rangle}\)</li></ul>
-
-    After:
-      Definition norm, \(\|v\|\)<br><br>For \(v \in V\), the norm of \(v\), denoted \(\|v\|\), is defined by<br><br><ul><li>\(\|v\|\) = \(\sqrt{\langle v, v\rangle}\)</li></ul>
-
-============================================================
-
-Note ID: 1696575250539
-  Field: Text
-    Before:
-      <b>Definition</b> orthogonal<br><br>Two vectors \(u, v \in V\) are called orthogonal if \(\langle u, v\rangle\) = \(0\).
-
-    After:
-      <b>Definition</b> orthogonal<br><br>Two vectors \(u, v \in V\) are called orthogonal if \(\langle u, v\rangle\) = \(0\).
-
-============================================================
-
-Note ID: 1696575541031
-  Field: Text
-    Before:
-      Orthogonality and 0<br><br><ul><li>(a) 0 is orthogonal to every vector in \(V\).</li><li>(b) 0 is also the only vector in \(V\) that is orthogonal to itself.</li></ul>
-
-    After:
-      Orthogonality and 0<br><br><ul><li>(a) 0 is orthogonal to every vector in \(V\).</li><li>(b) 0 is also the only vector in \(V\) that is orthogonal to itself.</li></ul>
-
-============================================================
-
-Note ID: 1696576676043
-  Field: Text
-    Before:
-      <b>The</b> Cauchy-Schwarz Inequality<br><br>Suppose \(u, v \in V\). Then<br><br><ul><li>\(|\langle u, v\rangle|\)\(\leq\)\(\|u\|\|v\| .\)</li></ul><br>This inequality is an equality if and only if one of \(u, v\) is a scalar multiple of the other.<br>
-
-    After:
-      <b>The</b> Cauchy-Schwarz Inequality<br><br>Suppose \(u, v \in V\). Then<br><br><ul><li>\(|\langle u, v\rangle|\)\(\leq\)\(\|u\|\|v\| .\)</li></ul><br>This inequality is an equality if and only if one of \(u, v\) is a scalar multiple of the other.<br>
-
-============================================================
-
-Note ID: 1696579055196
-  Field: Text
-    Before:
-      Triangle Inequality<br><br>Suppose \(u, v \in V\). Then<br><br><ul><li>\(\|u+v\|\)&nbsp;\(\leq\) \(\|u\|+\|v\| .\)</li></ul><br>This inequality is an equality if and only if one of \(u, v\) is a nonnegative multiple of the other.<br>
-
-    After:
-      Triangle Inequality<br><br>Suppose \(u, v \in V\). Then<br><br><ul><li>\(\|u+v\|\)&nbsp;\(\leq\) \(\|u\|+\|v\| .\)</li></ul><br>This inequality is an equality if and only if one of \(u, v\) is a nonnegative multiple of the other.<br>
-
-============================================================
-
-Note ID: 1696579547355
-  Field: Text
-    Before:
-      <img src="paste-d1816ecb0211e3b519d82b39b5c6bb028b1276cd.jpg"><br><br><ul><li>\(\|u+v\|^2+\|u-v\|^2\)=<br></li><ul><li>= \(\langle u+v, u+v\rangle\) +\(\langle u-v, u-v\rangle\)</li><li>=&nbsp;\(\|u\|^2\) + \(\|v\|^2\) + \(\langle u, v\rangle+\langle v, u\rangle\) + \(\|u\|^2\) + \(\|v\|^2\) - \(\langle u, v\rangle\) - \(\langle v, u\rangle\)</li><li>=&nbsp;\(2\left(\|u\|^2+\|v\|^2\right)\)</li></ul></ul>
-
-    After:
-      <img src="paste-d1816ecb0211e3b519d82b39b5c6bb028b1276cd.jpg"><br><br><ul><li>\(\|u+v\|^2+\|u-v\|^2\)=<br></li><ul><li>= \(\langle u+v, u+v\rangle\) +\(\langle u-v, u-v\rangle\)</li><li>=&nbsp;\(\|u\|^2\) + \(\|v\|^2\) + \(\langle u, v\rangle+\langle v, u\rangle\) + \(\|u\|^2\) + \(\|v\|^2\) - \(\langle u, v\rangle\) - \(\langle v, u\rangle\)</li><li>=&nbsp;\(2\left(\|u\|^2+\|v\|^2\right)\)</li></ul></ul>
-
-============================================================
-
-Note ID: 1696580053681
-  Field: Text
-    Before:
-      8 Suppose \(u, v \in V\) and \(\|u\|\) =\(\|v\| \) = \(1\) and \(\langle u, v\rangle\) = \(1\). Prove that \(u\) = \(v\).
-
-    After:
-      8 Suppose \(u, v \in V\) and \(\|u\|\) =\(\|v\| \) = \(1\) and \(\langle u, v\rangle\) = \(1\). Prove that \(u\) = \(v\).
-
-============================================================
-
-Note ID: 1696580229834
-  Field: Text
-    Before:
-      9 Suppose \(u, v \in V\) and \(\|u\|\)&nbsp;<br>\(\leq\) \(1\) and \(\|v\|\) \(\leq\) \(1\). Prove that<br><br><ul><li>\(\sqrt{1-\|u\|^{2} }\)\(\sqrt{1-\|v\|^{2} }\) \( \leq\) \( 1\)-\(|\langle u, v\rangle|\)</li></ul>
-
-    After:
-      9 Suppose \(u, v \in V\) and \(\|u\|\)&nbsp;<br>\(\leq\) \(1\) and \(\|v\|\) \(\leq\) \(1\). Prove that<br><br><ul><li>\(\sqrt{1-\|u\|^{2} }\)\(\sqrt{1-\|v\|^{2} }\) \( \leq\) \( 1\)-\(|\langle u, v\rangle|\)</li></ul>
-
-============================================================
-
-Note ID: 1696581855455
-  Field: Text
-    Before:
-      21 A norm on a vector space \(U\) is a function \|\|:&nbsp; \(U\)&nbsp; \(\rightarrow\) \([0, \infty)\) such that:<br><ul><li>&nbsp;\(\|u\|\) = \(0\) if and only if \(u\) = \(0\)</li><li>&nbsp;\(\|\alpha u\|\) = \(|\alpha|\|u\|\)&nbsp;</li><li>&nbsp;\(\|u+v\|\)&nbsp; \(\leq\) \(\|u\|+\|v\|\)&nbsp;</li></ul>
-
-    After:
-      21 A norm on a vector space \(U\) is a function \|\|:&nbsp; \(U\)&nbsp; \(\rightarrow\) \([0, \infty)\) such that:<br><ul><li>&nbsp;\(\|u\|\) = \(0\) if and only if \(u\) = \(0\)</li><li>&nbsp;\(\|\alpha u\|\) = \(|\alpha|\|u\|\)&nbsp;</li><li>&nbsp;\(\|u+v\|\)&nbsp; \(\leq\) \(\|u\|+\|v\|\)&nbsp;</li></ul>
-
-============================================================
-
-Note ID: 1696582089663
-  Field: Text
-    Before:
-      24 Suppose \(S \in \mathcal{L}(V)\) is an injective::inj/surj? operator on \(V\). Define \(\langle\cdot, \cdot\rangle_{1}\) by<br><br><ul><li>\(\langle u, v\rangle_{1}\)&nbsp;= \(\langle S u, S v\rangle\)</li></ul><br>for \(u, v \in V\). Show that \(\langle\cdot, \cdot\rangle_{1}\) is an inner product on \(V\).<br>
-
-    After:
-      24 Suppose \(S \in \mathcal{L}(V)\) is an injective::inj/surj? operator on \(V\). Define \(\langle\cdot, \cdot\rangle_{1}\) by<br><br><ul><li>\(\langle u, v\rangle_{1}\)&nbsp;= \(\langle S u, S v\rangle\)</li></ul><br>for \(u, v \in V\). Show that \(\langle\cdot, \cdot\rangle_{1}\) is an inner product on \(V\).<br>
-
-============================================================
-
-Note ID: 1696600499455
-  Field: Text
-    Before:
-      A basis is a sufficient choice to determine an isomorphism between V and its dual V' and sufficient to provide a definition for the norm
-
-    After:
-      A basis is a sufficient choice to determine an isomorphism between V and its dual V' and sufficient to provide a definition for the norm
-
-============================================================
-
-Note ID: 1696600778748
-  Field: Text
-    Before:
-      Ideally we would like an inner product to be bilinear rather than sequilinear( linear only in the first coordinate), but this is only possible on real vector spaceses
-
-    After:
-      Ideally we would like an inner product to be bilinear rather than sequilinear( linear only in the first coordinate), but this is only possible on real vector spaceses
-
-============================================================
-
-Note ID: 1696600871921
-  Field: Text
-    Before:
-      While a basis can help define an inner product via composing vectors with their duals, an inner product can exist independent of a basis. This is why we can talk about the length of vectors in R^3 without specifying a basis
-
-    After:
-      While a basis can help define an inner product via composing vectors with their duals, an inner product can exist independent of a basis. This is why we can talk about the length of vectors in R^3 without specifying a basis
-
-============================================================
-
-Note ID: 1696602772145
-  Field: Text
-    Before:
-      Given an inner product, we can define a norm such that:<br><ul><li>\(\vert\vert v \vert\vert\) =&nbsp;\(\sqrt{\langle v,v\rangle}\)</li><li>Note that the axioms of the inner product make it clear that&nbsp;\(\vert\vert v \vert\vert\) =&nbsp; \(0 \) requires that&nbsp;\(v=0\)<br><br></li></ul>
-
-    After:
-      Given an inner product, we can define a norm such that:<br><ul><li>\(\vert\vert v \vert\vert\) =&nbsp;\(\sqrt{\langle v,v\rangle}\)</li><li>Note that the axioms of the inner product make it clear that&nbsp;\(\vert\vert v \vert\vert\) =&nbsp; \(0 \) requires that&nbsp;\(v=0\)<br><br></li></ul>
-
-============================================================
-
-Note ID: 1696603035112
-  Field: Text
-    Before:
-      The norm of a scalar times a vector equals the norm of the scalar times the norm of the vector
-
-    After:
-      The norm of a scalar times a vector equals the norm of the scalar times the norm of the vector
-
-============================================================
-
-Note ID: 1696605366984
-  Field: Text
-    Before:
-      Two vectors being orthogonal implies that all vectors in their span are orthogonal
-
-    After:
-      Two vectors being orthogonal implies that all vectors in their span are orthogonal
-
-============================================================
-
-Note ID: 1696846574485
-  Field: Text
-    Before:
-      <b>Definition</b>&nbsp;orthonormal<br><br>- A list of vectors is called orthonormal if each vector in the list has norm 1 and is orthogonal to all the other vectors in the list.<br><br>- In other words, a list \(e_{1}, \ldots, e_{m}\) of vectors in \(V\) is orthonormal if<br><br><ul><li>\(\left\langle e_{j}, e_{k}\right\rangle\)&nbsp;= \( \begin{cases}1 &amp; \text { if } j =k, \\ 0 &amp; \text { if } j \neq k\end{cases}\)</li></ul>
-
-    After:
-      <b>Definition</b>&nbsp;orthonormal<br><br>- A list of vectors is called orthonormal if each vector in the list has norm 1 and is orthogonal to all the other vectors in the list.<br><br>- In other words, a list \(e_{1}, \ldots, e_{m}\) of vectors in \(V\) is orthonormal if<br><br><ul><li>\(\left\langle e_{j}, e_{k}\right\rangle\)&nbsp;= \( \begin{cases}1 &amp; \text { if } j =k, \\ 0 &amp; \text { if } j \neq k\end{cases}\)</li></ul>
-
-============================================================
-
-Note ID: 1696846717144
-  Field: Text
-    Before:
-      The norm of an orthonormal linear combination<br><br>If \(e_{1}, \ldots, e_{m}\) is an orthonormal list of vectors in \(V\), then<br><br><ul><li>\(\left\|a_{1} e_{1}+\cdots+a_{m} e_{m}\right\|^{2}\) =\(\left|a_{1}\right|^{2}+\cdots+\left|a_{m}\right|^{2}\)</li></ul><br>for all \(a_{1}, \ldots, a_{m} \in \mathbf{F}\).<br>
-
-    After:
-      The norm of an orthonormal linear combination<br><br>If \(e_{1}, \ldots, e_{m}\) is an orthonormal list of vectors in \(V\), then<br><br><ul><li>\(\left\|a_{1} e_{1}+\cdots+a_{m} e_{m}\right\|^{2}\) =\(\left|a_{1}\right|^{2}+\cdots+\left|a_{m}\right|^{2}\)</li></ul><br>for all \(a_{1}, \ldots, a_{m} \in \mathbf{F}\).<br>
-
-============================================================
-
-Note ID: 1696846797658
-  Field: Text
-    Before:
-      An orthonormal list is linearly independent<br><br>Every orthonormal list of vectors is linearly independent.
-
-    After:
-      An orthonormal list is linearly independent<br><br>Every orthonormal list of vectors is linearly independent.
-
-============================================================
-
-Note ID: 1696847053177
-  Field: Text
-    Before:
-      An orthonormal list of the right length is an orthonormal basis<br><br>Every orthonormal list of vectors in \(V\) with length \(\operatorname{dim} V\) is an orthonormal basis of \(V\).
-
-    After:
-      An orthonormal list of the right length is an orthonormal basis<br><br>Every orthonormal list of vectors in \(V\) with length \(\operatorname{dim} V\) is an orthonormal basis of \(V\).
-
-============================================================
-
-Note ID: 1696848990759
-  Field: Text
-    Before:
-      \subsection{Gram-Schmidt Procedure}<br><br>Suppose \(v_{1}, \ldots, v_{m}\) is a linearly independent list of vectors in \(V\). Let \(e_{1}=v_{1} /\left\|v_{1}\right\|\). For \(j=2, \ldots, m\), define \(e_{j}\) inductively by<br><br><ul><li>\(e_{j}\) =&nbsp;</li><ul><li>\((v_{j}-\left\langle v_{j}, e_{1}\right\rangle e_{1}\) \(-\)&nbsp;</li><li>\(\cdots\)&nbsp;</li><li>\(-\) \(\left\langle v_{j}, e_{j-1}\right\rangle e_{j-1}) \)&nbsp;&nbsp;</li><li>\(/\)&nbsp;&nbsp;</li><ul><li>\(\|v_{j}-\left\langle v_{j}, e_{1}\right\rangle e_{1} \) \(-\)&nbsp;</li><li>\(\cdots\)&nbsp;</li><li>\(-\) \(\left\langle v_{j}, e_{j-1}\right\rangle e_{j-1}\| .\)</li></ul></ul></ul>
-
-    After:
-      \subsection{Gram-Schmidt Procedure}<br><br>Suppose \(v_{1}, \ldots, v_{m}\) is a linearly independent list of vectors in \(V\). Let \(e_{1}=v_{1} /\left\|v_{1}\right\|\). For \(j=2, \ldots, m\), define \(e_{j}\) inductively by<br><br><ul><li>\(e_{j}\) =&nbsp;</li><ul><li>\((v_{j}-\left\langle v_{j}, e_{1}\right\rangle e_{1}\) \(-\)&nbsp;</li><li>\(\cdots\)&nbsp;</li><li>\(-\) \(\left\langle v_{j}, e_{j-1}\right\rangle e_{j-1}) \)&nbsp;&nbsp;</li><li>\(/\)&nbsp;&nbsp;</li><ul><li>\(\|v_{j}-\left\langle v_{j}, e_{1}\right\rangle e_{1} \) \(-\)&nbsp;</li><li>\(\cdots\)&nbsp;</li><li>\(-\) \(\left\langle v_{j}, e_{j-1}\right\rangle e_{j-1}\| .\)</li></ul></ul></ul>
-
-============================================================
-
-Note ID: 1696856359647
-  Field: Text
-    Before:
-      Compact version of gram-schmidt process for orthonormal basis creation:<br><ul><li>\(e_{1}\) = \(v_{1} /\left\|v_{1}\right\|\).<br></li><li>\(e_j\) =&nbsp;\(v_j\) \(-\)&nbsp;\((\sum_{i=1}^{j-1} \left\langle v_j, e_i\right\rangle e_i) \)&nbsp;\(/\)&nbsp;\(\) \(\|v_j\) \(-\)&nbsp; \((\sum_{i=1}^{j-1} \left\langle v_j, e_i\right\rangle e_i)&nbsp;\|\)<br></li></ul>
-
-    After:
-      Compact version of gram-schmidt process for orthonormal basis creation:<br><ul><li>\(e_{1}\) = \(v_{1} /\left\|v_{1}\right\|\).<br></li><li>\(e_j\) =&nbsp;\(v_j\) \(-\)&nbsp;\((\sum_{i=1}^{j-1} \left\langle v_j, e_i\right\rangle e_i) \)&nbsp;\(/\)&nbsp;\(\) \(\|v_j\) \(-\)&nbsp; \((\sum_{i=1}^{j-1} \left\langle v_j, e_i\right\rangle e_i)&nbsp;\|\)<br></li></ul>
-
-============================================================
-
-Note ID: 1696856955192
-  Field: Text
-    Before:
-      Existence of orthonormal basis<br><br>Every finite-dimensional inner product space has an orthonormal basis.
-
-    After:
-      Existence of orthonormal basis<br><br>Every finite-dimensional inner product space has an orthonormal basis.
-
-============================================================
-
-Note ID: 1696857076470
-  Field: Text
-    Before:
-      Orthonormal list extends to orthonormal basis<br><br>Suppose \(V\) is finite-dimensional. Then every orthonormal list of vectors in \(V\) can be extended to an orthonormal basis of \(V\).
-
-    After:
-      Orthonormal list extends to orthonormal basis<br><br>Suppose \(V\) is finite-dimensional. Then every orthonormal list of vectors in \(V\) can be extended to an orthonormal basis of \(V\).
-
-============================================================
-
-Note ID: 1696857253285
-  Field: Text
-    Before:
-      Upper-triangular matrix with respect to orthonormal basis<br><br>Suppose \(T \in \mathcal{L}(V)\). If \(T\) has an upper-triangular matrix with respect to some basis of \(V\), then \(T\) has an upper-triangular matrix with respect to some orthonormal basis of \(V\).
-
-    After:
-      Upper-triangular matrix with respect to orthonormal basis<br><br>Suppose \(T \in \mathcal{L}(V)\). If \(T\) has an upper-triangular matrix with respect to some basis of \(V\), then \(T\) has an upper-triangular matrix with respect to some orthonormal basis of \(V\).
-
-============================================================
-
-Note ID: 1696857718252
-  Field: Text
-    Before:
-      Definition linear functional<br><br>A linear functional on \(V\) is a linear map from \(V\) to \(\mathbf{F}\). In other words, a linear functional is an element of \(\mathcal{L}(V, \mathbf{F})\).
-
-    After:
-      Definition linear functional<br><br>A linear functional on \(V\) is a linear map from \(V\) to \(\mathbf{F}\). In other words, a linear functional is an element of \(\mathcal{L}(V, \mathbf{F})\).
-
-============================================================
-
-Note ID: 1696858443346
-  Field: Text
-    Before:
-      <img src="paste-cfcd6e590022102504c02a0eedc7c40ebaec55ef.jpg"><br><br>Proof First we show there exists a vector \(u \in V\) such that \(\varphi(v)=\langle v, u\rangle\) for every \(v \in V\). Let \(e_{1}, \ldots, e_{n}\) be an orthonormal basis of \(V\). Then<br><br>\[<br>\begin{aligned}<br>\varphi(v) &amp; =\varphi\left(\left\langle v, e_{1}\right\rangle e_{1}+\cdots+\left\langle v, e_{n}\right\rangle e_{n}\right) \\<br>&amp; =\left\langle v, e_{1}\right\rangle \varphi\left(e_{1}\right)+\cdots+\left\langle v, e_{n}\right\rangle \varphi\left(e_{n}\right) \\<br>&amp; =\left\langle v, \overline{\varphi\left(e_{1}\right)} e_{1}+\cdots+\overline{\varphi\left(e_{n}\right)} e_{n}\right\rangle<br>\end{aligned}<br>\]<br><br>for every \(v \in V\), where the first equality comes from 6.30. Thus setting<br><br>\[<br>u=\overline{\varphi\left(e_{1}\right)} e_{1}+\cdots+\overline{\varphi\left(e_{n}\right)} e_{n},<br>\]<br><br>we have \(\varphi(v)=\langle v, u\rangle\) for every \(v \in V\), as desired.<br><br>Now we prove that only one vector \(u \in V\) has the desired behavior. Suppose \(u_{1}, u_{2} \in V\) are such that<br><br>\[<br>\varphi(v)=\left\langle v, u_{1}\right\rangle=\left\langle v, u_{2}\right\rangle<br>\]<br><br>for every \(v \in V\). Then<br><br><ul><li>\(0\)&nbsp;= \(\left\langle v, u_{1}\right\rangle-\left\langle v, u_{2}\right\rangle\) = \(\left\langle v, u_{1}-u_{2}\right\rangle\)</li></ul><br>for every \(v \in V\). Taking \(v\) = \(u_{1}-u_{2}\) shows that \(u_{1}-u_{2}\) = \(0\). In other words, \(u_{1}=u_{2}\), completing the proof of the uniqueness part of the result.<br>
-
-    After:
-      <img src="paste-cfcd6e590022102504c02a0eedc7c40ebaec55ef.jpg"><br><br>Proof First we show there exists a vector \(u \in V\) such that \(\varphi(v)=\langle v, u\rangle\) for every \(v \in V\). Let \(e_{1}, \ldots, e_{n}\) be an orthonormal basis of \(V\). Then<br><br>\[<br>\begin{aligned}<br>\varphi(v) &amp; =\varphi\left(\left\langle v, e_{1}\right\rangle e_{1}+\cdots+\left\langle v, e_{n}\right\rangle e_{n}\right) \\<br>&amp; =\left\langle v, e_{1}\right\rangle \varphi\left(e_{1}\right)+\cdots+\left\langle v, e_{n}\right\rangle \varphi\left(e_{n}\right) \\<br>&amp; =\left\langle v, \overline{\varphi\left(e_{1}\right)} e_{1}+\cdots+\overline{\varphi\left(e_{n}\right)} e_{n}\right\rangle<br>\end{aligned}<br>\]<br><br>for every \(v \in V\), where the first equality comes from 6.30. Thus setting<br><br>\[<br>u=\overline{\varphi\left(e_{1}\right)} e_{1}+\cdots+\overline{\varphi\left(e_{n}\right)} e_{n},<br>\]<br><br>we have \(\varphi(v)=\langle v, u\rangle\) for every \(v \in V\), as desired.<br><br>Now we prove that only one vector \(u \in V\) has the desired behavior. Suppose \(u_{1}, u_{2} \in V\) are such that<br><br>\[<br>\varphi(v)=\left\langle v, u_{1}\right\rangle=\left\langle v, u_{2}\right\rangle<br>\]<br><br>for every \(v \in V\). Then<br><br><ul><li>\(0\)&nbsp;= \(\left\langle v, u_{1}\right\rangle-\left\langle v, u_{2}\right\rangle\) = \(\left\langle v, u_{1}-u_{2}\right\rangle\)</li></ul><br>for every \(v \in V\). Taking \(v\) = \(u_{1}-u_{2}\) shows that \(u_{1}-u_{2}\) = \(0\). In other words, \(u_{1}=u_{2}\), completing the proof of the uniqueness part of the result.<br>
-
-============================================================
-
-Note ID: 1696858540291
-  Field: Text
-    Before:
-      Suppose \(V\) is finite-dimensional and \(\varphi\) a linear functional on \(V\). Then 6.43 gives a formula for the vector \(u\) that satisfies \(\varphi(v)=\langle v, u\rangle\) for all \(v \in V\). Specifically, we have<br><br><ul><li>\(u\) = \(\overline{\varphi\left(e_{1}\right)} e_{1}\) \(+\)\(\cdots\)\(+\) \(\overline{\varphi\left(e_{n}\right)} e_{n} .\)</li></ul>
-
-    After:
-      Suppose \(V\) is finite-dimensional and \(\varphi\) a linear functional on \(V\). Then 6.43 gives a formula for the vector \(u\) that satisfies \(\varphi(v)=\langle v, u\rangle\) for all \(v \in V\). Specifically, we have<br><br><ul><li>\(u\) = \(\overline{\varphi\left(e_{1}\right)} e_{1}\) \(+\)\(\cdots\)\(+\) \(\overline{\varphi\left(e_{n}\right)} e_{n} .\)</li></ul>
-
-============================================================
-
-Note ID: 1696858602337
-  Field: Text
-    Before:
-      1 (a) Suppose \(\theta \in \mathbf{R}\). Show that \((\cos \theta, \sin \theta)\) ,\((-\sin \theta, \cos \theta)\) and \((\cos \theta, \sin \theta)\),\((\sin \theta,-\cos \theta)\) are orthonormal bases of \(\mathbf{R}^{2}\).
-
-    After:
-      1 (a) Suppose \(\theta \in \mathbf{R}\). Show that \((\cos \theta, \sin \theta)\) ,\((-\sin \theta, \cos \theta)\) and \((\cos \theta, \sin \theta)\),\((\sin \theta,-\cos \theta)\) are orthonormal bases of \(\mathbf{R}^{2}\).
-
-============================================================
-
-Note ID: 1696859577811
-  Field: Text
-    Before:
-      11 Suppose \(\langle\cdot, \cdot\rangle_{1}\) and \(\langle\cdot, \cdot\rangle_{2}\) are inner products on \(V\) such that:<br><ul><li>&nbsp;\(\langle v, w\rangle_{1}\) = \(0\) if and only if \(\langle v, w\rangle_{2}\) = \(0\).&nbsp;</li><li>Prove that:</li><ul><li>&nbsp;There is a positive number \(c\) such that&nbsp;</li><li>\(\langle v, w\rangle_{1}\) =\(c\langle v, w\rangle_{2}\) for every \(v, w \in V\).</li></ul></ul>
-
-    After:
-      11 Suppose \(\langle\cdot, \cdot\rangle_{1}\) and \(\langle\cdot, \cdot\rangle_{2}\) are inner products on \(V\) such that:<br><ul><li>&nbsp;\(\langle v, w\rangle_{1}\) = \(0\) if and only if \(\langle v, w\rangle_{2}\) = \(0\).&nbsp;</li><li>Prove that:</li><ul><li>&nbsp;There is a positive number \(c\) such that&nbsp;</li><li>\(\langle v, w\rangle_{1}\) =\(c\langle v, w\rangle_{2}\) for every \(v, w \in V\).</li></ul></ul>
-
-============================================================
-
-Note ID: 1696951947700
-  Field: Text
-    Before:
-      <b>Definition</b>: The orthogonal complement<br><br>The orthogonal complement of a subspace U of an inner product space V, denoted <img src="paste-e44b8da29e4394a8ff7a653eb7b4ad1189fcd710.jpg">,&nbsp;&nbsp;is the set of vectors in V which are orthogonal to every vector in U
-
-    After:
-      <b>Definition</b>: The orthogonal complement<br><br>The orthogonal complement of a subspace U of an inner product space V, denoted <img src="paste-e44b8da29e4394a8ff7a653eb7b4ad1189fcd710.jpg">,&nbsp;&nbsp;is the set of vectors in V which are orthogonal to every vector in U
-
-============================================================
-
-Note ID: 1696952791531
-  Field: Text
-    Before:
-      If U is a subspace of an inner product space V then V can be decomposed into a direct sum of U and its orthogonal complement
-
-    After:
-      If U is a subspace of an inner product space V then V can be decomposed into a direct sum of U and its orthogonal complement
-
-============================================================
-
-Note ID: 1697008645430
-  Field: Text
-    Before:
-      <b>Definition</b> orthogonal complement, \(U^{\perp}\)<br><br>If \(U\) is a subset of \(V\), then the orthogonal complement of \(U\), denoted \(U^{\perp}\), is the set of all vectors in \(V\) that are orthogonal to every vector in \(U\) :<br><br><ul><li>\(U^{\perp}\) =\(\{v \in V:\langle v, u\rangle=0 \text { for every } u \in U\} \)</li></ul>
-
-    After:
-      <b>Definition</b> orthogonal complement, \(U^{\perp}\)<br><br>If \(U\) is a subset of \(V\), then the orthogonal complement of \(U\), denoted \(U^{\perp}\), is the set of all vectors in \(V\) that are orthogonal to every vector in \(U\) :<br><br><ul><li>\(U^{\perp}\) =\(\{v \in V:\langle v, u\rangle=0 \text { for every } u \in U\} \)</li></ul>
-
-============================================================
-
-Note ID: 1697008807681
-  Field: Text
-    Before:
-      <b>Basic properties of orthogonal complement</b><br><br><ul><li>(a) If \(U\)&nbsp;is a subset of \(V\) then \(U^{\perp}\) is a subspace of \(V\).</li><li>(b) \(\{0\}^{\perp}\) =\(V\).</li><li>(c) \(V^{\perp}\) = \(\{0\}\).</li><li>(d) If \(U\) is a subset of \(V\), then \(U \cap U^{\perp} \subset\{0\}\).</li><li>(e) If \(U\) and \(W\) are subsets of \(V\) and \(U \subset W\), then \(W^{\perp} \subset U^{\perp}\).</li></ul>
-
-    After:
-      <b>Basic properties of orthogonal complement</b><br><br><ul><li>(a) If \(U\)&nbsp;is a subset of \(V\) then \(U^{\perp}\) is a subspace of \(V\).</li><li>(b) \(\{0\}^{\perp}\) =\(V\).</li><li>(c) \(V^{\perp}\) = \(\{0\}\).</li><li>(d) If \(U\) is a subset of \(V\), then \(U \cap U^{\perp} \subset\{0\}\).</li><li>(e) If \(U\) and \(W\) are subsets of \(V\) and \(U \subset W\), then \(W^{\perp} \subset U^{\perp}\).</li></ul>
-
-============================================================
-
-Note ID: 1697009058697
-  Field: Text
-    Before:
-      <b>Basic properties of orthogonal complement</b><br><br><br><br><ul><li>(d) If \(U\) is a subset of \(V\), then \(U\) \(\cap\) \( U^{\perp}\)&nbsp; \(\subset\) \(\{0\}\).</li></ul>
-
-    After:
-      <b>Basic properties of orthogonal complement</b><br><br><br><br><ul><li>(d) If \(U\) is a subset of \(V\), then \(U\) \(\cap\) \( U^{\perp}\)&nbsp; \(\subset\) \(\{0\}\).</li></ul>
-
-============================================================
-
-Note ID: 1697009530228
-  Field: Text
-    Before:
-      \subsection{Basic properties of orthogonal complement}<br><br><br>(b) \(\{0\}^{\perp}=V\).<br><br><br>Proof:<br><ul><li>(b) Suppose \(v \in V\). Then \(\langle v, 0\rangle\) = \(0\), which implies that \(v\) \(\in\) \(\{0\}^{\perp}\). Thus \(\{0\}^{\perp}\) = \(V\).<br></li></ul><br>
-
-    After:
-      \subsection{Basic properties of orthogonal complement}<br><br><br>(b) \(\{0\}^{\perp}=V\).<br><br><br>Proof:<br><ul><li>(b) Suppose \(v \in V\). Then \(\langle v, 0\rangle\) = \(0\), which implies that \(v\) \(\in\) \(\{0\}^{\perp}\). Thus \(\{0\}^{\perp}\) = \(V\).<br></li></ul><br>
-
-============================================================
-
-Note ID: 1697009581415
-  Field: Text
-    Before:
-      \subsection{Basic properties of orthogonal complement}<br><br><br>(c) \(V^{\perp}=\{0\}\).<br><br>Proof:<br><ul><li>(c) Suppose \(v \in V^{\perp}\). Then \(\langle v, v\rangle\) = \(0\), which implies that \(v\) = \(0\). Thus \(V^{\perp}\) = \(\{0\}\).<br></li></ul>
-
-    After:
-      \subsection{Basic properties of orthogonal complement}<br><br><br>(c) \(V^{\perp}=\{0\}\).<br><br>Proof:<br><ul><li>(c) Suppose \(v \in V^{\perp}\). Then \(\langle v, v\rangle\) = \(0\), which implies that \(v\) = \(0\). Thus \(V^{\perp}\) = \(\{0\}\).<br></li></ul>
-
-============================================================
-
-Note ID: 1697009650525
-  Field: Text
-    Before:
-      \subsection{Basic properties of orthogonal complement}<br><br>(d) If \(U\) is a subset of \(V\), then \(U \cap U^{\perp} \subset\{0\}\).<br><br>Proof:<br><ul><li>(d) Suppose \(U\) is a subset of \(V\) and \(v\)&nbsp; \(\in\) \(U \cap U^{\perp}\). Then \(\langle v, v\rangle\) = \(0\), which implies that \(v\) = \(0\). Thus \(U \cap U^{\perp} \subset\{0\}\)</li></ul>
-
-    After:
-      \subsection{Basic properties of orthogonal complement}<br><br>(d) If \(U\) is a subset of \(V\), then \(U \cap U^{\perp} \subset\{0\}\).<br><br>Proof:<br><ul><li>(d) Suppose \(U\) is a subset of \(V\) and \(v\)&nbsp; \(\in\) \(U \cap U^{\perp}\). Then \(\langle v, v\rangle\) = \(0\), which implies that \(v\) = \(0\). Thus \(U \cap U^{\perp} \subset\{0\}\)</li></ul>
-
-============================================================
-
-Note ID: 1697009756689
-  Field: Text
-    Before:
-      \subsection{Basic properties of orthogonal complement}<br><br>(e) If \(U\) and \(W\) are subsets of \(V\) and \(U \subset W\), then \(W^{\perp} \subset U^{\perp}\).<br><br>Proof:<br><ul><li>(e) Suppose \(U\) and \(W\) are subsets of \(V\) and \(U \subset W\). Suppose \(v\) \(\in\) \(W^{\perp}\). Then \(\langle v, u\rangle\) = \(0\) for every \(u \in W\), which implies that \(\langle v, u\rangle\) = \(0\) for every \(u \in U\). Hence \(v \in U^{\perp}\). Thus \(W^{\perp} \subset U^{\perp}\).<br></li></ul>
-
-    After:
-      \subsection{Basic properties of orthogonal complement}<br><br>(e) If \(U\) and \(W\) are subsets of \(V\) and \(U \subset W\), then \(W^{\perp} \subset U^{\perp}\).<br><br>Proof:<br><ul><li>(e) Suppose \(U\) and \(W\) are subsets of \(V\) and \(U \subset W\). Suppose \(v\) \(\in\) \(W^{\perp}\). Then \(\langle v, u\rangle\) = \(0\) for every \(u \in W\), which implies that \(\langle v, u\rangle\) = \(0\) for every \(u \in U\). Hence \(v \in U^{\perp}\). Thus \(W^{\perp} \subset U^{\perp}\).<br></li></ul>
-
-============================================================
-
-Note ID: 1697010860121
-  Field: Text
-    Before:
-      <img src="paste-63bed7b30dc6453d921aa714f2982932d33a701a.jpg"><br>Proof:<br><ul><li>Proof First we will show that</li><ul><li>6.48:&nbsp;</li><li>\(V\) = \(U+U^{\perp} .\)</li></ul><li>To do this, suppose \(v \in V\). Let \(e_{1}, \ldots, e_{m}\) be an orthonormal basis of \(U\). Obviously</li><ul><li>6.49:&nbsp;</li><li>\(&nbsp;v\) = \(\underbrace{\left\langle v, e_{1}\right\rangle e_{1}+\cdots+\left\langle v, e_{m}\right\rangle e_{m} }_{u}\)  + \(\underbrace{v-\left\langle v, e_{1}\right\rangle e_{1}-\cdots-\left\langle v, e_{m}\right\rangle e_{m} }_{w} .\)</li><li>where u is in U&nbsp;</li></ul><li>Because \(e_{1}, \ldots, e_{m}\) is an orthonormal basis, for each j up to m we have:</li><ul><li>\(\left\langle w, e_j\right\rangle\)&nbsp;</li><li>= \(\left\langle v, e_j\right\rangle-\left\langle v, e_j\right\rangle\)</li><li>= \(0\)</li></ul><li>Thus \(w\) is orthogonal to every vector in \(\operatorname{span}\left(e_{1}, \ldots, e_{m}\right)\). In other words,&nbsp;\(w \in U^{\perp}\).&nbsp;<br></li><li>This completes the proof that&nbsp;</li><ul><li>\(V\) = \(U+U^{\perp} .\)</li></ul></ul>
-
-    After:
-      <img src="paste-63bed7b30dc6453d921aa714f2982932d33a701a.jpg"><br>Proof:<br><ul><li>Proof First we will show that</li><ul><li>6.48:&nbsp;</li><li>\(V\) = \(U+U^{\perp} .\)</li></ul><li>To do this, suppose \(v \in V\). Let \(e_{1}, \ldots, e_{m}\) be an orthonormal basis of \(U\). Obviously</li><ul><li>6.49:&nbsp;</li><li>\(&nbsp;v\) = \(\underbrace{\left\langle v, e_{1}\right\rangle e_{1}+\cdots+\left\langle v, e_{m}\right\rangle e_{m} }_{u}\)  + \(\underbrace{v-\left\langle v, e_{1}\right\rangle e_{1}-\cdots-\left\langle v, e_{m}\right\rangle e_{m} }_{w} .\)</li><li>where u is in U&nbsp;</li></ul><li>Because \(e_{1}, \ldots, e_{m}\) is an orthonormal basis, for each j up to m we have:</li><ul><li>\(\left\langle w, e_j\right\rangle\)&nbsp;</li><li>= \(\left\langle v, e_j\right\rangle-\left\langle v, e_j\right\rangle\)</li><li>= \(0\)</li></ul><li>Thus \(w\) is orthogonal to every vector in \(\operatorname{span}\left(e_{1}, \ldots, e_{m}\right)\). In other words,&nbsp;\(w \in U^{\perp}\).&nbsp;<br></li><li>This completes the proof that&nbsp;</li><ul><li>\(V\) = \(U+U^{\perp} .\)</li></ul></ul>
-
-============================================================
-
-Note ID: 1697011100153
-  Field: Text
-    Before:
-      The orthogonal complement of the orthogonal complement<br><br>Suppose \(U\) is a finite-dimensional subspace of \(V\). Then<br><br><ul><li>\(U\)=\(\left(U^{\perp}\right)^{\perp}\)</li></ul>
-
-    After:
-      The orthogonal complement of the orthogonal complement<br><br>Suppose \(U\) is a finite-dimensional subspace of \(V\). Then<br><br><ul><li>\(U\)=\(\left(U^{\perp}\right)^{\perp}\)</li></ul>
-
-============================================================
-
-Note ID: 1697011834281
-  Field: Text
-    Before:
-      <b>Properties of the orthogonal projection \(P_{U}\)</b><br><br>Suppose \(U\) is a finite-dimensional subspace of \(V\) and \(v \in V\). Then<ul><li>(b) \(P_{U}\) \(u\) = \(u\) for every \(u \in U\);</li></ul>
-
-    After:
-      <b>Properties of the orthogonal projection \(P_{U}\)</b><br><br>Suppose \(U\) is a finite-dimensional subspace of \(V\) and \(v \in V\). Then<ul><li>(b) \(P_{U}\) \(u\) = \(u\) for every \(u \in U\);</li></ul>
-
-============================================================
-
-Note ID: 1697012174244
-  Field: Text
-    Before:
-      \subsection{Properties of the orthogonal projection \(P_{U}\)}<br><br>Suppose \(U\) is a finite-dimensional subspace of \(V\) and \(v \in V\). Then<br><ul><li>(g) \(P_{U}\)\(^{2}\) = \(P_{U}\)</li></ul>
-
-    After:
-      \subsection{Properties of the orthogonal projection \(P_{U}\)}<br><br>Suppose \(U\) is a finite-dimensional subspace of \(V\) and \(v \in V\). Then<br><ul><li>(g) \(P_{U}\)\(^{2}\) = \(P_{U}\)</li></ul>
-
-============================================================
-
-Note ID: 1697012280546
-  Field: Text
-    Before:
-      \subsection{Properties of the orthogonal projection \(P_{U}\)}<br><br>Suppose \(U\) is a finite-dimensional subspace of \(V\) and \(v \in V\). Then<br><br><ul><li>(i) for every orthonormal basis \(e_{1}, \ldots, e_{m}\) of \(U\),<br></li><li>\(P_{U} v\)= \(\left\langle v, e_{1}\right\rangle e_{1}\)\(+\)\(\cdots\)\(+\)\(\left\langle v, e_{m}\right\rangle e_{m} .\)<br></li></ul>
-
-    After:
-      \subsection{Properties of the orthogonal projection \(P_{U}\)}<br><br>Suppose \(U\) is a finite-dimensional subspace of \(V\) and \(v \in V\). Then<br><br><ul><li>(i) for every orthonormal basis \(e_{1}, \ldots, e_{m}\) of \(U\),<br></li><li>\(P_{U} v\)= \(\left\langle v, e_{1}\right\rangle e_{1}\)\(+\)\(\cdots\)\(+\)\(\left\langle v, e_{m}\right\rangle e_{m} .\)<br></li></ul>
-
-============================================================
-
-Note ID: 1697013273244
-  Field: Text
-    Before:
-      Suppose \(U\) is a finite-dimensional subspace of \(V, v \in V\), and \(u \in U\). Then<br><ul><li>\(\left\|v-P_{U} v\right\|\)\(\leq\)\(\|v-u\|\)</li><li>Furthermore, the inequality above is an equality if and only if \(u\) = \(P_{U} v\).</li></ul>
-
-    After:
-      Suppose \(U\) is a finite-dimensional subspace of \(V, v \in V\), and \(u \in U\). Then<br><ul><li>\(\left\|v-P_{U} v\right\|\)\(\leq\)\(\|v-u\|\)</li><li>Furthermore, the inequality above is an equality if and only if \(u\) = \(P_{U} v\).</li></ul>
-
-============================================================
-
-Note ID: 1697013519392
-  Field: Text
-    Before:
-      <img src="paste-81d56a8739ee61a20b8ed7889ba69c4b1a1ab3be.jpg"><br>Proof:<br><ul><li>\(\left\|v-P_U v\right\|^2\)&nbsp;</li><li>\(\leq\) \(\left\|v-P_U v\right\|^2\) +\(\left\|P_U v-u\right\|^2\)<br></li><li>=&nbsp;\(\|\) \(\left(v-P_U v\right)\) +\(\left(P_U v-u\right)\)\(\|\)\(^2\)</li><li>=&nbsp;\(\|v-u\|\) \(^2\)</li></ul>
-
-    After:
-      <img src="paste-81d56a8739ee61a20b8ed7889ba69c4b1a1ab3be.jpg"><br>Proof:<br><ul><li>\(\left\|v-P_U v\right\|^2\)&nbsp;</li><li>\(\leq\) \(\left\|v-P_U v\right\|^2\) +\(\left\|P_U v-u\right\|^2\)<br></li><li>=&nbsp;\(\|\) \(\left(v-P_U v\right)\) +\(\left(P_U v-u\right)\)\(\|\)\(^2\)</li><li>=&nbsp;\(\|v-u\|\) \(^2\)</li></ul>
-
-============================================================
-
-Note ID: 1697014050845
-  Field: Text
-    Before:
-      6 Suppose \(U\) and \(W\) are finite-dimensional subspaces of \(V\). <br><ul><li>Prove that \(P_{U}\)\(P_{W}\) = \(0\) if and only if \(\langle u, w\rangle\)=\(0\) for all \(u \in U\) and all \(w \in W\).</li></ul>
-
-    After:
-      6 Suppose \(U\) and \(W\) are finite-dimensional subspaces of \(V\). <br><ul><li>Prove that \(P_{U}\)\(P_{W}\) = \(0\) if and only if \(\langle u, w\rangle\)=\(0\) for all \(u \in U\) and all \(w \in W\).</li></ul>
-
-============================================================
-
-Note ID: 1697014134548
-  Field: Text
-    Before:
-      7 <br><br><ul><li>Suppose \(V\) is finite-dimensional&nbsp;</li><li>And \(P \in \mathcal{L}(V)\) is such that \(P^{2}\) = \(P\)&nbsp;</li><li>And every vector in null \(P\) is orthogonal to every vector in range \(P\).&nbsp;</li><li>Prove that there exists a subspace \(U\) of \(V\) such that \(P\)=\(P_{U}\).</li></ul>
-
-    After:
-      7 <br><br><ul><li>Suppose \(V\) is finite-dimensional&nbsp;</li><li>And \(P \in \mathcal{L}(V)\) is such that \(P^{2}\) = \(P\)&nbsp;</li><li>And every vector in null \(P\) is orthogonal to every vector in range \(P\).&nbsp;</li><li>Prove that there exists a subspace \(U\) of \(V\) such that \(P\)=\(P_{U}\).</li></ul>
-
-============================================================
-
-Note ID: 1697014221624
-  Field: Text
-    Before:
-      8 Suppose \(V\) is finite-dimensional and \(P \in \mathcal{L}(V)\) is such that:<br><ul><li>&nbsp;\(P^{2}\)=\(P\) and</li><li>\(\|P v\|\) \(\leq\)\(\|v\|\) for every \(v \in V\).&nbsp;</li><li>Prove that there exists a subspace \(U\) of \(V\) such that \(P\)=\(P_{U}\).</li></ul>
-
-    After:
-      8 Suppose \(V\) is finite-dimensional and \(P \in \mathcal{L}(V)\) is such that:<br><ul><li>&nbsp;\(P^{2}\)=\(P\) and</li><li>\(\|P v\|\) \(\leq\)\(\|v\|\) for every \(v \in V\).&nbsp;</li><li>Prove that there exists a subspace \(U\) of \(V\) such that \(P\)=\(P_{U}\).</li></ul>
-
-============================================================
-
-Note ID: 1697016044715
-  Field: Text
-    Before:
-      If \(U\) is a subset of \(V\), then \(U \cap U^{\perp} \subset\{0\}\).<br><br>Why does this only require containment and not equality?<br><ul><li>Because we only require U to be a subset and not a subspace, there is no guarantee the origin is in U</li><li>If we were require it to be a subset then we could use equiality</li></ul>
-
-    After:
-      If \(U\) is a subset of \(V\), then \(U \cap U^{\perp} \subset\{0\}\).<br><br>Why does this only require containment and not equality?<br><ul><li>Because we only require U to be a subset and not a subspace, there is no guarantee the origin is in U</li><li>If we were require it to be a subset then we could use equiality</li></ul>
-
-============================================================
-
-Note ID: 1697016987800
-  Field: Text
-    Before:
-      <img src="paste-f0ee3eb59c1cdc0d47dfd909a0162907a48105fa.jpg"><br>Why is the property useful?<br><ul><li>Because&nbsp;\(P_U v\) is an element of U</li><li>This means that the element of U which minimizes the norm/distance must be&nbsp;\(P_U v\)</li></ul>
-
-    After:
-      <img src="paste-f0ee3eb59c1cdc0d47dfd909a0162907a48105fa.jpg"><br>Why is the property useful?<br><ul><li>Because&nbsp;\(P_U v\) is an element of U</li><li>This means that the element of U which minimizes the norm/distance must be&nbsp;\(P_U v\)</li></ul>
-
-============================================================
-
-Note ID: 1697118024126
-  Field: Text
-    Before:
-      Proof that vector spaces can be decomposed as a direct sum of a subspace U and its orthogonal complement \(U^\perp\):<br><ul><li>Choose an orthonormal basis for U</li><li>Then, it can be extended to a basis of V</li><li>As such, any vector in V can be written as the sum of&nbsp;\(v_u\), which equals&nbsp;\(\vec{c} \cdot \vec{u}\), and&nbsp;\(v-v_u\)</li><li>In this case,&nbsp;\(v-v_u\) must be orthogonal to every vector in U because we have removed the components of v which correspond to&nbsp;\(\vec{u}\)</li><li>Since the intersection of U and its orthogonal complement is 0, this is a direct sum</li></ul>
-
-    After:
-      Proof that vector spaces can be decomposed as a direct sum of a subspace U and its orthogonal complement \(U^\perp\):<br><ul><li>Choose an orthonormal basis for U</li><li>Then, it can be extended to a basis of V</li><li>As such, any vector in V can be written as the sum of&nbsp;\(v_u\), which equals&nbsp;\(\vec{c} \cdot \vec{u}\), and&nbsp;\(v-v_u\)</li><li>In this case,&nbsp;\(v-v_u\) must be orthogonal to every vector in U because we have removed the components of v which correspond to&nbsp;\(\vec{u}\)</li><li>Since the intersection of U and its orthogonal complement is 0, this is a direct sum</li></ul>
-
-============================================================
-
-Note ID: 1697119638422
-  Field: Text
-    Before:
-      We can think of a projection&nbsp;\(P_u\) in two ways<br><ul><li>\(P_u\)&nbsp;\(\in\)&nbsp;\(L(V)\)<br></li><li>\(P_u\)&nbsp;\(\in\)&nbsp;\(L(V,U)\)<br></li></ul>
-
-    After:
-      We can think of a projection&nbsp;\(P_u\) in two ways<br><ul><li>\(P_u\)&nbsp;\(\in\)&nbsp;\(L(V)\)<br></li><li>\(P_u\)&nbsp;\(\in\)&nbsp;\(L(V,U)\)<br></li></ul>
-
-============================================================
-
-Note ID: 1697120754541
-  Field: Text
-    Before:
-      How can we think of using orthogonal projections as a means of solving minimization problems conceptually?<br><img src="paste-9a3bc3fb59f7c39d492add8e0057da0f3f0b3e29.jpg"><br><ul><li>Logically: if U is a model of reality and v is a noisy observation, what is the closest point in reality to the noisy v. With U being lower-dimensional.</li><ul><li>This requires that the phenomena actually be linear</li></ul><li>Statistically: if U is a staistical model based on a&nbsp; lower-dimensional choice of parameters and v is a phenomena we want the point in the model that best explains v</li></ul>
-
-    After:
-      How can we think of using orthogonal projections as a means of solving minimization problems conceptually?<br><img src="paste-9a3bc3fb59f7c39d492add8e0057da0f3f0b3e29.jpg"><br><ul><li>Logically: if U is a model of reality and v is a noisy observation, what is the closest point in reality to the noisy v. With U being lower-dimensional.</li><ul><li>This requires that the phenomena actually be linear</li></ul><li>Statistically: if U is a staistical model based on a&nbsp; lower-dimensional choice of parameters and v is a phenomena we want the point in the model that best explains v</li></ul>
-
-============================================================
-
-Note ID: 1697782460196
-  Field: Text
-    Before:
-      <b>Definition</b> adjoint, \(T^{*}\)<br><br>Suppose \(T \in \mathcal{L}(V, W)\). The adjoint of \(T\) is the function \(T^{*}\) : \(W\)&nbsp; \(\rightarrow\) \(V\) such that<br><br><ul><li>\(\langle T v, w\rangle\) = \(\left\langle v, T^{*} w\right\rangle\)</li></ul><br>for every \(v \in V\) and every \(w \in W\).<br>
-
-    After:
-      <b>Definition</b> adjoint, \(T^{*}\)<br><br>Suppose \(T \in \mathcal{L}(V, W)\). The adjoint of \(T\) is the function \(T^{*}\) : \(W\)&nbsp; \(\rightarrow\) \(V\) such that<br><br><ul><li>\(\langle T v, w\rangle\) = \(\left\langle v, T^{*} w\right\rangle\)</li></ul><br>for every \(v \in V\) and every \(w \in W\).<br>
-
-============================================================
-
-Note ID: 1697784369263
-  Field: Text
-    Before:
-      Properties of the adjoint<br><br><ul><li>(a) \(\quad(S+T)^{*}\) = \(S^{*}+T^{*}\) for all \(S, T \in \mathcal{L}(V, W)\);</li><li><br></li><li>(b) \((\lambda T)^{*}\) = \(\bar{\lambda} T^{*}\) for all \(\lambda \in \mathbf{F}\) and \(T \in \mathcal{L}(V, W)\);</li><li><br></li><li>(c) \(\left(T^{*}\right)^{*}\) = \(T\) for all \(T \in \mathcal{L}(V, W)\);</li><li><br></li><li>(d) \(I^{*}\) = \(I\), where \(I\) is the identity operator on \(V\);</li><li><br></li><li>(e) \((S T)^{*}\) = \(T^{*} S^{*}\) for all \(T \in \mathcal{L}(V, W)\) and \(S \in \mathcal{L}(W, U)\).</li></ul>
-
-    After:
-      Properties of the adjoint<br><br><ul><li>(a) \(\quad(S+T)^{*}\) = \(S^{*}+T^{*}\) for all \(S, T \in \mathcal{L}(V, W)\);</li><li><br></li><li>(b) \((\lambda T)^{*}\) = \(\bar{\lambda} T^{*}\) for all \(\lambda \in \mathbf{F}\) and \(T \in \mathcal{L}(V, W)\);</li><li><br></li><li>(c) \(\left(T^{*}\right)^{*}\) = \(T\) for all \(T \in \mathcal{L}(V, W)\);</li><li><br></li><li>(d) \(I^{*}\) = \(I\), where \(I\) is the identity operator on \(V\);</li><li><br></li><li>(e) \((S T)^{*}\) = \(T^{*} S^{*}\) for all \(T \in \mathcal{L}(V, W)\) and \(S \in \mathcal{L}(W, U)\).</li></ul>
-
-============================================================
-
-Note ID: 1697784536542
-  Field: Text
-    Before:
-      \subsection{Properties of the adjoint}<br><br>(d) \(I^{*}\) = \(I\), where \(I\) is the identity operator on \(V\);
-
-    After:
-      \subsection{Properties of the adjoint}<br><br>(d) \(I^{*}\) = \(I\), where \(I\) is the identity operator on \(V\);
-
-============================================================
-
-Note ID: 1697785805313
-  Field: Text
-    Before:
-      <img src="paste-4679dd4a3c60e3380cda6e152f581258b49de687.jpg"><br><br>Proof We begin by proving (a). Let \(w \in W\). Then<br><br><ul><li>\(w \in \operatorname{null} T^{*} \)&nbsp;&nbsp;</li><li>\(\Longleftrightarrow\) \( T^{*} w\)::by def::by def::by def::by def = \(0 \)::by def::by def::by def::by def</li><li>\(\Longleftrightarrow\) \(\left\langle v, T^{*} w\right\rangle\)::expand def::expand def::expand def::expand def::expand def = \(0\) \(\text { for all } v \in V \)</li><li>\(\Longleftrightarrow\) \(\langle T v, w\rangle\)::further expand def::further expand def::further expand def::further expand def::further expand def = \(0\) \(\text { for all } v \in V \)</li><li>\(\Longleftrightarrow\) \(w\) \(\in\) \((\text { range } T)^{\perp} .\)</li></ul><br>Thus null \(T^{*}=(\operatorname{range} T)^{\perp}\), proving (a).<br><br>
-
-    After:
-      <img src="paste-4679dd4a3c60e3380cda6e152f581258b49de687.jpg"><br><br>Proof We begin by proving (a). Let \(w \in W\). Then<br><br><ul><li>\(w \in \operatorname{null} T^{*} \)&nbsp;&nbsp;</li><li>\(\Longleftrightarrow\) \( T^{*} w\)::by def::by def::by def::by def = \(0 \)::by def::by def::by def::by def</li><li>\(\Longleftrightarrow\) \(\left\langle v, T^{*} w\right\rangle\)::expand def::expand def::expand def::expand def::expand def = \(0\) \(\text { for all } v \in V \)</li><li>\(\Longleftrightarrow\) \(\langle T v, w\rangle\)::further expand def::further expand def::further expand def::further expand def::further expand def = \(0\) \(\text { for all } v \in V \)</li><li>\(\Longleftrightarrow\) \(w\) \(\in\) \((\text { range } T)^{\perp} .\)</li></ul><br>Thus null \(T^{*}=(\operatorname{range} T)^{\perp}\), proving (a).<br><br>
-
-============================================================
-
-Note ID: 1697785949680
-  Field: Text
-    Before:
-      <img src="paste-3a17ead3b362bcf2c581832d899b7b869839ee01.jpg"><br>Proof We begin by proving (a). Let \(w \in W\). Then<br><br>\[<br>\begin{aligned}<br>w \in \operatorname{null} T^{*} &amp; \Longleftrightarrow T^{*} w=0 \\<br>&amp; \Longleftrightarrow\left\langle v, T^{*} w\right\rangle=0 \text { for all } v \in V \\<br>&amp; \Longleftrightarrow\langle T v, w\rangle=0 \text { for all } v \in V \\<br>&amp; \Longleftrightarrow w \in(\text { range } T)^{\perp} .<br>\end{aligned}<br>\]<br><br>Thus null \(T^{*}=(\operatorname{range} T)^{\perp}\), proving (a).<br><br>THen:<br><br><ul><li>If we take the orthogonal complement of both sides of (a), we get (d),&nbsp;</li><li>Replacing \(T\) with \(T^{*}\) in (a) gives (c)</li><li>Finally, replacing \(T\) with \(T^{*}\) in (d) gives (b).</li></ul>
-
-    After:
-      <img src="paste-3a17ead3b362bcf2c581832d899b7b869839ee01.jpg"><br>Proof We begin by proving (a). Let \(w \in W\). Then<br><br>\[<br>\begin{aligned}<br>w \in \operatorname{null} T^{*} &amp; \Longleftrightarrow T^{*} w=0 \\<br>&amp; \Longleftrightarrow\left\langle v, T^{*} w\right\rangle=0 \text { for all } v \in V \\<br>&amp; \Longleftrightarrow\langle T v, w\rangle=0 \text { for all } v \in V \\<br>&amp; \Longleftrightarrow w \in(\text { range } T)^{\perp} .<br>\end{aligned}<br>\]<br><br>Thus null \(T^{*}=(\operatorname{range} T)^{\perp}\), proving (a).<br><br>THen:<br><br><ul><li>If we take the orthogonal complement of both sides of (a), we get (d),&nbsp;</li><li>Replacing \(T\) with \(T^{*}\) in (a) gives (c)</li><li>Finally, replacing \(T\) with \(T^{*}\) in (d) gives (b).</li></ul>
-
-============================================================
-
-Note ID: 1697786059177
-  Field: Text
-    Before:
-      <b>Definition</b> conjugate transpose<br><br>The conjugate transpose of an \(m\)-by- \(n\)::size::size::size::size::size matrix is the \(n\)-by- \(m\)::size::size::size::size::size matrix obtained by interchanging the rows and columns and then taking the complex conjugate of each entry.
-
-    After:
-      <b>Definition</b> conjugate transpose<br><br>The conjugate transpose of an \(m\)-by- \(n\)::size::size::size::size::size matrix is the \(n\)-by- \(m\)::size::size::size::size::size matrix obtained by interchanging the rows and columns and then taking the complex conjugate of each entry.
-
-============================================================
-
-Note ID: 1698179738196
-  Field: Text
-    Before:
-      <b>Definition</b> conjugate transpose<br><br>The conjugate transpose of an \(m\)-by- \(n\)::size?::size?::size?::size?::size? matrix is the \(n\)-by- \(m\)::size?::size?::size?::size?::size? matrix obtained by interchanging the rows and columns::step 1::step 1::step 1::step 1::step 1 and then taking the complex conjugate of each entry.::step 2::step 2::step 2
-
-    After:
-      <b>Definition</b> conjugate transpose<br><br>The conjugate transpose of an \(m\)-by- \(n\)::size?::size?::size?::size?::size? matrix is the \(n\)-by- \(m\)::size?::size?::size?::size?::size? matrix obtained by interchanging the rows and columns::step 1::step 1::step 1::step 1::step 1 and then taking the complex conjugate of each entry.::step 2::step 2::step 2
-
-============================================================
-
-Note ID: 1698179922292
-  Field: Text
-    Before:
-      The matrix of \(T^{*}\)<br><br>Let \(T \in \mathcal{L}(V, W)\). Suppose \(e_{1}, \ldots, e_{n}\) is an orthonormal basis of \(V\) and \(f_{1}, \ldots, f_{m}\) is an orthonormal basis of \(W\). Then<br><br><ul><li>\(\mathcal{M}\left(T^{*},\left(f_{1}, \ldots, f_{m}\right),\left(e_{1}, \ldots, e_{n}\right)\right)\)</li></ul><br>is the conjugate transpose of<br><br><ul><li>\(\mathcal{M}\left(T,\left(e_{1}, \ldots, e_{n}\right),\left(f_{1}, \ldots, f_{m}\right)\right) .\)</li></ul>
-
-    After:
-      The matrix of \(T^{*}\)<br><br>Let \(T \in \mathcal{L}(V, W)\). Suppose \(e_{1}, \ldots, e_{n}\) is an orthonormal basis of \(V\) and \(f_{1}, \ldots, f_{m}\) is an orthonormal basis of \(W\). Then<br><br><ul><li>\(\mathcal{M}\left(T^{*},\left(f_{1}, \ldots, f_{m}\right),\left(e_{1}, \ldots, e_{n}\right)\right)\)</li></ul><br>is the conjugate transpose of<br><br><ul><li>\(\mathcal{M}\left(T,\left(e_{1}, \ldots, e_{n}\right),\left(f_{1}, \ldots, f_{m}\right)\right) .\)</li></ul>
-
-============================================================
-
-Note ID: 1698180120123
-  Field: Text
-    Before:
-      <b>Definition</b> self-adjoint<br><br>An operator \(T \in \mathcal{L}(V)\) is called self-adjoint if \(T\) = \(T^{*}\). In other words, \(T \in \mathcal{L}(V)\) is self-adjoint if and only if<br><br><ul><li>\(\langle T v, w\rangle\) = \(\langle v, T w\rangle\)</li></ul><br>for all \(v, w \in V\).<br>
-
-    After:
-      <b>Definition</b> self-adjoint<br><br>An operator \(T \in \mathcal{L}(V)\) is called self-adjoint if \(T\) = \(T^{*}\). In other words, \(T \in \mathcal{L}(V)\) is self-adjoint if and only if<br><br><ul><li>\(\langle T v, w\rangle\) = \(\langle v, T w\rangle\)</li></ul><br>for all \(v, w \in V\).<br>
-
-============================================================
-
-Note ID: 1698180204868
-  Field: Text
-    Before:
-      You should verify that the sum of two self-adjoint operators is self-adjoint and that the product of a real scalar and a self-adjoint operator is self-adjoint.
-
-    After:
-      You should verify that the sum of two self-adjoint operators is self-adjoint and that the product of a real scalar and a self-adjoint operator is self-adjoint.
-
-============================================================
-
-Note ID: 1698180341439
-  Field: Text
-    Before:
-      Eigenvalues of self-adjoint operators are real<br><br>Every eigenvalue of a self-adjoint operator is real.
-
-    After:
-      Eigenvalues of self-adjoint operators are real<br><br>Every eigenvalue of a self-adjoint operator is real.
-
-============================================================
-
-Note ID: 1698181134195
-  Field: Text
-    Before:
-      If \(T\) = \(T^{*}\) and \(\langle T v, v\rangle\) = \(0\) for all \(v\), then \(T\) = \(0\)
-
-    After:
-      If \(T\) = \(T^{*}\) and \(\langle T v, v\rangle\) = \(0\) for all \(v\), then \(T\) = \(0\)
-
-============================================================
-
-Note ID: 1698181355163
-  Field: Text
-    Before:
-      Definition normal<br><br>- An operator on an inner product space is called normal if it commutes with its adjoint.<br><br>- In other words, \(T \in \mathcal{L}(V)\) is normal if<br><br><ul><li>\(T T^{*}\) = \(T^{*} T \text {. }\)</li></ul>
-
-    After:
-      Definition normal<br><br>- An operator on an inner product space is called normal if it commutes with its adjoint.<br><br>- In other words, \(T \in \mathcal{L}(V)\) is normal if<br><br><ul><li>\(T T^{*}\) = \(T^{*} T \text {. }\)</li></ul>
-
-============================================================
-
-Note ID: 1698181535908
-  Field: Text
-    Before:
-      Obviously every self-adjoint operator is normal, because if \(T\) is self-adjoint then \(T^{*}\) = \(T\).
-
-    After:
-      Obviously every self-adjoint operator is normal, because if \(T\) is self-adjoint then \(T^{*}\) = \(T\).
-
-============================================================
-
-Note ID: 1698182150227
-  Field: Text
-    Before:
-      Orthogonal eigenvectors for normal operators<br><br>Suppose \(T \in \mathcal{L}(V)\) is normal. Then eigenvectors of \(T\) corresponding to distinct eigenvalues are orthogonal.
-
-    After:
-      Orthogonal eigenvectors for normal operators<br><br>Suppose \(T \in \mathcal{L}(V)\) is normal. Then eigenvectors of \(T\) corresponding to distinct eigenvalues are orthogonal.
-
-============================================================
-
-Note ID: 1698182354081
-  Field: Text
-    Before:
-      4 Suppose \(T \in \mathcal{L}(V, W)\). Prove that<br><br>(a) \(T\) is injective if and only if \(T^{*}\) is surjective;<br><br>(b) \(T\) is surjective if and only if \(T^{*}\) is injective.
-
-    After:
-      4 Suppose \(T \in \mathcal{L}(V, W)\). Prove that<br><br>(a) \(T\) is injective if and only if \(T^{*}\) is surjective;<br><br>(b) \(T\) is surjective if and only if \(T^{*}\) is injective.
-
-============================================================
-
-Note ID: 1698182654404
-  Field: Text
-    Before:
-      5 Prove that<br><br><ul><li>dim range\(T^*\) = dim rangeT</li></ul><br>
-
-    After:
-      5 Prove that<br><br><ul><li>dim range\(T^*\) = dim rangeT</li></ul><br>
-
-============================================================
-
-Note ID: 1698182702101
-  Field: Text
-    Before:
-      7 Suppose \(S, T \in \mathcal{L}(V)\) are self-adjoint. Prove that \(S T\) is self-adjoint if and only if \(S T\) = \(T S\).
-
-    After:
-      7 Suppose \(S, T \in \mathcal{L}(V)\) are self-adjoint. Prove that \(S T\) is self-adjoint if and only if \(S T\) = \(T S\).
-
-============================================================
-
-Note ID: 1698182878959
-  Field: Text
-    Before:
-      Suppose \(P \in \mathcal{L}(V)\) is such that \(P^{2}\) = \(P\). Prove that there is a subspace \(U\) of \(V\) such that \(P\) = \(P_{U}\) if and only if \(P\) is self-adjoint.
-
-    After:
-      Suppose \(P \in \mathcal{L}(V)\) is such that \(P^{2}\) = \(P\). Prove that there is a subspace \(U\) of \(V\) such that \(P\) = \(P_{U}\) if and only if \(P\) is self-adjoint.
-
-============================================================
-
-Note ID: 1698183596900
-  Field: Text
-    Before:
-      17 Suppose \(T \in \mathcal{L}(V)\) is normal. Prove that<br><br><ul><li>\(\text { null }\) \(T^{k}\) = \(\operatorname{null}\) \(T\)&nbsp;</li><li>\( \operatorname{range }\)\(T^{k}\)= \(\operatorname{range}\)\(T\)</li></ul><br>for every positive integer \(k\).<br>
-
-    After:
-      17 Suppose \(T \in \mathcal{L}(V)\) is normal. Prove that<br><br><ul><li>\(\text { null }\) \(T^{k}\) = \(\operatorname{null}\) \(T\)&nbsp;</li><li>\( \operatorname{range }\)\(T^{k}\)= \(\operatorname{range}\)\(T\)</li></ul><br>for every positive integer \(k\).<br>
-
-============================================================
-
-Note ID: 1698269581351
-  Field: Text
-    Before:
-      5. Define \(T \in \mathcal{L}\left(\mathbf{F}^{2}\right)\) by<br><br>\[<br>T(w, z)=(z, w)<br>\]<br><br>Find all eigenvalues and eigenvectors of \(T\).<br><br><ul><li>\(w\)&nbsp; =&nbsp; \( \lambda z\)<br></li><li>\(z\) =&nbsp;\(\lambda w\)<br></li><li>Thus:</li><li>\(\lambda^2 w \) = \(w\)<br></li><li>\(w (\lambda -1) (\lambda + 1)\) = 0<br></li><li>This implies that&nbsp;\(\lambda\) = 1 or&nbsp;\(\lambda\) = -1</li><li>For&nbsp;\(\lambda \) = 1</li><ul><li>The eigenvector is (w,w)</li></ul><li>For&nbsp;\(\lambda\) = -1</li><ul><li>The eigenvector is (w,-w)</li></ul></ul>
-
-    After:
-      5. Define \(T \in \mathcal{L}\left(\mathbf{F}^{2}\right)\) by<br><br>\[<br>T(w, z)=(z, w)<br>\]<br><br>Find all eigenvalues and eigenvectors of \(T\).<br><br><ul><li>\(w\)&nbsp; =&nbsp; \( \lambda z\)<br></li><li>\(z\) =&nbsp;\(\lambda w\)<br></li><li>Thus:</li><li>\(\lambda^2 w \) = \(w\)<br></li><li>\(w (\lambda -1) (\lambda + 1)\) = 0<br></li><li>This implies that&nbsp;\(\lambda\) = 1 or&nbsp;\(\lambda\) = -1</li><li>For&nbsp;\(\lambda \) = 1</li><ul><li>The eigenvector is (w,w)</li></ul><li>For&nbsp;\(\lambda\) = -1</li><ul><li>The eigenvector is (w,-w)</li></ul></ul>
-
-============================================================
-
-Note ID: 1698270134037
-  Field: Text
-    Before:
-      7. Suppose \(n\) is a positive integer and \(T \in \mathcal{L}\left(\mathrm{F}^{n}\right)\) is defined by<br><br>\[<br>T\left(x_{1}, \ldots, x_{n}\right)=\left(x_{1}+\cdots+x_{n}, \ldots, x_{1}+\cdots+x_{n}\right) ;<br>\]<br><br>in other words, \(T\) is the operator whose matrix (with respect to the standard basis) consists of all 1's. Find all eigenvalues and eigenvectors of \(T\).<br><br>Solution:<br>Solution: Suppose \(\lambda\) is an eigenvalue of \(T\). For this particular operator, the eigenvalue-eigenvector equation \(T x=\lambda x\) becomes the system of equations<br><br><ul><li>\(x_{1}+\cdots+x_{n}\) = \(\lambda x_{1} \)</li><li>\(\vdots \)</li><li>\(x_{1}+\cdots+x_{n}\) = \(\lambda x_{n} .\)</li></ul><br>Thus<br><br><ul><li>\(\lambda x_{1}\) = \(\cdots\) = \(\lambda x_{n} .\)</li></ul><div>Then either:</div><div><br></div><div><ul><li>\(\lambda\) = 0<br></li><ul><li>In which case the corresponding set of eigenvectors is&nbsp;</li><li>\[<br>\left\{\left(x_{1}, \ldots, x_{n}\right) \in \mathbf{F}^{n}: x_{1}+\cdots+x_{n}=0\right\} .<br>\]<br></li></ul><li>\(\lambda\) = n<br></li><ul><li>In which cas the corresponding set of eigenvectors equals:</li><li>\[<br>\left\{\left(x_{1}, \ldots, x_{n}\right) \in \mathbf{F}^{n}: x_{1}=\cdots=x_{n}\right\} .<br>\]<br></li></ul></ul></div>
-
-    After:
-      7. Suppose \(n\) is a positive integer and \(T \in \mathcal{L}\left(\mathrm{F}^{n}\right)\) is defined by<br><br>\[<br>T\left(x_{1}, \ldots, x_{n}\right)=\left(x_{1}+\cdots+x_{n}, \ldots, x_{1}+\cdots+x_{n}\right) ;<br>\]<br><br>in other words, \(T\) is the operator whose matrix (with respect to the standard basis) consists of all 1's. Find all eigenvalues and eigenvectors of \(T\).<br><br>Solution:<br>Solution: Suppose \(\lambda\) is an eigenvalue of \(T\). For this particular operator, the eigenvalue-eigenvector equation \(T x=\lambda x\) becomes the system of equations<br><br><ul><li>\(x_{1}+\cdots+x_{n}\) = \(\lambda x_{1} \)</li><li>\(\vdots \)</li><li>\(x_{1}+\cdots+x_{n}\) = \(\lambda x_{n} .\)</li></ul><br>Thus<br><br><ul><li>\(\lambda x_{1}\) = \(\cdots\) = \(\lambda x_{n} .\)</li></ul><div>Then either:</div><div><br></div><div><ul><li>\(\lambda\) = 0<br></li><ul><li>In which case the corresponding set of eigenvectors is&nbsp;</li><li>\[<br>\left\{\left(x_{1}, \ldots, x_{n}\right) \in \mathbf{F}^{n}: x_{1}+\cdots+x_{n}=0\right\} .<br>\]<br></li></ul><li>\(\lambda\) = n<br></li><ul><li>In which cas the corresponding set of eigenvectors equals:</li><li>\[<br>\left\{\left(x_{1}, \ldots, x_{n}\right) \in \mathbf{F}^{n}: x_{1}=\cdots=x_{n}\right\} .<br>\]<br></li></ul></ul></div>
-
-============================================================
-
-Note ID: 1698270631386
-  Field: Text
-    Before:
-      11. Suppose \(S, T \in \mathcal{L}(V)\). Prove that \(S T\) and \(T S\) have the same eigenvalues.<br><br>Solution: Suppose that \(\lambda \in \mathbf{F}\) is an eigenvalue of \(S T\). We want to prove that \(\lambda\) is an eigenvalue of \(T S\). Because \(\lambda\) is an eigenvalue of \(S T\), there exists a nonzero vector \(v \in V\) such that<br><br><ul><li>\((ST)v\) =&nbsp;\( \lambda v\)</li><li>If&nbsp;</li><li>\((TS)v\) =&nbsp;\( \alpha v\)</li><li>Then</li><li>\(T(ST)v\) =&nbsp;\( T \lambda v\)</li><li>\(T(ST)v\) =&nbsp;\(&nbsp; \lambda Tv\)</li><li>\(TST v\) =&nbsp;\(&nbsp; \lambda Tv\)</li><li>Which implies that&nbsp;\(Tv\) is an eigvenvector of&nbsp;\(TS\) and&nbsp;\(\lambda\) is also an eigenvalue of T</li><li>Mutatis mulandis</li></ul>
-
-    After:
-      11. Suppose \(S, T \in \mathcal{L}(V)\). Prove that \(S T\) and \(T S\) have the same eigenvalues.<br><br>Solution: Suppose that \(\lambda \in \mathbf{F}\) is an eigenvalue of \(S T\). We want to prove that \(\lambda\) is an eigenvalue of \(T S\). Because \(\lambda\) is an eigenvalue of \(S T\), there exists a nonzero vector \(v \in V\) such that<br><br><ul><li>\((ST)v\) =&nbsp;\( \lambda v\)</li><li>If&nbsp;</li><li>\((TS)v\) =&nbsp;\( \alpha v\)</li><li>Then</li><li>\(T(ST)v\) =&nbsp;\( T \lambda v\)</li><li>\(T(ST)v\) =&nbsp;\(&nbsp; \lambda Tv\)</li><li>\(TST v\) =&nbsp;\(&nbsp; \lambda Tv\)</li><li>Which implies that&nbsp;\(Tv\) is an eigvenvector of&nbsp;\(TS\) and&nbsp;\(\lambda\) is also an eigenvalue of T</li><li>Mutatis mulandis</li></ul>
-
-============================================================
-
-Note ID: 1698270919652
-  Field: Text
-    Before:
-      15. Suppose \(\mathbf{F}=\mathbf{C}, T \in \mathcal{L}(V), p \in \mathcal{P}(\mathbf{C})\), and \(a \in \mathbf{C}\). Prove that \(a\) is an eigenvalue of \(p(T)\) if and only if \(a=p(\lambda)\) for some eigenvalue \(\lambda\) of \(T\).<br><br>Solution:<br><br><br>SOLUTION: First suppose that \(a\) is an eigenvalue of \(p(T)\). Thus \(p(T)-a I\) is not injective. Write the polynomial \(p(z)-a\) in factored form:<br><br>\[<br>p(z)-a=c\left(z-\lambda_{1}\right) \ldots\left(z-\lambda_{m}\right),<br>\]<br><br>where \(c, \lambda_{1}, \ldots, \lambda_{m} \in \mathrm{C}\). We can assume that \(c \neq 0\) (otherwise \(p\) is a constant polynomial, in which case the desired result clearly holds). The equation above implies that<br><br><ul><li>\(p(T)-a I\) = \(c\left(T-\lambda_{1} I\right) \ldots\left(T-\lambda_{m} I\right) .\)</li><li>Because \(p(T)-a I\) is not injective</li><ul><li>This implies that \(T-\lambda_{j} I\) is not injective for some \(j\).&nbsp;</li><li>In other words, some \(\lambda_{j}\) is an eigenvalue of \(T\).&nbsp;</li><li>The formula above for \(p(z)-a\) shows that \(p\left(\lambda_{j}\right)-a\) = \(0\).&nbsp;</li><li>Hence \(a=p\left(\lambda_{j}\right)\), as desired.</li></ul></ul>
-
-    After:
-      15. Suppose \(\mathbf{F}=\mathbf{C}, T \in \mathcal{L}(V), p \in \mathcal{P}(\mathbf{C})\), and \(a \in \mathbf{C}\). Prove that \(a\) is an eigenvalue of \(p(T)\) if and only if \(a=p(\lambda)\) for some eigenvalue \(\lambda\) of \(T\).<br><br>Solution:<br><br><br>SOLUTION: First suppose that \(a\) is an eigenvalue of \(p(T)\). Thus \(p(T)-a I\) is not injective. Write the polynomial \(p(z)-a\) in factored form:<br><br>\[<br>p(z)-a=c\left(z-\lambda_{1}\right) \ldots\left(z-\lambda_{m}\right),<br>\]<br><br>where \(c, \lambda_{1}, \ldots, \lambda_{m} \in \mathrm{C}\). We can assume that \(c \neq 0\) (otherwise \(p\) is a constant polynomial, in which case the desired result clearly holds). The equation above implies that<br><br><ul><li>\(p(T)-a I\) = \(c\left(T-\lambda_{1} I\right) \ldots\left(T-\lambda_{m} I\right) .\)</li><li>Because \(p(T)-a I\) is not injective</li><ul><li>This implies that \(T-\lambda_{j} I\) is not injective for some \(j\).&nbsp;</li><li>In other words, some \(\lambda_{j}\) is an eigenvalue of \(T\).&nbsp;</li><li>The formula above for \(p(z)-a\) shows that \(p\left(\lambda_{j}\right)-a\) = \(0\).&nbsp;</li><li>Hence \(a=p\left(\lambda_{j}\right)\), as desired.</li></ul></ul>
-
-============================================================
-
-Note ID: 1698774906462
-  Field: Text
-    Before:
-      Theorem 0.3. Suppose \(\left(f_{1}, \ldots, f_{n}\right)\) is an orthogonal basis of \(V\). Then<br><br><ul><li>\(v\) = \(\frac{\left\langle v, f_{1}\right\rangle}{\left\langle f_{1}, f_{1}\right\rangle} f_{1}\) +\(\cdots\)+\(\frac{\left\langle v, f_{n}\right\rangle}{\left\langle f_{n}, f_{n}\right\rangle} f_{n}\)</li></ul><br>and<br><br><ul><li>\(\|v\|^{2}\) = \(\frac{\left|\left\langle v, f_{1}\right\rangle\right|^{2} }{\left\langle f_{1}, f_{1}\right\rangle}\)+\(\cdots\)+\(\frac{\left|\left\langle v, f_{n}\right\rangle\right|^{2} }{\left\langle f_{n}, f_{n}\right\rangle}\)</li></ul><br>for every \(v \in V\).<br>
-
-    After:
-      Theorem 0.3. Suppose \(\left(f_{1}, \ldots, f_{n}\right)\) is an orthogonal basis of \(V\). Then<br><br><ul><li>\(v\) = \(\frac{\left\langle v, f_{1}\right\rangle}{\left\langle f_{1}, f_{1}\right\rangle} f_{1}\) +\(\cdots\)+\(\frac{\left\langle v, f_{n}\right\rangle}{\left\langle f_{n}, f_{n}\right\rangle} f_{n}\)</li></ul><br>and<br><br><ul><li>\(\|v\|^{2}\) = \(\frac{\left|\left\langle v, f_{1}\right\rangle\right|^{2} }{\left\langle f_{1}, f_{1}\right\rangle}\)+\(\cdots\)+\(\frac{\left|\left\langle v, f_{n}\right\rangle\right|^{2} }{\left\langle f_{n}, f_{n}\right\rangle}\)</li></ul><br>for every \(v \in V\).<br>
-
-============================================================
-
-Note ID: 1698791362067
-  Field: Text
-    Before:
-      5. (4 points) This problem is about the inner product space \(C[0,1]\) of real-valued continuous functions on the interval \([0,1]\), with inner product<br><br>\[<br>\langle p, q\rangle=\int_{0}^{1} p(x) q(x) d x<br>\]<br><br>It's like the example in the text about finding a good polynomial approximation to \(\sin (x)\).<br><br>b) Define \(f(x)=\sqrt{x}\), regarded as a function in \(C[0,1]\). Calculate the orthogonal projection \(p_{U}(f)\) of \(f\) on the subspace \(U\). (Your answer should be a cubic polynomial in \(x\) with rational numbers as coefficients.)<br><br><br>According to the formula from the notes,<br><br><ul><li>\(P_{U}(f)\)&nbsp;</li><li>= \(\frac{\left\langle f, f_{1}\right\rangle}{\left\langle f_{1}, f_{1}\right\rangle} f_{1}\) + \(\frac{\left\langle f, f_{2}\right\rangle}{\left\langle f_{2}, f_{2}\right\rangle} f_{2}\) + \(\frac{\left\langle f, f_{3}\right\rangle}{\left\langle f_{3}, f_{3}\right.} f_{3}\) + \(\frac{\left\langle f, f_{4}\right\rangle}{\left\langle f_{4}, f_{f}\right\rangle} f_{4}\)</li></ul><div><br></div><div>This projection is equivalent to finding the best approximation of&nbsp;\(\sqrt{x}\) using polynomials of dgree 3,2,1,0</div>
-
-    After:
-      5. (4 points) This problem is about the inner product space \(C[0,1]\) of real-valued continuous functions on the interval \([0,1]\), with inner product<br><br>\[<br>\langle p, q\rangle=\int_{0}^{1} p(x) q(x) d x<br>\]<br><br>It's like the example in the text about finding a good polynomial approximation to \(\sin (x)\).<br><br>b) Define \(f(x)=\sqrt{x}\), regarded as a function in \(C[0,1]\). Calculate the orthogonal projection \(p_{U}(f)\) of \(f\) on the subspace \(U\). (Your answer should be a cubic polynomial in \(x\) with rational numbers as coefficients.)<br><br><br>According to the formula from the notes,<br><br><ul><li>\(P_{U}(f)\)&nbsp;</li><li>= \(\frac{\left\langle f, f_{1}\right\rangle}{\left\langle f_{1}, f_{1}\right\rangle} f_{1}\) + \(\frac{\left\langle f, f_{2}\right\rangle}{\left\langle f_{2}, f_{2}\right\rangle} f_{2}\) + \(\frac{\left\langle f, f_{3}\right\rangle}{\left\langle f_{3}, f_{3}\right.} f_{3}\) + \(\frac{\left\langle f, f_{4}\right\rangle}{\left\langle f_{4}, f_{f}\right\rangle} f_{4}\)</li></ul><div><br></div><div>This projection is equivalent to finding the best approximation of&nbsp;\(\sqrt{x}\) using polynomials of dgree 3,2,1,0</div>
-
-============================================================
-
-Note ID: 1698949200186
-  Field: Text
-    Before:
-      1. (25 points) Let<br><br>\[<br>A=\left(\begin{array}{lll}<br>1 &amp; 1 &amp; 1 \\<br>0 &amp; 2 &amp; 2 \\<br>0 &amp; 0 &amp; 2<br>\end{array}\right)<br>\]<br><br>a matrix of real numbers.<br><br>b) For each eigenvalue (1,2), find all the corresponding eigenvectors of \(A\).<br><br>Solution:<br><ul><li>For&nbsp;\(\lambda = 1\), we have&nbsp;\(Av \) =&nbsp;\( [x+y+z, 2(y+z), 2z]\) =&nbsp;\([x,y,z]\) which automatically implies z and y must be 0. Thus&nbsp;\(E(1, A) \)=&nbsp; \(\mathrm{span}(1,0,0)\)</li><li>For&nbsp;\(\lambda = 2\), we have&nbsp;\(Av \) =&nbsp;\( [x+y+z, 2(y+z), 2z]\) =&nbsp;\([2x,2y,2z]\) which automatically implies that&nbsp;\(2y+2z \) = 0 which implies that&nbsp;\(z\) = \(0\) and x,y can be any value.&nbsp; Thus&nbsp;\(E(2,A)\) =&nbsp;\(\mathrm{span}(1,1,0)\)</li></ul>
-
-    After:
-      1. (25 points) Let<br><br>\[<br>A=\left(\begin{array}{lll}<br>1 &amp; 1 &amp; 1 \\<br>0 &amp; 2 &amp; 2 \\<br>0 &amp; 0 &amp; 2<br>\end{array}\right)<br>\]<br><br>a matrix of real numbers.<br><br>b) For each eigenvalue (1,2), find all the corresponding eigenvectors of \(A\).<br><br>Solution:<br><ul><li>For&nbsp;\(\lambda = 1\), we have&nbsp;\(Av \) =&nbsp;\( [x+y+z, 2(y+z), 2z]\) =&nbsp;\([x,y,z]\) which automatically implies z and y must be 0. Thus&nbsp;\(E(1, A) \)=&nbsp; \(\mathrm{span}(1,0,0)\)</li><li>For&nbsp;\(\lambda = 2\), we have&nbsp;\(Av \) =&nbsp;\( [x+y+z, 2(y+z), 2z]\) =&nbsp;\([2x,2y,2z]\) which automatically implies that&nbsp;\(2y+2z \) = 0 which implies that&nbsp;\(z\) = \(0\) and x,y can be any value.&nbsp; Thus&nbsp;\(E(2,A)\) =&nbsp;\(\mathrm{span}(1,1,0)\)</li></ul>
-
-============================================================
-
-Note ID: 1698956231228
-  Field: Text
-    Before:
-      2. (30 points) Define a sequence of real numbers by \(a_{0}=0, a_{1}=1\), and<br><br>\[<br>a_{n+1}=\left(a_{n}+a_{n-1}\right) / 2 \quad(n \geq 1)<br>\]<br><br>that is, each term is the average of the two preceding terms. The first few terms of the sequence are<br><br>\[<br>0,1,1 / 2,3 / 4,5 / 8,11 / 16,21 / 32,43 / 64,85 / 128, \ldots<br>\]<br><br>a) Find a \(2 \times 2\) real matrix \(A\) such that the following is true<br><br>\[<br>\left(\begin{array}{c}<br>a_{n+1} \\<br>a_{n}<br>\end{array}\right)=A\left(\begin{array}{c}<br>a_{n} \\<br>a_{n-1}<br>\end{array}\right)<br>\]<br><br>\[<br>A=\left(\begin{array}{cc}<br>1 / 2 &amp; 1 / 2 \\<br>1 &amp; 0<br>\end{array}\right)<br>\]<br><br>b) Find all the eigenvalues and eigenvectors of your matrix \(A\).<br><br>We want to find \(\lambda\) such that<br><br>\[<br>A-\lambda I=\left(\begin{array}{cc}<br>1 / 2-\lambda &amp; 1 / 2 \\<br>1 &amp; -\lambda<br>\end{array}\right)<br>\]<br><br>is not invertible. Using Gauss elimination (or just be speculating), we found this is the same as saying \(\lambda^{2}-\frac{1}{2} \lambda-\frac{1}{2}=0\). So \(\lambda\) has two solutions \(1,-\frac{1}{2}\).<br><br>The eigenvectors for 1 are the solutions of \((A-1 \cdot I)\left(\begin{array}{l}x_{1} \\ x_{2}\end{array}\right)=0\), or<br><br><ul><li>\(-(1 / 2) x_{1}+(1 / 2) x_{2}\) = \(0\)</li><li>\(x_{1}-x_{2}\)&nbsp;= \(0\)<br></li><li>Thus the solutions are&nbsp;\(\left(\begin{array}{l}x \\ x\end{array}\right)\) with a basis&nbsp;\(u\)= \(\left(\begin{array}{l}1 \\ 1\end{array}\right)\)</li><li>Mutatis mulandis for -1/2</li></ul><div><br></div>
-
-    After:
-      2. (30 points) Define a sequence of real numbers by \(a_{0}=0, a_{1}=1\), and<br><br>\[<br>a_{n+1}=\left(a_{n}+a_{n-1}\right) / 2 \quad(n \geq 1)<br>\]<br><br>that is, each term is the average of the two preceding terms. The first few terms of the sequence are<br><br>\[<br>0,1,1 / 2,3 / 4,5 / 8,11 / 16,21 / 32,43 / 64,85 / 128, \ldots<br>\]<br><br>a) Find a \(2 \times 2\) real matrix \(A\) such that the following is true<br><br>\[<br>\left(\begin{array}{c}<br>a_{n+1} \\<br>a_{n}<br>\end{array}\right)=A\left(\begin{array}{c}<br>a_{n} \\<br>a_{n-1}<br>\end{array}\right)<br>\]<br><br>\[<br>A=\left(\begin{array}{cc}<br>1 / 2 &amp; 1 / 2 \\<br>1 &amp; 0<br>\end{array}\right)<br>\]<br><br>b) Find all the eigenvalues and eigenvectors of your matrix \(A\).<br><br>We want to find \(\lambda\) such that<br><br>\[<br>A-\lambda I=\left(\begin{array}{cc}<br>1 / 2-\lambda &amp; 1 / 2 \\<br>1 &amp; -\lambda<br>\end{array}\right)<br>\]<br><br>is not invertible. Using Gauss elimination (or just be speculating), we found this is the same as saying \(\lambda^{2}-\frac{1}{2} \lambda-\frac{1}{2}=0\). So \(\lambda\) has two solutions \(1,-\frac{1}{2}\).<br><br>The eigenvectors for 1 are the solutions of \((A-1 \cdot I)\left(\begin{array}{l}x_{1} \\ x_{2}\end{array}\right)=0\), or<br><br><ul><li>\(-(1 / 2) x_{1}+(1 / 2) x_{2}\) = \(0\)</li><li>\(x_{1}-x_{2}\)&nbsp;= \(0\)<br></li><li>Thus the solutions are&nbsp;\(\left(\begin{array}{l}x \\ x\end{array}\right)\) with a basis&nbsp;\(u\)= \(\left(\begin{array}{l}1 \\ 1\end{array}\right)\)</li><li>Mutatis mulandis for -1/2</li></ul><div><br></div>
-
-============================================================
-
-Note ID: 1698999646952
-  Field: Text
-    Before:
-      <img src="paste-a8ca47d946f7f00d5c54658f32e74eca4f4cc5b8.jpg"><br><br>Lets prove (a), let w be in W Then<br><ul><li>\(w \in \operatorname{null} T^*\)&nbsp;</li><li>\(\Longleftrightarrow\) \(T^* w=0\)</li><li>\(\Longleftrightarrow\) \(\left\langle v, T^* w\right\rangle\) =\(0\) for all \(v \in V\)<br></li><li>\(\Longleftrightarrow\) \(\langle T v, w\rangle\) = \(0\) for all \(v \in V\)<br></li></ul>
-
-    After:
-      <img src="paste-a8ca47d946f7f00d5c54658f32e74eca4f4cc5b8.jpg"><br><br>Lets prove (a), let w be in W Then<br><ul><li>\(w \in \operatorname{null} T^*\)&nbsp;</li><li>\(\Longleftrightarrow\) \(T^* w=0\)</li><li>\(\Longleftrightarrow\) \(\left\langle v, T^* w\right\rangle\) =\(0\) for all \(v \in V\)<br></li><li>\(\Longleftrightarrow\) \(\langle T v, w\rangle\) = \(0\) for all \(v \in V\)<br></li></ul>
-
-============================================================
-
-Note ID: 1699003127975
-  Field: Text
-    Before:
-      <img src="paste-cfb62edf1a09ee4b7ca829651b4d80b40ebd0f4e.jpg"><br><br>Proof, let v be a vector in V, then:<br><ul><li>\(\langle T v, v\rangle\) - \(\overline{\langle T v, v\rangle}\)&nbsp;</li><li>= \(\langle T v, v\rangle\) - \(\langle v, T v\rangle\)<br></li><li>=&nbsp;\(\langle T v, v\rangle\) - \(\left\langle T^* v, v\right\rangle\)</li><li>= \(\left\langle\left(T-T^*\right) v, v\right\rangle\).<br></li></ul><div><br></div><div><ul><li>If \(\langle T v, v\rangle \in \mathbf{R}\) for every \(v \in V\), then the left side of the first equation above equals 0 ,&nbsp;</li><li>So \(\left\langle\left(T-T^*\right) v, v\right\rangle\) = \(0\) for every \(v \in V\).&nbsp;</li><li>This implies that \(T-T^*\) = \(0\). Hence \(T\) is self-adjoint.</li></ul></div>
-
-    After:
-      <img src="paste-cfb62edf1a09ee4b7ca829651b4d80b40ebd0f4e.jpg"><br><br>Proof, let v be a vector in V, then:<br><ul><li>\(\langle T v, v\rangle\) - \(\overline{\langle T v, v\rangle}\)&nbsp;</li><li>= \(\langle T v, v\rangle\) - \(\langle v, T v\rangle\)<br></li><li>=&nbsp;\(\langle T v, v\rangle\) - \(\left\langle T^* v, v\right\rangle\)</li><li>= \(\left\langle\left(T-T^*\right) v, v\right\rangle\).<br></li></ul><div><br></div><div><ul><li>If \(\langle T v, v\rangle \in \mathbf{R}\) for every \(v \in V\), then the left side of the first equation above equals 0 ,&nbsp;</li><li>So \(\left\langle\left(T-T^*\right) v, v\right\rangle\) = \(0\) for every \(v \in V\).&nbsp;</li><li>This implies that \(T-T^*\) = \(0\). Hence \(T\) is self-adjoint.</li></ul></div>
-
-============================================================
-
-Note ID: 1699037262586
-  Field: Text
-    Before:
-      <img src="paste-a24bfef9e14eef153c19c7edcdde6d4f0f3b680e.jpg"><br><br>Proof<br><ul><li>T is normal</li><li>\(\Longleftrightarrow\)&nbsp; \(T^* T-T T^*\) = \(0\)<br></li><li>\(<br>\Longleftrightarrow\)&nbsp; \(\left\langle\left(T^* T-T T^*\right) v, v\right\rangle\)::inner prod::inner prod::inner prod::inner prod = \(0\)&nbsp;for all \(v \in V\)<br></li><li>\(\Longleftrightarrow\) \(\left\langle T^* T v, v\right\rangle\)::inner prod::inner prod::inner prod = \(\left\langle T T^* v, v\right\rangle\)::inner prod::inner prod::inner prod for all v in V<br></li><li>\(\Longleftrightarrow\) \(\|T v\|^2\)::norm::norm = \(\left\|T^* v\right\|^2\)::norm::norm for all v in V<br></li></ul>
-
-    After:
-      <img src="paste-a24bfef9e14eef153c19c7edcdde6d4f0f3b680e.jpg"><br><br>Proof<br><ul><li>T is normal</li><li>\(\Longleftrightarrow\)&nbsp; \(T^* T-T T^*\) = \(0\)<br></li><li>\(<br>\Longleftrightarrow\)&nbsp; \(\left\langle\left(T^* T-T T^*\right) v, v\right\rangle\)::inner prod::inner prod::inner prod::inner prod = \(0\)&nbsp;for all \(v \in V\)<br></li><li>\(\Longleftrightarrow\) \(\left\langle T^* T v, v\right\rangle\)::inner prod::inner prod::inner prod = \(\left\langle T T^* v, v\right\rangle\)::inner prod::inner prod::inner prod for all v in V<br></li><li>\(\Longleftrightarrow\) \(\|T v\|^2\)::norm::norm = \(\left\|T^* v\right\|^2\)::norm::norm for all v in V<br></li></ul>
-
-============================================================
-
-Note ID: 1701881706874
-  Field: Text
-    Before:
-      <ul><li>Smaller models can use mixed precision fp32 parameters and bf16 activations</li><li>While larger models use bf16 for both</li><li>bf16 parameters are updated using stochastic rounding to maintain stability</li></ul>
-
-    After:
-      <ul><li>Smaller models can use mixed precision fp32 parameters and bf16 activations</li><li>While larger models use bf16 for both</li><li>bf16 parameters are updated using stochastic rounding to maintain stability</li></ul>
-
-============================================================
-
-Note ID: 1701881882199
-  Field: Text
-    Before:
-      <span style="font-style:italic">The&nbsp;</span>continuation<span style="font-style:italic">&nbsp;</span>toxicity<span style="font-style:italic">&nbsp;of larger models is more consistent with&nbsp;</span>prompt toxicity<span style="font-style:italic">&nbsp;than for smaller models</span>
-
-    After:
-      <span style="font-style:italic">The&nbsp;</span>continuation<span style="font-style:italic">&nbsp;</span>toxicity<span style="font-style:italic">&nbsp;of larger models is more consistent with&nbsp;</span>prompt toxicity<span style="font-style:italic">&nbsp;than for smaller models</span>
-
-============================================================
-
-Note ID: 1701882251288
-  Field: Text
-    Before:
-      Distillation from large to small or small to large seems to bring little benefit to LLMs.
-
-    After:
-      Distillation from large to small or small to large seems to bring little benefit to LLMs.
-
-============================================================
-
-Note ID: 1701882675893
-  Field: Text
-    Before:
-      <ul><li>Unlike the model parameters and activations, it is best to maintain optimzier states in fp32</li><li>The fp32 parameters are used for the update and then cast to bf16 during the fowrad pass</li></ul>
-
-    After:
-      <ul><li>Unlike the model parameters and activations, it is best to maintain optimzier states in fp32</li><li>The fp32 parameters are used for the update and then cast to bf16 during the fowrad pass</li></ul>
-
-============================================================
-
-Note ID: 1701882726063
-  Field: Text
-    Before:
-      Keeping only the optimiser state in fp32 matches the performance of fp32 training and only slightly increases memory compared to bf16
+      <ol>

=== Pair #2 ===
Content differs beyond just cloze markup!
Differences (normalized text):
--- before (normalized)
+++ after (normalized)
@@ -1,11 +1,8 @@
-      While both local SGD and minibatch SGD greatly improve with intra-node parallelisation, local SGD is the only one to scale well to multi-node settings (approaches linear::what kind of speedup?::what kind of speedup? speedup).
-
-    After:
-      While both local SGD and minibatch SGD greatly improve with intra-node parallelisation, local SGD is the only one to scale well to multi-node settings (approaches linear::what kind of speedup?::what kind of speedup? speedup).
+      <span style="color: inherit; font-style: italic;">Performing local SGD comes at a price:&nbsp;</span>lower<span style="color: inherit; font-style: italic;">&nbsp;</span>communication costs<span style="color: inherit; font-style: italic;">&nbsp;(and thereby&nbsp;</span>faster<span style="color: inherit; font-style: italic;">&nbsp;training) are accompanied by&nbsp;</span>lower<span style="color: inherit; font-style: italic;">&nbsp;</span>accuracy<span style="color: inherit; font-style: italic;">.</span>
 
 ============================================================
 
-Note ID: 1701948371327
+Note ID: 1702021813378
   Field: Text
     Before:
-      <span style="color: inherit; font-style: italic;">Performing local SGD comes at a price:&nbsp;</span>lower<span style="color: inherit; font-style: italic;">&nbsp;</span>communication costs<span style="color: inherit; font-style: italic;">&nbsp;(and thereby&nbsp;</span>faster<span style="color: inherit; font-style: italic;">&nbsp;training) are accompanied by&nbsp;</span>lower<span style="color: inherit; font-style: italic;">&nbsp;</span>accuracy<span style="color: inherit; font-style: italic;">.</span>
+      <ol>

=== Pair #3 ===
Content differs beyond just cloze markup!
Differences (normalized text):
--- before (normalized)
+++ after (normalized)
@@ -1,351 +1,8 @@
-      <ul><li>From a probabilistic perspective, translation is equivalent to finding a target sentence \(\mathbf{y}\) that maximizes the conditional probability of \(\mathbf{y}\) given a source sentence \(\mathbf{x}\), i.e., :<br><br></li><ul><li>\(\arg \max _{\mathbf{y} }\) p(&nbsp;\(\mathbf{y} \mid \mathbf{x}\) ).&nbsp;</li></ul></ul>
-
-    After:
-      <ul><li>From a probabilistic perspective, translation is equivalent to finding a target sentence \(\mathbf{y}\) that maximizes the conditional probability of \(\mathbf{y}\) given a source sentence \(\mathbf{x}\), i.e., :<br><br></li><ul><li>\(\arg \max _{\mathbf{y} }\) p(&nbsp;\(\mathbf{y} \mid \mathbf{x}\) ).&nbsp;</li></ul></ul>
+      Self-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.
 
 ============================================================
 
-Note ID: 1702122699247
+Note ID: 1702395142696
   Field: Text
     Before:
-      <img src="paste-4f405f4122a2f4b25a46858f63800308295a137f.jpg"><br>In the attention-based RNN architecture:<br><ul><li>We define each conditional probability in Eq. (2) as:</li><ul><li>\(p\) \( (y_i \mid y_1, \ldots, y_{i-1}, \mathbf{x})\) = \(g\)\((y_{i-1}, s_i, c_i)\)</li></ul><li>where \(s_i\) is an RNN hidden state for time \(i\), computed by</li><ul><li>\(s_i\) = \(f\) \(\left(s_{i-1}, y_{i-1}, c_i\right) \)</li></ul><li>It should be noted that unlike the existing encoder-decoder approach (see Eq. (2)), here the probability is conditioned on a distinct context vector \(c_i\) for each target word \(y_i\).</li></ul>
-
-    After:
-      <img src="paste-4f405f4122a2f4b25a46858f63800308295a137f.jpg"><br>In the attention-based RNN architecture:<br><ul><li>We define each conditional probability in Eq. (2) as:</li><ul><li>\(p\) \( (y_i \mid y_1, \ldots, y_{i-1}, \mathbf{x})\) = \(g\)\((y_{i-1}, s_i, c_i)\)</li></ul><li>where \(s_i\) is an RNN hidden state for time \(i\), computed by</li><ul><li>\(s_i\) = \(f\) \(\left(s_{i-1}, y_{i-1}, c_i\right) \)</li></ul><li>It should be noted that unlike the existing encoder-decoder approach (see Eq. (2)), here the probability is conditioned on a distinct context vector \(c_i\) for each target word \(y_i\).</li></ul>
-
-============================================================
-
-Note ID: 1702125852562
-  Field: Text
-    Before:
-      A Bidirectional RNN:<br><ul><li>Consists of forward and backward RNN's.&nbsp;</li><li>The forward RNN \(\vec{f}\) reads the input sequence as it is ordered (from \(x_1\) to \(x_{T_x}\) ) and calculates a sequence of forward hidden states \(\left(\vec{h}_1, \cdots, \vec{h}_{T_x}\right)\).&nbsp;</li><li>The backward RNN \(\overleftarrow{f}\) reads the sequence in the reverse order (from \(x_{T_x}\) to \(x_1\) ), resulting in a sequence of backward hidden states \(\left(\overleftarrow{h}_1, \cdots, \overleftarrow{h}_{T_x}\right)\)</li></ul>
-
-    After:
-      A Bidirectional RNN:<br><ul><li>Consists of forward and backward RNN's.&nbsp;</li><li>The forward RNN \(\vec{f}\) reads the input sequence as it is ordered (from \(x_1\) to \(x_{T_x}\) ) and calculates a sequence of forward hidden states \(\left(\vec{h}_1, \cdots, \vec{h}_{T_x}\right)\).&nbsp;</li><li>The backward RNN \(\overleftarrow{f}\) reads the sequence in the reverse order (from \(x_{T_x}\) to \(x_1\) ), resulting in a sequence of backward hidden states \(\left(\overleftarrow{h}_1, \cdots, \overleftarrow{h}_{T_x}\right)\)</li></ul>
-
-============================================================
-
-Note ID: 1702131761215
-  Field: Text
-    Before:
-      <ul><li><img src="paste-d7fe315a3abdaa5d8ff22fb35d4eae8fd319a56c.jpg"><br></li><li>The strength of the soft-alignment, opposed to a hard-alignment, is evident, for instance.</li><li>Consider the source phrase [the man] which was translated into [l' homme].&nbsp;</li><li>Any hard alignment will map [the] to [l'] and [man] to [homme].&nbsp;</li><li>This is not helpful for translation, as one must consider the word following [the] to determine whether it should be translated into [le], [la], [les] or [l'].&nbsp;</li><li>Our soft-alignment solves this issue naturally by letting the model look at both [the] and [man].&nbsp;</li><li>An additional benefit of the soft alignment is that it naturally deals with source and target phrases of different lengths, without requiring a counter-intuitive way of mapping some words to or from nowhere ([NULL])&nbsp;</li></ul>
-
-    After:
-      <ul><li><img src="paste-d7fe315a3abdaa5d8ff22fb35d4eae8fd319a56c.jpg"><br></li><li>The strength of the soft-alignment, opposed to a hard-alignment, is evident, for instance.</li><li>Consider the source phrase [the man] which was translated into [l' homme].&nbsp;</li><li>Any hard alignment will map [the] to [l'] and [man] to [homme].&nbsp;</li><li>This is not helpful for translation, as one must consider the word following [the] to determine whether it should be translated into [le], [la], [les] or [l'].&nbsp;</li><li>Our soft-alignment solves this issue naturally by letting the model look at both [the] and [man].&nbsp;</li><li>An additional benefit of the soft alignment is that it naturally deals with source and target phrases of different lengths, without requiring a counter-intuitive way of mapping some words to or from nowhere ([NULL])&nbsp;</li></ul>
-
-============================================================
-
-Note ID: 1702132482922
-  Field: Text
-    Before:
-      The model takes a source sentence of 1 -of-K coded word vectors as input<br>\[<br>\mathbf{x}=\left(x_1, \ldots, x_{T_x}\right), x_i \in \mathbb{R}^{K_x}<br>\]<br>and outputs a translated sentence of 1 -of-K coded word vectors<br>\[<br>\mathbf{y}=\left(y_1, \ldots, y_{T_y}\right), y_i \in \mathbb{R}^{K_y}<br>\]<br>where \(K_x\) and \(K_y\) are the vocabulary sizes of source and target languages, respectively. \(T_x\) and \(T_y\) respectively denote the lengths of source and target sentences.
-
-    After:
-      The model takes a source sentence of 1 -of-K coded word vectors as input<br>\[<br>\mathbf{x}=\left(x_1, \ldots, x_{T_x}\right), x_i \in \mathbb{R}^{K_x}<br>\]<br>and outputs a translated sentence of 1 -of-K coded word vectors<br>\[<br>\mathbf{y}=\left(y_1, \ldots, y_{T_y}\right), y_i \in \mathbb{R}^{K_y}<br>\]<br>where \(K_x\) and \(K_y\) are the vocabulary sizes of source and target languages, respectively. \(T_x\) and \(T_y\) respectively denote the lengths of source and target sentences.
-
-============================================================
-
-Note ID: 1702201627835
-  Field: Text
-    Before:
-      For complext inner product spaces<br><ul><li>\(\langle Tv, v \rangle\) = 0<br></li><li>Implies T = 0</li></ul>
-
-    After:
-      For complext inner product spaces<br><ul><li>\(\langle Tv, v \rangle\) = 0<br></li><li>Implies T = 0</li></ul>
-
-============================================================
-
-Note ID: 1702201961636
-  Field: Text
-    Before:
-      Proof that \(\langle Tv,v \rangle \in R\) for all v implies that T is self-adjoint:<br><ul><li>\(\langle\)&nbsp; \(Tv,v\) \(\rangle\) - \(\langle\) \(\overline{Tv,v}\) \(\rangle\)</li><li>= \(\langle\)&nbsp; \(Tv,v\) \(\rangle\) - \(\langle\)&nbsp; \(v,Tv\) \(\rangle\)&nbsp;<br></li><li>= \(\langle\)&nbsp; \(Tv,v\) \(\rangle\) -&nbsp;\(\langle\)&nbsp; \(T^*v,v\) \(\rangle\)&nbsp;</li><li>= \(\langle\)&nbsp; \( (T-T*)v,v\) \(\rangle\) = 0</li><li>Which implies&nbsp;\(T^*\) = \(T\)</li></ul>
-
-    After:
-      Proof that \(\langle Tv,v \rangle \in R\) for all v implies that T is self-adjoint:<br><ul><li>\(\langle\)&nbsp; \(Tv,v\) \(\rangle\) - \(\langle\) \(\overline{Tv,v}\) \(\rangle\)</li><li>= \(\langle\)&nbsp; \(Tv,v\) \(\rangle\) - \(\langle\)&nbsp; \(v,Tv\) \(\rangle\)&nbsp;<br></li><li>= \(\langle\)&nbsp; \(Tv,v\) \(\rangle\) -&nbsp;\(\langle\)&nbsp; \(T^*v,v\) \(\rangle\)&nbsp;</li><li>= \(\langle\)&nbsp; \( (T-T*)v,v\) \(\rangle\) = 0</li><li>Which implies&nbsp;\(T^*\) = \(T\)</li></ul>
-
-============================================================
-
-Note ID: 1702203378249
-  Field: Text
-    Before:
-      <ul><li>The nicest operators on \(V\) are those for which there is an orthonormal basis of \(V\) with respect to which the operator has a diagonal matrix.</li><li>&nbsp;These are precisely the operators \(T \in \mathcal{L}(V)\) such that there is an orthonormal basis of \(V\) consisting of eigenvectors of \(T\).&nbsp;</li></ul>
-
-    After:
-      <ul><li>The nicest operators on \(V\) are those for which there is an orthonormal basis of \(V\) with respect to which the operator has a diagonal matrix.</li><li>&nbsp;These are precisely the operators \(T \in \mathcal{L}(V)\) such that there is an orthonormal basis of \(V\) consisting of eigenvectors of \(T\).&nbsp;</li></ul>
-
-============================================================
-
-Note ID: 1702203840360
-  Field: Text
-    Before:
-      <img src="paste-530a5e94b1b46c19656893fea1bedce3bd55f09a.jpg"><br>Proof:<br><ul><li>Proof First suppose (c) holds.&nbsp;</li><li>So \(T\) has a diagonal matrix with respect to some orthonormal basis of \(V\).&nbsp;</li><li>The matrix of \(T^{*}\) (with respect to the same basis) is obtained by taking the conjugate transpose of the matrix of \(T\)&nbsp;</li><li>Hence \(T^{*}\) also has a diagonal matrix.&nbsp;</li><li>Any two diagonal matrices commute; thus \(T\) commutes with \(T^{*}\), which means that \(T\) is normal.&nbsp;</li><li>In other words, (a) holds.<br></li></ul>
-
-    After:
-      <img src="paste-530a5e94b1b46c19656893fea1bedce3bd55f09a.jpg"><br>Proof:<br><ul><li>Proof First suppose (c) holds.&nbsp;</li><li>So \(T\) has a diagonal matrix with respect to some orthonormal basis of \(V\).&nbsp;</li><li>The matrix of \(T^{*}\) (with respect to the same basis) is obtained by taking the conjugate transpose of the matrix of \(T\)&nbsp;</li><li>Hence \(T^{*}\) also has a diagonal matrix.&nbsp;</li><li>Any two diagonal matrices commute; thus \(T\) commutes with \(T^{*}\), which means that \(T\) is normal.&nbsp;</li><li>In other words, (a) holds.<br></li></ul>
-
-============================================================
-
-Note ID: 1702204465110
-  Field: Text
-    Before:
-      <img src="paste-b177df9f882ef3397e176a2b7100706a58dab475.jpg"><br>Now suppose (a) holds, so \(T\) is normal. By Schur's Theorem (6.38), there is an orthonormal basis \(e_{1}, \ldots, e_{n}\) of \(V\) with respect to which \(T\) has an upper-triangular matrix. Thus we can write<br><ul><li>\(\mathcal{M}\left(T,\left(e_1, \ldots, e_n\right)\right)\) =&nbsp;\(\left(\begin{array}{ccc}a_{1,1} &amp; \cdots &amp; a_{1, n} \\ &amp; \ddots &amp; \vdots \\ 0 &amp; &amp; a_{n, n}\end{array}\right)\).<br></li><li>We will show that this matrix is actually a diagonal matrix.<br></li><li>We see from the matrix above that&nbsp;</li><ul><li>\(\|\) \(T e_{1}\) \(\|^{2}\) = \(|\) \(a_{1,1}\) \(|^{2}\)</li><li>and</li><li>\(\| \) \(T^{*} e_{1}\) \(\|^{2}\) = \(\left|a_{1,1}\right|^{2}+\left|a_{1,2}\right|^{2}+\cdots+\left|a_{1, n}\right|^{2} .\)</li></ul><li>Because \(T\) is normal:</li><ul><li>&nbsp;\(\|\)&nbsp; \(T e_{1}\)&nbsp; \( \|\) = \(\| \)&nbsp; \(T^{*} e_{1}\) \(\|\).&nbsp;</li></ul><li>Thus the two equation above implity that all entries in the first row of the matrix, except possibly the first entry, equal 0</li></ul>
-
-    After:
-      <img src="paste-b177df9f882ef3397e176a2b7100706a58dab475.jpg"><br>Now suppose (a) holds, so \(T\) is normal. By Schur's Theorem (6.38), there is an orthonormal basis \(e_{1}, \ldots, e_{n}\) of \(V\) with respect to which \(T\) has an upper-triangular matrix. Thus we can write<br><ul><li>\(\mathcal{M}\left(T,\left(e_1, \ldots, e_n\right)\right)\) =&nbsp;\(\left(\begin{array}{ccc}a_{1,1} &amp; \cdots &amp; a_{1, n} \\ &amp; \ddots &amp; \vdots \\ 0 &amp; &amp; a_{n, n}\end{array}\right)\).<br></li><li>We will show that this matrix is actually a diagonal matrix.<br></li><li>We see from the matrix above that&nbsp;</li><ul><li>\(\|\) \(T e_{1}\) \(\|^{2}\) = \(|\) \(a_{1,1}\) \(|^{2}\)</li><li>and</li><li>\(\| \) \(T^{*} e_{1}\) \(\|^{2}\) = \(\left|a_{1,1}\right|^{2}+\left|a_{1,2}\right|^{2}+\cdots+\left|a_{1, n}\right|^{2} .\)</li></ul><li>Because \(T\) is normal:</li><ul><li>&nbsp;\(\|\)&nbsp; \(T e_{1}\)&nbsp; \( \|\) = \(\| \)&nbsp; \(T^{*} e_{1}\) \(\|\).&nbsp;</li></ul><li>Thus the two equation above implity that all entries in the first row of the matrix, except possibly the first entry, equal 0</li></ul>
-
-============================================================
-
-Note ID: 1702204668168
-  Field: Text
-    Before:
-      <img src="paste-b177df9f882ef3397e176a2b7100706a58dab475.jpg"><br>Now suppose (a) holds, so \(T\) is normal. By Schur's Theorem (6.38), there is an orthonormal basis \(e_{1}, \ldots, e_{n}\) of \(V\) with respect to which \(T\) has an upper-triangular matrix. Thus we can write<br><ul><li>\(\mathcal{M}\left(T,\left(e_1, \ldots, e_n\right)\right)\) =&nbsp;\(\left(\begin{array}{ccc}a_{1,1} &amp; \cdots &amp; a_{1, n} \\ &amp; \ddots &amp; \vdots \\ 0 &amp; &amp; a_{n, n}\end{array}\right)\).<br></li><li>We will show that this matrix is actually a diagonal matrix.<br></li><li>We see from the matrix above that&nbsp;</li><ul><li>\(\left\|T e_{1}\right\|^{2}\) = \(\left|a_{1,1}\right|^{2}\)</li><li>and</li><li>\(\left\|T^{*} e_{1}\right\|^{2}\) = \(\left|a_{1,1}\right|^{2}+\left|a_{1,2}\right|^{2}+\cdots+\left|a_{1, n}\right|^{2} .\)</li></ul><li>Because \(T\) is normal:</li><ul><li>&nbsp;\(\left\|T e_{1}\right\|\) = \(\left\|T^{*} e_{1}\right\|\) (see 7.20).&nbsp;</li></ul><li>Thus the two equations above imply that all entries in the first row of the matrix in 7.25 , except possibly the first entry \(a_{1,1}\), equal 0 .</li><li>Now from the matrix we see that</li><ul><li>\(\|\) \(T e_{2}\) \(\|^{2}\) = \(|\) \(a_{2,2}\) \( |^{2}\)</li><li>(because \(a_{1,2}=0\), as we showed in the paragraph above) and</li><li>\(\|\) \(T^{*} e_{2}\) \(|^{2}\) =\(\left|a_{2,2}\right|^{2}+\left|a_{2,3}\right|^{2}+\cdots+\left|a_{2, n}\right|^{2} .\)</li></ul><li>Because \(T\) is normal, \(\| \) \(T e_{2}\) \(\|\) = \(\|\) \(T^{*} e_{2}\) \(\|\).&nbsp;</li><li>Thus the two equations above imply that all entries in the second row of the matrix in 7.25, except possibly the diagonal entry \(a_{2,2}\), equal 0 .</li><li>Continuing in this fashion, we see that all the nondiagonal entries in the matrix 7.25 equal 0 . Thus (c) holds.</li></ul>
-
-    After:
-      <img src="paste-b177df9f882ef3397e176a2b7100706a58dab475.jpg"><br>Now suppose (a) holds, so \(T\) is normal. By Schur's Theorem (6.38), there is an orthonormal basis \(e_{1}, \ldots, e_{n}\) of \(V\) with respect to which \(T\) has an upper-triangular matrix. Thus we can write<br><ul><li>\(\mathcal{M}\left(T,\left(e_1, \ldots, e_n\right)\right)\) =&nbsp;\(\left(\begin{array}{ccc}a_{1,1} &amp; \cdots &amp; a_{1, n} \\ &amp; \ddots &amp; \vdots \\ 0 &amp; &amp; a_{n, n}\end{array}\right)\).<br></li><li>We will show that this matrix is actually a diagonal matrix.<br></li><li>We see from the matrix above that&nbsp;</li><ul><li>\(\left\|T e_{1}\right\|^{2}\) = \(\left|a_{1,1}\right|^{2}\)</li><li>and</li><li>\(\left\|T^{*} e_{1}\right\|^{2}\) = \(\left|a_{1,1}\right|^{2}+\left|a_{1,2}\right|^{2}+\cdots+\left|a_{1, n}\right|^{2} .\)</li></ul><li>Because \(T\) is normal:</li><ul><li>&nbsp;\(\left\|T e_{1}\right\|\) = \(\left\|T^{*} e_{1}\right\|\) (see 7.20).&nbsp;</li></ul><li>Thus the two equations above imply that all entries in the first row of the matrix in 7.25 , except possibly the first entry \(a_{1,1}\), equal 0 .</li><li>Now from the matrix we see that</li><ul><li>\(\|\) \(T e_{2}\) \(\|^{2}\) = \(|\) \(a_{2,2}\) \( |^{2}\)</li><li>(because \(a_{1,2}=0\), as we showed in the paragraph above) and</li><li>\(\|\) \(T^{*} e_{2}\) \(|^{2}\) =\(\left|a_{2,2}\right|^{2}+\left|a_{2,3}\right|^{2}+\cdots+\left|a_{2, n}\right|^{2} .\)</li></ul><li>Because \(T\) is normal, \(\| \) \(T e_{2}\) \(\|\) = \(\|\) \(T^{*} e_{2}\) \(\|\).&nbsp;</li><li>Thus the two equations above imply that all entries in the second row of the matrix in 7.25, except possibly the diagonal entry \(a_{2,2}\), equal 0 .</li><li>Continuing in this fashion, we see that all the nondiagonal entries in the matrix 7.25 equal 0 . Thus (c) holds.</li></ul>
-
-============================================================
-
-Note ID: 1702205631760
-  Field: Text
-    Before:
-      <img src="paste-83b3831e34c826764b0093760f2f884050344989.jpg"><br>What are the high-level points of the complex spectral theorem proof?<br><ul><li>We can create an upper-triangular matrix w.r.t some orthonormal basis for any vectors over C</li><li>For the first element \(\left\|T e_1\right\|^2\) = \(\left|a_{1,1}\right|^2\) since the column is zero<br></li><li>Because T is normal, the same applies to the adjoint/complex transpose and thus the row must also be zero:</li><li><img src="paste-caa70de9bb00431a9366f94d7841a6d2253f01e0.jpg"><br></li><ul><li>I.e for normal operators the norms of columns and their matching rows must match</li></ul><li>Since we know this row is zero, repeat for all other elements by induction</li></ul>
-
-    After:
-      <img src="paste-83b3831e34c826764b0093760f2f884050344989.jpg"><br>What are the high-level points of the complex spectral theorem proof?<br><ul><li>We can create an upper-triangular matrix w.r.t some orthonormal basis for any vectors over C</li><li>For the first element \(\left\|T e_1\right\|^2\) = \(\left|a_{1,1}\right|^2\) since the column is zero<br></li><li>Because T is normal, the same applies to the adjoint/complex transpose and thus the row must also be zero:</li><li><img src="paste-caa70de9bb00431a9366f94d7841a6d2253f01e0.jpg"><br></li><ul><li>I.e for normal operators the norms of columns and their matching rows must match</li></ul><li>Since we know this row is zero, repeat for all other elements by induction</li></ul>
-
-============================================================
-
-Note ID: 1702206379921
-  Field: Text
-    Before:
-      Self-adjoint operators and invariant subspaces<br><br>Suppose \(T \in \mathcal{L}(V)\) is self-adjoint and \(U\) is a subspace of \(V\) that is invariant under \(T\). Then<br><br>(a) \(U^{\perp}\) is invariant under \(T\);<br><br>(b) \(\left.T\right|_{U} \in \mathcal{L}(U)\) is self-adjoint;<br><br>(c) \(\left.T\right|_{U^{\perp} }  \in \mathcal{L}\left(U^{\perp}\right)\) is self-adjoint.
-
-    After:
-      Self-adjoint operators and invariant subspaces<br><br>Suppose \(T \in \mathcal{L}(V)\) is self-adjoint and \(U\) is a subspace of \(V\) that is invariant under \(T\). Then<br><br>(a) \(U^{\perp}\) is invariant under \(T\);<br><br>(b) \(\left.T\right|_{U} \in \mathcal{L}(U)\) is self-adjoint;<br><br>(c) \(\left.T\right|_{U^{\perp} }  \in \mathcal{L}\left(U^{\perp}\right)\) is self-adjoint.
-
-============================================================
-
-Note ID: 1702207086330
-  Field: Text
-    Before:
-      <img src="paste-347f8df15eca2c89ed5a9727d6b582e0021fda62.jpg"><br>What are the broad steps of the proof ?<br><ul><li>Make non-lin-indp list via powers of Tv for v not 0</li><li>Decompose into degree-1 and degree-2 polynomials&nbsp;</li><ul><li>The degree-2 polynomials must by-definition have b^2 &lt; 4ac since otherwise they would have been broken down into degree 1 polynomials</li></ul></ul><br>
-
-    After:
-      <img src="paste-347f8df15eca2c89ed5a9727d6b582e0021fda62.jpg"><br>What are the broad steps of the proof ?<br><ul><li>Make non-lin-indp list via powers of Tv for v not 0</li><li>Decompose into degree-1 and degree-2 polynomials&nbsp;</li><ul><li>The degree-2 polynomials must by-definition have b^2 &lt; 4ac since otherwise they would have been broken down into degree 1 polynomials</li></ul></ul><br>
-
-============================================================
-
-Note ID: 1702209131622
-  Field: Text
-    Before:
-      9 Suppose \(V\) is a complex inner product space. Prove that every normal operator on \(V\) has a square root.<br><ul><li>&nbsp;(An operator \(S \in \mathcal{L}(V)\) is called a square root of \(T \in \mathcal{L}(V)\) if \(S^{2}\) = \(T\).)</li></ul>
-
-    After:
-      9 Suppose \(V\) is a complex inner product space. Prove that every normal operator on \(V\) has a square root.<br><ul><li>&nbsp;(An operator \(S \in \mathcal{L}(V)\) is called a square root of \(T \in \mathcal{L}(V)\) if \(S^{2}\) = \(T\).)</li></ul>
-
-============================================================
-
-Note ID: 1702209220210
-  Field: Text
-    Before:
-      12 Suppose \(T \in \mathcal{L}(V)\) is self-adjoint, \(\lambda \in \mathbf{F}\), and \(\epsilon&gt;0\). Suppose there exists \(v \in V\) such that \(\|v\|=1\) and<br><br><ul><li>\(\|T v-\lambda v\|\)&lt; \(\epsilon .\)</li></ul><br>Prove that \(T\) has an eigenvalue \(\lambda^{\prime}\) such that:<br><ul><li>&nbsp;\(\left|\lambda-\lambda^{\prime}\right|\) &lt; \(\epsilon\).</li></ul>
-
-    After:
-      12 Suppose \(T \in \mathcal{L}(V)\) is self-adjoint, \(\lambda \in \mathbf{F}\), and \(\epsilon&gt;0\). Suppose there exists \(v \in V\) such that \(\|v\|=1\) and<br><br><ul><li>\(\|T v-\lambda v\|\)&lt; \(\epsilon .\)</li></ul><br>Prove that \(T\) has an eigenvalue \(\lambda^{\prime}\) such that:<br><ul><li>&nbsp;\(\left|\lambda-\lambda^{\prime}\right|\) &lt; \(\epsilon\).</li></ul>
-
-============================================================
-
-Note ID: 1702209350114
-  Field: Text
-    Before:
-      <br>Definition positive operator<br><br>An operator \(T \in \mathcal{L}(V)\) is called positive if \(T\) is self-adjoint and<br><br><ul><li>\(\langle T v, v\rangle\)&nbsp; \(\geq\) \(0\)</li><li>for all \(v \in V\).</li></ul>
-
-    After:
-      <br>Definition positive operator<br><br>An operator \(T \in \mathcal{L}(V)\) is called positive if \(T\) is self-adjoint and<br><br><ul><li>\(\langle T v, v\rangle\)&nbsp; \(\geq\) \(0\)</li><li>for all \(v \in V\).</li></ul>
-
-============================================================
-
-Note ID: 1702209470978
-  Field: Text
-    Before:
-      <br>Definition square root<br><br>An operator \(R\) is called a square root of an operator \(T\) if \(R^{2}\) = \(T\).
-
-    After:
-      <br>Definition square root<br><br>An operator \(R\) is called a square root of an operator \(T\) if \(R^{2}\) = \(T\).
-
-============================================================
-
-Note ID: 1702209525930
-  Field: Text
-    Before:
-      The positive operators correspond to the numbers \([0, \infty)\), so better terminology would use the term nonnegative instead of positive. However, operator theorists consistently call these the positive operators, so we will follow that custom.
-
-    After:
-      The positive operators correspond to the numbers \([0, \infty)\), so better terminology would use the term nonnegative instead of positive. However, operator theorists consistently call these the positive operators, so we will follow that custom.
-
-============================================================
-
-Note ID: 1702209741686
-  Field: Text
-    Before:
-      Characterization of positive operators<br><br>Let \(T \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(c) \(T\) has a positive square root;<br></li><li><br></li><li>(d) \(T\) has a self-adjoint square root;</li></ul>
-
-    After:
-      Characterization of positive operators<br><br>Let \(T \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(c) \(T\) has a positive square root;<br></li><li><br></li><li>(d) \(T\) has a self-adjoint square root;</li></ul>
-
-============================================================
-
-Note ID: 1702210314559
-  Field: Text
-    Before:
-      Each nonnegative number has a unique nonnegative square root.
-
-    After:
-      Each nonnegative number has a unique nonnegative square root.
-
-============================================================
-
-Note ID: 1702210345950
-  Field: Text
-    Before:
-      Each positive operator has only one positive square root<br><br>Every positive operator on \(V\) has a unique positive square root.<br>
-
-    After:
-      Each positive operator has only one positive square root<br><br>Every positive operator on \(V\) has a unique positive square root.<br>
-
-============================================================
-
-Note ID: 1702210371566
-  Field: Text
-    Before:
-      <br>Some mathematicians also use the term positive semidefinite operator, which means the same as positive operator.
-
-    After:
-      <br>Some mathematicians also use the term positive semidefinite operator, which means the same as positive operator.
-
-============================================================
-
-Note ID: 1702210860335
-  Field: Text
-    Before:
-      Definition isometry<br><br>- An operator \(S \in \mathcal{L}(V)\) is called an isometry if<br><br><ul><li>\(\|S v\|\) = \(\|v\|\)</li><li>for all \(v \in V\).</li></ul><br>In other words, an operator is an isometry if it preserves norms.<br>
-
-    After:
-      Definition isometry<br><br>- An operator \(S \in \mathcal{L}(V)\) is called an isometry if<br><br><ul><li>\(\|S v\|\) = \(\|v\|\)</li><li>for all \(v \in V\).</li></ul><br>In other words, an operator is an isometry if it preserves norms.<br>
-
-============================================================
-
-Note ID: 1702211338609
-  Field: Text
-    Before:
-      Characterization of isometries<br><br>Suppose \(S \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(S\) is an isometry;</li><li>(d) there exists an orthonormal basis \(e_{1}, \ldots, e_{n}\) of \(V\) such that \(S e_{1}, \ldots, S e_{n}\) is orthonormal;</li></ul>
-
-    After:
-      Characterization of isometries<br><br>Suppose \(S \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(S\) is an isometry;</li><li>(d) there exists an orthonormal basis \(e_{1}, \ldots, e_{n}\) of \(V\) such that \(S e_{1}, \ldots, S e_{n}\) is orthonormal;</li></ul>
-
-============================================================
-
-Note ID: 1702211356480
-  Field: Text
-    Before:
-      Characterization of isometries<br><br>Suppose \(S \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(S\) is an isometry;</li><li>(e) \(S^{*} S\) = \(I\);</li><li>(f) \(S S^{*}\) = \(I\);</li></ul>
-
-    After:
-      Characterization of isometries<br><br>Suppose \(S \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(S\) is an isometry;</li><li>(e) \(S^{*} S\) = \(I\);</li><li>(f) \(S S^{*}\) = \(I\);</li></ul>
-
-============================================================
-
-Note ID: 1702211390938
-  Field: Text
-    Before:
-      Characterization of isometries<br><br>Suppose \(S \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(S\) is an isometry;</li><li>(g) \(S^{*}\) is an isometry;</li><li>(h) \(S\) is invertible and \(S^{-1}\) = \(S^{*}\).</li></ul>
-
-    After:
-      Characterization of isometries<br><br>Suppose \(S \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(S\) is an isometry;</li><li>(g) \(S^{*}\) is an isometry;</li><li>(h) \(S\) is invertible and \(S^{-1}\) = \(S^{*}\).</li></ul>
-
-============================================================
-
-Note ID: 1702211878776
-  Field: Text
-    Before:
-      <ul><li>Every isometry is normal<br></li><li>Thus characterization of normal operators can be used to give descriptions of isometries</li></ul>
-
-    After:
-      <ul><li>Every isometry is normal<br></li><li>Thus characterization of normal operators can be used to give descriptions of isometries</li></ul>
-
-============================================================
-
-Note ID: 1702212316004
-  Field: Text
-    Before:
-      Suppose \(T \in \mathcal{L}(V, W)\). Prove that \(T^{*}\) \(\circ\) \(T\) is a positive operator on \(V\) and \(T\) \(\circ\) \(T^{*}\) is a positive operator on \(W\).
-
-    After:
-      Suppose \(T \in \mathcal{L}(V, W)\). Prove that \(T^{*}\) \(\circ\) \(T\) is a positive operator on \(V\) and \(T\) \(\circ\) \(T^{*}\) is a positive operator on \(W\).
-
-============================================================
-
-Note ID: 1702212326595
-  Field: Text
-    Before:
-      5 Prove that the sum of two positive operators on \(V\) is positive.
-
-    After:
-      5 Prove that the sum of two positive operators on \(V\) is positive.
-
-============================================================
-
-Note ID: 1702212362887
-  Field: Text
-    Before:
-      Suppose \(T \in \mathcal{L}(V)\) is positive. <br>Prove that \(T^{k}\) is positive for every positive integer \(k\).
-
-    After:
-      Suppose \(T \in \mathcal{L}(V)\) is positive. <br>Prove that \(T^{k}\) is positive for every positive integer \(k\).
-
-============================================================
-
-Note ID: 1702212593662
-  Field: Text
-    Before:
-      10 Suppose \(S \in \mathcal{L}(V)\). Prove that the following are equivalent:<br><br><ul><li>(a) \(S\) is an isometry;</li><li>(c) \(S^{*} e_{1}, \ldots, S^{*} e_{m}\) is an orthonormal list for every orthonormal list of vectors \(e_{1}, \ldots, e_{m}\) in \(V\);</li></ul>
-
-    After:
-      10 Suppose \(S \in \mathcal{L}(V)\). Prove that the following are equivalent:<br><br><ul><li>(a) \(S\) is an isometry;</li><li>(c) \(S^{*} e_{1}, \ldots, S^{*} e_{m}\) is an orthonormal list for every orthonormal list of vectors \(e_{1}, \ldots, e_{m}\) in \(V\);</li></ul>
-
-============================================================
-
-Note ID: 1702213237454
-  Field: Text
-    Before:
-      <img src="paste-c7360bede1694146732269f8d646b388bb42064d.jpg"><br>Proof First suppose (c) holds.<br>The matrix of \(T^*\) is obtained by taking the conjugate transpose of the matrix of \(T\); hence \(T^*\) also has a diagonal matrix.<br><br>Any two diagonal matrices commute; thus \(T\) commutes with \(T^*\).<br><br>Thus \(T\) is normal. In other words, (a) holds.
-
-    After:
-      <img src="paste-c7360bede1694146732269f8d646b388bb42064d.jpg"><br>Proof First suppose (c) holds.<br>The matrix of \(T^*\) is obtained by taking the conjugate transpose of the matrix of \(T\); hence \(T^*\) also has a diagonal matrix.<br><br>Any two diagonal matrices commute; thus \(T\) commutes with \(T^*\).<br><br>Thus \(T\) is normal. In other words, (a) holds.
-
-============================================================
-
-Note ID: 1702215912788
-  Field: Text
-    Before:
-      - If \(\mathcal{M}(T)\) is a diagonal matrix with nonnegative entries on the diagonal, then \(T\) is a positive operator.
-
-    After:
-      - If \(\mathcal{M}(T)\) is a diagonal matrix with nonnegative entries on the diagonal, then \(T\) is a positive operator.
-
-============================================================
-
-Note ID: 1702220190504
-  Field: Text
-    Before:
-      <img src="paste-0629f4e38b9bf7aa8b7c49773aaa24db716252dc.jpg"><br>Proof Suppose (a) holds, so \(S\) is an isometry. By the Complex Spectral Theorem, there is an orthonormal basis \(e_1, \ldots, e_n\) of \(V\) consisting of eigenvectors of \(S\). For \(j \in\{1, \ldots, n\}\), let \(\lambda_j\) be the eigenvalue corresponding to \(e_j\). Then<br><ul><li>\(\left|\lambda_j\right|\)</li><li>=&nbsp;\(\left\|\lambda_j e_j\right\|\) because the norm of orthonormal vectors is 1</li><li>= \(\left\|S e_j\right\|\) because eigenvector</li><li>=&nbsp;\(\left\|e_j\right\|\) because isometry</li><li>= 1 because norm of orthonormal vectors is 1</li></ul><div>Thus each eigenvalue of S has absolute<br></div>value 1. Hence (b) holds.
-
-    After:
-      <img src="paste-0629f4e38b9bf7aa8b7c49773aaa24db716252dc.jpg"><br>Proof Suppose (a) holds, so \(S\) is an isometry. By the Complex Spectral Theorem, there is an orthonormal basis \(e_1, \ldots, e_n\) of \(V\) consisting of eigenvectors of \(S\). For \(j \in\{1, \ldots, n\}\), let \(\lambda_j\) be the eigenvalue corresponding to \(e_j\). Then<br><ul><li>\(\left|\lambda_j\right|\)</li><li>=&nbsp;\(\left\|\lambda_j e_j\right\|\) because the norm of orthonormal vectors is 1</li><li>= \(\left\|S e_j\right\|\) because eigenvector</li><li>=&nbsp;\(\left\|e_j\right\|\) because isometry</li><li>= 1 because norm of orthonormal vectors is 1</li></ul><div>Thus each eigenvalue of S has absolute<br></div>value 1. Hence (b) holds.
-
-============================================================
-
-Note ID: 1702394082186
-  Field: Text
-    Before:
-      Self-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.
+      An original encoder block does the following operations:<br><ol>

=== Pair #4 ===
Content differs beyond just cloze markup!
Differences (normalized text):
--- before (normalized)
+++ after (normalized)
@@ -1,121 +1,8 @@
-      An original encoder block does the following operations:<br><ol><li>x = positional_encoding(input)</li><li>prev =x</li><li>x = Multi-headed Attention (MHA)(x)</li><li>x = prev + x</li><li>x = LayerNorm(x)</li><li>prev = x</li><li>x = FeedForward(x)</li><li>x = prev + x</li><li>x = LayerNorm(x)</li></ol>
-
-    After:
-      An original encoder block does the following operations:<br><ol><li>x = positional_encoding(input)</li><li>prev =x</li><li>x = Multi-headed Attention (MHA)(x)</li><li>x = prev + x</li><li>x = LayerNorm(x)</li><li>prev = x</li><li>x = FeedForward(x)</li><li>x = prev + x</li><li>x = LayerNorm(x)</li></ol>
+      An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.
 
 ============================================================
 
-Note ID: 1702398919559
+Note ID: 1702570136805
   Field: Text
     Before:
-      The transformer's decoder can be summarised as follows:<br><ol><li></li><li>x = positional_encoding(right_shifted_input)<br></li><li>prev = x<br></li><li>x = MHA(x)<br></li><li>x = prev+x<br></li><li>x = LayerNorm(x)<br></li><li>x = MHA( encoder(input), encoder(input), x)<br></li><li>x = prev + x<br></li><li>x = LayerNorm(x)<br></li><li>prev = x<br></li><li>x = FeedForward(x)<br></li><li>x = prev + x<br></li><li>x = LayerNorm(x)</li></ol>
-
-    After:
-      The transformer's decoder can be summarised as follows:<br><ol><li></li><li>x = positional_encoding(right_shifted_input)<br></li><li>prev = x<br></li><li>x = MHA(x)<br></li><li>x = prev+x<br></li><li>x = LayerNorm(x)<br></li><li>x = MHA( encoder(input), encoder(input), x)<br></li><li>x = prev + x<br></li><li>x = LayerNorm(x)<br></li><li>prev = x<br></li><li>x = FeedForward(x)<br></li><li>x = prev + x<br></li><li>x = LayerNorm(x)</li></ol>
-
-============================================================
-
-Note ID: 1702398951276
-  Field: Text
-    Before:
-      The transformer's decoder can be summarised as follows:<br><ol><li></li><li>x = positional_encoding(right_shifted_input)<br></li><li>prev = x<br></li><li>x = MHA(x)<br></li><li>x = prev+x<br></li><li>x = LayerNorm(x)<br></li><li>x = MHA( encoder(input), encoder(input), x)<br></li><li>x = prev + x<br></li><li>x = LayerNorm(x)<br></li><li>prev = x<br></li><li>x = FeedForward(x)<br></li><li>x = prev + x<br></li><li>x = LayerNorm(x)</li></ol>
-
-    After:
-      The transformer's decoder can be summarised as follows:<br><ol><li></li><li>x = positional_encoding(right_shifted_input)<br></li><li>prev = x<br></li><li>x = MHA(x)<br></li><li>x = prev+x<br></li><li>x = LayerNorm(x)<br></li><li>x = MHA( encoder(input), encoder(input), x)<br></li><li>x = prev + x<br></li><li>x = LayerNorm(x)<br></li><li>prev = x<br></li><li>x = FeedForward(x)<br></li><li>x = prev + x<br></li><li>x = LayerNorm(x)</li></ol>
-
-============================================================
-
-Note ID: 1702399018840
-  Field: Text
-    Before:
-      The self-attention sub-layer in the decoder stack is modified compared to the encoder to prevent positions from attending to subsequent positions via masking
-
-    After:
-      The self-attention sub-layer in the decoder stack is modified compared to the encoder to prevent positions from attending to subsequent positions via masking
-
-============================================================
-
-Note ID: 1702545421830
-  Field: Text
-    Before:
-      An operator T is normal iff :<br><ul><li>\(T T^*\) =&nbsp;\(T^* T\)<br></li><li>so every self-adjoint operator is normal by default</li></ul>
-
-    After:
-      An operator T is normal iff :<br><ul><li>\(T T^*\) =&nbsp;\(T^* T\)<br></li><li>so every self-adjoint operator is normal by default</li></ul>
-
-============================================================
-
-Note ID: 1702545541381
-  Field: Text
-    Before:
-      An operator is also normal, besides the full definition, iff:<br><ul><li>\(&nbsp; \vert \vert\)&nbsp; \(Tv\)&nbsp;\(&nbsp; \vert \vert\)&nbsp; = \(&nbsp; \vert \vert\)&nbsp; \(T^*v\)&nbsp;\(&nbsp; \vert \vert\)</li></ul>
-
-    After:
-      An operator is also normal, besides the full definition, iff:<br><ul><li>\(&nbsp; \vert \vert\)&nbsp; \(Tv\)&nbsp;\(&nbsp; \vert \vert\)&nbsp; = \(&nbsp; \vert \vert\)&nbsp; \(T^*v\)&nbsp;\(&nbsp; \vert \vert\)</li></ul>
-
-============================================================
-
-Note ID: 1702545958895
-  Field: Text
-    Before:
-      For an arbitrary operator T and&nbsp;\(\lambda \in F\):<br><ul><li>&nbsp;\(\lambda\) is an eigenvalue of T iff&nbsp;\(\bar{\lambda}\) is an eigenvalue of&nbsp;\(T^*\)</li></ul>
-
-    After:
-      For an arbitrary operator T and&nbsp;\(\lambda \in F\):<br><ul><li>&nbsp;\(\lambda\) is an eigenvalue of T iff&nbsp;\(\bar{\lambda}\) is an eigenvalue of&nbsp;\(T^*\)</li></ul>
-
-============================================================
-
-Note ID: 1702547032646
-  Field: Text
-    Before:
-      Proof that for normal operators eigenvectors corresponding to different eigenvalues are orthogonal:<br><ul><li>Take two pairs of eigenvectors-eigenvalues for T and its adjoint</li><li>&nbsp;\(T v\) = \(\alpha v\)</li><li>\(T^* u\) =&nbsp;\(\beta u\)<br></li><li>(\(\alpha - \beta\)) \(\langle\) \(u,v\) \(\rangle\)<br></li><ul><li>= \(\langle\) \(\alpha u,v\) \(\rangle\) -&nbsp;&nbsp;\(\langle\) \(u, \bar{\beta} v\) \(\rangle\) due to distributivity and conjugate symmetry</li><li>= \(\langle\) \(Tu,v\) \(\rangle\) -&nbsp;&nbsp;\(\langle\) \(u, T^* v\) \(\rangle\) due to eigenvectors</li><li>= \(\langle\) \(Tu,v\) \(\rangle\) -&nbsp;&nbsp;\(\langle\) \(Tu, v\) \(\rangle\) due to adjoint definition</li><li>&nbsp;= 0</li></ul><li>Thus the vectors must have been orthogonal since the eigenvalues were distinct</li></ul>
-
-    After:
-      Proof that for normal operators eigenvectors corresponding to different eigenvalues are orthogonal:<br><ul><li>Take two pairs of eigenvectors-eigenvalues for T and its adjoint</li><li>&nbsp;\(T v\) = \(\alpha v\)</li><li>\(T^* u\) =&nbsp;\(\beta u\)<br></li><li>(\(\alpha - \beta\)) \(\langle\) \(u,v\) \(\rangle\)<br></li><ul><li>= \(\langle\) \(\alpha u,v\) \(\rangle\) -&nbsp;&nbsp;\(\langle\) \(u, \bar{\beta} v\) \(\rangle\) due to distributivity and conjugate symmetry</li><li>= \(\langle\) \(Tu,v\) \(\rangle\) -&nbsp;&nbsp;\(\langle\) \(u, T^* v\) \(\rangle\) due to eigenvectors</li><li>= \(\langle\) \(Tu,v\) \(\rangle\) -&nbsp;&nbsp;\(\langle\) \(Tu, v\) \(\rangle\) due to adjoint definition</li><li>&nbsp;= 0</li></ul><li>Thus the vectors must have been orthogonal since the eigenvalues were distinct</li></ul>
-
-============================================================
-
-Note ID: 1702547445165
-  Field: Text
-    Before:
-      For an operator T with eigenvalues \(\lambda_i\)&nbsp;the following are equivalent:<br><ul><li>T has a diagonalizing basis</li><li>V =&nbsp;\(\oplus_{i=1}^n\)&nbsp;\(U_i\) each of which is is invariant under T</li><li>V =&nbsp;\(\oplus_{i=1}^n\)&nbsp;\(\mathrm{null}(T-\lambda_i I)\)&nbsp;</li><li>dim V =&nbsp;\(\sum_{i=1}^n\) \(\mathrm{dim} \,\mathrm{null} (T-\lambda_i I)\)</li></ul>
-
-    After:
-      For an operator T with eigenvalues \(\lambda_i\)&nbsp;the following are equivalent:<br><ul><li>T has a diagonalizing basis</li><li>V =&nbsp;\(\oplus_{i=1}^n\)&nbsp;\(U_i\) each of which is is invariant under T</li><li>V =&nbsp;\(\oplus_{i=1}^n\)&nbsp;\(\mathrm{null}(T-\lambda_i I)\)&nbsp;</li><li>dim V =&nbsp;\(\sum_{i=1}^n\) \(\mathrm{dim} \,\mathrm{null} (T-\lambda_i I)\)</li></ul>
-
-============================================================
-
-Note ID: 1702556807253
-  Field: Text
-    Before:
-      <br>To prove the real spectral theorem we use an induction on V<br><ul><li>Use an inductive technique, shwoing that it is self-evident for dim V = 1</li><li>For other ones decompose the vector space into an eigenvector, which exists because of self-adjointness,&nbsp; and its orthogonal complement</li><li>Show that the orthogonal complement has an orthonormal basis formed of eigenvectors by showing that it is self-adjoint and using the I.H</li><li>Since the extra vector is orthogonal to all vectors in the orthogonal complement, just concatenate it to get a new orthonormal eigenbasis</li></ul>
-
-    After:
-      <br>To prove the real spectral theorem we use an induction on V<br><ul><li>Use an inductive technique, shwoing that it is self-evident for dim V = 1</li><li>For other ones decompose the vector space into an eigenvector, which exists because of self-adjointness,&nbsp; and its orthogonal complement</li><li>Show that the orthogonal complement has an orthonormal basis formed of eigenvectors by showing that it is self-adjoint and using the I.H</li><li>Since the extra vector is orthogonal to all vectors in the orthogonal complement, just concatenate it to get a new orthonormal eigenbasis</li></ul>
-
-============================================================
-
-Note ID: 1702557494110
-  Field: Text
-    Before:
-      Suppose V is a 2-dimensional inner product space over R and T is an operator. Then the following are equivalent:<br><ul><li>T is normal but not self-adjoint</li><li>The matirix of T is of the form &nbsp;\(\left[\begin{array}{cc}a &amp; -b \\ b &amp; a\end{array}\right]\) for any orthonormal basis with b != 0</li><li>There exists an orthonormal basis such that the matrix is of the previous form with b != 0</li></ul>
-
-    After:
-      Suppose V is a 2-dimensional inner product space over R and T is an operator. Then the following are equivalent:<br><ul><li>T is normal but not self-adjoint</li><li>The matirix of T is of the form &nbsp;\(\left[\begin{array}{cc}a &amp; -b \\ b &amp; a\end{array}\right]\) for any orthonormal basis with b != 0</li><li>There exists an orthonormal basis such that the matrix is of the previous form with b != 0</li></ul>
-
-============================================================
-
-Note ID: 1702564689508
-  Field: Text
-    Before:
-      Self-attention<span style="font-weight:600">, sometimes called&nbsp;</span>intra-attention<span style="font-weight:600">&nbsp;is an&nbsp;</span>attention<span style="font-weight:600">&nbsp;mechanism&nbsp;</span>relating different positions<span style="font-weight:600">&nbsp;of&nbsp;</span>a single sequence<span style="font-weight:600">&nbsp;in order to compute&nbsp;</span>a representation of the sequence.
-
-    After:
-      Self-attention<span style="font-weight:600">, sometimes called&nbsp;</span>intra-attention<span style="font-weight:600">&nbsp;is an&nbsp;</span>attention<span style="font-weight:600">&nbsp;mechanism&nbsp;</span>relating different positions<span style="font-weight:600">&nbsp;of&nbsp;</span>a single sequence<span style="font-weight:600">&nbsp;in order to compute&nbsp;</span>a representation of the sequence.
-
-============================================================
-
-Note ID: 1702565340841
-  Field: Text
-    Before:
-      An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.
+      <ol><li>\( \operatorname{MultiHeadAttn}\) ( \(Q, K, V\))&nbsp; = Concat ( \(\text {head }1, \ldots, \text { head }{\mathrm{h}} \) )&nbsp; \(W^O \)</li><li>head \(_i\) = \(\operatorname{Attention}\) ( \(Q W_i^Q, K W_i^K, V W_i^V\))&nbsp;</li><li>The projections are simple parameter matrices

=== Pair #5 ===
Content differs beyond just cloze markup!
Differences (normalized text):
--- before (normalized)
+++ after (normalized)
@@ -1,11 +1,8 @@
-      <ol><li>\( \operatorname{MultiHeadAttn}\) ( \(Q, K, V\))&nbsp; = Concat ( \(\text {head }1, \ldots, \text { head }{\mathrm{h}} \) )&nbsp; \(W^O \)</li><li>head \(_i\) = \(\operatorname{Attention}\) ( \(Q W_i^Q, K W_i^K, V W_i^V\))&nbsp;</li><li>The projections are simple parameter matrices<ol><li>\(W_i^Q\) \(\in\) \( \mathbb{R}^{d_{\text {model } } \times d_k}\)</li><li>\(W_i^K\) \(\in\) \(\mathbb{R}^{d_{\text {model } }  \times d_k}\)</li><li>\(W_i^V\) \(\in\) \(\mathbb{R}^{d_{\text {model } }  \times d_v}\)</li><li>\(W^O\) \(\in\) \(\mathbb{R}^{h d_v \times d_{\text {model &nbsp;} } }\)</li></ol></li></ol>
-
-    After:
-      <ol><li>\( \operatorname{MultiHeadAttn}\) ( \(Q, K, V\))&nbsp; = Concat ( \(\text {head }1, \ldots, \text { head }{\mathrm{h}} \) )&nbsp; \(W^O \)</li><li>head \(_i\) = \(\operatorname{Attention}\) ( \(Q W_i^Q, K W_i^K, V W_i^V\))&nbsp;</li><li>The projections are simple parameter matrices<ol><li>\(W_i^Q\) \(\in\) \( \mathbb{R}^{d_{\text {model } } \times d_k}\)</li><li>\(W_i^K\) \(\in\) \(\mathbb{R}^{d_{\text {model } }  \times d_k}\)</li><li>\(W_i^V\) \(\in\) \(\mathbb{R}^{d_{\text {model } }  \times d_v}\)</li><li>\(W^O\) \(\in\) \(\mathbb{R}^{h d_v \times d_{\text {model &nbsp;} } }\)</li></ol></li></ol>
+      The connections between the encoder and decoder passes through an attention layer which is not<b>&nbsp;</b>a<b>&nbsp;</b>self-attention<b>&nbsp;</b>layer. Rather it is an attention layer between different sequences.
 
 ============================================================
 
-Note ID: 1702571374309
+Note ID: 1702571923436
   Field: Text
     Before:
-      The connections between the encoder and decoder passes through an attention layer which is not<b>&nbsp;</b>a<b>&nbsp;</b>self-attention<b>&nbsp;</b>layer. Rather it is an attention layer between different sequences.
+      To do the final prediction, the original Transformer:<br><ul><li>Take the decoder outputs after the last block<br></li><li>Apply a linear layer to them

=== Pair #6 ===
Content differs beyond just cloze markup!
Differences (normalized text):
--- before (normalized)
+++ after (normalized)
@@ -1,281 +1,9 @@
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">EncoderDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="sd">"""</span><br><span class="sd">    A standard Encoder-Decoder architecture. Base for this and many</span><br><span class="sd">    other models.</span><br><span class="sd">    """</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">src_embed</span><span class="p">,</span> <span class="n">tgt_embed</span><span class="p">,</span> <span class="n">generator</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">src_embed</span> <span class="o">=</span> <span class="n">src_embed</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">tgt_embed</span> <span class="o">=</span> <span class="n">tgt_embed</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">generator</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span><br>        <span class="s2">"Take in and process masked src and target sequences."</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">),</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">):</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">src_embed</span><span class="p">(</span><span class="n">src</span><span class="p">)</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tgt_embed</span><span class="p">(</span><span class="n">tgt</span><span class="p">),</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
-
-    After:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">EncoderDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="sd">"""</span><br><span class="sd">    A standard Encoder-Decoder architecture. Base for this and many</span><br><span class="sd">    other models.</span><br><span class="sd">    """</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">src_embed</span><span class="p">,</span> <span class="n">tgt_embed</span><span class="p">,</span> <span class="n">generator</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">src_embed</span> <span class="o">=</span> <span class="n">src_embed</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">tgt_embed</span> <span class="o">=</span> <span class="n">tgt_embed</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">generator</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span><br>        <span class="s2">"Take in and process masked src and target sequences."</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">),</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">):</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">src_embed</span><span class="p">(</span><span class="n">src</span><span class="p">)</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tgt_embed</span><span class="p">(</span><span class="n">tgt</span><span class="p">),</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
+      18 Suppose \(T \in \mathcal{L}(V)\). Let \(\hat{s}\) denote the smallest singular value of \(T\), and let \(s\) denote the largest singular value of \(T\).<br><ul><li>(b) Suppose \(\lambda\) is an eigenvalue of \(T\). Prove that \(\hat{s}\) \(\leq\) \(|\lambda|\) \(\leq\) \(s\).<br></li></ul>
 
 ============================================================
 
-Note ID: 1702629245485
+Note ID: 1702838148958
   Field: Text
     Before:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">EncoderDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="sd">"""</span><br><span class="sd">    A standard Encoder-Decoder architecture. Base for this and many</span><br><span class="sd">    other models.</span><br><span class="sd">    """</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">src_embed</span><span class="p">,</span> <span class="n">tgt_embed</span><span class="p">,</span> <span class="n">generator</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">src_embed</span> <span class="o">=</span> <span class="n">src_embed</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">tgt_embed</span> <span class="o">=</span> <span class="n">tgt_embed</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">generator</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span><br>        <span class="s2">"Take in and process masked src and target sequences."</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">),</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">):</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">src_embed</span><span class="p">(</span><span class="n">src</span><span class="p">),</span> <span class="n">src_mask</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tgt_embed</span><span class="p">(</span><span class="n">tgt</span><span class="p">),</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
-
-    After:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">EncoderDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="sd">"""</span><br><span class="sd">    A standard Encoder-Decoder architecture. Base for this and many</span><br><span class="sd">    other models.</span><br><span class="sd">    """</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">src_embed</span><span class="p">,</span> <span class="n">tgt_embed</span><span class="p">,</span> <span class="n">generator</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">src_embed</span> <span class="o">=</span> <span class="n">src_embed</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">tgt_embed</span> <span class="o">=</span> <span class="n">tgt_embed</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">generator</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span><br>        <span class="s2">"Take in and process masked src and target sequences."</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">),</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">):</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">src_embed</span><span class="p">(</span><span class="n">src</span><span class="p">),</span> <span class="n">src_mask</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tgt_embed</span><span class="p">(</span><span class="n">tgt</span><span class="p">),</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
-
-============================================================
-
-Note ID: 1702629797899
-  Field: Text
-    Before:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Encoder is made up of self-attn and feed forward (defined below)"</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">feed_forward</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span><br>        <span class="s2">"Follow Figure 1 (left) for connections."</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">))</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
-
-    After:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Encoder is made up of self-attn and feed forward (defined below)"</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">feed_forward</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span><br>        <span class="s2">"Follow Figure 1 (left) for connections."</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">))</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
-
-============================================================
-
-Note ID: 1702629840290
-  Field: Text
-    Before:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Encoder is made up of self-attn and feed forward (defined below)"</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">feed_forward</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span><br>        <span class="s2">"Follow Figure 1 (left) for connections."</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">))</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
-
-    After:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Encoder is made up of self-attn and feed forward (defined below)"</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">feed_forward</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span><br>        <span class="s2">"Follow Figure 1 (left) for connections."</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">))</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
-
-============================================================
-
-Note ID: 1702630130628
-  Field: Text
-    Before:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">src_attn</span><span class="p">,</span> <span class="n">feed_forward</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span> <span class="o">=</span> <span class="n">src_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">),</span> <span class="mi">3</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span><br>        <span class="s2">"Follow Figure 1 (right) for connections."</span><br>        <span class="n">m</span> <span class="o">=</span> <span class="n">memory</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">))</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">))</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
-
-    After:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">src_attn</span><span class="p">,</span> <span class="n">feed_forward</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span> <span class="o">=</span> <span class="n">src_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">),</span> <span class="mi">3</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span><br>        <span class="s2">"Follow Figure 1 (right) for connections."</span><br>        <span class="n">m</span> <span class="o">=</span> <span class="n">memory</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">))</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">))</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
-
-============================================================
-
-Note ID: 1702630145721
-  Field: Text
-    Before:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">src_attn</span><span class="p">,</span> <span class="n">feed_forward</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span> <span class="o">=</span> <span class="n">src_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">),</span> <span class="mi">3</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span><br>        <span class="s2">"Follow Figure 1 (right) for connections."</span><br>        <span class="n">m</span> <span class="o">=</span> <span class="n">memory</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">))</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">))</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
-
-    After:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">src_attn</span><span class="p">,</span> <span class="n">feed_forward</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span> <span class="o">=</span> <span class="n">src_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">),</span> <span class="mi">3</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span><br>        <span class="s2">"Follow Figure 1 (right) for connections."</span><br>        <span class="n">m</span> <span class="o">=</span> <span class="n">memory</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">))</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">))</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
-
-============================================================
-
-Note ID: 1702630171744
-  Field: Text
-    Before:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">src_attn</span><span class="p">,</span> <span class="n">feed_forward</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span> <span class="o">=</span> <span class="n">src_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">),</span> <span class="mi">3</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span><br>        <span class="s2">"Follow Figure 1 (right) for connections."</span><br>        <span class="n">m</span> <span class="o">=</span> <span class="n">memory</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">))</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">))</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
-
-    After:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">src_attn</span><span class="p">,</span> <span class="n">feed_forward</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span> <span class="o">=</span> <span class="n">src_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">),</span> <span class="mi">3</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span><br>        <span class="s2">"Follow Figure 1 (right) for connections."</span><br>        <span class="n">m</span> <span class="o">=</span> <span class="n">memory</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">))</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">))</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
-
-============================================================
-
-Note ID: 1702630193637
-  Field: Text
-    Before:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">src_attn</span><span class="p">,</span> <span class="n">feed_forward</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span> <span class="o">=</span> <span class="n">src_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">),</span> <span class="mi">3</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span><br>        <span class="s2">"Follow Figure 1 (right) for connections."</span><br>        <span class="n">m</span> <span class="o">=</span> <span class="n">memory</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">))</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">))</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
-
-    After:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">src_attn</span><span class="p">,</span> <span class="n">feed_forward</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span> <span class="o">=</span> <span class="n">src_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">),</span> <span class="mi">3</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span><br>        <span class="s2">"Follow Figure 1 (right) for connections."</span><br>        <span class="n">m</span> <span class="o">=</span> <span class="n">memory</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">))</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">))</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
-
-============================================================
-
-Note ID: 1702630477937
-  Field: Text
-    Before:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span><br>    <span class="s2">"Compute 'Scaled Dot Product Attention'"</span><br>    <br></code></pre></div></td></tr></tbody></table><br>
-
-    After:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span><br>    <span class="s2">"Compute 'Scaled Dot Product Attention'"</span><br>    <br></code></pre></div></td></tr></tbody></table><br>
-
-============================================================
-
-Note ID: 1702630650293
-  Field: Text
-    Before:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span><br>    <span class="s2">"Compute 'Scaled Dot Product Attention'"</span><br>    <span class="n">d_k</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><br>    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span><br>    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span><br>        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span><br>    <span class="n">p_attn</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><br>    <span class="k">if</span> <span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span><br>        <span class="n">p_attn</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">p_attn</span><span class="p">)</span><br>    <span class="k">return</span> torch.<span class="n">matmul</span><span class="p">(</span><span class="n">p_attn</span><span class="p">,</span> <span class="n">value</span><span class="p">),</span> <span class="n">p_attn</span><br></code></pre></div></td></tr></tbody></table><br>
-
-    After:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span><br>    <span class="s2">"Compute 'Scaled Dot Product Attention'"</span><br>    <span class="n">d_k</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><br>    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span><br>    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span><br>        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span><br>    <span class="n">p_attn</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><br>    <span class="k">if</span> <span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span><br>        <span class="n">p_attn</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">p_attn</span><span class="p">)</span><br>    <span class="k">return</span> torch.<span class="n">matmul</span><span class="p">(</span><span class="n">p_attn</span><span class="p">,</span> <span class="n">value</span><span class="p">),</span> <span class="n">p_attn</span><br></code></pre></div></td></tr></tbody></table><br>
-
-============================================================
-
-Note ID: 1702630764250
-  Field: Text
-    Before:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">MultiHeadedAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span><br>        <span class="s2">"Take in model size and number of heads."</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadedAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">h</span> <span class="o">==</span> <span class="mi">0</span><br>        <span class="c1"># We assume d_v always equals d_k</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">h</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">h</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">linears</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span> <span class="mi">{{c2</span></code>::4}}<span class="p" style="font-family: Arial;">)</span></pre><pre><code>        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="kc">None</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span><br>        <span class="s2">"Implements Figure 2"</span><br>        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span><br>            <span class="c1"># Same mask applied to all h heads.</span><br>            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><br>        <span class="n">nbatches</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><br><br>        <span class="c1"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k</span><br>        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="p">[</span><br>            <span class="n">lin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">nbatches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><br>            <span class="k">for</span> <span class="n">lin</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">,</span> <span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">))</span><br>        <span class="p">]</span><br><br>        <span class="c1"># 2) Apply attention on all the projected vectors in batch.</span><br>        <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><br>            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><br>        <span class="p">)</span><br><br>        <span class="c1"># 3) "Concat" using a view and apply a final linear.</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><br>            <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><br>            <span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><br>            <span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">nbatches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><br>        <span class="p">)</span><br>        <span class="k">del</span> <span class="n">query</span><br>        <span class="k">del</span> <span class="n">key</span><br>        <span class="k">del</span> <span class="n">value</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
-
-    After:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">MultiHeadedAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span><br>        <span class="s2">"Take in model size and number of heads."</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadedAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">h</span> <span class="o">==</span> <span class="mi">0</span><br>        <span class="c1"># We assume d_v always equals d_k</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">h</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">h</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">linears</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span> <span class="mi">{{c2</span></code>::4}}<span class="p" style="font-family: Arial;">)</span></pre><pre><code>        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="kc">None</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span><br>        <span class="s2">"Implements Figure 2"</span><br>        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span><br>            <span class="c1"># Same mask applied to all h heads.</span><br>            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><br>        <span class="n">nbatches</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><br><br>        <span class="c1"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k</span><br>        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="p">[</span><br>            <span class="n">lin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">nbatches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><br>            <span class="k">for</span> <span class="n">lin</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">,</span> <span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">))</span><br>        <span class="p">]</span><br><br>        <span class="c1"># 2) Apply attention on all the projected vectors in batch.</span><br>        <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><br>            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><br>        <span class="p">)</span><br><br>        <span class="c1"># 3) "Concat" using a view and apply a final linear.</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><br>            <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><br>            <span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><br>            <span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">nbatches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><br>        <span class="p">)</span><br>        <span class="k">del</span> <span class="n">query</span><br>        <span class="k">del</span> <span class="n">key</span><br>        <span class="k">del</span> <span class="n">value</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
-
-============================================================
-
-Note ID: 1702633787686
-  Field: Text
-    Before:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">MultiHeadedAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span><br>        <span class="s2">"Take in model size and number of heads."</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadedAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">h</span> <span class="o">==</span> <span class="mi">0</span><br>        <span class="c1"># We assume d_v always equals d_k</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">h</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">h</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">linears</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span> <span class="mi">4</span><span class="p">)</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="kc">None</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span><br>        <span class="s2">"Implements Figure 2"</span><br>        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span><br>            <span class="c1"># Same mask applied to all h heads.</span><br>            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><br>        <span class="n">nbatches</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><br><br>        <span class="c1"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k</span><br>        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="p">[</span><br>            <span class="n">lin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">nbatches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><br>            <span class="k">for</span> <span class="n">lin</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">,</span> <span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">))</span><br>        <span class="p">]</span><br><br>        <span class="c1"># 2) Apply attention on all the projected vectors in batch.</span><br>        <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><br>            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><br>        <span class="p">)</span><br><br>        <span class="c1"># 3) "Concat" using a view and apply a final linear.</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><br>            <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><br>            <span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><br>            <span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">nbatches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><br>        <span class="p">)</span><br>        <span class="k">del</span> <span class="n">query</span><br>        <span class="k">del</span> <span class="n">key</span><br>        <span class="k">del</span> <span class="n">value</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
-
-    After:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">MultiHeadedAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span><br>        <span class="s2">"Take in model size and number of heads."</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadedAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">h</span> <span class="o">==</span> <span class="mi">0</span><br>        <span class="c1"># We assume d_v always equals d_k</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">h</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">h</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">linears</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span> <span class="mi">4</span><span class="p">)</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="kc">None</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span><br>        <span class="s2">"Implements Figure 2"</span><br>        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span><br>            <span class="c1"># Same mask applied to all h heads.</span><br>            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><br>        <span class="n">nbatches</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><br><br>        <span class="c1"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k</span><br>        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="p">[</span><br>            <span class="n">lin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">nbatches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><br>            <span class="k">for</span> <span class="n">lin</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">,</span> <span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">))</span><br>        <span class="p">]</span><br><br>        <span class="c1"># 2) Apply attention on all the projected vectors in batch.</span><br>        <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><br>            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><br>        <span class="p">)</span><br><br>        <span class="c1"># 3) "Concat" using a view and apply a final linear.</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><br>            <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><br>            <span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><br>            <span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">nbatches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><br>        <span class="p">)</span><br>        <span class="k">del</span> <span class="n">query</span><br>        <span class="k">del</span> <span class="n">key</span><br>        <span class="k">del</span> <span class="n">value</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
-
-============================================================
-
-Note ID: 1702634062132
-  Field: Text
-    Before:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">Embeddings</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">vocab</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">Embeddings</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">lut</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span><br><br></code></pre></div></td></tr></tbody></table><br>
-
-    After:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">Embeddings</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">vocab</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">Embeddings</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">lut</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span><br><br></code></pre></div></td></tr></tbody></table><br>
-
-============================================================
-
-Note ID: 1702634163797
-  Field: Text
-    Before:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Implement the PE function."</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span><br><br>        <span class="c1"># Compute the positional encodings once in log space.</span><br>        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span><br>        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><br>        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><br>            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span><br>        <span class="p">)</span><br>        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span><br>        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span><br>        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">"pe"</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
-
-    After:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Implement the PE function."</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span><br><br>        <span class="c1"># Compute the positional encodings once in log space.</span><br>        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span><br>        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><br>        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><br>            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span><br>        <span class="p">)</span><br>        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span><br>        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span><br>        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">"pe"</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
-
-============================================================
-
-Note ID: 1702634237360
-  Field: Text
-    Before:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Implement the PE function."</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span><br><br>        <span class="c1"># Compute the positional encodings once in log space.</span><br>        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span><br>        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><br>        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><br>            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span><br>        <span class="p">)</span><br>        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span><br>        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span><br>        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">"pe"</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
-
-    After:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Implement the PE function."</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span><br><br>        <span class="c1"># Compute the positional encodings once in log space.</span><br>        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span><br>        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><br>        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><br>            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span><br>        <span class="p">)</span><br>        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span><br>        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span><br>        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">"pe"</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
-
-============================================================
-
-Note ID: 1702634285670
-  Field: Text
-    Before:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Implement the PE function."</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span><br><br>        <span class="c1"># Compute the positional encodings once in log space.</span><br>        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span><br>        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><br>        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><br>            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span><br>        <span class="p">)</span><br>        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span><br>        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span><br>        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">"pe"</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
-
-    After:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Implement the PE function."</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span><br><br>        <span class="c1"># Compute the positional encodings once in log space.</span><br>        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span><br>        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><br>        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><br>            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span><br>        <span class="p">)</span><br>        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span><br>        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span><br>        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">"pe"</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
-
-============================================================
-
-Note ID: 1702634332321
-  Field: Text
-    Before:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Implement the PE function."</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span><br><br>        <span class="c1"># Compute the positional encodings once in log space.</span><br>        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span><br>        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><br>        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><br>            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span><br>        <span class="p">)</span><br>        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span><br>        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span><br>        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">"pe"</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="p">]</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
-
-    After:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Implement the PE function."</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span><br><br>        <span class="c1"># Compute the positional encodings once in log space.</span><br>        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span><br>        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><br>        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><br>            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span><br>        <span class="p">)</span><br>        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span><br>        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span><br>        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">"pe"</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="p">]</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
-
-============================================================
-
-Note ID: 1702635173135
-  Field: Text
-    Before:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">def</span> <span class="nf">make_model</span><span class="p">(</span><br>    <span class="n">src_vocab</span><span class="p">,</span> <span class="n">tgt_vocab</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><br><span class="p">):</span><br>    <span class="s2">"Helper: Construct a model from hyperparameters."</span><br>    <span class="n">c</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><br>    <span class="n">attn</span> <span class="o">=</span> <span class="n">MultiHeadedAttention</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span><br>    <span class="n">ff</span> <span class="o">=</span> <span class="n">PositionwiseFeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span><br>    <span class="n">position</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span><br>    <span class="n">model</span> <span class="o">=</span> <span class="n">EncoderDecoder</span><span class="p">(</span><br>        <span class="n">Encoder</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">c</span><span class="p">(</span><span class="n">attn</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">ff</span><span class="p">),</span> <span class="n">dropout</span><span class="p">),</span> <span class="n">N</span><span class="p">),</span><br>        <span class="n">Decoder</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">c</span><span class="p">(</span><span class="n">attn</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">attn</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">ff</span><span class="p">),</span> <span class="n">dropout</span><span class="p">),</span> <span class="n">N</span><span class="p">),</span><br>        <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">Embeddings</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">src_vocab</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">position</span><span class="p">)),</span><br>        <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">Embeddings</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tgt_vocab</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">position</span><span class="p">)),</span><br>        <span class="n">Generator</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tgt_vocab</span><span class="p">),</span><br>    <span class="p">)</span><br><br>    <span class="c1"># This was important from their code.</span><br>    <span class="c1"># Initialize parameters with Glorot / fan_avg.</span><br>    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span><br>        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span><br>            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><br>    <span class="k">return</span> <span class="n">model</span><br></code></pre></div></td></tr></tbody></table><br>
-
-    After:
-      <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">def</span> <span class="nf">make_model</span><span class="p">(</span><br>    <span class="n">src_vocab</span><span class="p">,</span> <span class="n">tgt_vocab</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><br><span class="p">):</span><br>    <span class="s2">"Helper: Construct a model from hyperparameters."</span><br>    <span class="n">c</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><br>    <span class="n">attn</span> <span class="o">=</span> <span class="n">MultiHeadedAttention</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span><br>    <span class="n">ff</span> <span class="o">=</span> <span class="n">PositionwiseFeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span><br>    <span class="n">position</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span><br>    <span class="n">model</span> <span class="o">=</span> <span class="n">EncoderDecoder</span><span class="p">(</span><br>        <span class="n">Encoder</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">c</span><span class="p">(</span><span class="n">attn</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">ff</span><span class="p">),</span> <span class="n">dropout</span><span class="p">),</span> <span class="n">N</span><span class="p">),</span><br>        <span class="n">Decoder</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">c</span><span class="p">(</span><span class="n">attn</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">attn</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">ff</span><span class="p">),</span> <span class="n">dropout</span><span class="p">),</span> <span class="n">N</span><span class="p">),</span><br>        <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">Embeddings</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">src_vocab</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">position</span><span class="p">)),</span><br>        <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">Embeddings</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tgt_vocab</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">position</span><span class="p">)),</span><br>        <span class="n">Generator</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tgt_vocab</span><span class="p">),</span><br>    <span class="p">)</span><br><br>    <span class="c1"># This was important from their code.</span><br>    <span class="c1"># Initialize parameters with Glorot / fan_avg.</span><br>    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span><br>        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span><br>            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><br>    <span class="k">return</span> <span class="n">model</span><br></code></pre></div></td></tr></tbody></table><br>
-
-============================================================
-
-Note ID: 1702638382651
-  Field: Text
-    Before:
-      <div>Second, an attention&nbsp;<span style="font-style: inherit; font-weight: inherit; color: rgb(179, 106, 226);">decoder</span>&nbsp;does an extra step before producing its output. In order to focus on the parts of the input that are relevant to this decoding time step, the&nbsp;<span style="font-style: inherit; font-weight: inherit; color: rgb(179, 106, 226);">decoder</span>&nbsp;does the following:</div><ol><li>Look at the set of encoder&nbsp;<span style="font-style: inherit; font-weight: inherit; color: rgb(243, 144, 25);">hidden states</span>&nbsp;it received – each&nbsp;encoder hidden state&nbsp;is most associated with a certain word in the input sentence</li><li>Give each&nbsp;hidden state&nbsp;a score</li><li>Multiply each&nbsp;hidden state&nbsp;by its softmaxed score, thus amplifying&nbsp;hidden states&nbsp;with high scores, and drowning out&nbsp;hidden states&nbsp;with low scores</li></ol>
-
-    After:
-      <div>Second, an attention&nbsp;<span style="font-style: inherit; font-weight: inherit; color: rgb(179, 106, 226);">decoder</span>&nbsp;does an extra step before producing its output. In order to focus on the parts of the input that are relevant to this decoding time step, the&nbsp;<span style="font-style: inherit; font-weight: inherit; color: rgb(179, 106, 226);">decoder</span>&nbsp;does the following:</div><ol><li>Look at the set of encoder&nbsp;<span style="font-style: inherit; font-weight: inherit; color: rgb(243, 144, 25);">hidden states</span>&nbsp;it received – each&nbsp;encoder hidden state&nbsp;is most associated with a certain word in the input sentence</li><li>Give each&nbsp;hidden state&nbsp;a score</li><li>Multiply each&nbsp;hidden state&nbsp;by its softmaxed score, thus amplifying&nbsp;hidden states&nbsp;with high scores, and drowning out&nbsp;hidden states&nbsp;with low scores</li></ol>
-
-============================================================
-
-Note ID: 1702645185923
-  Field: Text
-    Before:
-      <span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it&nbsp;</span>creates its Queries matrix<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;</span>from the layer below it<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">, and takes the&nbsp;</span>Keys<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;and&nbsp;</span>Values<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;</span>matrix<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;from&nbsp;</span>the output of the encoder stack.
-
-    After:
-      <span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it&nbsp;</span>creates its Queries matrix<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;</span>from the layer below it<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">, and takes the&nbsp;</span>Keys<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;and&nbsp;</span>Values<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;</span>matrix<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;from&nbsp;</span>the output of the encoder stack.
-
-============================================================
-
-Note ID: 1702645233698
-  Field: Text
-    Before:
-      <div><ul><li>The decoder stack outputs a vector of floats. How do we turn that into a word?&nbsp;</li><li>That’s the job of the final Linear layer which is followed by a Softmax Layer.</li><li>The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.</li></ul></div>
-
-    After:
-      <div><ul><li>The decoder stack outputs a vector of floats. How do we turn that into a word?&nbsp;</li><li>That’s the job of the final Linear layer which is followed by a Softmax Layer.</li><li>The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.</li></ul></div>
-
-============================================================
-
-Note ID: 1702651158497
-  Field: Text
-    Before:
-      Continuing with our analogy, note that each complex number \(z\) except 0 can be written in the form<br><br><ul><li>\(z\)</li><li>=\((\frac{z}{|z|})\) \(|z|\)</li><li>=\((\frac{z}{|z|})\)&nbsp; \(\sqrt{\bar{z} z}\)</li></ul><br>where the first factor, namely, \(z /|z|\), is an element of the unit circle.<br>
-
-    After:
-      Continuing with our analogy, note that each complex number \(z\) except 0 can be written in the form<br><br><ul><li>\(z\)</li><li>=\((\frac{z}{|z|})\) \(|z|\)</li><li>=\((\frac{z}{|z|})\)&nbsp; \(\sqrt{\bar{z} z}\)</li></ul><br>where the first factor, namely, \(z /|z|\), is an element of the unit circle.<br>
-
-============================================================
-
-Note ID: 1702651309182
-  Field: Text
-    Before:
-      Notation \(\sqrt{T}\)<br><br>If \(T\) is a positive operator, then \(\sqrt{T}\) denotes the unique positive square root of \(T\).
-
-    After:
-      Notation \(\sqrt{T}\)<br><br>If \(T\) is a positive operator, then \(\sqrt{T}\) denotes the unique positive square root of \(T\).
-
-============================================================
-
-Note ID: 1702651882859
-  Field: Text
-    Before:
-      Consider the case \(\mathbf{F}=\mathbf{C}\), and suppose \(T=S \sqrt{T^{*} T}\) is a Polar Decomposition of an operator \(T \in \mathcal{L}(V)\), where \(S\) is an isometry.<br><ul><li>Then there is an orthonormal basis of \(V\) with respect to which \(S\) has a diagonal matrix, and there is an orthonormal basis of \(V\) with respect to which \(\sqrt{T^{*} T}\) has a diagonal matrix.&nbsp;</li></ul>
-
-    After:
-      Consider the case \(\mathbf{F}=\mathbf{C}\), and suppose \(T=S \sqrt{T^{*} T}\) is a Polar Decomposition of an operator \(T \in \mathcal{L}(V)\), where \(S\) is an isometry.<br><ul><li>Then there is an orthonormal basis of \(V\) with respect to which \(S\) has a diagonal matrix, and there is an orthonormal basis of \(V\) with respect to which \(\sqrt{T^{*} T}\) has a diagonal matrix.&nbsp;</li></ul>
-
-============================================================
-
-Note ID: 1702652061664
-  Field: Text
-    Before:
-      Definition singular values<br><br>Suppose \(T \in \mathcal{L}(V)\). The singular values of \(T\) are the eigenvalues of \(\sqrt{T^{*} T}\), with each eigenvalue \(\lambda\) repeated \(\operatorname{dim} E\left(\lambda, \sqrt{T^{*} T}\right)\) times.
-
-    After:
-      Definition singular values<br><br>Suppose \(T \in \mathcal{L}(V)\). The singular values of \(T\) are the eigenvalues of \(\sqrt{T^{*} T}\), with each eigenvalue \(\lambda\) repeated \(\operatorname{dim} E\left(\lambda, \sqrt{T^{*} T}\right)\) times.
-
-============================================================
-
-Note ID: 1702652445767
-  Field: Text
-    Before:
-      \subsection{Example Define \(T \in \mathcal{L}\left(\mathbf{F}^{4}\right)\) by}<br><br>\[<br>T\left(z_{1}, z_{2}, z_{3}, z_{4}\right)=\left(0,3 z_{1}, 2 z_{2},-3 z_{4}\right) .<br>\]<br><br>Find the singular values of \(T\).<br><br>Solution A calculation shows \(T^{*} T\left(z_{1}, z_{2}, z_{3}, z_{4}\right)=\left(9 z_{1}, 4 z_{2}, 0,9 z_{4}\right)\), as you should verify. Thus<br><br>\[<br>\sqrt{T^{*} T}\left(z_{1}, z_{2}, z_{3}, z_{4}\right)=\left(3 z_{1}, 2 z_{2}, 0,3 z_{4}\right),<br>\]<br><br>and we see that the eigenvalues of \(\sqrt{T^{*} T}\) are 3,2,0 and<br><br><ul><li>\(\operatorname{dim} E\) \(\left(3, \sqrt{T^{*} T}\right)\) = 2</li><li>\(\operatorname{dim} E\) \(\left(2, \sqrt{T^{*} T}\right)\)</li><li>=1</li><li>\(\operatorname{dim} E\) \(\left(0, \sqrt{T^{*} T}\right)\) =1.</li></ul><br>Hence the singular values of \(T\) are 3,3,2,0.<br><br>
-
-    After:
-      \subsection{Example Define \(T \in \mathcal{L}\left(\mathbf{F}^{4}\right)\) by}<br><br>\[<br>T\left(z_{1}, z_{2}, z_{3}, z_{4}\right)=\left(0,3 z_{1}, 2 z_{2},-3 z_{4}\right) .<br>\]<br><br>Find the singular values of \(T\).<br><br>Solution A calculation shows \(T^{*} T\left(z_{1}, z_{2}, z_{3}, z_{4}\right)=\left(9 z_{1}, 4 z_{2}, 0,9 z_{4}\right)\), as you should verify. Thus<br><br>\[<br>\sqrt{T^{*} T}\left(z_{1}, z_{2}, z_{3}, z_{4}\right)=\left(3 z_{1}, 2 z_{2}, 0,3 z_{4}\right),<br>\]<br><br>and we see that the eigenvalues of \(\sqrt{T^{*} T}\) are 3,2,0 and<br><br><ul><li>\(\operatorname{dim} E\) \(\left(3, \sqrt{T^{*} T}\right)\) = 2</li><li>\(\operatorname{dim} E\) \(\left(2, \sqrt{T^{*} T}\right)\)</li><li>=1</li><li>\(\operatorname{dim} E\) \(\left(0, \sqrt{T^{*} T}\right)\) =1.</li></ul><br>Hence the singular values of \(T\) are 3,3,2,0.<br><br>
-
-============================================================
-
-Note ID: 1702654648908
-  Field: Text
-    Before:
-      4 Suppose \(T \in \mathcal{L}(V)\) and \(s\) is a singular value of \(T\). Prove that there exists:<br><ul><li>A vector \(v \in V\) such that:</li><ul><li>&nbsp;\(\|\) \(v\) \(\|\) = \(1\)&nbsp;</li><li>\(\|\) \(T\) \(v\|\) = \(s\).</li></ul></ul>
-
-    After:
-      4 Suppose \(T \in \mathcal{L}(V)\) and \(s\) is a singular value of \(T\). Prove that there exists:<br><ul><li>A vector \(v \in V\) such that:</li><ul><li>&nbsp;\(\|\) \(v\) \(\|\) = \(1\)&nbsp;</li><li>\(\|\) \(T\) \(v\|\) = \(s\).</li></ul></ul>
-
-============================================================
-
-Note ID: 1702654746209
-  Field: Text
-    Before:
-      8 Suppose \(T \in \mathcal{L}(V), S \in \mathcal{L}(V)\) is an isometry, and \(R \in \mathcal{L}(V)\) is a positive operator such that \(T=S R\). Prove that \(R=\sqrt{T^{*} T}\).<br><br>[The exercise above shows that if we write \(T\) as the product of an isometry and a positive operator (as in the Polar Decomposition, then the positive operator equals \(\sqrt{T^{*} T}\).]
-
-    After:
-      8 Suppose \(T \in \mathcal{L}(V), S \in \mathcal{L}(V)\) is an isometry, and \(R \in \mathcal{L}(V)\) is a positive operator such that \(T=S R\). Prove that \(R=\sqrt{T^{*} T}\).<br><br>[The exercise above shows that if we write \(T\) as the product of an isometry and a positive operator (as in the Polar Decomposition, then the positive operator equals \(\sqrt{T^{*} T}\).]
-
-============================================================
-
-Note ID: 1702655602837
-  Field: Text
-    Before:
-      18 Suppose \(T \in \mathcal{L}(V)\). Let \(\hat{s}\) denote the smallest singular value of \(T\), and let \(s\) denote the largest singular value of \(T\).<br><ul><li>(b) Suppose \(\lambda\) is an eigenvalue of \(T\). Prove that \(\hat{s}\) \(\leq\) \(|\lambda|\) \(\leq\) \(s\).<br></li></ul>
+      
+<div>

=== Pair #7 ===
Content differs beyond just cloze markup!
Differences (normalized text):
--- before (normalized)
+++ after (normalized)
@@ -1,41 +1,8 @@
-      Nesterov Accelerated Gradient formula:<br><ul><li>\(v_{t+1}\) = \(\mu v_t\) \(-\) \(\eta \nabla f\) \((\)\(\theta_t+\mu v_t\)\()\)<br></li><li>\(\theta_{t+1}\) = \(\theta_t\) \(+\) \(v_{t+1}\)</li></ul>
-
-    After:
-      Nesterov Accelerated Gradient formula:<br><ul><li>\(v_{t+1}\) = \(\mu v_t\) \(-\) \(\eta \nabla f\) \((\)\(\theta_t+\mu v_t\)\()\)<br></li><li>\(\theta_{t+1}\) = \(\theta_t\) \(+\) \(v_{t+1}\)</li></ul>
+      <ol><li>FedMOM Nesterov Accelerated Gradient:<ol><li>\(v_{t+1}\) = \(w_t\) - \(\eta\) \(\sum_{k=1}^K\) \(\frac{n_k}{n}\)\((\underbrace{w_t-w_{t+1}^k}_{\Delta \omega})\),</li><li>\(w_{t+1}\) = \(v_{t+1}\) + \(\beta\left(v_{t+1}-v_t\right)\)</li></ol></li></ol>
 
 ============================================================
 
-Note ID: 1702896614681
+Note ID: 1702978099854
   Field: Text
     Before:
-      <ul><li>While local convergence is all that matters in terms of asymptotic convergence rates &nbsp;in practice, the "transient phase" of convergence seems to matter a lot more for optimizing deep neural networks.</li><li>In this transient phase of learning, directions of reduction in the objective tend to persist across many successive gradient estimates and are not completely swamped by noise.</li></ul>
-
-    After:
-      <ul><li>While local convergence is all that matters in terms of asymptotic convergence rates &nbsp;in practice, the "transient phase" of convergence seems to matter a lot more for optimizing deep neural networks.</li><li>In this transient phase of learning, directions of reduction in the objective tend to persist across many successive gradient estimates and are not completely swamped by noise.</li></ul>
-
-============================================================
-
-Note ID: 1702897146780
-  Field: Text
-    Before:
-      <div><ul><li>The convergence rate of stochastic gradient descent on smooth convex functions is given by \(O\) \((\)\(L / T+\sigma / \sqrt{T}\)\()\), where \(\sigma\) is the variance in the gradient estimate and \(L\) is the Lipshits coefficient of \(\nabla f\).&nbsp;</li><li>The convergence rate of an accelerated gradient method of Lan (2010) (which is related to but different from NAG, in that it combines Nesterov style momentum with dual averaging) is \(O\) \((\)\(L / T^2\)\(+\)\(\sigma / \sqrt{T}\)\()\).&nbsp;</li><li>Thus, for convex objectives, momentum-based methods will outperform SGD in the early/transient stages of the optimization where \(L / T\) is the dominant term.&nbsp;</li><li>However, the two methods will be equally effective during the final stages&nbsp;of the optimization where \(\sigma / \sqrt{T}\) is the dominant term (i.e., when the optimization problem resembles an estimation one).</li></ul></div>
-
-    After:
-      <div><ul><li>The convergence rate of stochastic gradient descent on smooth convex functions is given by \(O\) \((\)\(L / T+\sigma / \sqrt{T}\)\()\), where \(\sigma\) is the variance in the gradient estimate and \(L\) is the Lipshits coefficient of \(\nabla f\).&nbsp;</li><li>The convergence rate of an accelerated gradient method of Lan (2010) (which is related to but different from NAG, in that it combines Nesterov style momentum with dual averaging) is \(O\) \((\)\(L / T^2\)\(+\)\(\sigma / \sqrt{T}\)\()\).&nbsp;</li><li>Thus, for convex objectives, momentum-based methods will outperform SGD in the early/transient stages of the optimization where \(L / T\) is the dominant term.&nbsp;</li><li>However, the two methods will be equally effective during the final stages&nbsp;of the optimization where \(\sigma / \sqrt{T}\) is the dominant term (i.e., when the optimization problem resembles an estimation one).</li></ul></div>
-
-============================================================
-
-Note ID: 1702899184266
-  Field: Text
-    Before:
-      <ul><li>The authors found it beneficial to reduce \(\mu\) during the final few parameter updates of the optimization without reducing the learning rate&nbsp;</li><li>They think that reducing the momentum coefficient allows for finer convergence to take place whereas otherwise the overly aggressive nature of Classical Momentum \(\mathrm{CM}\) or Nesterov Accelerated Gradient would prevent this</li></ul>
-
-    After:
-      <ul><li>The authors found it beneficial to reduce \(\mu\) during the final few parameter updates of the optimization without reducing the learning rate&nbsp;</li><li>They think that reducing the momentum coefficient allows for finer convergence to take place whereas otherwise the overly aggressive nature of Classical Momentum \(\mathrm{CM}\) or Nesterov Accelerated Gradient would prevent this</li></ul>
-
-============================================================
-
-Note ID: 1702976691118
-  Field: Text
-    Before:
-      <ol><li>FedMOM Nesterov Accelerated Gradient:<ol><li>\(v_{t+1}\) = \(w_t\) - \(\eta\) \(\sum_{k=1}^K\) \(\frac{n_k}{n}\)\((\underbrace{w_t-w_{t+1}^k}_{\Delta \omega})\),</li><li>\(w_{t+1}\) = \(v_{t+1}\) + \(\beta\left(v_{t+1}-v_t\right)\)</li></ol></li></ol>
+      <ol>

=== Pair #8 ===
Content differs beyond just cloze markup!
Differences (normalized text):
--- before (normalized)
+++ after (normalized)
@@ -1 +1,8 @@
       <ul><li>The inner product of FedAvg with the optimal direction is generally larger than FedSGD.&nbsp;</li><li>FedAvg converges faster on training loss and testing loss and testing accuracy.</li></ul>
+
+============================================================
+
+Note ID: 1702982734871
+  Field: Text
+    Before:
+      <ol>

=== Pair #9 ===
Content differs beyond just cloze markup!
Differences (normalized text):
--- before (normalized)
+++ after (normalized)
@@ -1 +1,8 @@
       Using a single GPU to aggregate gradients in data-parallel training requires O(N) communication where N is the number of GPUs since:<br><ul><li>Every worker has to send its gradients to the parameter server&nbsp;&nbsp;</li><li>Then the parameter server must send back the averaged gradient to all workers.</li></ul>
+
+============================================================
+
+Note ID: 1703279251860
+  Field: Text
+    Before:
+      <ol>

=== Pair #10 ===
Content differs beyond just cloze markup!
Differences (normalized text):
--- before (normalized)
+++ after (normalized)
@@ -1 +1,8 @@
       Scatter-Reduce Phase:<ol><li>The GPUs will do N-1 iterations of the scatter-reduce; in every iteration:<ol><li>A GPU will send one of its chunks to its right neighbor,</li><li>It will then receive a chunk from its left neighbor and accumulate into that chunk.</li></ol></li><li>The chunk being sent and received by every GPU is different every iteration:<ol><li>The n-th GPU starts by sending chunk n and receiving chunk n - 1</li><li>It then proceeds backwards from there, each iteration sending the chunk it received in the previous iteration.</li></ol></li></ol>
+
+============================================================
+
+Note ID: 1703345627892
+  Field: Text
+    Before:
+      <ol>

=== Pair #11 ===
Content differs beyond just cloze markup!
Differences (normalized text):
--- before (normalized)
+++ after (normalized)
@@ -1,11 +1,8 @@
-      The Allgather phase:<br><ol><li>The nth GPU starts by sending the n+1th chunk and receiving the nth chunk, and then in future iterations always sends the chunk it has just received.</li></ol>
-
-    After:
-      The Allgather phase:<br><ol><li>The nth GPU starts by sending the n+1th chunk and receiving the nth chunk, and then in future iterations always sends the chunk it has just received.</li></ol>
+      <div><ul><li>At the start of the training phase, we create two matrices – an&nbsp;<code>Embedding</code>&nbsp;matrix and a&nbsp;<code>Context</code>&nbsp;matrix.&nbsp;</li><li>These two matrices have an embedding for each word in our vocabulary (So&nbsp;<code>vocab_size</code>&nbsp;is one of their dimensions). The second dimension is how long we want each embedding to be.</li></ul></div><br>
 
 ============================================================
 
-Note ID: 1703433958415
+Note ID: 1703500765527
   Field: Text
     Before:
-      <div><ul><li>At the start of the training phase, we create two matrices – an&nbsp;<code>Embedding</code>&nbsp;matrix and a&nbsp;<code>Context</code>&nbsp;matrix.&nbsp;</li><li>These two matrices have an embedding for each word in our vocabulary (So&nbsp;<code>vocab_size</code>&nbsp;is one of their dimensions). The second dimension is how long we want each embedding to be.</li></ul></div><br>
+      <ol>

=== Pair #12 ===
Content differs beyond just cloze markup!
Differences (normalized text):
--- before (normalized)
+++ after (normalized)
@@ -1 +1,8 @@
       The “next sentence prediction” task involves a binarized choice: determining if sentence B is the actual next sentence after A in the corpus. There's a 50% chance that B is the actual next sentence and a 50% chance it is a random sentence.
+
+============================================================
+
+Note ID: 1703504726322
+  Field: Text
+    Before:
+      <ul>

=== Pair #13 ===
Content differs beyond just cloze markup!
Differences (normalized text):
--- before (normalized)
+++ after (normalized)
@@ -1,11 +1,8 @@
-      The approach taken by GPT is semi-supervised, being composed of unsupervised pre-training via language modelling and and supervised fine-tuning.
-
-    After:
-      The approach taken by GPT is semi-supervised, being composed of unsupervised pre-training via language modelling and and supervised fine-tuning.
+      Transformer structure:<br><ul><li>\(h_0\) = \(U W_e\) \(+\) \(W_p\)<br></li><li>\(h_l\) = transformer_block(\(h_{l-1}\) )\(\forall i \in [1,n]\)<br></li><li>\(P(u)\) = softmax ( \( h_n, \mathbf{W}_e^t \) )<br></li><li>where \( U\) is the context vector of tokens, n is the number of layers</li><li>&nbsp;\(\mathbf{W}_e\) is the token embedding matrix</li><li>\(W_p\) is the position embedding matrix</li></ul>
 
 ============================================================
 
-Note ID: 1703526911103
+Note ID: 1703620610472
   Field: Text
     Before:
-      Transformer structure:<br><ul><li>\(h_0\) = \(U W_e\) \(+\) \(W_p\)<br></li><li>\(h_l\) = transformer_block(\(h_{l-1}\) )\(\forall i \in [1,n]\)<br></li><li>\(P(u)\) = softmax ( \( h_n, \mathbf{W}_e^t \) )<br></li><li>where \( U\) is the context vector of tokens, n is the number of layers</li><li>&nbsp;\(\mathbf{W}_e\) is the token embedding matrix</li><li>\(W_p\) is the position embedding matrix</li></ul>
+      <ol>

=== Pair #14 ===
Content differs beyond just cloze markup!
Differences (normalized text):
--- before (normalized)
+++ after (normalized)
@@ -1,51 +1,8 @@
-      <ul><li><span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">The simplest way to run a trained GPT-2 is to allow it to&nbsp;</span>generate unconditional samples</li><li><span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">Alternatively, we can give it a prompt&nbsp;to&nbsp;</span><span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">generate&nbsp;</span><em>interactive conditional samples</em></li><li><span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">In the generate unconditional samples&nbsp;case, we can simply hand it the start token and have it start generating words&nbsp;</span></li></ul>
-
-    After:
-      <ul><li><span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">The simplest way to run a trained GPT-2 is to allow it to&nbsp;</span>generate unconditional samples</li><li><span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">Alternatively, we can give it a prompt&nbsp;to&nbsp;</span><span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">generate&nbsp;</span><em>interactive conditional samples</em></li><li><span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">In the generate unconditional samples&nbsp;case, we can simply hand it the start token and have it start generating words&nbsp;</span></li></ul>
+      <ul><li><span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">Each trnasformer block has</span> its own set of these weights.&nbsp;</li><li><span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">On the other hand, the model has only one&nbsp;token embedding matrix&nbsp;and one&nbsp;</span>positional encoding matrix:</li></ul>
 
 ============================================================
 
-Note ID: 1703624004159
+Note ID: 1703768562592
   Field: Text
     Before:
-      <span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">GPT-2 has a parameter called&nbsp;</span>top-k<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;that we can use to have the model&nbsp;</span>consider sampling words<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;</span>other than the top word<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;(which is the case when</span> top-k<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;=&nbsp;</span>1<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">).</span>
-
-    After:
-      <span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">GPT-2 has a parameter called&nbsp;</span>top-k<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;that we can use to have the model&nbsp;</span>consider sampling words<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;</span>other than the top word<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;(which is the case when</span> top-k<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;=&nbsp;</span>1<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">).</span>
-
-============================================================
-
-Note ID: 1703629974113
-  Field: Text
-    Before:
-      <ul><li>Query: The query is a representation of the current word used to score against all the other words (using their keys). We only care about the query of the token we’re currently processing.</li><li>Key: Key vectors are like labels for all the words in the segment. They’re what we match against in our search for relevant words.</li><li>Value: Value vectors are actual word representations, once we’ve scored how relevant each word is, these are the values we add up to represent the current word.</li></ul>
-
-    After:
-      <ul><li>Query: The query is a representation of the current word used to score against all the other words (using their keys). We only care about the query of the token we’re currently processing.</li><li>Key: Key vectors are like labels for all the words in the segment. They’re what we match against in our search for relevant words.</li><li>Value: Value vectors are actual word representations, once we’ve scored how relevant each word is, these are the values we add up to represent the current word.</li></ul>
-
-============================================================
-
-Note ID: 1703668003522
-  Field: Text
-    Before:
-      After applyinh self attention to one query&nbsp;<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">we end up with&nbsp;</span>a vector<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;representing&nbsp;</span>each token<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;containing&nbsp;</span>the appropriate context<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;of that&nbsp;</span>token<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">.</span>
-
-    After:
-      After applyinh self attention to one query&nbsp;<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">we end up with&nbsp;</span>a vector<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;representing&nbsp;</span>each token<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;containing&nbsp;</span>the appropriate context<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;of that&nbsp;</span>token<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">.</span>
-
-============================================================
-
-Note ID: 1703694592195
-  Field: Text
-    Before:
-      How GPT-2 optimizes attention for the inference step<br><ul><li>Avoid re-computing self-attention for previous tokens in light of new tokens</li><li>Storing the the query,key,and value of previous tokens at every single attention layer</li></ul>
-
-    After:
-      How GPT-2 optimizes attention for the inference step<br><ul><li>Avoid re-computing self-attention for previous tokens in light of new tokens</li><li>Storing the the query,key,and value of previous tokens at every single attention layer</li></ul>
-
-============================================================
-
-Note ID: 1703696597787
-  Field: Text
-    Before:
-      <ul><li><span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">Each trnasformer block has</span> its own set of these weights.&nbsp;</li><li><span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">On the other hand, the model has only one&nbsp;token embedding matrix&nbsp;and one&nbsp;</span>positional encoding matrix:</li></ul>
+      <ol>

=== Pair #15 ===
Content differs beyond just cloze markup!
Differences (normalized text):
--- before (normalized)
+++ after (normalized)
@@ -1 +1,8 @@
       <span style="font-style:italic">ZeRO</span>-DP has three main optimization stages, which correspond to partitioning optimizer states, gradients and params:<br><ul><li><span style="font-weight:600">Optimizer State&nbsp;</span>Partitioning<br></li><li><span style="font-weight:600">Optimizer State&nbsp;+ Gradient&nbsp;Partitioning<br></span></li><li><span style="font-weight:600">Optimizer State&nbsp;+ Gradient&nbsp;+ Parameter&nbsp;Partitioning<br></span></li></ul>
+
+============================================================
+
+Note ID: 1703862505041
+  Field: Text
+    Before:
+      <ol>

=== Pair #16 ===
Content differs beyond just cloze markup!
Differences (normalized text):
--- before (normalized)
+++ after (normalized)
@@ -1 +1,8 @@
       MP splits the model vertically, partitioning the computation and parameters in each layer across multiple devices, requiring significant communication between each layer
+
+============================================================
+
+Note ID: 1704202466826
+  Field: Text
+    Before:
+      <ol>

=== Pair #17 ===
Content differs beyond just cloze markup!
Differences (normalized text):
--- before (normalized)
+++ after (normalized)
@@ -1 +1,8 @@
       Few-shot can significantly reduce the need for task-specific data and reduce the potential to learn an overly narrow distribution from a large but narrow fine-tuning dataset.
+
+============================================================
+
+Note ID: 1704203089694
+  Field: Text
+    Before:
+      <ol>

=== Pair #18 ===
Content differs beyond just cloze markup!
Differences (normalized text):
--- before (normalized)
+++ after (normalized)
@@ -1 +1,8 @@
       To train the larger models without running out of memory, GPT-3 uses a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network.
+
+============================================================
+
+Note ID: 1704205571176
+  Field: Text
+    Before:
+      <ol>

=== Pair #19 ===
Content differs beyond just cloze markup!
Differences (normalized text):
--- before (normalized)
+++ after (normalized)
@@ -1,11 +1,8 @@
-      Human abilities to detect model-generated text decrease as model size increases: there appears to be a trend towards chance accuracy with model size, and human detection of GPT-3 is close to chance. This is true even though participants spend more time on each output as the model size increases
-
-    After:
-      Human abilities to detect model-generated text decrease as model size increases: there appears to be a trend towards chance accuracy with model size, and human detection of GPT-3 is close to chance. This is true even though participants spend more time on each output as the model size increases
+      Byte-pair encoding allows for the representation of an open vocabulary through a fixed-size vocabulary of variable-length character sequences.
 
 ============================================================
 
-Note ID: 1704219020264
+Note ID: 1704277966602
   Field: Text
     Before:
-      Byte-pair encoding allows for the representation of an open vocabulary through a fixed-size vocabulary of variable-length character sequences.
+      <ol>

=== Pair #20 ===
Content differs beyond just cloze markup!
Differences (normalized text):
--- before (normalized)
+++ after (normalized)
@@ -1,61 +1,8 @@
-      <ul><li>Offloading optimizer computation requires CPU to perform <span style="font-style:italic">O</span>(M) computation compared to <span style="font-style:italic">O</span>(MB) on GPU&nbsp;</li><li>where M&nbsp;and <span style="font-style:italic">B</span> are the model size and batch size.&nbsp;</li><li>In most cases, the batch size is large, and CPU computation is not a bottleneck, but for small batch sizes, the CPU compute can be a bottleneck.</li></ul>
-
-    After:
-      <ul><li>Offloading optimizer computation requires CPU to perform <span style="font-style:italic">O</span>(M) computation compared to <span style="font-style:italic">O</span>(MB) on GPU&nbsp;</li><li>where M&nbsp;and <span style="font-style:italic">B</span> are the model size and batch size.&nbsp;</li><li>In most cases, the batch size is large, and CPU computation is not a bottleneck, but for small batch sizes, the CPU compute can be a bottleneck.</li></ul>
+      13. **Delayed-Parameter Updates training schedule**<br><br>1) The first *N-1* steps are trained without DPU to avoid destabilizing the training during the early stages where gradients change rapidly. 2) On step *N*, we obtain the gradients from the GPU, but we skip the CPU optimizer step, and do not update the fp16 parameters on the GPU either. 3) At step *N+1*, we compute the parameter updates on the CPU using gradients from step *N*, while computing the forward and backward pass on the GPU in parallel using parameters updated at step *N-1*. From this step onwards, the model at \((i+1)^{th}\) step will be trained using the parameters updated with gradients from \((i-1)^{th}\) step instead of parameters updated at \(i^{th}\) step, overlapping CPU compute with GPU compute.
 
 ============================================================
 
-Note ID: 1704319429751
+Note ID: 1704320011332
   Field: Text
     Before:
-      2. Limiting CPU Computation:<br><br>The compute complexity of DL training per iteration is generally given by \(O(MB)\), where \(M\) is the model size and \(B\) is the effective batch size.<br><br>To prevent CPU computation from becoming a bottleneck, only those computations that have a compute complexity lower than \(O(MB)\) should be offloaded to the CPU.<br><br>Therefore, ZeRO-Offload does forward and backward propagation on the GPU and remaining computations that have a complexity of \(O(M)\) on the CPU.
-
-    After:
-      2. Limiting CPU Computation:<br><br>The compute complexity of DL training per iteration is generally given by \(O(MB)\), where \(M\) is the model size and \(B\) is the effective batch size.<br><br>To prevent CPU computation from becoming a bottleneck, only those computations that have a compute complexity lower than \(O(MB)\) should be offloaded to the CPU.<br><br>Therefore, ZeRO-Offload does forward and backward propagation on the GPU and remaining computations that have a complexity of \(O(M)\) on the CPU.
-
-============================================================
-
-Note ID: 1704319462595
-  Field: Text
-    Before:
-      7. Multi-GPU Scheduling:<br><br>Using ZeRO DP allows each DP process to update a subset of parameters, thus decreasing the CPU compute load linearly with the number of compute nodes.<br><br>ZeRO-Offload partitions gradients and optimizer states among different GPUs, and each GPU offloads the partition it owns to the CPU memory and keeps it there for the entire training.<br><br>During the backward propagation, gradients are computed and averaged using reduce-scatter on the GPU, and each GPU only offloads the averaged gradients belonging to its partition to the CPU memory.<br><br>Once the gradients are available on the CPU, optimizer state partitions are updated in parallel by each data parallel process directly on the CPU.
-
-    After:
-      7. Multi-GPU Scheduling:<br><br>Using ZeRO DP allows each DP process to update a subset of parameters, thus decreasing the CPU compute load linearly with the number of compute nodes.<br><br>ZeRO-Offload partitions gradients and optimizer states among different GPUs, and each GPU offloads the partition it owns to the CPU memory and keeps it there for the entire training.<br><br>During the backward propagation, gradients are computed and averaged using reduce-scatter on the GPU, and each GPU only offloads the averaged gradients belonging to its partition to the CPU memory.<br><br>Once the gradients are available on the CPU, optimizer state partitions are updated in parallel by each data parallel process directly on the CPU.
-
-============================================================
-
-Note ID: 1704319467351
-  Field: Text
-    Before:
-      8. CPU Tricks:<br><br>To make CPU compute more efficient, ZeRO-Offload provides a highly optimized CPU-only ADAM.<br><br>This achieves a performance comparable to the PyTorch GPU ADAM.<br><br>Additionally, it adds a one-step delayed parameter update to overlap CPU and GPU compute more for small-batch settings where the CPU may still be a bottleneck.<br><br>They show this has little impact on accuracy.
-
-    After:
-      8. CPU Tricks:<br><br>To make CPU compute more efficient, ZeRO-Offload provides a highly optimized CPU-only ADAM.<br><br>This achieves a performance comparable to the PyTorch GPU ADAM.<br><br>Additionally, it adds a one-step delayed parameter update to overlap CPU and GPU compute more for small-batch settings where the CPU may still be a bottleneck.<br><br>They show this has little impact on accuracy.
-
-============================================================
-
-Note ID: 1704319639505
-  Field: Text
-    Before:
-      4. The original ZeRO DP approach has three stages, ZeRO-1, ZeRO-2, and ZeRO-3, corresponding to the partitioning of the three different model states, optimizer states, gradients, and parameters, respectively. ZeRO-1 partitions the optimizer states only, while ZeRO-2 partitions gradients in addition to optimizer states, and ZeRO-3 partitions all model states.<br><br>ZeRO-Offload works symbiotically with ZeRO-2.
-
-    After:
-      4. The original ZeRO DP approach has three stages, ZeRO-1, ZeRO-2, and ZeRO-3, corresponding to the partitioning of the three different model states, optimizer states, gradients, and parameters, respectively. ZeRO-1 partitions the optimizer states only, while ZeRO-2 partitions gradients in addition to optimizer states, and ZeRO-3 partitions all model states.<br><br>ZeRO-Offload works symbiotically with ZeRO-2.
-
-============================================================
-
-Note ID: 1704319810097
-  Field: Text
-    Before:
-      10. Notice that any partitioning strategy that does not co-locate the fp32 model states with its producer and consumer nodes cannot achieve the minimum communication volume of 4*M. Such a partition must cut at least one edge with a weight of 4*M, and the other with at least 2*M, resulting in a communication volume of at least 6*M.
-
-    After:
-      10. Notice that any partitioning strategy that does not co-locate the fp32 model states with its producer and consumer nodes cannot achieve the minimum communication volume of 4*M. Such a partition must cut at least one edge with a weight of 4*M, and the other with at least 2*M, resulting in a communication volume of at least 6*M.
-
-============================================================
-
-Note ID: 1704319868500
-  Field: Text
-    Before:
-      13. **Delayed-Parameter Updates training schedule**<br><br>1) The first *N-1* steps are trained without DPU to avoid destabilizing the training during the early stages where gradients change rapidly. 2) On step *N*, we obtain the gradients from the GPU, but we skip the CPU optimizer step, and do not update the fp16 parameters on the GPU either. 3) At step *N+1*, we compute the parameter updates on the CPU using gradients from step *N*, while computing the forward and backward pass on the GPU in parallel using parameters updated at step *N-1*. From this step onwards, the model at \((i+1)^{th}\) step will be trained using the parameters updated with gradients from \((i-1)^{th}\) step instead of parameters updated at \(i^{th}\) step, overlapping CPU compute with GPU compute.
+      <ol>

=== Pair #21 ===
Content differs beyond just cloze markup!
Differences (normalized text):
--- before (normalized)
+++ after (normalized)
@@ -1,31 +1,8 @@
-      Pipeline parallelism partitions a model instance into stages and distributes stages across multiple devices, where activations and gradients are communicated across stage boundaries.
-
-    After:
-      Pipeline parallelism partitions a model instance into stages and distributes stages across multiple devices, where activations and gradients are communicated across stage boundaries.
+      <ul><li>For \(\Psi\) number of parameter elements , \(K_{\text {low }}\) bytes per low precision element, and \(K_{\text {full }}\) bytes per full precision element, this approach to mixed precision normally increases the memory overhead from \(K_{\text {full }} \Psi\) to ( \(\left.K_{\text {low }}+K_{\text {full }}\right) \Psi\) due to maintaining both precision copies.<br></li><li>For \(N\) FlatParameters with numels given by \(\psi_1, \ldots, \psi_N\), the parameter peak memory contribution for FSDP actually decreases from:</li><ul><li>&nbsp;\(\frac{K_{\text {full } } }{F} \sum_{i=1}^N \psi_i\) &nbsp;+ \(K_{\text {full } } \max {i=1}^N \psi_i \) &nbsp;</li></ul><li>to<br></li><ul><li>\(\frac{K{\text {full } } }{F} \sum_{i=1}^N \psi_i\) &nbsp;+ \(K_{\text {low } }&nbsp; \max {i=1}^N \psi_i\) &nbsp;bytes.&nbsp;</li></ul><li>In other words, FSDP directly reduces the second \(K{\text {full } } \max {i=1}^N \psi_i\) term to \(K{\text {low } } \max _{i=1}^N \psi_i\).<br></li></ul>
 
 ============================================================
 
-Note ID: 1704454535294
+Note ID: 1704474558237
   Field: Text
     Before:
-      During forward and backward computation, FSDP only materializes unsharded parameters and gradients of one unit at a time, and otherwise, it keeps parameters and gradients sharded together with the optimizer states.
-
-    After:
-      During forward and backward computation, FSDP only materializes unsharded parameters and gradients of one unit at a time, and otherwise, it keeps parameters and gradients sharded together with the optimizer states.
-
-============================================================
-
-Note ID: 1704468497677
-  Field: Text
-    Before:
-      <ul><li><span style="color: rgb(53, 55, 64);">Card 5: Full Sharding's {c1::NCCL efficiency}} is impacted by factors like input size, with larger&nbsp;inputs and even-sized inputs across ranks yielding higher efficiency measures in collective communications such as all-gather and reduce-scatter, as shown in experiments detailed in a study.</span></li></ul>
-
-    After:
-      <ul><li><span style="color: rgb(53, 55, 64);">Card 5: Full Sharding's {c1::NCCL efficiency}} is impacted by factors like input size, with larger&nbsp;inputs and even-sized inputs across ranks yielding higher efficiency measures in collective communications such as all-gather and reduce-scatter, as shown in experiments detailed in a study.</span></li></ul>
-
-============================================================
-
-Note ID: 1704473266154
-  Field: Text
-    Before:
-      <ul><li>For \(\Psi\) number of parameter elements , \(K_{\text {low }}\) bytes per low precision element, and \(K_{\text {full }}\) bytes per full precision element, this approach to mixed precision normally increases the memory overhead from \(K_{\text {full }} \Psi\) to ( \(\left.K_{\text {low }}+K_{\text {full }}\right) \Psi\) due to maintaining both precision copies.<br></li><li>For \(N\) FlatParameters with numels given by \(\psi_1, \ldots, \psi_N\), the parameter peak memory contribution for FSDP actually decreases from:</li><ul><li>&nbsp;\(\frac{K_{\text {full } } }{F} \sum_{i=1}^N \psi_i\) &nbsp;+ \(K_{\text {full } } \max {i=1}^N \psi_i \) &nbsp;</li></ul><li>to<br></li><ul><li>\(\frac{K{\text {full } } }{F} \sum_{i=1}^N \psi_i\) &nbsp;+ \(K_{\text {low } }&nbsp; \max {i=1}^N \psi_i\) &nbsp;bytes.&nbsp;</li></ul><li>In other words, FSDP directly reduces the second \(K{\text {full } } \max {i=1}^N \psi_i\) term to \(K{\text {low } } \max _{i=1}^N \psi_i\).<br></li></ul>
+      <ol>

=== Pair #22 ===
Content differs beyond just cloze markup!
Differences (normalized text):
--- before (normalized)
+++ after (normalized)
@@ -1,71 +1,8 @@
-      <ol><li><div><strong>Estimation of Current Data Stocks:</strong>&nbsp;The paper presents aggregated models to estimate the current total stock of language and image data. The range of growth rates is quite wide, indicating uncertainty in the long-term availability of such data.</div></li></ol>
-
-    After:
-      <ol><li><div><strong>Estimation of Current Data Stocks:</strong>&nbsp;The paper presents aggregated models to estimate the current total stock of language and image data. The range of growth rates is quite wide, indicating uncertainty in the long-term availability of such data.</div></li></ol>
+      {c1::Tensor}} parallelism is typically used within a single multi-GPU server or closely interconnected TPU cores.
 
 ============================================================
 
-Note ID: 1704709798900
+Note ID: 1704722725271
   Field: Text
     Before:
-      <ol><li><div>Distributed Mini-batch SGD is shown to suffer from generalization issues compared to local SGD or single-machine SGD even when using the same local batch sizes as local SGD.</div></li></ol>
-
-    After:
-      <ol><li><div>Distributed Mini-batch SGD is shown to suffer from generalization issues compared to local SGD or single-machine SGD even when using the same local batch sizes as local SGD.</div></li></ol>
-
-============================================================
-
-Note ID: 1704709830560
-  Field: Text
-    Before:
-      <ol><li><div>In direct comparison, post-local SGD is more communication-efficient than mini-batch SGD (while less than local SGD) and achieves better generalization performance than both.</div></li></ol>
-
-    After:
-      <ol><li><div>In direct comparison, post-local SGD is more communication-efficient than mini-batch SGD (while less than local SGD) and achieves better generalization performance than both.</div></li></ol>
-
-============================================================
-
-Note ID: 1704710496774
-  Field: Text
-    Before:
-      <span style="color: rgb(53, 55, 64);">For post-local SGD, local SGD is only started in the&nbsp;</span>second phase<span style="color: rgb(53, 55, 64);">&nbsp;of training, after (t) initial steps with&nbsp;</span>standard mini-batch SGD.
-
-    After:
-      <span style="color: rgb(53, 55, 64);">For post-local SGD, local SGD is only started in the&nbsp;</span>second phase<span style="color: rgb(53, 55, 64);">&nbsp;of training, after (t) initial steps with&nbsp;</span>standard mini-batch SGD.
-
-============================================================
-
-Note ID: 1704710539589
-  Field: Text
-    Before:
-      <span style="color: rgb(53, 55, 64);">Tthe&nbsp;</span>effective batch size<span style="color: rgb(53, 55, 64);">&nbsp;of Post-local SGD scales from (B) = (B_{loc}) in the&nbsp;</span>initial<span style="color: rgb(53, 55, 64);">&nbsp;phase to (B) = (H B_{loc})</span>
-
-    After:
-      <span style="color: rgb(53, 55, 64);">Tthe&nbsp;</span>effective batch size<span style="color: rgb(53, 55, 64);">&nbsp;of Post-local SGD scales from (B) = (B_{loc}) in the&nbsp;</span>initial<span style="color: rgb(53, 55, 64);">&nbsp;phase to (B) = (H B_{loc})</span>
-
-============================================================
-
-Note ID: 1704710678046
-  Field: Text
-    Before:
-      <ol><li><div>In direct comparison, post-local SGD is more communication-efficient than mini-batch SGD (while less than local SGD) and achieves better generalization performance than both.</div></li></ol>
-
-    After:
-      <ol><li><div>In direct comparison, post-local SGD is more communication-efficient than mini-batch SGD (while less than local SGD) and achieves better generalization performance than both.</div></li></ol>
-
-============================================================
-
-Note ID: 1704710760876
-  Field: Text
-    Before:
-      <ol><li><div>For post-local SGD, local SGD is only started in the second phase of training, after t initial steps with standard mini-batch SGD.</div><ol><li>Thus the effective batch size scales from \(B\) = \(B_{loc}\) in the initial phase to \(B\) = \(H B_{loc}\).</li><li>It is crucial to use local SGD in the second phase, as e.g. just resorting to large batch training does achieve worse performance.</li></ol></li></ol>
-
-    After:
-      <ol><li><div>For post-local SGD, local SGD is only started in the second phase of training, after t initial steps with standard mini-batch SGD.</div><ol><li>Thus the effective batch size scales from \(B\) = \(B_{loc}\) in the initial phase to \(B\) = \(H B_{loc}\).</li><li>It is crucial to use local SGD in the second phase, as e.g. just resorting to large batch training does achieve worse performance.</li></ol></li></ol>
-
-============================================================
-
-Note ID: 1704722555507
-  Field: Text
-    Before:
-      {c1::Tensor}} parallelism is typically used within a single multi-GPU server or closely interconnected TPU cores.
+      <ol>

=== Pair #23 ===
Content differs beyond just cloze markup!
Differences (normalized text):
--- before (normalized)
+++ after (normalized)
@@ -1 +1,8 @@
       Both re-starting on every network and node failure and recomputing attention caches on every node struggle to scale to long sequences.
+
+============================================================
+
+Note ID: 1704730054470
+  Field: Text
+    Before:
+      <ol>

=== Pair #24 ===
Content differs beyond just cloze markup!
Differences (normalized text):
--- before (normalized)
+++ after (normalized)
@@ -1,8185 +1,3 @@
-      If a server detects that reblancing would improve throughput, they switch layers until the throughput becomes near-optimal. In particular, if all peers serving certain blocks suddenly leave the system, this procedure quickly redistributes the remaining resources to close the gaps that have emerged.
-
-    After:
-      If a server detects that reblancing would improve throughput, they switch layers until the throughput becomes near-optimal. In particular, if all peers serving certain blocks suddenly leave the system, this procedure quickly redistributes the remaining resources to close the gaps that have emerged.
+      Lemma 3.1.12 If \(a\) and \(b\) are objects, then \(\{a, b\}\)=\(\{a\} \cup\{b\}\). If \(A, B, C\) are sets, then the union operation is commutative and associative. Also, we have \(A\) \(\cup\) \(A\)=\(A\) \(\cup\) \(\emptyset\)=\(\emptyset\) \(\cup\) \(A\)=\(A\).
 
 ============================================================
-
-Note ID: 1704736441336
-  Field: Text
-    Before:
-      Before joining for the first time, each server measures its Internet connection throughput (in tokens/second, using one of public web APIs for doing that) and GPU throughput (in tokens/second, using a small benchmark running several forward passes). The minimum of these values becomes the overall server throughput, which is then cached for future runs.
-
-    After:
-      Before joining for the first time, each server measures its Internet connection throughput (in tokens/second, using one of public web APIs for doing that) and GPU throughput (in tokens/second, using a small benchmark running several forward passes). The minimum of these values becomes the overall server throughput, which is then cached for future runs.
-
-============================================================
-
-Note ID: 1706082635686
-  Field: Text
-    Before:
-      4. Suppose \(P \in \mathcal{L}(V)\) is such that \(P^{2}=P\). Prove that \(P\) is an orthogonal projection if and only if \(P\) is self-adjoint.<br><br><ul><li>Suppose that P is an orthogonal projection</li><li>Thus we can make an orthogonal decomposition and</li><ul><li>\(v_1\) =&nbsp;\(u_1 + w_1\)</li><li>\(v_2\) =&nbsp;\(u_2 + w_2\)</li></ul><li>Then \(\langle\) \(P v_1,v_2\) \(\rangle\) =&nbsp;</li><ul><li>&nbsp;\(\langle\)&nbsp;\(u_1, u_2 + w_2\) \(\rangle\) by orthogonal projection</li><li>&nbsp;\(\langle\)&nbsp;\(u_1, u_2\) \(\rangle\) +&nbsp; \(\langle\)&nbsp;\(u_1, w_2\) \(\rangle\) by linearity&nbsp;</li><li>&nbsp;\(\langle\)&nbsp; \(u_1,u_2\) \(\rangle\) by orthogonal complement</li><li>&nbsp;\(\langle\)&nbsp; \(u_1,u_2\) \(\rangle\) +&nbsp;&nbsp;\(\langle\)&nbsp; \(w_1,u_2\) \(\rangle\) by orthogonal complement in the other direction<br></li><li>&nbsp;\(\langle\)&nbsp; \(u_1 + w_1,u_2\) \(\rangle\) by linearity<br></li><li>&nbsp;\(\langle\)&nbsp; \(v_1 P v_2\) \(\rangle\) by orthognal projection<br></li></ul><li>Thus&nbsp;\(P\) =&nbsp;\(P*\) and hence P is self-adjoint</li></ul>
-
-    After:
-      4. Suppose \(P \in \mathcal{L}(V)\) is such that \(P^{2}=P\). Prove that \(P\) is an orthogonal projection if and only if \(P\) is self-adjoint.<br><br><ul><li>Suppose that P is an orthogonal projection</li><li>Thus we can make an orthogonal decomposition and</li><ul><li>\(v_1\) =&nbsp;\(u_1 + w_1\)</li><li>\(v_2\) =&nbsp;\(u_2 + w_2\)</li></ul><li>Then \(\langle\) \(P v_1,v_2\) \(\rangle\) =&nbsp;</li><ul><li>&nbsp;\(\langle\)&nbsp;\(u_1, u_2 + w_2\) \(\rangle\) by orthogonal projection</li><li>&nbsp;\(\langle\)&nbsp;\(u_1, u_2\) \(\rangle\) +&nbsp; \(\langle\)&nbsp;\(u_1, w_2\) \(\rangle\) by linearity&nbsp;</li><li>&nbsp;\(\langle\)&nbsp; \(u_1,u_2\) \(\rangle\) by orthogonal complement</li><li>&nbsp;\(\langle\)&nbsp; \(u_1,u_2\) \(\rangle\) +&nbsp;&nbsp;\(\langle\)&nbsp; \(w_1,u_2\) \(\rangle\) by orthogonal complement in the other direction<br></li><li>&nbsp;\(\langle\)&nbsp; \(u_1 + w_1,u_2\) \(\rangle\) by linearity<br></li><li>&nbsp;\(\langle\)&nbsp; \(v_1 P v_2\) \(\rangle\) by orthognal projection<br></li></ul><li>Thus&nbsp;\(P\) =&nbsp;\(P*\) and hence P is self-adjoint</li></ul>
-
-============================================================
-
-Note ID: 1706084748658
-  Field: Text
-    Before:
-      8 Suppose \(T \in \mathcal{L}(V), S \in \mathcal{L}(V)\) is an isometry, and \(R \in \mathcal{L}(V)\) is a positive operator such that \(T=S R\). Prove that \(R=\sqrt{T^* T}\).<br>[The exercise above shows that if we write \(T\) as the product of an isometry and a positive operator (as in the Polar Decomposition 7.45), then the positive operator equals \(\sqrt{T^* T}\).]<br><br>Proof:<br><ul><li>\(T = SR\) implies&nbsp;\(R\) =&nbsp;\(S^* T\) since isometries are their own inverses<br></li><li>||&nbsp;\(Rv\) ||\(^2\) =&nbsp; ||&nbsp;\(S^* Tv\) ||\(^2\) =&nbsp;</li><ul><li>\(\langle\)&nbsp;\(S^* Tv, S^* T\) \(\rangle\) by definition of squared norm<br></li><li>\(\langle\)&nbsp;\(Tv, Tv\) \(\rangle\) since isometries and their adjoints preserver inner products</li><li>\(\langle\)&nbsp;\(T^* Tv, v\) \(\rangle\) by definition of adjoint</li><li>\(\langle\)&nbsp;&nbsp;\(\sqrt{T^* Tv} \sqrt{T^* Tv}, v\)&nbsp;\(\rangle\) by definition of square root</li><li>|| \(\sqrt{T^* Tv}&nbsp;\) ||\(^2\)</li></ul><li>Which implies that&nbsp;\(R\) = \(\sqrt{T^* Tv}&nbsp;\)</li></ul>
-
-    After:
-      8 Suppose \(T \in \mathcal{L}(V), S \in \mathcal{L}(V)\) is an isometry, and \(R \in \mathcal{L}(V)\) is a positive operator such that \(T=S R\). Prove that \(R=\sqrt{T^* T}\).<br>[The exercise above shows that if we write \(T\) as the product of an isometry and a positive operator (as in the Polar Decomposition 7.45), then the positive operator equals \(\sqrt{T^* T}\).]<br><br>Proof:<br><ul><li>\(T = SR\) implies&nbsp;\(R\) =&nbsp;\(S^* T\) since isometries are their own inverses<br></li><li>||&nbsp;\(Rv\) ||\(^2\) =&nbsp; ||&nbsp;\(S^* Tv\) ||\(^2\) =&nbsp;</li><ul><li>\(\langle\)&nbsp;\(S^* Tv, S^* T\) \(\rangle\) by definition of squared norm<br></li><li>\(\langle\)&nbsp;\(Tv, Tv\) \(\rangle\) since isometries and their adjoints preserver inner products</li><li>\(\langle\)&nbsp;\(T^* Tv, v\) \(\rangle\) by definition of adjoint</li><li>\(\langle\)&nbsp;&nbsp;\(\sqrt{T^* Tv} \sqrt{T^* Tv}, v\)&nbsp;\(\rangle\) by definition of square root</li><li>|| \(\sqrt{T^* Tv}&nbsp;\) ||\(^2\)</li></ul><li>Which implies that&nbsp;\(R\) = \(\sqrt{T^* Tv}&nbsp;\)</li></ul>
-
-============================================================
-
-Note ID: 1706167648563
-  Field: Text
-    Before:
-      1. (6 points) Suppose that \(V\) is a complex inner product space with orthogonal basis \(\left(f_{1}, \ldots, f_{n}\right)\), and \(T \in \mathcal{L}(V)\).<br><br>a) Prove that any vector \(v \in V\) can be written<br><br>\[<br>v=\sum_{i=1}^{n} \frac{\left\langle v, f_{i}\right\rangle}{\left\langle f_{i}, f_{i}\right\rangle} f_{i}<br>\]<br><br>Proof:<br><ul><li>First, take inner product with&nbsp;\(f_j\)&nbsp;and use the fact that \(\langle\) \(f_i, f_j\) \(\rangle\) = \(0\) for \(i \neq j\).</li><ul><li>\(\langle\) \(v, f_j\) \(\rangle\) =&nbsp;\(a_j\) \(\langle\)&nbsp;\(f_j, f_j\) \(\rangle\)<br></li></ul><li>Now, because \(\langle\)&nbsp;\(f_j, f_j\) \(\rangle\) is guaranteed to be nonzero due to basis vector we can rearrange the equation into</li><ul><li>\(a_j\) =&nbsp;\(\frac{\left\langle v, f_j\right\rangle}{\left\langle f_j, f_j\right\rangle}\),<br></li></ul></ul>
-
-    After:
-      1. (6 points) Suppose that \(V\) is a complex inner product space with orthogonal basis \(\left(f_{1}, \ldots, f_{n}\right)\), and \(T \in \mathcal{L}(V)\).<br><br>a) Prove that any vector \(v \in V\) can be written<br><br>\[<br>v=\sum_{i=1}^{n} \frac{\left\langle v, f_{i}\right\rangle}{\left\langle f_{i}, f_{i}\right\rangle} f_{i}<br>\]<br><br>Proof:<br><ul><li>First, take inner product with&nbsp;\(f_j\)&nbsp;and use the fact that \(\langle\) \(f_i, f_j\) \(\rangle\) = \(0\) for \(i \neq j\).</li><ul><li>\(\langle\) \(v, f_j\) \(\rangle\) =&nbsp;\(a_j\) \(\langle\)&nbsp;\(f_j, f_j\) \(\rangle\)<br></li></ul><li>Now, because \(\langle\)&nbsp;\(f_j, f_j\) \(\rangle\) is guaranteed to be nonzero due to basis vector we can rearrange the equation into</li><ul><li>\(a_j\) =&nbsp;\(\frac{\left\langle v, f_j\right\rangle}{\left\langle f_j, f_j\right\rangle}\),<br></li></ul></ul>
-
-============================================================
-
-Note ID: 1706169059472
-  Field: Text
-    Before:
-      19 Suppose \(V\) is a real inner product space. Prove that<br>\[<br>\langle u, v\rangle=\frac{\|u+v\|^2-\|u-v\|^2}{4}<br>\]<br>for all \(u, v \in V\).<br><br>What is the trick?<br><ul><li>Expand until you get (\( 2\) \(\langle\) u,v \(\rangle\) + \(2\) \(\langle\) v, u \(\rangle\) )&nbsp;\(/ 4\)&nbsp;</li><li>And then use the fact that the vector space is real to conclude that&nbsp;\(\langle\)&nbsp;\(v,u\) \(\rangle\)&nbsp; = \(\langle\)&nbsp;\(u,v\) \(\rangle\)&nbsp;</li></ul>
-
-    After:
-      19 Suppose \(V\) is a real inner product space. Prove that<br>\[<br>\langle u, v\rangle=\frac{\|u+v\|^2-\|u-v\|^2}{4}<br>\]<br>for all \(u, v \in V\).<br><br>What is the trick?<br><ul><li>Expand until you get (\( 2\) \(\langle\) u,v \(\rangle\) + \(2\) \(\langle\) v, u \(\rangle\) )&nbsp;\(/ 4\)&nbsp;</li><li>And then use the fact that the vector space is real to conclude that&nbsp;\(\langle\)&nbsp;\(v,u\) \(\rangle\)&nbsp; = \(\langle\)&nbsp;\(u,v\) \(\rangle\)&nbsp;</li></ul>
-
-============================================================
-
-Note ID: 1706281373927
-  Field: Text
-    Before:
-      30. Suppose \(T \in \mathcal{L}(V, W)\). Prove that<br><br>(a) \(T\) is injective if and only if \(T^{*}\) is surjective;<br><br><ul><li>\(T\) is injective \(\Longleftrightarrow\) null \(T\)=\(\{0\}\)<br></li><li>\(\Longleftrightarrow\)&nbsp;\(\left(\text { range } T^*\right)^{\perp}\) = \(\{0\}\)<br></li><li>\(\Longleftrightarrow\)&nbsp;\(\operatorname{range} T^*\) = \(W\)<br></li><li>\(\Longleftrightarrow\)&nbsp;\(T^*\) is surjective<br></li></ul>
-
-    After:
-      30. Suppose \(T \in \mathcal{L}(V, W)\). Prove that<br><br>(a) \(T\) is injective if and only if \(T^{*}\) is surjective;<br><br><ul><li>\(T\) is injective \(\Longleftrightarrow\) null \(T\)=\(\{0\}\)<br></li><li>\(\Longleftrightarrow\)&nbsp;\(\left(\text { range } T^*\right)^{\perp}\) = \(\{0\}\)<br></li><li>\(\Longleftrightarrow\)&nbsp;\(\operatorname{range} T^*\) = \(W\)<br></li><li>\(\Longleftrightarrow\)&nbsp;\(T^*\) is surjective<br></li></ul>
-
-============================================================
-
-Note ID: 1706287270553
-  Field: Text
-    Before:
-      <img src="paste-ddc1d44cb2a70885dac2caf27b8c6caa23806516.jpg"><br>Why do we understand both operators in the polar decomposition pretty well?<br><ul><li>The second operator&nbsp;\(\sqrt{T^* T}\). is a positive opeartor thus it is self-adjoint and normal meaning that it is also diagonalizable i.e a basis of orthonormal eigenvectors exists for it</li><li>The first is an isometry, so over C since all isometries are normal then S is also diagonalizable by the spectral theorem</li><li>Warning: the orthonormal eigenbasse of the two operators are&nbsp; not guaranteed to be the same.</li></ul>
-
-    After:
-      <img src="paste-ddc1d44cb2a70885dac2caf27b8c6caa23806516.jpg"><br>Why do we understand both operators in the polar decomposition pretty well?<br><ul><li>The second operator&nbsp;\(\sqrt{T^* T}\). is a positive opeartor thus it is self-adjoint and normal meaning that it is also diagonalizable i.e a basis of orthonormal eigenvectors exists for it</li><li>The first is an isometry, so over C since all isometries are normal then S is also diagonalizable by the spectral theorem</li><li>Warning: the orthonormal eigenbasse of the two operators are&nbsp; not guaranteed to be the same.</li></ul>
-
-============================================================
-
-Note ID: 1706685486391
-  Field: Text
-    Before:
-      <img src="paste-6594a741e8f9def1ba601b6671b48342832c0e5e.jpg"><br>Proof:<br><ul><li>By the Spectral Theorem, there is an orthonormal basis \(e_1, \ldots, e_n\) of \(V\) such that:</li><ul><li>&nbsp;\(\sqrt{T^* T} e_j\) = \(s_j e_j\) for \(j=1, \ldots, n\).</li><li>the associated eigenvalues of this are precisely the singular values by definitiojn</li></ul><li>Write&nbsp;\(v\) =&nbsp;\(\sum\)&nbsp;\(\langle\)\(v,e_i\)&nbsp;\(\rangle\) \(e_i\)</li><li>Apply \(\sqrt{T^* T}\) to both sides of this equation, getting<br></li><ul><li>\(\sqrt{T^* T}v\) = \(\sum\)&nbsp;\(\langle\)\(v,e_i\)&nbsp;\(\rangle\) \(\sqrt{T^* T}\)\(e_i\) by linearity<br></li><li>= \(\sum\)&nbsp;\(s_i\)\(\langle\)\(v,e_i\)&nbsp;\(\rangle\)&nbsp;\(e_i\) by definition</li></ul><li>By the polar decomposition there exists an isometry S such that&nbsp;\(T\) =&nbsp;\(S \sqrt{T^* T}\).</li><li>Apply S to both sides of the equation above to get</li><ul><li>\(T\) = \(\sum\)&nbsp;\(s_i\)\(\langle\)\(v,e_i\)&nbsp;\(\rangle\)&nbsp;\(S\)\(e_i\)&nbsp;<br></li></ul><li>For each j let&nbsp;\(f_i\) =&nbsp;\(S e_i\), this is an orthonormal basis since S is an isometry</li><li>The equation now becomes</li><ul><li>\(T\) = \(\sum\)&nbsp;\(s_i\)\(\langle\)\(v,e_i\)&nbsp;\(\rangle\)&nbsp;\(f_i\)</li></ul></ul>
-
-    After:
-      <img src="paste-6594a741e8f9def1ba601b6671b48342832c0e5e.jpg"><br>Proof:<br><ul><li>By the Spectral Theorem, there is an orthonormal basis \(e_1, \ldots, e_n\) of \(V\) such that:</li><ul><li>&nbsp;\(\sqrt{T^* T} e_j\) = \(s_j e_j\) for \(j=1, \ldots, n\).</li><li>the associated eigenvalues of this are precisely the singular values by definitiojn</li></ul><li>Write&nbsp;\(v\) =&nbsp;\(\sum\)&nbsp;\(\langle\)\(v,e_i\)&nbsp;\(\rangle\) \(e_i\)</li><li>Apply \(\sqrt{T^* T}\) to both sides of this equation, getting<br></li><ul><li>\(\sqrt{T^* T}v\) = \(\sum\)&nbsp;\(\langle\)\(v,e_i\)&nbsp;\(\rangle\) \(\sqrt{T^* T}\)\(e_i\) by linearity<br></li><li>= \(\sum\)&nbsp;\(s_i\)\(\langle\)\(v,e_i\)&nbsp;\(\rangle\)&nbsp;\(e_i\) by definition</li></ul><li>By the polar decomposition there exists an isometry S such that&nbsp;\(T\) =&nbsp;\(S \sqrt{T^* T}\).</li><li>Apply S to both sides of the equation above to get</li><ul><li>\(T\) = \(\sum\)&nbsp;\(s_i\)\(\langle\)\(v,e_i\)&nbsp;\(\rangle\)&nbsp;\(S\)\(e_i\)&nbsp;<br></li></ul><li>For each j let&nbsp;\(f_i\) =&nbsp;\(S e_i\), this is an orthonormal basis since S is an isometry</li><li>The equation now becomes</li><ul><li>\(T\) = \(\sum\)&nbsp;\(s_i\)\(\langle\)\(v,e_i\)&nbsp;\(\rangle\)&nbsp;\(f_i\)</li></ul></ul>
-
-============================================================
-
-Note ID: 1706685649536
-  Field: Text
-    Before:
-      <img src="paste-6594a741e8f9def1ba601b6671b48342832c0e5e.jpg"><br>Proof:<br><ul><li>By the Spectral Theorem, there is an orthonormal basis \(e_1, \ldots, e_n\) of \(V\) such that:</li><ul><li>&nbsp;\(\sqrt{T^* T} e_j\) = \(s_j e_j\) for \(j=1, \ldots, n\).</li><li>the associated eigenvalues of this are precisely the singular values by definitiojn</li></ul><li>Write&nbsp;\(v\) =&nbsp;\(\sum\)&nbsp;\(\langle\)\(v,e_i\)&nbsp;\(\rangle\) \(e_i\)</li><li>Apply \(\sqrt{T^* T}\) to both sides of this equation, getting<br></li><ul><li>\(\sqrt{T^* T}v\) = \(\sum\)&nbsp;\(\langle\)\(v,e_i\)&nbsp;\(\rangle\) \(\sqrt{T^* T}\)\(e_i\) by linearity<br></li><li>= \(\sum\)&nbsp;\(s_i\)\(\langle\)\(v,e_i\)&nbsp;\(\rangle\)&nbsp;\(e_i\) by definition</li></ul><li>By the polar decomposition there exists an isometry S such that&nbsp;\(T\) =&nbsp;\(S \sqrt{T^* T}\).</li><li>Apply S to both sides of the equation above to get</li><ul><li>\(T\) = \(\sum\)&nbsp;\(s_i\)\(\langle\)\(v,e_i\)&nbsp;\(\rangle\)&nbsp;\(S\)\(e_i\)&nbsp;<br></li></ul><li>For each j let&nbsp;\(f_i\) =&nbsp;\(S e_i\), this is an orthonormal basis since S is an isometry</li><li>The equation now becomes</li><ul><li>\(T\) = \(\sum\)&nbsp;\(s_i\)\(\langle\)\(v,e_i\)&nbsp;\(\rangle\)&nbsp;\(f_i\)</li></ul></ul>
-
-    After:
-      <img src="paste-6594a741e8f9def1ba601b6671b48342832c0e5e.jpg"><br>Proof:<br><ul><li>By the Spectral Theorem, there is an orthonormal basis \(e_1, \ldots, e_n\) of \(V\) such that:</li><ul><li>&nbsp;\(\sqrt{T^* T} e_j\) = \(s_j e_j\) for \(j=1, \ldots, n\).</li><li>the associated eigenvalues of this are precisely the singular values by definitiojn</li></ul><li>Write&nbsp;\(v\) =&nbsp;\(\sum\)&nbsp;\(\langle\)\(v,e_i\)&nbsp;\(\rangle\) \(e_i\)</li><li>Apply \(\sqrt{T^* T}\) to both sides of this equation, getting<br></li><ul><li>\(\sqrt{T^* T}v\) = \(\sum\)&nbsp;\(\langle\)\(v,e_i\)&nbsp;\(\rangle\) \(\sqrt{T^* T}\)\(e_i\) by linearity<br></li><li>= \(\sum\)&nbsp;\(s_i\)\(\langle\)\(v,e_i\)&nbsp;\(\rangle\)&nbsp;\(e_i\) by definition</li></ul><li>By the polar decomposition there exists an isometry S such that&nbsp;\(T\) =&nbsp;\(S \sqrt{T^* T}\).</li><li>Apply S to both sides of the equation above to get</li><ul><li>\(T\) = \(\sum\)&nbsp;\(s_i\)\(\langle\)\(v,e_i\)&nbsp;\(\rangle\)&nbsp;\(S\)\(e_i\)&nbsp;<br></li></ul><li>For each j let&nbsp;\(f_i\) =&nbsp;\(S e_i\), this is an orthonormal basis since S is an isometry</li><li>The equation now becomes</li><ul><li>\(T\) = \(\sum\)&nbsp;\(s_i\)\(\langle\)\(v,e_i\)&nbsp;\(\rangle\)&nbsp;\(f_i\)</li></ul></ul>
-
-============================================================
-
-Note ID: 1706687074577
-  Field: Text
-    Before:
-      Sequence of increasing null spaces<br><br>Suppose \(T \in \mathcal{L}(V)\). Then<br><br><ul><li>\(\{0\}\) = \(\operatorname{null} T^{0}\) \(\subset\) \(\text { null } T^{1}\) \(\subset\) \(\cdots\) \(\subset\) \(\text { null } T^{k}\) \(\subset\) \(\text { null } T^{k+1}\) \(\subset\) \(\cdots .\)</li></ul>
-
-    After:
-      Sequence of increasing null spaces<br><br>Suppose \(T \in \mathcal{L}(V)\). Then<br><br><ul><li>\(\{0\}\) = \(\operatorname{null} T^{0}\) \(\subset\) \(\text { null } T^{1}\) \(\subset\) \(\cdots\) \(\subset\) \(\text { null } T^{k}\) \(\subset\) \(\text { null } T^{k+1}\) \(\subset\) \(\cdots .\)</li></ul>
-
-============================================================
-
-Note ID: 1706687244135
-  Field: Text
-    Before:
-      <img src="paste-b8c98635f068d0821371de190b579e04dea816cb.jpg"><br><span style="color: rgb(0, 0, 0);">Proof:</span><br><ul><li>Proof Suppose \(k\) is a nonnegative integer and \(v \in \operatorname{null} T^{k}\).&nbsp;</li><li>Then \(T^{k} v\) = \(0\), and hence&nbsp;</li><ul><li>\(T^{k+1} v\) = \(T\left(T^{k} v\right) \)= \(T(0)\)= \(0\).&nbsp;</li></ul><li>Thus \(v \in\) \(\operatorname{null} T^{k+1}\).&nbsp;</li><li>Hence null \(T^{k} \subset\) null \(T^{k+1}\), as desired.</li></ul>
-
-    After:
-      <img src="paste-b8c98635f068d0821371de190b579e04dea816cb.jpg"><br><span style="color: rgb(0, 0, 0);">Proof:</span><br><ul><li>Proof Suppose \(k\) is a nonnegative integer and \(v \in \operatorname{null} T^{k}\).&nbsp;</li><li>Then \(T^{k} v\) = \(0\), and hence&nbsp;</li><ul><li>\(T^{k+1} v\) = \(T\left(T^{k} v\right) \)= \(T(0)\)= \(0\).&nbsp;</li></ul><li>Thus \(v \in\) \(\operatorname{null} T^{k+1}\).&nbsp;</li><li>Hence null \(T^{k} \subset\) null \(T^{k+1}\), as desired.</li></ul>
-
-============================================================
-
-Note ID: 1706687384414
-  Field: Text
-    Before:
-      Equality in the sequence of null spaces<br><br>Suppose \(T \in \mathcal{L}(V)\). Suppose \(m\) is a nonnegative integer such that null \(T^{m}\) = \(\operatorname{null} T^{m+1}\). Then<br><br><ul><li>\(\operatorname{null} T^{m}\) = \(\operatorname{null} T^{m+1}\) = \(\operatorname{null} T^{m+2}\) = \(\operatorname{null} T^{m+3}\) = \(\cdots .\)</li></ul>
-
-    After:
-      Equality in the sequence of null spaces<br><br>Suppose \(T \in \mathcal{L}(V)\). Suppose \(m\) is a nonnegative integer such that null \(T^{m}\) = \(\operatorname{null} T^{m+1}\). Then<br><br><ul><li>\(\operatorname{null} T^{m}\) = \(\operatorname{null} T^{m+1}\) = \(\operatorname{null} T^{m+2}\) = \(\operatorname{null} T^{m+3}\) = \(\cdots .\)</li></ul>
-
-============================================================
-
-Note ID: 1706687590895
-  Field: Text
-    Before:
-      Null spaces stop growing<br><br>Suppose \(T \in \mathcal{L}(V)\). Let \(n\) = \(\operatorname{dim} V\). Then<br><br><ul><li>\(\operatorname{null} T^{n}\) = \(\operatorname{null} T^{n+1}\) = \(\operatorname{null} T^{n+2}\) = \(\cdots .\)</li></ul>
-
-    After:
-      Null spaces stop growing<br><br>Suppose \(T \in \mathcal{L}(V)\). Let \(n\) = \(\operatorname{dim} V\). Then<br><br><ul><li>\(\operatorname{null} T^{n}\) = \(\operatorname{null} T^{n+1}\) = \(\operatorname{null} T^{n+2}\) = \(\cdots .\)</li></ul>
-
-============================================================
-
-Note ID: 1706687730510
-  Field: Text
-    Before:
-      Why is this true?<br><img src="paste-40474fac57ce81a59681cd605a5adf81d42817d1.jpg"><br>Because:<br><ul><li>Null spaces of increasing powers of an operator are contained within the null space of the larger power</li><li>Null spaces are subspaces&nbsp;</li><li>A subspaces cannot have dimension larger than the vector space so the growth must stop</li></ul>
-
-    After:
-      Why is this true?<br><img src="paste-40474fac57ce81a59681cd605a5adf81d42817d1.jpg"><br>Because:<br><ul><li>Null spaces of increasing powers of an operator are contained within the null space of the larger power</li><li>Null spaces are subspaces&nbsp;</li><li>A subspaces cannot have dimension larger than the vector space so the growth must stop</li></ul>
-
-============================================================
-
-Note ID: 1706687909107
-  Field: Text
-    Before:
-      \(V\) is the direct sum of null \(T^{\operatorname{dim} V}\) and \(\operatorname{range} T^{\operatorname{dim} V}\)<br><br>Suppose \(T \in \mathcal{L}(V)\). Let \(n\) = \(\operatorname{dim} V\). Then<br><br><ul><li>\(V\) = \(\operatorname{null} T^{n}\) \(\oplus\) \(\text { range } T^{n} .\)</li></ul>
-
-    After:
-      \(V\) is the direct sum of null \(T^{\operatorname{dim} V}\) and \(\operatorname{range} T^{\operatorname{dim} V}\)<br><br>Suppose \(T \in \mathcal{L}(V)\). Let \(n\) = \(\operatorname{dim} V\). Then<br><br><ul><li>\(V\) = \(\operatorname{null} T^{n}\) \(\oplus\) \(\text { range } T^{n} .\)</li></ul>
-
-============================================================
-
-Note ID: 1706688232781
-  Field: Text
-    Before:
-      Definition generalized eigenvector<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(\lambda\) is an eigenvalue of \(T\). A vector \(v \in V\) is called a generalized eigenvector of \(T\) corresponding to \(\lambda\) if \(v\) \(\neq\) \(0\)&nbsp;and<br><br><ul><li>\((T-\lambda I)^{j}\) \(v\) = \(0\)</li></ul><br>for some positive integer \(j\).<br>
-
-    After:
-      Definition generalized eigenvector<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(\lambda\) is an eigenvalue of \(T\). A vector \(v \in V\) is called a generalized eigenvector of \(T\) corresponding to \(\lambda\) if \(v\) \(\neq\) \(0\)&nbsp;and<br><br><ul><li>\((T-\lambda I)^{j}\) \(v\) = \(0\)</li></ul><br>for some positive integer \(j\).<br>
-
-============================================================
-
-Note ID: 1706688302939
-  Field: Text
-    Before:
-      <ul><li>We do not define the concept of a generalized eigenvalue, because this would not lead to anything new.&nbsp;</li><li>Reason: if \((T-\lambda I)^j\) is not injective for some positive integer \(j\), then \(T-\lambda I\) is not injective, and hence \(\lambda\) is an eigenvalue of \(T\).</li></ul>
-
-    After:
-      <ul><li>We do not define the concept of a generalized eigenvalue, because this would not lead to anything new.&nbsp;</li><li>Reason: if \((T-\lambda I)^j\) is not injective for some positive integer \(j\), then \(T-\lambda I\) is not injective, and hence \(\lambda\) is an eigenvalue of \(T\).</li></ul>
-
-============================================================
-
-Note ID: 1706688406713
-  Field: Text
-    Before:
-      Definition generalized eigenspace, \(G(\lambda, T)\)<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(\lambda \in \mathbf{F}\). The generalized eigenspace of \(T\) corresponding to \(\lambda\), denoted \(G(\lambda, T)\), is defined to be the set of all generalized eigenvectors of \(T\) corresponding to \(\lambda\), along with the 0 vector.
-
-    After:
-      Definition generalized eigenspace, \(G(\lambda, T)\)<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(\lambda \in \mathbf{F}\). The generalized eigenspace of \(T\) corresponding to \(\lambda\), denoted \(G(\lambda, T)\), is defined to be the set of all generalized eigenvectors of \(T\) corresponding to \(\lambda\), along with the 0 vector.
-
-============================================================
-
-Note ID: 1706688911201
-  Field: Text
-    Before:
-      Linearly independent generalized eigenvectors<br><br>Let \(T \in \mathcal{L}(V)\). Suppose \(\lambda_{1}, \ldots, \lambda_{m}\) are distinct eigenvalues of \(T\) and \(v_{1}, \ldots, v_{m}\) are corresponding generalized eigenvectors. Then \(v_{1}, \ldots, v_{m}\) is linearly independent.
-
-    After:
-      Linearly independent generalized eigenvectors<br><br>Let \(T \in \mathcal{L}(V)\). Suppose \(\lambda_{1}, \ldots, \lambda_{m}\) are distinct eigenvalues of \(T\) and \(v_{1}, \ldots, v_{m}\) are corresponding generalized eigenvectors. Then \(v_{1}, \ldots, v_{m}\) is linearly independent.
-
-============================================================
-
-Note ID: 1706770183244
-  Field: Text
-    Before:
-      <img src="paste-e9899024693e7c56ae0e87d2ec9f8b6081e05ae0.jpg"><br>Proof Suppose \(a_{1}, \ldots, a_{m}\) are complex numbers such that<br><br>8.14<br><br>\[<br>0=a_{1} v_{1}+\cdots+a_{m} v_{m} .<br>\]<br><br><ul><li>Let \(k\) be the largest nonnegative integer such that:</li><ul><li>&nbsp;\(\left(T-\lambda_{1} I\right)^{k}\) \(v_{1}\)&nbsp; \(\neq\) \(0\).&nbsp;</li></ul><li>Let \(w\) = \(\left(T-\lambda_{1} I\right)^{k}\) \(v_{1} .\)</li></ul><br>Thus<br><br><ul><li>\(\left(T-\lambda_{1} I\right)\) \(w\) = \(\left(T-\lambda_{1} I\right)^{k+1}\) \(w\) = \(0,\)</li></ul><br>and hence \(T w\) = \(\lambda_{1} w\). Thus \((T-\lambda I) w=\left(\lambda_{1}-\lambda\right) w\) for every \(\lambda \in \mathbf{F}\) and hence<br><br><ul><li>\((T-\lambda I)^{n} w=\left(\lambda_{1}-\lambda\right)^{n} w\)</li></ul><br>for every \(\lambda \in \mathbf{F}\), where \(n=\operatorname{dim} V\).<br><br>Apply the operator<br><br>\(\left(T-\lambda_{1} I\right)^{k}\) \(\left(T-\lambda_{2} I\right)^{n}\) \(\cdots\) \(\left(T-\lambda_{m} I\right)^{n}\)<br><br>to both sides of 8.14 , getting<br><br><ul><li>0 = \(a_1\left(T-\lambda_1 I\right)^k\left(T-\lambda_2 I\right)^n \cdots\left(T-\lambda_m I\right)^n v_1\)</li><li>=&nbsp;\(a_1\left(T-\lambda_2 I\right)^n \cdots\left(T-\lambda_m I\right)^n w\)</li><li>&nbsp;=&nbsp;\(a_1\left(\lambda_1-\lambda_2\right)^n \cdots\left(\lambda_1-\lambda_m\right)^n w\),</li></ul>where we have used 8.11 to get the first equation above and 8.15 to get the last equation above.<br><br>The equation above implies that \(a_{1}=0\). In a similar fashion, \(a_{j}=0\) for each \(j\), which implies that \(v_{1}, \ldots, v_{m}\) is linearly independent.<br>
-
-    After:
-      <img src="paste-e9899024693e7c56ae0e87d2ec9f8b6081e05ae0.jpg"><br>Proof Suppose \(a_{1}, \ldots, a_{m}\) are complex numbers such that<br><br>8.14<br><br>\[<br>0=a_{1} v_{1}+\cdots+a_{m} v_{m} .<br>\]<br><br><ul><li>Let \(k\) be the largest nonnegative integer such that:</li><ul><li>&nbsp;\(\left(T-\lambda_{1} I\right)^{k}\) \(v_{1}\)&nbsp; \(\neq\) \(0\).&nbsp;</li></ul><li>Let \(w\) = \(\left(T-\lambda_{1} I\right)^{k}\) \(v_{1} .\)</li></ul><br>Thus<br><br><ul><li>\(\left(T-\lambda_{1} I\right)\) \(w\) = \(\left(T-\lambda_{1} I\right)^{k+1}\) \(w\) = \(0,\)</li></ul><br>and hence \(T w\) = \(\lambda_{1} w\). Thus \((T-\lambda I) w=\left(\lambda_{1}-\lambda\right) w\) for every \(\lambda \in \mathbf{F}\) and hence<br><br><ul><li>\((T-\lambda I)^{n} w=\left(\lambda_{1}-\lambda\right)^{n} w\)</li></ul><br>for every \(\lambda \in \mathbf{F}\), where \(n=\operatorname{dim} V\).<br><br>Apply the operator<br><br>\(\left(T-\lambda_{1} I\right)^{k}\) \(\left(T-\lambda_{2} I\right)^{n}\) \(\cdots\) \(\left(T-\lambda_{m} I\right)^{n}\)<br><br>to both sides of 8.14 , getting<br><br><ul><li>0 = \(a_1\left(T-\lambda_1 I\right)^k\left(T-\lambda_2 I\right)^n \cdots\left(T-\lambda_m I\right)^n v_1\)</li><li>=&nbsp;\(a_1\left(T-\lambda_2 I\right)^n \cdots\left(T-\lambda_m I\right)^n w\)</li><li>&nbsp;=&nbsp;\(a_1\left(\lambda_1-\lambda_2\right)^n \cdots\left(\lambda_1-\lambda_m\right)^n w\),</li></ul>where we have used 8.11 to get the first equation above and 8.15 to get the last equation above.<br><br>The equation above implies that \(a_{1}=0\). In a similar fashion, \(a_{j}=0\) for each \(j\), which implies that \(v_{1}, \ldots, v_{m}\) is linearly independent.<br>
-
-============================================================
-
-Note ID: 1706770722152
-  Field: Text
-    Before:
-      <img src="paste-e9899024693e7c56ae0e87d2ec9f8b6081e05ae0.jpg"><br>Proof Suppose \(a_{1}, \ldots, a_{m}\) are complex numbers such that<br><br>8.14<br><br>\[<br>0=a_{1} v_{1}+\cdots+a_{m} v_{m} .<br>\]<br><br><ul><li>Let \(k\) be the largest nonnegative integer such that:</li><ul><li>&nbsp;\(\left(T-\lambda_{1} I\right)^{k} v_{1}\)&nbsp; \(\neq\) \(0\).&nbsp;</li></ul><li>Let \(w\) = \(\left(T-\lambda_{1} I\right)^{k} v_{1} .\)</li></ul><br>Thus<br><br><ul><li>\(\left(T-\lambda_{1} I\right) w\) = \(\left(T-\lambda_{1} I\right)^{k+1} w\) = \(0,\)</li></ul><br>Hence:<br><ul><li>\(T w\) = \(\lambda_{1} w\).&nbsp;</li><li>Thus:</li><ul><li>&nbsp;\((T-\lambda I)\) \(w\) = \(\left(\lambda_{1}-\lambda\right) w\) for every \(\lambda \in \mathbf{F}\)&nbsp;</li></ul><li>&nbsp;hence</li><ul><li>\((T-\lambda I)^{n} w=\left(\lambda_{1}-\lambda\right)^{n} w\)</li><li>for every \(\lambda \in \mathbf{F}\), where \(n=\operatorname{dim} V\).</li></ul></ul><br>Apply the operator<br><ul><li>\(\left(T-\lambda_{1} I\right)^{k}\) \(\left(T-\lambda_{2} I\right)^{n}\) \(\cdots\) \(\left(T-\lambda_{m} I\right)^{n}\)</li></ul><br>to both sides of 8.14 , getting<br><br><ul><li>0 = \(a_1\)\(\left(T-\lambda_1 I\right)^k\left(T-\lambda_2 I\right)^n \cdots\left(T-\lambda_m I\right)^n\) \(v_1\)</li><li>=&nbsp;\(a_1\) \(\left(T-\lambda_2 I\right)^n \cdots\left(T-\lambda_m I\right)^n\) \(w\)</li><li>&nbsp;=&nbsp;\(a_1\) \(\left(\lambda_1-\lambda_2\right)^n \cdots\left(\lambda_1-\lambda_m\right)^n\) \(w\),</li></ul>where we have used 8.11 to get the first equation above and 8.15 to get the last equation above.<br><br>The equation above implies that \(a_{1}\) = \(0\). In a similar fashion, \(a_{j}\) = \(0\) for each \(j\), which implies that \(v_{1}, \ldots, v_{m}\) is linearly independent.<br>
-
-    After:
-      <img src="paste-e9899024693e7c56ae0e87d2ec9f8b6081e05ae0.jpg"><br>Proof Suppose \(a_{1}, \ldots, a_{m}\) are complex numbers such that<br><br>8.14<br><br>\[<br>0=a_{1} v_{1}+\cdots+a_{m} v_{m} .<br>\]<br><br><ul><li>Let \(k\) be the largest nonnegative integer such that:</li><ul><li>&nbsp;\(\left(T-\lambda_{1} I\right)^{k} v_{1}\)&nbsp; \(\neq\) \(0\).&nbsp;</li></ul><li>Let \(w\) = \(\left(T-\lambda_{1} I\right)^{k} v_{1} .\)</li></ul><br>Thus<br><br><ul><li>\(\left(T-\lambda_{1} I\right) w\) = \(\left(T-\lambda_{1} I\right)^{k+1} w\) = \(0,\)</li></ul><br>Hence:<br><ul><li>\(T w\) = \(\lambda_{1} w\).&nbsp;</li><li>Thus:</li><ul><li>&nbsp;\((T-\lambda I)\) \(w\) = \(\left(\lambda_{1}-\lambda\right) w\) for every \(\lambda \in \mathbf{F}\)&nbsp;</li></ul><li>&nbsp;hence</li><ul><li>\((T-\lambda I)^{n} w=\left(\lambda_{1}-\lambda\right)^{n} w\)</li><li>for every \(\lambda \in \mathbf{F}\), where \(n=\operatorname{dim} V\).</li></ul></ul><br>Apply the operator<br><ul><li>\(\left(T-\lambda_{1} I\right)^{k}\) \(\left(T-\lambda_{2} I\right)^{n}\) \(\cdots\) \(\left(T-\lambda_{m} I\right)^{n}\)</li></ul><br>to both sides of 8.14 , getting<br><br><ul><li>0 = \(a_1\)\(\left(T-\lambda_1 I\right)^k\left(T-\lambda_2 I\right)^n \cdots\left(T-\lambda_m I\right)^n\) \(v_1\)</li><li>=&nbsp;\(a_1\) \(\left(T-\lambda_2 I\right)^n \cdots\left(T-\lambda_m I\right)^n\) \(w\)</li><li>&nbsp;=&nbsp;\(a_1\) \(\left(\lambda_1-\lambda_2\right)^n \cdots\left(\lambda_1-\lambda_m\right)^n\) \(w\),</li></ul>where we have used 8.11 to get the first equation above and 8.15 to get the last equation above.<br><br>The equation above implies that \(a_{1}\) = \(0\). In a similar fashion, \(a_{j}\) = \(0\) for each \(j\), which implies that \(v_{1}, \ldots, v_{m}\) is linearly independent.<br>
-
-============================================================
-
-Note ID: 1706771367335
-  Field: Text
-    Before:
-      Matrix of a nilpotent operator<br><br>Suppose \(N\) is a nilpotent operator on \(V\). Then there is a basis of \(V\) with respect to which the matrix of \(N\) has the form<br><br>\[<br>\left(\begin{array}{ccc}<br>0 &amp; &amp; * \\<br>&amp; \ddots &amp; \\<br>0 &amp; &amp; 0<br>\end{array}\right) \text {; }<br>\]<br><br>here all entries on and below the diagonal are 0's.
-
-    After:
-      Matrix of a nilpotent operator<br><br>Suppose \(N\) is a nilpotent operator on \(V\). Then there is a basis of \(V\) with respect to which the matrix of \(N\) has the form<br><br>\[<br>\left(\begin{array}{ccc}<br>0 &amp; &amp; * \\<br>&amp; \ddots &amp; \\<br>0 &amp; &amp; 0<br>\end{array}\right) \text {; }<br>\]<br><br>here all entries on and below the diagonal are 0's.
-
-============================================================
-
-Note ID: 1706771853459
-  Field: Text
-    Before:
-      <img src="paste-4c50ecf5917708940cd50b108f4f45d3b1fc9680.jpg"><br><br>Proof:<br><ul><li>First choose a basis of null \(N\).&nbsp;</li><li>Then extend this to a basis of null \(N^{2}\), then null \(N^{3}\), eventually getting a basis of \(V\)&nbsp;</li><li>Because 8.18 states that null \(N^{\operatorname{dim} V}\) = \(V\) .</li></ul><br>Now let's think about the matrix of \(N\) with respect to this basis. :<br><ul><li>The first column, and perhaps additional columns at the beginning, consists of all 0 's, because the corresponding basis vectors are in null \(N\).&nbsp;</li><li>The next set of columns comes from basis vectors in null \(N^{2}\).&nbsp;</li><li>Applying \(N\) to any such vector, we get a vector in null \(N\); in other words, we get a vector that is a linear combination of the previous basis vectors.&nbsp;</li><li>Thus all nonzero entries in these columns lie above the diagonal.&nbsp;</li><li>Repeat like this&nbsp;</li></ul>
-
-    After:
-      <img src="paste-4c50ecf5917708940cd50b108f4f45d3b1fc9680.jpg"><br><br>Proof:<br><ul><li>First choose a basis of null \(N\).&nbsp;</li><li>Then extend this to a basis of null \(N^{2}\), then null \(N^{3}\), eventually getting a basis of \(V\)&nbsp;</li><li>Because 8.18 states that null \(N^{\operatorname{dim} V}\) = \(V\) .</li></ul><br>Now let's think about the matrix of \(N\) with respect to this basis. :<br><ul><li>The first column, and perhaps additional columns at the beginning, consists of all 0 's, because the corresponding basis vectors are in null \(N\).&nbsp;</li><li>The next set of columns comes from basis vectors in null \(N^{2}\).&nbsp;</li><li>Applying \(N\) to any such vector, we get a vector in null \(N\); in other words, we get a vector that is a linear combination of the previous basis vectors.&nbsp;</li><li>Thus all nonzero entries in these columns lie above the diagonal.&nbsp;</li><li>Repeat like this&nbsp;</li></ul>
-
-============================================================
-
-Note ID: 1706771976297
-  Field: Text
-    Before:
-      3 Suppose \(T \in \mathcal{L}(V)\) is invertible. Prove that \(G\)( \(\lambda, T)\) ) = \(G\) ( \(\frac{1}{\lambda}, T^{-1}\) ) for every \(\lambda \in \mathbf{F}\) with \(\lambda \neq 0\).
-
-    After:
-      3 Suppose \(T \in \mathcal{L}(V)\) is invertible. Prove that \(G\)( \(\lambda, T)\) ) = \(G\) ( \(\frac{1}{\lambda}, T^{-1}\) ) for every \(\lambda \in \mathbf{F}\) with \(\lambda \neq 0\).
-
-============================================================
-
-Note ID: 1706772110894
-  Field: Text
-    Before:
-      4 Suppose \(T \in \mathcal{L}(V)\) and \(\alpha, \beta\)&nbsp; \(\in\) \(\mathbf{F}\) with \(\alpha \neq \beta\). Prove that<br><br><ul><li>G( \(\alpha, T\) ) \(\cap\) G( \(\beta, T\) ) = \(\{0\} .\)</li></ul>
-
-    After:
-      4 Suppose \(T \in \mathcal{L}(V)\) and \(\alpha, \beta\)&nbsp; \(\in\) \(\mathbf{F}\) with \(\alpha \neq \beta\). Prove that<br><br><ul><li>G( \(\alpha, T\) ) \(\cap\) G( \(\beta, T\) ) = \(\{0\} .\)</li></ul>
-
-============================================================
-
-Note ID: 1706772311428
-  Field: Text
-    Before:
-      5 Suppose \(T \in \mathcal{L}(V), m\) is a positive integer, and \(v \in V\) is such that \(T^{m-1}\) \(v\) \(\neq\) \(0\) but \(T^{m}\) \(v\)= \(0\). Prove that<br><br><ul><li>\(v\), \(T v\), \(T^{2} v\), \(\ldots\), \(T^{m-1} v\)</li></ul><br>is linearly independent.<br>
-
-    After:
-      5 Suppose \(T \in \mathcal{L}(V), m\) is a positive integer, and \(v \in V\) is such that \(T^{m-1}\) \(v\) \(\neq\) \(0\) but \(T^{m}\) \(v\)= \(0\). Prove that<br><br><ul><li>\(v\), \(T v\), \(T^{2} v\), \(\ldots\), \(T^{m-1} v\)</li></ul><br>is linearly independent.<br>
-
-============================================================
-
-Note ID: 1706772369902
-  Field: Text
-    Before:
-      9 Suppose \(S, T \in \mathcal{L}(V)\) and \(S T\) is nilpotent. Prove that \(T S\) is nilpotent.
-
-    After:
-      9 Suppose \(S, T \in \mathcal{L}(V)\) and \(S T\) is nilpotent. Prove that \(T S\) is nilpotent.
-
-============================================================
-
-Note ID: 1706772660862
-  Field: Text
-    Before:
-      14 Suppose \(V\) is an inner product space and \(N \in \mathcal{L}(V)\) is nilpotent. Prove that there exists an orthonormal basis of \(V\) with respect to which \(N\) has an upper-triangular matrix.<br><br>[If \(F\) = \(\mathbf{C}\), then the result above follows from Schur's Theorem (6.38) without the hypothesis that \(N\) is nilpotent. Thus the exercise above needs to be proved only when \(\mathbf{F}\) = \(\mathbf{R}\).]
-
-    After:
-      14 Suppose \(V\) is an inner product space and \(N \in \mathcal{L}(V)\) is nilpotent. Prove that there exists an orthonormal basis of \(V\) with respect to which \(N\) has an upper-triangular matrix.<br><br>[If \(F\) = \(\mathbf{C}\), then the result above follows from Schur's Theorem (6.38) without the hypothesis that \(N\) is nilpotent. Thus the exercise above needs to be proved only when \(\mathbf{F}\) = \(\mathbf{R}\).]
-
-============================================================
-
-Note ID: 1706772890415
-  Field: Text
-    Before:
-      15 Suppose \(N \in \mathcal{L}(V)\) is such that:<br><ul><li>&nbsp;null \(N^{\operatorname{dim} V-1}\) \(\neq\) \(\operatorname{null}\) \(N^{\operatorname{dim} V}\).&nbsp;</li></ul>Prove that \(N\) is nilpotent and that<br><ul><li>\(\operatorname{dim}\) \(\operatorname{null} N^{j}\) = \(j\)</li></ul><br>for every integer \(j\) with \(0\) \(\leq\) \(j\) \(\leq\) \(\operatorname{dim} V\).<br>
-
-    After:
-      15 Suppose \(N \in \mathcal{L}(V)\) is such that:<br><ul><li>&nbsp;null \(N^{\operatorname{dim} V-1}\) \(\neq\) \(\operatorname{null}\) \(N^{\operatorname{dim} V}\).&nbsp;</li></ul>Prove that \(N\) is nilpotent and that<br><ul><li>\(\operatorname{dim}\) \(\operatorname{null} N^{j}\) = \(j\)</li></ul><br>for every integer \(j\) with \(0\) \(\leq\) \(j\) \(\leq\) \(\operatorname{dim} V\).<br>
-
-============================================================
-
-Note ID: 1706773019395
-  Field: Text
-    Before:
-      16 Suppose \(T \in \mathcal{L}(V)\). Show that<br><br><ul><li>V = \(\operatorname{range} T^{0}\) \(\supset\) \(\text { range } T^{1}\) \(\supset\) \(\cdots\) \(\supset\) \(\text { range } T^{k}\) \(\supset\) \(\text { range } T^{k+1}\) \(\supset\) \(\cdots\)</li></ul>
-
-    After:
-      16 Suppose \(T \in \mathcal{L}(V)\). Show that<br><br><ul><li>V = \(\operatorname{range} T^{0}\) \(\supset\) \(\text { range } T^{1}\) \(\supset\) \(\cdots\) \(\supset\) \(\text { range } T^{k}\) \(\supset\) \(\text { range } T^{k+1}\) \(\supset\) \(\cdots\)</li></ul>
-
-============================================================
-
-Note ID: 1706773127832
-  Field: Text
-    Before:
-      17 Suppose \(T \in \mathcal{L}(V)\) and \(m\) is a nonnegative integer such that<br><br><ul><li>\(\text { range }\) \(T^{m}\) = \(\operatorname{range}\) \(T^{m+1} \text {. }\)</li></ul><br>Prove that range \(T^{k}\) = \(\operatorname{range}\) \(T^{m}\) for all \(k&gt;m\).<br>
-
-    After:
-      17 Suppose \(T \in \mathcal{L}(V)\) and \(m\) is a nonnegative integer such that<br><br><ul><li>\(\text { range }\) \(T^{m}\) = \(\operatorname{range}\) \(T^{m+1} \text {. }\)</li></ul><br>Prove that range \(T^{k}\) = \(\operatorname{range}\) \(T^{m}\) for all \(k&gt;m\).<br>
-
-============================================================
-
-Note ID: 1706773225413
-  Field: Text
-    Before:
-      18 Suppose \(T \in \mathcal{L}(V)\). Let \(n\) = \(\operatorname{dim} V\). Prove that<br><br><ul><li>\(\text { range }\) \(T^{n}\) = \(\operatorname{range}\) \(T^{n+1}\) = \(\operatorname{range}\) \(T^{n+2}\) = \(\cdots .\)</li></ul>
-
-    After:
-      18 Suppose \(T \in \mathcal{L}(V)\). Let \(n\) = \(\operatorname{dim} V\). Prove that<br><br><ul><li>\(\text { range }\) \(T^{n}\) = \(\operatorname{range}\) \(T^{n+1}\) = \(\operatorname{range}\) \(T^{n+2}\) = \(\cdots .\)</li></ul>
-
-============================================================
-
-Note ID: 1706773343817
-  Field: Text
-    Before:
-      19 Suppose \(T \in \mathcal{L}(V)\) and \(m\) is a nonnegative integer. <br>Prove that:<br><ul><li>\(\operatorname{null}\) \(T^{m}\) = \(\operatorname{null}\) \(T^{m+1}\)&nbsp;</li><li>if and only if:</li><ul><li>\(\operatorname{range}\) \(T^{m}\) = \(\operatorname{range}\) \( T^{m+1}\).</li></ul></ul>
-
-    After:
-      19 Suppose \(T \in \mathcal{L}(V)\) and \(m\) is a nonnegative integer. <br>Prove that:<br><ul><li>\(\operatorname{null}\) \(T^{m}\) = \(\operatorname{null}\) \(T^{m+1}\)&nbsp;</li><li>if and only if:</li><ul><li>\(\operatorname{range}\) \(T^{m}\) = \(\operatorname{range}\) \( T^{m+1}\).</li></ul></ul>
-
-============================================================
-
-Note ID: 1706857930056
-  Field: Text
-    Before:
-      <img src="paste-8c6fc87474d7fbea330a1f7f6f6bd1dc03017126.jpg"><br>Proof:<br><ul><li>Let k be a positive integer, we want to prove that</li><ul><li>null&nbsp;\(T^{m+k}\) = null&nbsp;\(T^{m+k+1}\)</li></ul><li>From this:</li><ul><li><img src="paste-c5fa00abe1b56df3a213f4abf509f5b1d86d056f.jpg"></li></ul><li>We already know that null&nbsp;\(T^{m+k}\) = null&nbsp;\(T^{m+k+1}\)</li><li>To prove the inclusion in the other direction suppose&nbsp;\(v\)&nbsp;\(\in\) null \(T^{m+k+1}\) then:</li><ul><li>\(T^{m+1} T^k\)&nbsp;\(v\) = \(T^{m+k+1}\)&nbsp;\(v\) = 0<br></li></ul><li>hence&nbsp;&nbsp;\(T^k\) \(v\) \(\in\) \(\operatorname{null}\) \(T^{m+1}\) = null&nbsp;\(T^m\)</li><li>Thus</li><li>\(T^{m+k} \)&nbsp;\(v\) =&nbsp;\(T^m T^k\)&nbsp;\(v\) = 0<br></li><li>which means that&nbsp;\(v\)&nbsp;\(\in\)&nbsp;\(null\) \(T^{m+k}\) and thus that&nbsp;\(\operatorname{null}\) \(T^{m+k+1}\) \(\subset\) \(\operatorname{null}\) \(T^{m+k}\)</li></ul><br><br><br>
-
-    After:
-      <img src="paste-8c6fc87474d7fbea330a1f7f6f6bd1dc03017126.jpg"><br>Proof:<br><ul><li>Let k be a positive integer, we want to prove that</li><ul><li>null&nbsp;\(T^{m+k}\) = null&nbsp;\(T^{m+k+1}\)</li></ul><li>From this:</li><ul><li><img src="paste-c5fa00abe1b56df3a213f4abf509f5b1d86d056f.jpg"></li></ul><li>We already know that null&nbsp;\(T^{m+k}\) = null&nbsp;\(T^{m+k+1}\)</li><li>To prove the inclusion in the other direction suppose&nbsp;\(v\)&nbsp;\(\in\) null \(T^{m+k+1}\) then:</li><ul><li>\(T^{m+1} T^k\)&nbsp;\(v\) = \(T^{m+k+1}\)&nbsp;\(v\) = 0<br></li></ul><li>hence&nbsp;&nbsp;\(T^k\) \(v\) \(\in\) \(\operatorname{null}\) \(T^{m+1}\) = null&nbsp;\(T^m\)</li><li>Thus</li><li>\(T^{m+k} \)&nbsp;\(v\) =&nbsp;\(T^m T^k\)&nbsp;\(v\) = 0<br></li><li>which means that&nbsp;\(v\)&nbsp;\(\in\)&nbsp;\(null\) \(T^{m+k}\) and thus that&nbsp;\(\operatorname{null}\) \(T^{m+k+1}\) \(\subset\) \(\operatorname{null}\) \(T^{m+k}\)</li></ul><br><br><br>
-
-============================================================
-
-Note ID: 1706858774806
-  Field: Text
-    Before:
-      <img src="paste-5b23b582d8d40ab6b0b3b40dd0a5b85fe4075b15.jpg"><br>Proof:<br><ul><li>First we need to show that</li><ul><li>\(\left(\right.\) null \(\left.T^n\right)\) \(\cap\) \(\left(\operatorname{range} T^n\right)\) = \(\{0\}\).<br></li></ul><li>Suppose&nbsp;\(v\)&nbsp;\(\in\) \(\left(\right.\) null \(\left.T^n\right) \cap\left(\operatorname{range} T^n\right)\).</li><li>Then:</li><ul><li>\(T^{n}\) \(v\) = 0 because it is in the null<br></li></ul><li>and there exists&nbsp;\(u\)&nbsp;\(\in\)&nbsp;\(V\) such that</li><ul><li>\(T^{n}\) \(u\) = \(v\) because v is in the range<br></li></ul><li>we can then transform the last equation:</li><ul><li>\(T^{2 n}\) \(u\) = \(T^n\) \(v\) = 0<br></li></ul><li>Since the null space of&nbsp;\(T^n\) is the same as that of&nbsp;\(T^{2n}\) given that n is the dimension of V, then&nbsp;\(T^{n}\) u = 0&nbsp;</li><li>Finally this shows that v is 0 meaning the intersection is empty</li></ul>
-
-    After:
-      <img src="paste-5b23b582d8d40ab6b0b3b40dd0a5b85fe4075b15.jpg"><br>Proof:<br><ul><li>First we need to show that</li><ul><li>\(\left(\right.\) null \(\left.T^n\right)\) \(\cap\) \(\left(\operatorname{range} T^n\right)\) = \(\{0\}\).<br></li></ul><li>Suppose&nbsp;\(v\)&nbsp;\(\in\) \(\left(\right.\) null \(\left.T^n\right) \cap\left(\operatorname{range} T^n\right)\).</li><li>Then:</li><ul><li>\(T^{n}\) \(v\) = 0 because it is in the null<br></li></ul><li>and there exists&nbsp;\(u\)&nbsp;\(\in\)&nbsp;\(V\) such that</li><ul><li>\(T^{n}\) \(u\) = \(v\) because v is in the range<br></li></ul><li>we can then transform the last equation:</li><ul><li>\(T^{2 n}\) \(u\) = \(T^n\) \(v\) = 0<br></li></ul><li>Since the null space of&nbsp;\(T^n\) is the same as that of&nbsp;\(T^{2n}\) given that n is the dimension of V, then&nbsp;\(T^{n}\) u = 0&nbsp;</li><li>Finally this shows that v is 0 meaning the intersection is empty</li></ul>
-
-============================================================
-
-Note ID: 1706861197311
-  Field: Text
-    Before:
-      The null space and range of \(p(T)\) are invariant under \(T\)<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(p \in \mathcal{P}(\mathbf{F})\). Then null \(p(T)\) and range \(p(T)\) are invariant under \(T\).
-
-    After:
-      The null space and range of \(p(T)\) are invariant under \(T\)<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(p \in \mathcal{P}(\mathbf{F})\). Then null \(p(T)\) and range \(p(T)\) are invariant under \(T\).
-
-============================================================
-
-Note ID: 1706946923158
-  Field: Text
-    Before:
-      A basis of generalized eigenvectors<br><br>Suppose \(V\) is a complex vector space and \(T \in \mathcal{L}(V)\). Then there is a basis of \(V\) consisting of generalized eigenvectors of \(T\).
-
-    After:
-      A basis of generalized eigenvectors<br><br>Suppose \(V\) is a complex vector space and \(T \in \mathcal{L}(V)\). Then there is a basis of \(V\) consisting of generalized eigenvectors of \(T\).
-
-============================================================
-
-Note ID: 1706947106102
-  Field: Text
-    Before:
-      Definition multiplicity<br><br><ul><li>Suppose \(T \in \mathcal{L}(V)\). The multiplicity of an eigenvalue \(\lambda\) of \(T\) is defined to be the dimension of the corresponding generalized eigenspace \(G(\lambda, T)\).</li><li>In other words, the multiplicity of an eigenvalue \(\lambda\) of \(T\) equals \(\operatorname{dim} \operatorname{null}(T-\lambda I)^{\operatorname{dim} V}\).</li></ul>
-
-    After:
-      Definition multiplicity<br><br><ul><li>Suppose \(T \in \mathcal{L}(V)\). The multiplicity of an eigenvalue \(\lambda\) of \(T\) is defined to be the dimension of the corresponding generalized eigenspace \(G(\lambda, T)\).</li><li>In other words, the multiplicity of an eigenvalue \(\lambda\) of \(T\) equals \(\operatorname{dim} \operatorname{null}(T-\lambda I)^{\operatorname{dim} V}\).</li></ul>
-
-============================================================
-
-Note ID: 1706947221054
-  Field: Text
-    Before:
-      Sum of the multiplicities equals \(\operatorname{dim} V\)<br><br>Suppose \(V\) is a complex vector space and \(T \in \mathcal{L}(V)\). Then the sum of the multiplicities of all the eigenvalues of \(T\) equals \(\operatorname{dim} V\).
-
-    After:
-      Sum of the multiplicities equals \(\operatorname{dim} V\)<br><br>Suppose \(V\) is a complex vector space and \(T \in \mathcal{L}(V)\). Then the sum of the multiplicities of all the eigenvalues of \(T\) equals \(\operatorname{dim} V\).
-
-============================================================
-
-Note ID: 1706947523983
-  Field: Text
-    Before:
-      If \(T \in \mathcal{L}(V)\) and \(\lambda\) is an eigenvalue of \(T\), then<br><br><ul><li>algebraic multiplicity of \(\lambda\) = \(\operatorname{dim} \) \(\operatorname{null}\) \((T-\lambda I)^{\operatorname{dim} V} \) = \(\operatorname{dim}\) \(G(\lambda, T)\)</li><li>geometric multiplicity of \(\lambda\)= \(\operatorname{dim}\) \(\operatorname{null}\) \((T-\lambda I)\)=\(\operatorname{dim}\) \(E(\lambda, T)\).</li></ul>
-
-    After:
-      If \(T \in \mathcal{L}(V)\) and \(\lambda\) is an eigenvalue of \(T\), then<br><br><ul><li>algebraic multiplicity of \(\lambda\) = \(\operatorname{dim} \) \(\operatorname{null}\) \((T-\lambda I)^{\operatorname{dim} V} \) = \(\operatorname{dim}\) \(G(\lambda, T)\)</li><li>geometric multiplicity of \(\lambda\)= \(\operatorname{dim}\) \(\operatorname{null}\) \((T-\lambda I)\)=\(\operatorname{dim}\) \(E(\lambda, T)\).</li></ul>
-
-============================================================
-
-Note ID: 1706947751796
-  Field: Text
-    Before:
-      Definition block diagonal matrix<br><br>A block diagonal matrix is a square matrix of the form<br><br>\[<br>\left(\begin{array}{ccc}<br>A_{1} &amp; &amp; 0 \\<br>&amp; \ddots &amp; \\<br>0 &amp; &amp; A_{m}<br>\end{array}\right),<br>\]<br><br>where \(A_{1}, \ldots, A_{m}\) are square matrices lying along the diagonal and all the other entries of the matrix equal 0 .
-
-    After:
-      Definition block diagonal matrix<br><br>A block diagonal matrix is a square matrix of the form<br><br>\[<br>\left(\begin{array}{ccc}<br>A_{1} &amp; &amp; 0 \\<br>&amp; \ddots &amp; \\<br>0 &amp; &amp; A_{m}<br>\end{array}\right),<br>\]<br><br>where \(A_{1}, \ldots, A_{m}\) are square matrices lying along the diagonal and all the other entries of the matrix equal 0 .
-
-============================================================
-
-Note ID: 1706948685291
-  Field: Text
-    Before:
-      Block diagonal matrix with upper-triangular blocks<br><br>Suppose \(V\) is a complex vector space and \(T \in \mathcal{L}(V)\). Let \(\lambda_{1}, \ldots, \lambda_{m}\) be the distinct eigenvalues of \(T\), with multiplicities \(d_{1}, \ldots, d_{m}\). Then there is a basis of \(V\) with respect to which \(T\) has a block diagonal matrix of the form<br><br>\[<br>\left(\begin{array}{ccc}<br>A_{1} &amp; &amp; 0 \\<br>&amp; \ddots &amp; \\<br>0 &amp; &amp; A_{m}<br>\end{array}\right)<br>\]<br><br>where each \(A_{j}\) is a \(d_{j}\)-by- \(d_{j}\) upper-triangular matrix of the form<br><br>\[<br>A_{j}=\left(\begin{array}{ccc}<br>\lambda_{j} &amp; &amp; * \\<br>&amp; \ddots &amp; \\<br>0 &amp; &amp; \lambda_{j}<br>\end{array}\right)<br>\]<br>
-
-    After:
-      Block diagonal matrix with upper-triangular blocks<br><br>Suppose \(V\) is a complex vector space and \(T \in \mathcal{L}(V)\). Let \(\lambda_{1}, \ldots, \lambda_{m}\) be the distinct eigenvalues of \(T\), with multiplicities \(d_{1}, \ldots, d_{m}\). Then there is a basis of \(V\) with respect to which \(T\) has a block diagonal matrix of the form<br><br>\[<br>\left(\begin{array}{ccc}<br>A_{1} &amp; &amp; 0 \\<br>&amp; \ddots &amp; \\<br>0 &amp; &amp; A_{m}<br>\end{array}\right)<br>\]<br><br>where each \(A_{j}\) is a \(d_{j}\)-by- \(d_{j}\) upper-triangular matrix of the form<br><br>\[<br>A_{j}=\left(\begin{array}{ccc}<br>\lambda_{j} &amp; &amp; * \\<br>&amp; \ddots &amp; \\<br>0 &amp; &amp; \lambda_{j}<br>\end{array}\right)<br>\]<br>
-
-============================================================
-
-Note ID: 1706949204170
-  Field: Text
-    Before:
-      Identity plus nilpotent has a square root<br><br>Suppose \(N \in \mathcal{L}(V)\) is nilpotent. Then \(I\) + \(N\) has a square root.
-
-    After:
-      Identity plus nilpotent has a square root<br><br>Suppose \(N \in \mathcal{L}(V)\) is nilpotent. Then \(I\) + \(N\) has a square root.
-
-============================================================
-
-Note ID: 1706949407550
-  Field: Text
-    Before:
-      Over \(\mathbf{C}\), invertible operators have square roots<br><br>Suppose \(V\) is a complex vector space and \(T \in \mathcal{L}(V)\) is invertible. Then \(T\) has a square root.
-
-    After:
-      Over \(\mathbf{C}\), invertible operators have square roots<br><br>Suppose \(V\) is a complex vector space and \(T \in \mathcal{L}(V)\) is invertible. Then \(T\) has a square root.
-
-============================================================
-
-Note ID: 1706949850028
-  Field: Text
-    Before:
-      <img src="paste-2796e54f31fed3b00816f8632242366536386ab3.jpg"><br><br>Proof Let \(\lambda_{1}, \ldots, \lambda_{m}\) be the distinct eigenvalues of \(T\). For each \(j\), there exists a nilpotent operator \(N_{j} \in \mathcal{L}\left(G\left(\lambda_{j}, T\right)\right)\) such that \(\left.T\right|_{G\left(\lambda_{j}, T\right)}=\lambda_{j} I+N_{j}\) [see 8.21(c)]. Because \(T\) is invertible, none of the \(\lambda_{j}\) 's equals 0 , so we can write<br><br>\[<br>\left.T\right|_{G\left(\lambda_{j}, T\right)}=\lambda_{j}\left(I+\frac{N_{j}}{\lambda_{j}}\right)<br>\]<br><br>for each \(j\). Clearly \(N_{j} / \lambda_{j}\) is nilpotent, and so \(I+N_{j} / \lambda_{j}\) has a square root (by 8.31). <br><br>Multiplying a square root of the complex number \(\lambda_{j}\) by a square root of \(I+N_{j} / \lambda_{j}\), we obtain a square root \(R_{j}\) of \(\left.T\right|_{G\left(\lambda_{j}, T\right)}\).<br><br>A typical vector \(v \in V\) can be written uniquely in the form<br><br>\[<br>v=u_{1}+\cdots+u_{m}<br>\]<br><br>where each \(u_{j}\) is in \(G\left(\lambda_{j}, T\right)\) (see 8.21). Using this decomposition, define an operator \(R \in \mathcal{L}(V)\) by<br><ul><li>\(R\) v= \(R_{1} u_{1}+\cdots+R_{m} u_{m} .\)<br></li></ul><br>You should verify that this operator \(R\) is a square root of \(T\), completing the proof.<br>
-
-    After:
-      <img src="paste-2796e54f31fed3b00816f8632242366536386ab3.jpg"><br><br>Proof Let \(\lambda_{1}, \ldots, \lambda_{m}\) be the distinct eigenvalues of \(T\). For each \(j\), there exists a nilpotent operator \(N_{j} \in \mathcal{L}\left(G\left(\lambda_{j}, T\right)\right)\) such that \(\left.T\right|_{G\left(\lambda_{j}, T\right)}=\lambda_{j} I+N_{j}\) [see 8.21(c)]. Because \(T\) is invertible, none of the \(\lambda_{j}\) 's equals 0 , so we can write<br><br>\[<br>\left.T\right|_{G\left(\lambda_{j}, T\right)}=\lambda_{j}\left(I+\frac{N_{j}}{\lambda_{j}}\right)<br>\]<br><br>for each \(j\). Clearly \(N_{j} / \lambda_{j}\) is nilpotent, and so \(I+N_{j} / \lambda_{j}\) has a square root (by 8.31). <br><br>Multiplying a square root of the complex number \(\lambda_{j}\) by a square root of \(I+N_{j} / \lambda_{j}\), we obtain a square root \(R_{j}\) of \(\left.T\right|_{G\left(\lambda_{j}, T\right)}\).<br><br>A typical vector \(v \in V\) can be written uniquely in the form<br><br>\[<br>v=u_{1}+\cdots+u_{m}<br>\]<br><br>where each \(u_{j}\) is in \(G\left(\lambda_{j}, T\right)\) (see 8.21). Using this decomposition, define an operator \(R \in \mathcal{L}(V)\) by<br><ul><li>\(R\) v= \(R_{1} u_{1}+\cdots+R_{m} u_{m} .\)<br></li></ul><br>You should verify that this operator \(R\) is a square root of \(T\), completing the proof.<br>
-
-============================================================
-
-Note ID: 1706949963245
-  Field: Text
-    Before:
-      3 Suppose \(T \in \mathcal{L}(V)\). Suppose \(S \in \mathcal{L}(V)\) is invertible. Prove that \(T\) and \(S^{-1} T S\) have the same eigenvalues with the same multiplicities.
-
-    After:
-      3 Suppose \(T \in \mathcal{L}(V)\). Suppose \(S \in \mathcal{L}(V)\) is invertible. Prove that \(T\) and \(S^{-1} T S\) have the same eigenvalues with the same multiplicities.
-
-============================================================
-
-Note ID: 1707118059081
-  Field: Text
-    Before:
-      Define \(T \in \mathcal{L}\left(\mathbf{C}^3\right)\) by<br>\[<br>\begin{aligned}<br>T\left(z_1, z_2, z_3\right) &amp; =\left(3 z_1+4 z_2, 3 z_2, 8 z_3\right) . \\<br>\mathcal{M}(T) &amp; =\left(\begin{array}{lll}<br>3 &amp; 4 &amp; 0 \\<br>0 &amp; 3 &amp; 0 \\<br>0 &amp; 0 &amp; 8<br>\end{array}\right)<br>\end{aligned}<br>\]<br><br>The eigenvalues of \(T\) are 3 and 8 .<br>The eigenspaces of \(T\) are<br>\[<br>\begin{aligned}<br>&amp; E(3, T)=\left\{\left(z_1, 0,0\right): z_1 \in \mathbf{C}\right\}, \\<br>&amp; E(8, T)=\left\{\left(0,0, z_3\right): z_3 \in \mathbf{C}\right\} .<br>\end{aligned}<br>\]<br><br>Thus the eigenvalue 3 has geometric multiplicity 1 and the eigenvalue 8 has geometric multiplicity 1 .<br><br>The generalized eigenspaces of \(T\) are<br>\[<br>\begin{aligned}<br>&amp; G(3, T)=\left\{\left(z_1, z_2, 0\right): z_1, z_2 \in \mathbf{C}\right\}, \\<br>&amp; G(8, T)=\left\{\left(0,0, z_3\right): z_3 \in \mathbf{C}\right\} .<br>\end{aligned}<br>\]<br><br>Thus the eigenvalue 3 has algebraic multiplicity 2 and the eigenvalue 8 has algebraic multiplicity 1 .<br><br>We have<br>\[<br>\mathbf{C}^3=G(3, T) \oplus G(8, T),<br>\]<br>as expected by the Decomposition Theorem.
-
-    After:
-      Define \(T \in \mathcal{L}\left(\mathbf{C}^3\right)\) by<br>\[<br>\begin{aligned}<br>T\left(z_1, z_2, z_3\right) &amp; =\left(3 z_1+4 z_2, 3 z_2, 8 z_3\right) . \\<br>\mathcal{M}(T) &amp; =\left(\begin{array}{lll}<br>3 &amp; 4 &amp; 0 \\<br>0 &amp; 3 &amp; 0 \\<br>0 &amp; 0 &amp; 8<br>\end{array}\right)<br>\end{aligned}<br>\]<br><br>The eigenvalues of \(T\) are 3 and 8 .<br>The eigenspaces of \(T\) are<br>\[<br>\begin{aligned}<br>&amp; E(3, T)=\left\{\left(z_1, 0,0\right): z_1 \in \mathbf{C}\right\}, \\<br>&amp; E(8, T)=\left\{\left(0,0, z_3\right): z_3 \in \mathbf{C}\right\} .<br>\end{aligned}<br>\]<br><br>Thus the eigenvalue 3 has geometric multiplicity 1 and the eigenvalue 8 has geometric multiplicity 1 .<br><br>The generalized eigenspaces of \(T\) are<br>\[<br>\begin{aligned}<br>&amp; G(3, T)=\left\{\left(z_1, z_2, 0\right): z_1, z_2 \in \mathbf{C}\right\}, \\<br>&amp; G(8, T)=\left\{\left(0,0, z_3\right): z_3 \in \mathbf{C}\right\} .<br>\end{aligned}<br>\]<br><br>Thus the eigenvalue 3 has algebraic multiplicity 2 and the eigenvalue 8 has algebraic multiplicity 1 .<br><br>We have<br>\[<br>\mathbf{C}^3=G(3, T) \oplus G(8, T),<br>\]<br>as expected by the Decomposition Theorem.
-
-============================================================
-
-Note ID: 1707118234366
-  Field: Text
-    Before:
-      A block diagonal matrix looks just like a diagonal matrix execpt for maybe having matrices rather than numbers on the daigonal
-
-    After:
-      A block diagonal matrix looks just like a diagonal matrix execpt for maybe having matrices rather than numbers on the daigonal
-
-============================================================
-
-Note ID: 1707120139677
-  Field: Text
-    Before:
-      Over C, invertible operators have square roots<br><br>Suppose \(V\) is a complex vector space and \(T \in \mathcal{L}(V)\) is invertible. Then \(T\) has a square root.
-
-    After:
-      Over C, invertible operators have square roots<br><br>Suppose \(V\) is a complex vector space and \(T \in \mathcal{L}(V)\) is invertible. Then \(T\) has a square root.
-
-============================================================
-
-Note ID: 1707120600097
-  Field: Text
-    Before:
-      <img src="paste-5423a84c5fb4f78ff3c0be8eebc6d33e230a9f71.jpg"><br>Proof:<br><ul><li>Let \(\lambda_1, \ldots, \lambda_m\) be the distinct eigenvalues of \(T\).&nbsp;</li><li>For each \(j\), there exists a nilpotent operator \(N_j \in \mathcal{L}\left(G\left(\lambda_j, T\right)\right)\) such that:</li><ul><li>&nbsp;\(\left.T\right|_{G\left(\lambda_j, T\right)}\) = \(\lambda_j I+N_j\).&nbsp;</li></ul><li>Because \(T\) is invertible, none of the \(\lambda_j\) 's equals 0 , so we can write</li><ul><li>\(\left.T\right|_{G\left(\lambda_j, T\right)}\) = \(\lambda_j\left(I+\frac{N_j}{\lambda_j}\right) .\)</li></ul><li>Because a nillpotent operator divided by a scalar remains nillpotent then:</li><li>Clearly \(N_j / \lambda_j\) is nilpotent, and so \(I+N_j / \lambda_j\) has a square root because all such sums have square roots.</li></ul>
-
-    After:
-      <img src="paste-5423a84c5fb4f78ff3c0be8eebc6d33e230a9f71.jpg"><br>Proof:<br><ul><li>Let \(\lambda_1, \ldots, \lambda_m\) be the distinct eigenvalues of \(T\).&nbsp;</li><li>For each \(j\), there exists a nilpotent operator \(N_j \in \mathcal{L}\left(G\left(\lambda_j, T\right)\right)\) such that:</li><ul><li>&nbsp;\(\left.T\right|_{G\left(\lambda_j, T\right)}\) = \(\lambda_j I+N_j\).&nbsp;</li></ul><li>Because \(T\) is invertible, none of the \(\lambda_j\) 's equals 0 , so we can write</li><ul><li>\(\left.T\right|_{G\left(\lambda_j, T\right)}\) = \(\lambda_j\left(I+\frac{N_j}{\lambda_j}\right) .\)</li></ul><li>Because a nillpotent operator divided by a scalar remains nillpotent then:</li><li>Clearly \(N_j / \lambda_j\) is nilpotent, and so \(I+N_j / \lambda_j\) has a square root because all such sums have square roots.</li></ul>
-
-============================================================
-
-Note ID: 1707120709736
-  Field: Text
-    Before:
-      <img src="paste-5423a84c5fb4f78ff3c0be8eebc6d33e230a9f71.jpg"><br>Proof:<br><ul><li>Let \(\lambda_1, \ldots, \lambda_m\) be the distinct eigenvalues of \(T\).&nbsp;</li><li>For each \(j\), there exists a nilpotent operator \(N_j \in \mathcal{L}\left(G\left(\lambda_j, T\right)\right)\) such that:</li><ul><li>&nbsp;\(\left.T\right|_{G\left(\lambda_j, T\right)}=\lambda_j I+N_j\).&nbsp;</li></ul><li>Because \(T\) is invertible, none of the \(\lambda_j\) 's equals 0 , so we can write</li><ul><li>\(\left.T\right|_{G\left(\lambda_j, T\right)}\) = \(\lambda_j\left(I+\frac{N_j}{\lambda_j}\right) .\)</li></ul><li>Because a nillpotent operator divided by a scalar remains nillpotent then:</li><li>Clearly \(N_j / \lambda_j\) is nilpotent, and so \(I+N_j / \lambda_j\) has a square root because all such sums have suare roots.<br></li><li>Then because every complex number has a complex square root:</li><li>Multiplying a square root of the number \(\lambda_j\) by a square root of \(I+N_j / \lambda_j\) gives a square root \(R_j\) of \(\left.T\right|_{G\left(\lambda_j, T\right)}\).<br></li><li>Then we can define an operator R by applying each&nbsp;\(R_j\) to the corresponding eigenvector from the generalized eigensapce decomposition</li><li>This operator is a square root of T.</li></ul>
-
-    After:
-      <img src="paste-5423a84c5fb4f78ff3c0be8eebc6d33e230a9f71.jpg"><br>Proof:<br><ul><li>Let \(\lambda_1, \ldots, \lambda_m\) be the distinct eigenvalues of \(T\).&nbsp;</li><li>For each \(j\), there exists a nilpotent operator \(N_j \in \mathcal{L}\left(G\left(\lambda_j, T\right)\right)\) such that:</li><ul><li>&nbsp;\(\left.T\right|_{G\left(\lambda_j, T\right)}=\lambda_j I+N_j\).&nbsp;</li></ul><li>Because \(T\) is invertible, none of the \(\lambda_j\) 's equals 0 , so we can write</li><ul><li>\(\left.T\right|_{G\left(\lambda_j, T\right)}\) = \(\lambda_j\left(I+\frac{N_j}{\lambda_j}\right) .\)</li></ul><li>Because a nillpotent operator divided by a scalar remains nillpotent then:</li><li>Clearly \(N_j / \lambda_j\) is nilpotent, and so \(I+N_j / \lambda_j\) has a square root because all such sums have suare roots.<br></li><li>Then because every complex number has a complex square root:</li><li>Multiplying a square root of the number \(\lambda_j\) by a square root of \(I+N_j / \lambda_j\) gives a square root \(R_j\) of \(\left.T\right|_{G\left(\lambda_j, T\right)}\).<br></li><li>Then we can define an operator R by applying each&nbsp;\(R_j\) to the corresponding eigenvector from the generalized eigensapce decomposition</li><li>This operator is a square root of T.</li></ul>
-
-============================================================
-
-Note ID: 1707201890607
-  Field: Text
-    Before:
-      Definition characteristic polynomial<br><br>Suppose \(V\) is a complex vector space and \(T \in \mathcal{L}(V)\). Let \(\lambda_{1}, \ldots, \lambda_{m}\) denote the distinct eigenvalues of \(T\), with multiplicities \(d_{1}, \ldots, d_{m}\). The polynomial<br><br><ul><li>\(\prod_{i=1}^m\)\((z-\lambda_i)\)\(^{d_i}\)</li></ul><br>is called the characteristic polynomial of \(T\).<br>
-
-    After:
-      Definition characteristic polynomial<br><br>Suppose \(V\) is a complex vector space and \(T \in \mathcal{L}(V)\). Let \(\lambda_{1}, \ldots, \lambda_{m}\) denote the distinct eigenvalues of \(T\), with multiplicities \(d_{1}, \ldots, d_{m}\). The polynomial<br><br><ul><li>\(\prod_{i=1}^m\)\((z-\lambda_i)\)\(^{d_i}\)</li></ul><br>is called the characteristic polynomial of \(T\).<br>
-
-============================================================
-
-Note ID: 1707203295964
-  Field: Text
-    Before:
-      <img src="paste-e1fc3bcece69a82f3536472bbc5ebb238a8984e0.jpg"><br><span style="color: rgb(0, 0, 0);">Proof:</span><br><ol><li>Let \(\lambda_{1}, \ldots, \lambda_{m}\) be the distinct eigenvalues of the operator \(T\).</li><li>Let \(d_{1}, \ldots, d_{m}\) be the dimensions of the corresponding generalized eigenspaces \(G\left(\lambda_{1}, T\right), \ldots, G\left(\lambda_{m}, T\right)\).&nbsp;<br></li><li>For each j,&nbsp;\(\left.\left(T-\lambda_j I\right)\right|_{G\left(\lambda_j, T\right)}\) is nilpotent</li><li>Thus we have:</li><ol><li>&nbsp;\(\left.\left(T-\lambda_{j} I\right)^{d_{j}}\right|_{G\left(\lambda_{j}, T\right)}\) = \(0\)&nbsp;</li><li>Because nill potent operators to the power of the dimension are 0</li></ol><li>Every vector v can be written as the sum of vectors in generalized eigenspaces, thus to prove that&nbsp;\(q(T)\) = 0 we need to show that&nbsp;\(\left.q(T)\right|_{G\left(\lambda_j, T\right)}\) = \(0\) for each j</li><li>Since the characteristic polynomial is defined as&nbsp;</li><ol><li>\(q(T)\) = \(\left(T-\lambda_{1} I\right)^{d_{1} } \cdots\left(T-\lambda_{m} I\right)^{d_{m} } .\)<br></li></ol><li>We can match the component corresponding to&nbsp;\(\lambda_j\) to vectors from&nbsp;\(G(\lambda_j,T)\).</li><li>Because \(\left.\left(T-\lambda_{j} I\right)^{d_{j} }\right|_{G\left(\lambda_{j}, T\right) }\) = \(0\), we conclude that \(\left.q(T)\right|_{G\left(\lambda_{j}, T\right)}\) = \(0\), as desired.</li></ol>
-
-    After:
-      <img src="paste-e1fc3bcece69a82f3536472bbc5ebb238a8984e0.jpg"><br><span style="color: rgb(0, 0, 0);">Proof:</span><br><ol><li>Let \(\lambda_{1}, \ldots, \lambda_{m}\) be the distinct eigenvalues of the operator \(T\).</li><li>Let \(d_{1}, \ldots, d_{m}\) be the dimensions of the corresponding generalized eigenspaces \(G\left(\lambda_{1}, T\right), \ldots, G\left(\lambda_{m}, T\right)\).&nbsp;<br></li><li>For each j,&nbsp;\(\left.\left(T-\lambda_j I\right)\right|_{G\left(\lambda_j, T\right)}\) is nilpotent</li><li>Thus we have:</li><ol><li>&nbsp;\(\left.\left(T-\lambda_{j} I\right)^{d_{j}}\right|_{G\left(\lambda_{j}, T\right)}\) = \(0\)&nbsp;</li><li>Because nill potent operators to the power of the dimension are 0</li></ol><li>Every vector v can be written as the sum of vectors in generalized eigenspaces, thus to prove that&nbsp;\(q(T)\) = 0 we need to show that&nbsp;\(\left.q(T)\right|_{G\left(\lambda_j, T\right)}\) = \(0\) for each j</li><li>Since the characteristic polynomial is defined as&nbsp;</li><ol><li>\(q(T)\) = \(\left(T-\lambda_{1} I\right)^{d_{1} } \cdots\left(T-\lambda_{m} I\right)^{d_{m} } .\)<br></li></ol><li>We can match the component corresponding to&nbsp;\(\lambda_j\) to vectors from&nbsp;\(G(\lambda_j,T)\).</li><li>Because \(\left.\left(T-\lambda_{j} I\right)^{d_{j} }\right|_{G\left(\lambda_{j}, T\right) }\) = \(0\), we conclude that \(\left.q(T)\right|_{G\left(\lambda_{j}, T\right)}\) = \(0\), as desired.</li></ol>
-
-============================================================
-
-Note ID: 1707203387841
-  Field: Text
-    Before:
-      Definition monic polynomial<br><br>A monic polynomial is a polynomial whose highest-degree coefficient equals 1 .
-
-    After:
-      Definition monic polynomial<br><br>A monic polynomial is a polynomial whose highest-degree coefficient equals 1 .
-
-============================================================
-
-Note ID: 1707204151415
-  Field: Text
-    Before:
-      <img src="paste-622c8f0ee70dfe028246bca066e500d732fed2c0.jpg"><br>Proof broad steps:<br><ol><li>Construct a list in L(V,V), since dim L(V,V) = \(n^2\) we can construct a list which is linearly dependent as \(<br>I, T, T^{2}, \ldots, T^{n^{2} } <br>\)&nbsp;because the list is longer than the dimension the of the space</li><li>Let m be the smallest positive integer such that the list&nbsp;\(I, T, T^2, \ldots, T^m\) is linearly dependent</li></ol>
-
-    After:
-      <img src="paste-622c8f0ee70dfe028246bca066e500d732fed2c0.jpg"><br>Proof broad steps:<br><ol><li>Construct a list in L(V,V), since dim L(V,V) = \(n^2\) we can construct a list which is linearly dependent as \(<br>I, T, T^{2}, \ldots, T^{n^{2} } <br>\)&nbsp;because the list is longer than the dimension the of the space</li><li>Let m be the smallest positive integer such that the list&nbsp;\(I, T, T^2, \ldots, T^m\) is linearly dependent</li></ol>
-
-============================================================
-
-Note ID: 1707204330801
-  Field: Text
-    Before:
-      <img src="paste-622c8f0ee70dfe028246bca066e500d732fed2c0.jpg"><br>Proof broad steps:<br><ol><li>Construct a list in L(V,V), since dim L(V,V) = \(n^2\) we can construct a list which is linearly dependent as \(<br>I, T, T^{2}, \ldots, T^{n^{2}}<br>\) because the list is longer than the dimension the of the space</li><li>Let m be the smallest positive integer such that the list&nbsp;\(I, T, T^2, \ldots, T^m\) is linearly dependent</li><li>Thus one of the powers of T in the list is a linear combination of the other ones by the linear dependence lema</li><li>Because m was chosen as the smallest positive integer such that the list is lin dep, we can conclude&nbsp;\(T^m\) is a linear combination of the rest of the list</li><li>Thus there exist scalars such tthat</li><ol><li>\(\sum_{i=0}^{m-1}\) \(a_i T^{i}\) +&nbsp;\(T^m\) = 0<br></li></ol><li>Define a monic polynomial p by:</li><ol><li>\(p(z)\) = \(\sum_{i=0}^{m-1}\)&nbsp;\(a_i z^{i}\) +&nbsp;\(z^m\)<br></li><li>p(T) = 0 by the above</li></ol><li>Since m is the smallest degree, the polynomial is unique</li></ol>
-
-    After:
-      <img src="paste-622c8f0ee70dfe028246bca066e500d732fed2c0.jpg"><br>Proof broad steps:<br><ol><li>Construct a list in L(V,V), since dim L(V,V) = \(n^2\) we can construct a list which is linearly dependent as \(<br>I, T, T^{2}, \ldots, T^{n^{2}}<br>\) because the list is longer than the dimension the of the space</li><li>Let m be the smallest positive integer such that the list&nbsp;\(I, T, T^2, \ldots, T^m\) is linearly dependent</li><li>Thus one of the powers of T in the list is a linear combination of the other ones by the linear dependence lema</li><li>Because m was chosen as the smallest positive integer such that the list is lin dep, we can conclude&nbsp;\(T^m\) is a linear combination of the rest of the list</li><li>Thus there exist scalars such tthat</li><ol><li>\(\sum_{i=0}^{m-1}\) \(a_i T^{i}\) +&nbsp;\(T^m\) = 0<br></li></ol><li>Define a monic polynomial p by:</li><ol><li>\(p(z)\) = \(\sum_{i=0}^{m-1}\)&nbsp;\(a_i z^{i}\) +&nbsp;\(z^m\)<br></li><li>p(T) = 0 by the above</li></ol><li>Since m is the smallest degree, the polynomial is unique</li></ol>
-
-============================================================
-
-Note ID: 1707204808942
-  Field: Text
-    Before:
-      Definition minimal polynomial<br><br>Suppose \(T \in \mathcal{L}(V)\). Then the minimal polynomial of \(T\) is the unique monic polynomial \(p\) of smallest degree such that \(p(T)\) = \(0\).
-
-    After:
-      Definition minimal polynomial<br><br>Suppose \(T \in \mathcal{L}(V)\). Then the minimal polynomial of \(T\) is the unique monic polynomial \(p\) of smallest degree such that \(p(T)\) = \(0\).
-
-============================================================
-
-Note ID: 1707205284442
-  Field: Text
-    Before:
-      \(q(T)\) = \(0\) implies \(q\) is a multiple of the minimal polynomial<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(q \in \mathcal{P}(\mathbf{F})\). Then \(q(T)\) = \(0\) if and only if \(q\) is a polynomial multiple of the minimal polynomial of \(T\).
-
-    After:
-      \(q(T)\) = \(0\) implies \(q\) is a multiple of the minimal polynomial<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(q \in \mathcal{P}(\mathbf{F})\). Then \(q(T)\) = \(0\) if and only if \(q\) is a polynomial multiple of the minimal polynomial of \(T\).
-
-============================================================
-
-Note ID: 1707205718959
-  Field: Text
-    Before:
-      <img src="paste-87a16b8c0a0a94ae4ff1a161bd2d53bfb9533191.jpg"><br>Proof:<br><ol><li>First direction is easy, if q is a multiple then q(T) = p(T) s(T) = 0 s(T) = 0</li><li>The other direction:</li><ol><li>Decompose</li><ol><li>\(q\) = \(ps\) + \(r\)&nbsp;</li><li>and \(deg r\) &lt; \(deg p\) by division algorithm</li></ol><li>\(0\) = \(q(T)\) = \(p(T) s(T)\)+\(r(T)\)=\(r(T) .\)<br></li><li>Since dividing r by its highest-degree coefficient would produce a smaller minimal polynomial, r must be 0</li><li>Thus \(q\) = \(ps\) and q is a polynomial multiple of p</li></ol></ol>
-
-    After:
-      <img src="paste-87a16b8c0a0a94ae4ff1a161bd2d53bfb9533191.jpg"><br>Proof:<br><ol><li>First direction is easy, if q is a multiple then q(T) = p(T) s(T) = 0 s(T) = 0</li><li>The other direction:</li><ol><li>Decompose</li><ol><li>\(q\) = \(ps\) + \(r\)&nbsp;</li><li>and \(deg r\) &lt; \(deg p\) by division algorithm</li></ol><li>\(0\) = \(q(T)\) = \(p(T) s(T)\)+\(r(T)\)=\(r(T) .\)<br></li><li>Since dividing r by its highest-degree coefficient would produce a smaller minimal polynomial, r must be 0</li><li>Thus \(q\) = \(ps\) and q is a polynomial multiple of p</li></ol></ol>
-
-============================================================
-
-Note ID: 1707205801502
-  Field: Text
-    Before:
-      Characteristic polynomial is a multiple of minimal polynomial<br><br>Suppose \(\mathbf{F}=\mathbf{C}\) and \(T \in \mathcal{L}(V)\). Then the characteristic polynomial of \(T\) is a polynomial multiple of the minimal polynomial of \(T\).
-
-    After:
-      Characteristic polynomial is a multiple of minimal polynomial<br><br>Suppose \(\mathbf{F}=\mathbf{C}\) and \(T \in \mathcal{L}(V)\). Then the characteristic polynomial of \(T\) is a polynomial multiple of the minimal polynomial of \(T\).
-
-============================================================
-
-Note ID: 1707205900035
-  Field: Text
-    Before:
-      Eigenvalues are the zeros of the minimal polynomial<br><br>Let \(T \in \mathcal{L}(V)\). Then the zeros of the minimal polynomial of \(T\) are precisely the eigenvalues of \(T\).
-
-    After:
-      Eigenvalues are the zeros of the minimal polynomial<br><br>Let \(T \in \mathcal{L}(V)\). Then the zeros of the minimal polynomial of \(T\) are precisely the eigenvalues of \(T\).
-
-============================================================
-
-Note ID: 1707206593240
-  Field: Text
-    Before:
-      <img src="paste-ffefb39e579ba840ec32457ebf2aa829f125a9a8.jpg"><br>Proof:<br><ul><li>Let p be the minimal polynomial of T</li><li>First suppose \(\lambda \in \mathbf{F}\) is a zero of \(p\). Then \(p\) can be written in the form</li><ul><li>\(p(z)\) = \((z-\lambda)\) \(q(z)\)</li><li>where \(q\) is a monic polynomial with coefficients in \(\mathbf{F}\) (see 4.11).&nbsp;</li></ul><li>Because \(p(T)=0\), we have</li><ul><li>\(0\) =\((T-\lambda I)\) \((q(T) v)\) for al v</li></ul><li>Because the degree of q is less than the minimal polynomial, there must exist a vector such that&nbsp;\(q(T)\)&nbsp;\(v\)&nbsp;\(\neq\)&nbsp;\(0\)&nbsp;</li><ul><li>This implies that&nbsp;\(\lambda \) is an eigenvalue of T</li></ul><li>Conversely, if&nbsp;\(\lambda\) is an eigenvalue of T with&nbsp;\(v\)&nbsp; \(\neq 0\) then&nbsp;\(T^{j}\)&nbsp;\(v\) =&nbsp;\(\lambda^j\)&nbsp;\(v\)</li><ul><li>Thus:</li><ul><li>0 =&nbsp;</li><li>= \(p(T)\)&nbsp;\(v\)&nbsp;&nbsp;</li><li>= (\(\sum_{i=0}^{m-1}\)&nbsp;\(a_i T^{i}\) +&nbsp;\(T^m\))&nbsp;\(v\)&nbsp;</li><li>= (\(\sum_{i=0}^{m-1}\)&nbsp;\(a_i \lambda^{i}\) +&nbsp;\(\lambda^m\))&nbsp;\(v\)</li><li>=&nbsp;\(p(\lambda)\) \(v\).</li></ul><li>Because \(v \neq 0\), the equation above implies that \(p(\lambda)\) = \(0\), as desired.</li></ul><li></li></ul>
-
-    After:
-      <img src="paste-ffefb39e579ba840ec32457ebf2aa829f125a9a8.jpg"><br>Proof:<br><ul><li>Let p be the minimal polynomial of T</li><li>First suppose \(\lambda \in \mathbf{F}\) is a zero of \(p\). Then \(p\) can be written in the form</li><ul><li>\(p(z)\) = \((z-\lambda)\) \(q(z)\)</li><li>where \(q\) is a monic polynomial with coefficients in \(\mathbf{F}\) (see 4.11).&nbsp;</li></ul><li>Because \(p(T)=0\), we have</li><ul><li>\(0\) =\((T-\lambda I)\) \((q(T) v)\) for al v</li></ul><li>Because the degree of q is less than the minimal polynomial, there must exist a vector such that&nbsp;\(q(T)\)&nbsp;\(v\)&nbsp;\(\neq\)&nbsp;\(0\)&nbsp;</li><ul><li>This implies that&nbsp;\(\lambda \) is an eigenvalue of T</li></ul><li>Conversely, if&nbsp;\(\lambda\) is an eigenvalue of T with&nbsp;\(v\)&nbsp; \(\neq 0\) then&nbsp;\(T^{j}\)&nbsp;\(v\) =&nbsp;\(\lambda^j\)&nbsp;\(v\)</li><ul><li>Thus:</li><ul><li>0 =&nbsp;</li><li>= \(p(T)\)&nbsp;\(v\)&nbsp;&nbsp;</li><li>= (\(\sum_{i=0}^{m-1}\)&nbsp;\(a_i T^{i}\) +&nbsp;\(T^m\))&nbsp;\(v\)&nbsp;</li><li>= (\(\sum_{i=0}^{m-1}\)&nbsp;\(a_i \lambda^{i}\) +&nbsp;\(\lambda^m\))&nbsp;\(v\)</li><li>=&nbsp;\(p(\lambda)\) \(v\).</li></ul><li>Because \(v \neq 0\), the equation above implies that \(p(\lambda)\) = \(0\), as desired.</li></ul><li></li></ul>
-
-============================================================
-
-Note ID: 1707376475826
-  Field: Text
-    Before:
-      16 Suppose \(V\) is an inner product space and \(T \in \mathcal{L}(V)\). Suppose<br><br><ul><li>\(\sum_{i=0}^{m-1}\)&nbsp;\(a_i\) \(z^i\) +&nbsp;\(z^m\)<br></li><li>is the minimal polynomial of \(T\).&nbsp;</li></ul>Prove that<br><br><ul><li>\(\sum_{i=0}^{m-1}\)&nbsp;\(\overline{a_i}\) \(z^i\) +&nbsp;\(z^m\)</li><li>is the minimal polynomial of \(T^{*}\).</li></ul>
-
-    After:
-      16 Suppose \(V\) is an inner product space and \(T \in \mathcal{L}(V)\). Suppose<br><br><ul><li>\(\sum_{i=0}^{m-1}\)&nbsp;\(a_i\) \(z^i\) +&nbsp;\(z^m\)<br></li><li>is the minimal polynomial of \(T\).&nbsp;</li></ul>Prove that<br><br><ul><li>\(\sum_{i=0}^{m-1}\)&nbsp;\(\overline{a_i}\) \(z^i\) +&nbsp;\(z^m\)</li><li>is the minimal polynomial of \(T^{*}\).</li></ul>
-
-============================================================
-
-Note ID: 1707376571432
-  Field: Text
-    Before:
-      Suppose \(\mathbf{F}\) = \(\mathbf{C}\) and \(T \in \mathcal{L}(V)\). <br><ul><li>Suppose the minimal polynomial of \(T\) has degree \(\operatorname{dim} V\).&nbsp;</li><li>Prove that the characteristic polynomial of \(T\) equals the minimal polynomial of \(T\).</li></ul><br>
-
-    After:
-      Suppose \(\mathbf{F}\) = \(\mathbf{C}\) and \(T \in \mathcal{L}(V)\). <br><ul><li>Suppose the minimal polynomial of \(T\) has degree \(\operatorname{dim} V\).&nbsp;</li><li>Prove that the characteristic polynomial of \(T\) equals the minimal polynomial of \(T\).</li></ul><br>
-
-============================================================
-
-Note ID: 1707376750961
-  Field: Text
-    Before:
-      20 <br><br><ul><li>Suppose \(V\) is a complex vector space and \(V_{1}, \ldots, V_{m}\) are nonzero subspaces of \(V\) such that:</li><ul><li>&nbsp;\(V\)= \(V_{1} \oplus \cdots \oplus V_{m}\).&nbsp;</li></ul><li>Suppose \(T \in \mathcal{L}(V)\) and each \(V_{j}\) is invariant under \(T\).&nbsp;</li><li>For each \(j\), let \(p_{j}\) denote the characteristic polynomial of \(\left.T\right|_{V_{j} }\).&nbsp;</li><li>Prove that the characteristic polynomial of \(T\) equals \(p_{1} \cdots p_{m}\).</li></ul>
-
-    After:
-      20 <br><br><ul><li>Suppose \(V\) is a complex vector space and \(V_{1}, \ldots, V_{m}\) are nonzero subspaces of \(V\) such that:</li><ul><li>&nbsp;\(V\)= \(V_{1} \oplus \cdots \oplus V_{m}\).&nbsp;</li></ul><li>Suppose \(T \in \mathcal{L}(V)\) and each \(V_{j}\) is invariant under \(T\).&nbsp;</li><li>For each \(j\), let \(p_{j}\) denote the characteristic polynomial of \(\left.T\right|_{V_{j} }\).&nbsp;</li><li>Prove that the characteristic polynomial of \(T\) equals \(p_{1} \cdots p_{m}\).</li></ul>
-
-============================================================
-
-Note ID: 1707377976263
-  Field: Text
-    Before:
-      <img src="paste-cf0cefa25f25833e1845a70954b0a91d7fded35f.jpg"><br><br>Proof:<br><ul><li>Let&nbsp;\(\lambda_i\) be the distinct eigenvalues of T</li><li>Let&nbsp;\(d_i\) be the dimensions of their corresponding generalized eigenspaces&nbsp;\(G(\lambda_i, T)\)</li><li>Each \((T-\lambda_j I)\)&nbsp;\(|_{G\left(\lambda_j, T\right)}\) is nilpotent and:<br></li><ul><li>\( (T-\lambda_j I)\) \(|_{G\left(\lambda_j, T\right)}\) = 0<br></li></ul><li>V =&nbsp;\(\bigoplus\) \(G(\lambda_i,T)\)</li></ul>
-
-    After:
-      <img src="paste-cf0cefa25f25833e1845a70954b0a91d7fded35f.jpg"><br><br>Proof:<br><ul><li>Let&nbsp;\(\lambda_i\) be the distinct eigenvalues of T</li><li>Let&nbsp;\(d_i\) be the dimensions of their corresponding generalized eigenspaces&nbsp;\(G(\lambda_i, T)\)</li><li>Each \((T-\lambda_j I)\)&nbsp;\(|_{G\left(\lambda_j, T\right)}\) is nilpotent and:<br></li><ul><li>\( (T-\lambda_j I)\) \(|_{G\left(\lambda_j, T\right)}\) = 0<br></li></ul><li>V =&nbsp;\(\bigoplus\) \(G(\lambda_i,T)\)</li></ul>
-
-============================================================
-
-Note ID: 1707378172730
-  Field: Text
-    Before:
-      <img src="paste-cf0cefa25f25833e1845a70954b0a91d7fded35f.jpg" style="width: 793.987px;"><br><br>Proof:<br><ul><li>Let&nbsp;\(\lambda_i\) be the distinct eigenvalues of T</li><li>Let&nbsp;\(d_i\) be the dimensions of their corresponding generalized eigenspaces&nbsp;\(G(\lambda_i, T)\)</li><li>Each \(\left.\left(T-\lambda_j I\right)\right|_{G\left(\lambda_j, T\right)}\) is nilpotent and:<br></li><ul><li>\(\left.\left(T-\lambda_j I\right)\right|_{G\left(\lambda_j, T\right)}^{d_j}\) = 0<br></li></ul><li>V =&nbsp;\(\bigoplus G(\lambda_i,T)\)</li><li>Thus we only need to show that&nbsp;\(q(T)\)\(|_{G\left(\lambda_j, T\right)}\) = \(0\) for each \(j\).</li><li>We have</li><ul><li>\(q(T)\) =&nbsp;\(\prod_i\) \((T-\lambda_i)^{d_i}\)<br></li></ul><li>If we fix a j, we can comute the equation above so&nbsp;\(\left(T-\lambda_j I\right)^{d_j}\) is the last term on the RHS</li><li>Because :</li><ul><li>\((T-\lambda_j I)^{d_j}\) \(|_{G\left(\lambda_j, T\right)}\) = \(0\)</li></ul><li>We conclude that&nbsp;</li><ul><li>\(q(T)\) \(|_{G\left(\lambda_j, T\right)}\) = \(0\)</li></ul><li>As desired.</li></ul>
-
-    After:
-      <img src="paste-cf0cefa25f25833e1845a70954b0a91d7fded35f.jpg" style="width: 793.987px;"><br><br>Proof:<br><ul><li>Let&nbsp;\(\lambda_i\) be the distinct eigenvalues of T</li><li>Let&nbsp;\(d_i\) be the dimensions of their corresponding generalized eigenspaces&nbsp;\(G(\lambda_i, T)\)</li><li>Each \(\left.\left(T-\lambda_j I\right)\right|_{G\left(\lambda_j, T\right)}\) is nilpotent and:<br></li><ul><li>\(\left.\left(T-\lambda_j I\right)\right|_{G\left(\lambda_j, T\right)}^{d_j}\) = 0<br></li></ul><li>V =&nbsp;\(\bigoplus G(\lambda_i,T)\)</li><li>Thus we only need to show that&nbsp;\(q(T)\)\(|_{G\left(\lambda_j, T\right)}\) = \(0\) for each \(j\).</li><li>We have</li><ul><li>\(q(T)\) =&nbsp;\(\prod_i\) \((T-\lambda_i)^{d_i}\)<br></li></ul><li>If we fix a j, we can comute the equation above so&nbsp;\(\left(T-\lambda_j I\right)^{d_j}\) is the last term on the RHS</li><li>Because :</li><ul><li>\((T-\lambda_j I)^{d_j}\) \(|_{G\left(\lambda_j, T\right)}\) = \(0\)</li></ul><li>We conclude that&nbsp;</li><ul><li>\(q(T)\) \(|_{G\left(\lambda_j, T\right)}\) = \(0\)</li></ul><li>As desired.</li></ul>
-
-============================================================
-
-Note ID: 1707378611272
-  Field: Text
-    Before:
-      <img src="paste-64cacea41c038d23efa347a13437b5661ba6ef19.jpg"><br>Proof <br>Let \(n=\operatorname{dim} V\). The list<br>\[<br>I, T, T^2, \ldots, T^{n^2}<br>\]<br>is not linearly independent in \(\mathcal{L}(V)\), because \(\mathcal{L}(V)\) has dimension \(n^2\) and the list has length \(n^2+1\). Let \(m\) be the smallest positive integer such that<br>\[<br>I, T, T^2, \ldots, T^m<br>\]<br>is linearly dependent.<br><ul><li>The Linear Dependence Lemma implies that \(T^m\) is a linear combination of \(I, T, T^2, \ldots, T^{m-1}\).&nbsp;<br></li><li>Thus there exist scalars \(a_0, a_1, a_2, \ldots, a_{m-1} \in \mathbf{F}\) such that \(a_0 I+a_1 T+a_2 T^2+\cdots+a_{m-1} T^{m-1}+T^m\) = \(0\).&nbsp;</li><li>Define a monic polynomial \(p \in \mathcal{P}(\mathbf{F})\) by:</li><ul><li>&nbsp;\(p(z)\) = \(a_0+a_1 z+a_2 z^2+\cdots+a_{m-1} z^{m-1}+z^m\).&nbsp;</li></ul><li>Then \(p(T)\) = \(0\).&nbsp;<br></li><li>No monic polynomial \(q \in \mathcal{P}(\mathbf{F})\) with degree smaller than \(m\) can satisfy \(q(T)\) = \(0\).&nbsp; because we choose m to be the smallest degree</li></ul>
-
-    After:
-      <img src="paste-64cacea41c038d23efa347a13437b5661ba6ef19.jpg"><br>Proof <br>Let \(n=\operatorname{dim} V\). The list<br>\[<br>I, T, T^2, \ldots, T^{n^2}<br>\]<br>is not linearly independent in \(\mathcal{L}(V)\), because \(\mathcal{L}(V)\) has dimension \(n^2\) and the list has length \(n^2+1\). Let \(m\) be the smallest positive integer such that<br>\[<br>I, T, T^2, \ldots, T^m<br>\]<br>is linearly dependent.<br><ul><li>The Linear Dependence Lemma implies that \(T^m\) is a linear combination of \(I, T, T^2, \ldots, T^{m-1}\).&nbsp;<br></li><li>Thus there exist scalars \(a_0, a_1, a_2, \ldots, a_{m-1} \in \mathbf{F}\) such that \(a_0 I+a_1 T+a_2 T^2+\cdots+a_{m-1} T^{m-1}+T^m\) = \(0\).&nbsp;</li><li>Define a monic polynomial \(p \in \mathcal{P}(\mathbf{F})\) by:</li><ul><li>&nbsp;\(p(z)\) = \(a_0+a_1 z+a_2 z^2+\cdots+a_{m-1} z^{m-1}+z^m\).&nbsp;</li></ul><li>Then \(p(T)\) = \(0\).&nbsp;<br></li><li>No monic polynomial \(q \in \mathcal{P}(\mathbf{F})\) with degree smaller than \(m\) can satisfy \(q(T)\) = \(0\).&nbsp; because we choose m to be the smallest degree</li></ul>
-
-============================================================
-
-Note ID: 1707379133416
-  Field: Text
-    Before:
-      <img src="paste-0d7e8c1725ccdad8afdeb991e0554f8aa428adfd.jpg"><br>To prove the other direction, now suppose \(q(T)\) = \(0\). By the Division Algorithm for Polynomials, there exist polynomials \(s, r \in \mathcal{P}(\mathbf{F})\) such that<br><ul><li>\(q\) = \(p s\) +\(r\)</li></ul>and \(\operatorname{deg} r&lt;\operatorname{deg} p\). We have<br><br><ul><li>0 =&nbsp;\(q(T)\)</li><li>=&nbsp;\(p(T) s(T)\) +&nbsp;\(r(T)\)</li><li>= \(r(T)\)</li></ul><br>The equation above implies that \(r\) = \(0\).<br>
-
-    After:
-      <img src="paste-0d7e8c1725ccdad8afdeb991e0554f8aa428adfd.jpg"><br>To prove the other direction, now suppose \(q(T)\) = \(0\). By the Division Algorithm for Polynomials, there exist polynomials \(s, r \in \mathcal{P}(\mathbf{F})\) such that<br><ul><li>\(q\) = \(p s\) +\(r\)</li></ul>and \(\operatorname{deg} r&lt;\operatorname{deg} p\). We have<br><br><ul><li>0 =&nbsp;\(q(T)\)</li><li>=&nbsp;\(p(T) s(T)\) +&nbsp;\(r(T)\)</li><li>= \(r(T)\)</li></ul><br>The equation above implies that \(r\) = \(0\).<br>
-
-============================================================
-
-Note ID: 1707379958890
-  Field: Text
-    Before:
-      <ul><li>If you choose an operator at random it is extremely likely the characteristic polynomial and minimal polynomial are the same</li><li>Thus the easiest way to find the characteristic polynomial is usually to find the minimal polynomial</li><li>If the minimal polynomial has degree dim V then we are done, otherwise search by other means</li></ul>
-
-    After:
-      <ul><li>If you choose an operator at random it is extremely likely the characteristic polynomial and minimal polynomial are the same</li><li>Thus the easiest way to find the characteristic polynomial is usually to find the minimal polynomial</li><li>If the minimal polynomial has degree dim V then we are done, otherwise search by other means</li></ul>
-
-============================================================
-
-Note ID: 1707380316327
-  Field: Text
-    Before:
-      <img src="paste-f9fa7e823504027c64a83fb58982269824641558.jpg"><br>Proof:<br><ul><li>Let p be the minimal polynomial of T<br></li><li>First: suppose&nbsp;\(\lambda \in F\) is a zero of p</li><ul><li>Then we can write p in the form</li><ul><li>p(z) =&nbsp;\((z-\lambda)\)&nbsp;\(q(z)\)</li><li>where q is a monic polynomial</li></ul><li>For every v</li><ul><li>0 = p(T) v</li><li>=&nbsp;\((T-\lambda I)\) \(q(T)\) \(v\)</li></ul><li>Because the degree of \(q\) is less than the degree of the minimal polynomial \(p\), there exists at least one vector \(v \in V\) such that \(q(T) v\) \(\neq\) \(0\) but </li><li>&nbsp;\(T-\lambda I\) applied to v is 0</li><li>Thus&nbsp;\(\lambda\) is an eigenvalue of T</li><li>Thus every 0 of p is an eigenvalue of T</li></ul></ul>
-
-    After:
-      <img src="paste-f9fa7e823504027c64a83fb58982269824641558.jpg"><br>Proof:<br><ul><li>Let p be the minimal polynomial of T<br></li><li>First: suppose&nbsp;\(\lambda \in F\) is a zero of p</li><ul><li>Then we can write p in the form</li><ul><li>p(z) =&nbsp;\((z-\lambda)\)&nbsp;\(q(z)\)</li><li>where q is a monic polynomial</li></ul><li>For every v</li><ul><li>0 = p(T) v</li><li>=&nbsp;\((T-\lambda I)\) \(q(T)\) \(v\)</li></ul><li>Because the degree of \(q\) is less than the degree of the minimal polynomial \(p\), there exists at least one vector \(v \in V\) such that \(q(T) v\) \(\neq\) \(0\) but </li><li>&nbsp;\(T-\lambda I\) applied to v is 0</li><li>Thus&nbsp;\(\lambda\) is an eigenvalue of T</li><li>Thus every 0 of p is an eigenvalue of T</li></ul></ul>
-
-============================================================
-
-Note ID: 1707380851714
-  Field: Text
-    Before:
-      <img src="paste-779c6b5fbf7a19d21c5ba2a62669e4885ece5984.jpg"><br>Proof:<br><ul><li>Let p be the minimal polynomial</li><li>Suppose&nbsp;\(\lambda \in F\) is an eigenvalue of T</li><li>Thus there exists&nbsp;\(v\in V\) with&nbsp;\(v\) \(\neq 0\) such that</li><ul><li>\(T\)&nbsp;\(v\) =&nbsp;\(\lambda\)&nbsp;\(v\)<br></li></ul><li>Repeated applications of T to both sides show that</li><ul><li>\(T^j\)&nbsp;\(v\) =&nbsp;\(\lambda^j\)&nbsp;\(v\)</li><li>for every nonnegative integer j</li></ul><li>Since p(T) is just taking linear combinations of that we can conclude that</li><ul><li>\(p(T)\)&nbsp;\(v\) =&nbsp;\(p(\lambda)\)&nbsp;\(v\)<br></li></ul><li>Then:</li><ul><li>\(0\) =&nbsp;\(p(T)\)&nbsp;\(v\) because p is the minimal polynomial<br></li><li>=&nbsp;\(p(\lambda)\)&nbsp;\(v\)</li></ul><li>Because&nbsp;\(v\)&nbsp;\(\neq\)&nbsp;\(0\), the equation above implies that&nbsp;\(p(\lambda)\) =&nbsp;\(0\)</li><li>Thus we have shown that every eigenvalue of T is a zero of p</li></ul>
-
-    After:
-      <img src="paste-779c6b5fbf7a19d21c5ba2a62669e4885ece5984.jpg"><br>Proof:<br><ul><li>Let p be the minimal polynomial</li><li>Suppose&nbsp;\(\lambda \in F\) is an eigenvalue of T</li><li>Thus there exists&nbsp;\(v\in V\) with&nbsp;\(v\) \(\neq 0\) such that</li><ul><li>\(T\)&nbsp;\(v\) =&nbsp;\(\lambda\)&nbsp;\(v\)<br></li></ul><li>Repeated applications of T to both sides show that</li><ul><li>\(T^j\)&nbsp;\(v\) =&nbsp;\(\lambda^j\)&nbsp;\(v\)</li><li>for every nonnegative integer j</li></ul><li>Since p(T) is just taking linear combinations of that we can conclude that</li><ul><li>\(p(T)\)&nbsp;\(v\) =&nbsp;\(p(\lambda)\)&nbsp;\(v\)<br></li></ul><li>Then:</li><ul><li>\(0\) =&nbsp;\(p(T)\)&nbsp;\(v\) because p is the minimal polynomial<br></li><li>=&nbsp;\(p(\lambda)\)&nbsp;\(v\)</li></ul><li>Because&nbsp;\(v\)&nbsp;\(\neq\)&nbsp;\(0\), the equation above implies that&nbsp;\(p(\lambda)\) =&nbsp;\(0\)</li><li>Thus we have shown that every eigenvalue of T is a zero of p</li></ul>
-
-============================================================
-
-Note ID: 1707463982269
-  Field: Text
-    Before:
-      Basis corresponding to a nilpotent operator<br><br>Suppose \(N \in \mathcal{L}(V)\) is nilpotent. Then there exist vectors \(v_{1}, \ldots, v_{n} \in V\) and nonnegative integers \(m_{1}, \ldots, m_{n}\) such that<br><br><ul><li>(a) \(N^{m_{1} } v_{1}, \ldots, N v_{1}, v_{1}, \ldots, N^{m_{n} } v_{n}, \ldots, N v_{n}, v_{n} \) is a basis of \(V\);</li><li>(b) \(N^{m_{1}+1} v_{1}\) = \(\cdots=N^{m_{n}+1} v_{n}\) = \(0\).</li></ul>
-
-    After:
-      Basis corresponding to a nilpotent operator<br><br>Suppose \(N \in \mathcal{L}(V)\) is nilpotent. Then there exist vectors \(v_{1}, \ldots, v_{n} \in V\) and nonnegative integers \(m_{1}, \ldots, m_{n}\) such that<br><br><ul><li>(a) \(N^{m_{1} } v_{1}, \ldots, N v_{1}, v_{1}, \ldots, N^{m_{n} } v_{n}, \ldots, N v_{n}, v_{n} \) is a basis of \(V\);</li><li>(b) \(N^{m_{1}+1} v_{1}\) = \(\cdots=N^{m_{n}+1} v_{n}\) = \(0\).</li></ul>
-
-============================================================
-
-Note ID: 1707665323978
-  Field: Text
-    Before:
-      <img src="paste-e6c465f8f4dddf39cd587c1c082ae6c53e083170.jpg"><br>Proof by induction:<br><ul><li>Obviously true for dim V = 1</li><li>Inductive case:</li><li>Because N is nilpotent , N is not injective and thus not surjective, thus range N is a strict subspace of V</li><li>Thus we can apply I.H to the restriction operatotr&nbsp;\(\left.N\right|_{\text {range } N} \in \mathcal{L}(\) range \(N)\).</li><li>Thus the following is a basis of range N:</li><ul><li>\(N^{m_{1}} v_{1}, \ldots,\) \(N v_{1}, v_{1}, \ldots, \) \( N^{m_{n}} v_{n}, \ldots,\) \(N v_{n}, v_{n}\)<br></li></ul><li>And&nbsp;</li><ul><li>\(N^{m_{1}+1} v_{1}\) = \(\cdots\) =\(N^{m_{n}+1} v_{n}\) = \(0 .\)<br></li></ul><li>Because each&nbsp;\(v_j\) is in range N, there exists&nbsp;\(u_j\) such that&nbsp;\(v_j\)&nbsp; =&nbsp;&nbsp;\(N\)&nbsp;\(u_j\) which means we can claim the following basis to be linearly independent in V:</li><ul><li>\(N^{m_{1}+1} u_{1}, \ldots,\) \(N u_{1}, u_{1}, \ldots,\)\( N^{m_{n}+1} u_{n}, \ldots,\)\( N u_{n}, u_{n}\)</li></ul><li>Which can be proven by contradiction, since if it were to equal 0 and we applied N to it we would get the original list which we know is linearly independent</li><ul><li>Thus the only vectors which could have non-zero coefficients are:</li><ul><li>\[<br>N^{m_{1}+1} u_{1}, \ldots, N^{m_{n}+1} u_{n},<br>\]<br></li></ul><li>Which equal:</li><ul><li>\[<br>N^{m_{1} } v_{1}, \ldots, N^{m_{n} } v_{n}<br>\]<br></li></ul><li>Again, since the original list was linearly indepedent, these must have coefficients 0</li></ul><li>We can extend the u-list to a basis by concatenating basis vectors</li><ul><li>\(N^{m_{1}+1} u_{1}, \ldots,\) \(N u_{1}, u_{1}, \ldots,\)\( N^{m_{n}+1} u_{n}, \ldots,\) \(N u_{n}, u_{n}, \)\(w_{1}, \ldots, w_{p}\)</li></ul><li>Applying N to any of the new vectors would result in a vector in range N and thus in the span of the riginal list.&nbsp;</li><li>Since each vector in the original list can be written based on u's, there exists&nbsp;\(x_j\) in the span of the u-list such that&nbsp;\(N w_j\) =&nbsp;\(N x_j\).</li><li>Let: \(u_{n+j}\) = \(w_{j}-x_{j} .\)</li><li>Then&nbsp;\(N u_{n+j}\) = \(0\), furthermore the list:</li><ul><li>\[<br>N^{m_{1}+1} u_{1}, \ldots, N u_{1}, u_{1}, \ldots, N^{m_{n}+1} u_{n}, \ldots, N u_{n}, u_{n}, u_{n+1}, \ldots, u_{n+p}<br>\]&nbsp;</li><li>spans V becuase its span contains each&nbsp;\(x_j\), and each&nbsp;\(u_{n+j}\) and hence each&nbsp;\(w_j\)</li></ul><li>This is the basis of the entire space we were looking for</li></ul>
-
-    After:
-      <img src="paste-e6c465f8f4dddf39cd587c1c082ae6c53e083170.jpg"><br>Proof by induction:<br><ul><li>Obviously true for dim V = 1</li><li>Inductive case:</li><li>Because N is nilpotent , N is not injective and thus not surjective, thus range N is a strict subspace of V</li><li>Thus we can apply I.H to the restriction operatotr&nbsp;\(\left.N\right|_{\text {range } N} \in \mathcal{L}(\) range \(N)\).</li><li>Thus the following is a basis of range N:</li><ul><li>\(N^{m_{1}} v_{1}, \ldots,\) \(N v_{1}, v_{1}, \ldots, \) \( N^{m_{n}} v_{n}, \ldots,\) \(N v_{n}, v_{n}\)<br></li></ul><li>And&nbsp;</li><ul><li>\(N^{m_{1}+1} v_{1}\) = \(\cdots\) =\(N^{m_{n}+1} v_{n}\) = \(0 .\)<br></li></ul><li>Because each&nbsp;\(v_j\) is in range N, there exists&nbsp;\(u_j\) such that&nbsp;\(v_j\)&nbsp; =&nbsp;&nbsp;\(N\)&nbsp;\(u_j\) which means we can claim the following basis to be linearly independent in V:</li><ul><li>\(N^{m_{1}+1} u_{1}, \ldots,\) \(N u_{1}, u_{1}, \ldots,\)\( N^{m_{n}+1} u_{n}, \ldots,\)\( N u_{n}, u_{n}\)</li></ul><li>Which can be proven by contradiction, since if it were to equal 0 and we applied N to it we would get the original list which we know is linearly independent</li><ul><li>Thus the only vectors which could have non-zero coefficients are:</li><ul><li>\[<br>N^{m_{1}+1} u_{1}, \ldots, N^{m_{n}+1} u_{n},<br>\]<br></li></ul><li>Which equal:</li><ul><li>\[<br>N^{m_{1} } v_{1}, \ldots, N^{m_{n} } v_{n}<br>\]<br></li></ul><li>Again, since the original list was linearly indepedent, these must have coefficients 0</li></ul><li>We can extend the u-list to a basis by concatenating basis vectors</li><ul><li>\(N^{m_{1}+1} u_{1}, \ldots,\) \(N u_{1}, u_{1}, \ldots,\)\( N^{m_{n}+1} u_{n}, \ldots,\) \(N u_{n}, u_{n}, \)\(w_{1}, \ldots, w_{p}\)</li></ul><li>Applying N to any of the new vectors would result in a vector in range N and thus in the span of the riginal list.&nbsp;</li><li>Since each vector in the original list can be written based on u's, there exists&nbsp;\(x_j\) in the span of the u-list such that&nbsp;\(N w_j\) =&nbsp;\(N x_j\).</li><li>Let: \(u_{n+j}\) = \(w_{j}-x_{j} .\)</li><li>Then&nbsp;\(N u_{n+j}\) = \(0\), furthermore the list:</li><ul><li>\[<br>N^{m_{1}+1} u_{1}, \ldots, N u_{1}, u_{1}, \ldots, N^{m_{n}+1} u_{n}, \ldots, N u_{n}, u_{n}, u_{n+1}, \ldots, u_{n+p}<br>\]&nbsp;</li><li>spans V becuase its span contains each&nbsp;\(x_j\), and each&nbsp;\(u_{n+j}\) and hence each&nbsp;\(w_j\)</li></ul><li>This is the basis of the entire space we were looking for</li></ul>
-
-============================================================
-
-Note ID: 1707665514594
-  Field: Text
-    Before:
-      Definition Jordan basis<br><br>Suppose \(T \in \mathcal{L}(V)\). A basis of \(V\) is called a Jordan basis for \(T\) if with respect to this basis \(T\) has a block diagonal matrix<br><br>\[<br>\left(\begin{array}{ccc}<br>A_{1} &amp; &amp; 0 \\<br>&amp; \ddots &amp; \\<br>0 &amp; &amp; A_{p}<br>\end{array}\right),<br>\]<br><br>where each \(A_{j}\) is an upper-triangular matrix of the form<br><br>\[<br>A_{j}=\left(\begin{array}{cccc}<br>\lambda_{j} &amp; 1 &amp; &amp; 0 \\<br>&amp; \ddots &amp; \ddots &amp; \\<br>&amp; &amp; \ddots &amp; 1 \\<br>0 &amp; &amp; &amp; \lambda_{j}<br>\end{array}\right) .<br>\]
-
-    After:
-      Definition Jordan basis<br><br>Suppose \(T \in \mathcal{L}(V)\). A basis of \(V\) is called a Jordan basis for \(T\) if with respect to this basis \(T\) has a block diagonal matrix<br><br>\[<br>\left(\begin{array}{ccc}<br>A_{1} &amp; &amp; 0 \\<br>&amp; \ddots &amp; \\<br>0 &amp; &amp; A_{p}<br>\end{array}\right),<br>\]<br><br>where each \(A_{j}\) is an upper-triangular matrix of the form<br><br>\[<br>A_{j}=\left(\begin{array}{cccc}<br>\lambda_{j} &amp; 1 &amp; &amp; 0 \\<br>&amp; \ddots &amp; \ddots &amp; \\<br>&amp; &amp; \ddots &amp; 1 \\<br>0 &amp; &amp; &amp; \lambda_{j}<br>\end{array}\right) .<br>\]
-
-============================================================
-
-Note ID: 1707665569086
-  Field: Text
-    Before:
-      Jordan Form<br><br>Suppose \(V\) is a complex vector space. If \(T \in \mathcal{L}(V)\), then there is a basis of \(V\) that is a Jordan basis for \(T\).
-
-    After:
-      Jordan Form<br><br>Suppose \(V\) is a complex vector space. If \(T \in \mathcal{L}(V)\), then there is a basis of \(V\) that is a Jordan basis for \(T\).
-
-============================================================
-
-Note ID: 1707665929604
-  Field: Text
-    Before:
-      <img src="paste-38f854533fe9c300346a9fd950b3594d316f8b04.jpg"><br>ProofL<br><br><ul><li>First consider a nilpotent operator \(N \in \mathcal{L}(V)\) and the vectors \(v_{1}, \ldots, v_{n} \in V\) given by 8.55 .&nbsp;</li><ul><li>8.55: <img src="paste-469e4737e5f98e9ac1a08cabf2103e7ef4c2a7e2.jpg"></li></ul><li>For each \(j\), note that \(N\) sends the first vector in the list \(N^{m_{j}} v_{j}, \ldots, N v_{j}, v_{j}\) to 0 and that \(N\) sends each vector in this list other than the first vector to the previous vector.&nbsp;</li><li>In other words, 8.55 gives a basis of \(V\) with respect to which \(N\) has a block diagonal matrix, where each matrix on the diagonal has the form</li><ul><li>\[\left(\begin{array}{cccc}0 &amp; 1 &amp; &amp; 0 \\&amp; \ddots &amp; \ddots &amp; \\&amp;&amp; \ddots &amp; 1 \\0 &amp; &amp; &amp; 0\end{array}\right) .\]</li></ul></ul>Thus the desired result holds for nilpotent operators.
-
-    After:
-      <img src="paste-38f854533fe9c300346a9fd950b3594d316f8b04.jpg"><br>ProofL<br><br><ul><li>First consider a nilpotent operator \(N \in \mathcal{L}(V)\) and the vectors \(v_{1}, \ldots, v_{n} \in V\) given by 8.55 .&nbsp;</li><ul><li>8.55: <img src="paste-469e4737e5f98e9ac1a08cabf2103e7ef4c2a7e2.jpg"></li></ul><li>For each \(j\), note that \(N\) sends the first vector in the list \(N^{m_{j}} v_{j}, \ldots, N v_{j}, v_{j}\) to 0 and that \(N\) sends each vector in this list other than the first vector to the previous vector.&nbsp;</li><li>In other words, 8.55 gives a basis of \(V\) with respect to which \(N\) has a block diagonal matrix, where each matrix on the diagonal has the form</li><ul><li>\[\left(\begin{array}{cccc}0 &amp; 1 &amp; &amp; 0 \\&amp; \ddots &amp; \ddots &amp; \\&amp;&amp; \ddots &amp; 1 \\0 &amp; &amp; &amp; 0\end{array}\right) .\]</li></ul></ul>Thus the desired result holds for nilpotent operators.
-
-============================================================
-
-Note ID: 1707672965891
-  Field: Text
-    Before:
-      <img src="paste-16ac95ba2f11b60a610de9c7d53062babf1be5a6.jpg"><br><img src="paste-f102893410e4bd563bc3aff300b2318a2a446864.jpg"><br>Proof:<br><ul><li>Let&nbsp;\(\lambda_i\) be the eigenvalues of T</li><li>Decompose V into generalized eigenspaces&nbsp;</li><ul><li>\(V\) =&nbsp;\(\bigoplus\)&nbsp;\(G(\lambda_i,T)\) because the vector space is complex<br></li></ul><li>Where each&nbsp;\((T-\lambda_j I)\) \(|_{G(\lambda_j, T)}\) is nilpotent</li><li>Thus some basis of each generalized eigenspace is a Jordan basis for&nbsp;&nbsp;\((T-\lambda_j I)\) \(|_{G(\lambda_j, T)}\).</li><li>Thus &nbsp;\((T-\lambda_j I)\) \(|_{G(\lambda_j, T)}\).. has the form <img src="paste-501e90f5a0cb38845eb8352dfe16597aeb9c46da.jpg"></li><li>Put the bases together to get a jordan basis for T</li></ul>
-
-    After:
-      <img src="paste-16ac95ba2f11b60a610de9c7d53062babf1be5a6.jpg"><br><img src="paste-f102893410e4bd563bc3aff300b2318a2a446864.jpg"><br>Proof:<br><ul><li>Let&nbsp;\(\lambda_i\) be the eigenvalues of T</li><li>Decompose V into generalized eigenspaces&nbsp;</li><ul><li>\(V\) =&nbsp;\(\bigoplus\)&nbsp;\(G(\lambda_i,T)\) because the vector space is complex<br></li></ul><li>Where each&nbsp;\((T-\lambda_j I)\) \(|_{G(\lambda_j, T)}\) is nilpotent</li><li>Thus some basis of each generalized eigenspace is a Jordan basis for&nbsp;&nbsp;\((T-\lambda_j I)\) \(|_{G(\lambda_j, T)}\).</li><li>Thus &nbsp;\((T-\lambda_j I)\) \(|_{G(\lambda_j, T)}\).. has the form <img src="paste-501e90f5a0cb38845eb8352dfe16597aeb9c46da.jpg"></li><li>Put the bases together to get a jordan basis for T</li></ul>
-
-============================================================
-
-Note ID: 1707675426248
-  Field: Text
-    Before:
-      <ul><li>As we will soon see, a real vector space \(V\) can be embedded, in a natural way, in a complex vector space called the complexification of \(V\).&nbsp;</li><li>Each operator on \(V\) can be extended to an operator on the complexification of \(V\).&nbsp;</li></ul>
-
-    After:
-      <ul><li>As we will soon see, a real vector space \(V\) can be embedded, in a natural way, in a complex vector space called the complexification of \(V\).&nbsp;</li><li>Each operator on \(V\) can be extended to an operator on the complexification of \(V\).&nbsp;</li></ul>
-
-============================================================
-
-Note ID: 1707675651640
-  Field: Text
-    Before:
-      Definition complexification of \(V\), \(V_{\mathbf{C} }\)<br><br>Suppose \(V\) is a real vector space.<br><br><ul><li>The complexification of \(V\), denoted \(V_{\mathbf{C} }\), equals \(V \times V\). An element of \(V_{\mathbf{C}}\) is an ordered pair \((u, v)\), where \(u, v \in V\), but we will write this as \(u+i v\).</li></ul>
-
-    After:
-      Definition complexification of \(V\), \(V_{\mathbf{C} }\)<br><br>Suppose \(V\) is a real vector space.<br><br><ul><li>The complexification of \(V\), denoted \(V_{\mathbf{C} }\), equals \(V \times V\). An element of \(V_{\mathbf{C}}\) is an ordered pair \((u, v)\), where \(u, v \in V\), but we will write this as \(u+i v\).</li></ul>
-
-============================================================
-
-Note ID: 1707676205426
-  Field: Text
-    Before:
-      We think of \(V\) as a subset of \(V_{\mathbf{C} }\) by identifying \(u \in V\) with \(u+i 0\). The construction of \(V_{\mathbf{C} }\) from \(V\) can then be thought of as generalizing the construction of \(\mathbf{C}^{n}\) from \(\mathbf{R}^{n}\).
-
-    After:
-      We think of \(V\) as a subset of \(V_{\mathbf{C} }\) by identifying \(u \in V\) with \(u+i 0\). The construction of \(V_{\mathbf{C} }\) from \(V\) can then be thought of as generalizing the construction of \(\mathbf{C}^{n}\) from \(\mathbf{R}^{n}\).
-
-============================================================
-
-Note ID: 1707676315095
-  Field: Text
-    Before:
-      &nbsp;\(V_{\mathbf{C} }\) is a complex vector space.<br><br>Suppose \(V\) is a real vector space. Then with the definitions of addition and scalar multiplication as above, \(V_{\mathbf{C} }\) is a complex vector space.
-
-    After:
-      &nbsp;\(V_{\mathbf{C} }\) is a complex vector space.<br><br>Suppose \(V\) is a real vector space. Then with the definitions of addition and scalar multiplication as above, \(V_{\mathbf{C} }\) is a complex vector space.
-
-============================================================
-
-Note ID: 1707678155529
-  Field: Text
-    Before:
-      Basis of \(V\) is basis of \(V_{\mathbf{C} }\)<br><br>Suppose \(V\) is a real vector space.<br><ul><li>(a) If \(v_{1}, \ldots, v_{n}\) is a basis of \(V\) (as a real vector space), then \(v_{1}, \ldots, v_{n}\) is a basis of \(V_{\mathbf{C} }\) (as a complex vector space).<br></li><li>(b) The dimension of \(V_{\mathbf{C} }\) (as a complex vector space) equals the dimension of \(V\) (as a real vector space).</li></ul>
-
-    After:
-      Basis of \(V\) is basis of \(V_{\mathbf{C} }\)<br><br>Suppose \(V\) is a real vector space.<br><ul><li>(a) If \(v_{1}, \ldots, v_{n}\) is a basis of \(V\) (as a real vector space), then \(v_{1}, \ldots, v_{n}\) is a basis of \(V_{\mathbf{C} }\) (as a complex vector space).<br></li><li>(b) The dimension of \(V_{\mathbf{C} }\) (as a complex vector space) equals the dimension of \(V\) (as a real vector space).</li></ul>
-
-============================================================
-
-Note ID: 1707721377883
-  Field: Text
-    Before:
-      <img src="paste-2d07474ae11ae00257e7560eef98d3a972a46a78.jpg"><br>Proof:<br><ul><li>To show that \(v_{1}, \ldots, v_{n}\) is linearly independent in the complex vector space \(V_{\mathbf{C}}\), suppose \(\lambda_{1}, \ldots, \lambda_{n} \in \mathbf{C}\) and<br></li><ul><li>\(\sum\) \(\lambda_i\) \(v_i \) = \( 0 \)</li></ul><li>Then the equation above and our definitions imply that:</li><ul><li>&nbsp;\(\sum\) \((\operatorname{Re}\lambda_i)\)&nbsp;\(v_i\) = \(0\)&nbsp;</li><li>and&nbsp;</li><li>\(\sum\) \((\operatorname{Im}\lambda_i)&nbsp;\)&nbsp;\(v_i\)&nbsp;= \(0\).</li></ul><li>Because \(v_{1}, \ldots, v_{n}\) is linearly independent in \(V\), the equations above imply:</li><ul><li>&nbsp;\(\operatorname{Re}\lambda_i\)&nbsp;=&nbsp;\(0\) for all i</li><li>and&nbsp;</li><li>\(\operatorname{Im}\lambda_i\) =&nbsp;\(0\) for all i</li></ul><li>Thus we have \(\lambda_{1}=\cdots=\lambda_{n}\) = \(0\).&nbsp;</li><li>Hence \(v_{1}, \ldots, v_{n}\) is linearly independent in \(V_{\mathbf{C}}\), completing the proof of (a).<br></li></ul>
-
-    After:
-      <img src="paste-2d07474ae11ae00257e7560eef98d3a972a46a78.jpg"><br>Proof:<br><ul><li>To show that \(v_{1}, \ldots, v_{n}\) is linearly independent in the complex vector space \(V_{\mathbf{C}}\), suppose \(\lambda_{1}, \ldots, \lambda_{n} \in \mathbf{C}\) and<br></li><ul><li>\(\sum\) \(\lambda_i\) \(v_i \) = \( 0 \)</li></ul><li>Then the equation above and our definitions imply that:</li><ul><li>&nbsp;\(\sum\) \((\operatorname{Re}\lambda_i)\)&nbsp;\(v_i\) = \(0\)&nbsp;</li><li>and&nbsp;</li><li>\(\sum\) \((\operatorname{Im}\lambda_i)&nbsp;\)&nbsp;\(v_i\)&nbsp;= \(0\).</li></ul><li>Because \(v_{1}, \ldots, v_{n}\) is linearly independent in \(V\), the equations above imply:</li><ul><li>&nbsp;\(\operatorname{Re}\lambda_i\)&nbsp;=&nbsp;\(0\) for all i</li><li>and&nbsp;</li><li>\(\operatorname{Im}\lambda_i\) =&nbsp;\(0\) for all i</li></ul><li>Thus we have \(\lambda_{1}=\cdots=\lambda_{n}\) = \(0\).&nbsp;</li><li>Hence \(v_{1}, \ldots, v_{n}\) is linearly independent in \(V_{\mathbf{C}}\), completing the proof of (a).<br></li></ul>
-
-============================================================
-
-Note ID: 1707721587946
-  Field: Text
-    Before:
-      Definition complexification of \(T\), \(T_{\mathbf{C} }\)<br><br>Suppose \(V\) is a real vector space and \(T \in \mathcal{L}(V)\). <br><br>The complexification of \(T\), denoted \(T_{\mathbf{C} }\), is the operator \(T_{\mathbf{C} }\) \(\in\) \(\mathcal{L}\left(V_{\mathbf{C} }\right)\) defined by<br><br><ul><li>\(T_{\mathbf{C} }\) \((\) \(u+i v\) \()\)= \(T u+i T v\)</li><li>for \(u, v \in V\).<br></li></ul>
-
-    After:
-      Definition complexification of \(T\), \(T_{\mathbf{C} }\)<br><br>Suppose \(V\) is a real vector space and \(T \in \mathcal{L}(V)\). <br><br>The complexification of \(T\), denoted \(T_{\mathbf{C} }\), is the operator \(T_{\mathbf{C} }\) \(\in\) \(\mathcal{L}\left(V_{\mathbf{C} }\right)\) defined by<br><br><ul><li>\(T_{\mathbf{C} }\) \((\) \(u+i v\) \()\)= \(T u+i T v\)</li><li>for \(u, v \in V\).<br></li></ul>
-
-============================================================
-
-Note ID: 1707722272420
-  Field: Text
-    Before:
-      Every operator has an invariant subspace of dimension 1 or 2<br><br>Every operator on a nonzero finite-dimensional vector space has an invariant subspace of dimension 1 or 2 .
-
-    After:
-      Every operator has an invariant subspace of dimension 1 or 2<br><br>Every operator on a nonzero finite-dimensional vector space has an invariant subspace of dimension 1 or 2 .
-
-============================================================
-
-Note ID: 1707722737703
-  Field: Text
-    Before:
-      <img src="paste-9eb0c523b8c723672c35b0444259780a6274209c.jpg"><br>Proof:<br>For real vector spaces (since we know the complex case is guaranteed to have an eigenvalue and a 1-dim invariant subspace):<br><ul><li>Hence assume \(V\) is a real vector space and \(T \in \mathcal{L}(V)\).&nbsp;</li><li>The complexification \(T_{\mathbf{C}}\) has an eigenvalue \(a+b i\) (by 5.21), where \(a, b \in \mathbf{R}\).&nbsp;</li><li>Thus there exist \(u, v \in V\), not both 0 , such that:</li><ul><li>&nbsp;\(T_{\mathbf{C}}\) ( \(u+i v\) )= \((a+b i)\) \((u+i v)\)</li></ul><li>Using the definition of \(T_{\mathbf{C}}\), the last equation can be rewritten as</li><ul><li>\(T u\) + \(i T v\)=\((a u-b v)\) + \((a v+b u) i \)</li></ul><li>Thus</li><ul><li>\(T u\) = \(a u-b v\)&nbsp;</li><li>and&nbsp;</li><li>\(T v\) = \(a v+b u .\)</li></ul><li>Let \(U\) equal the span in \(V\) of the list \(u, v\).&nbsp;</li><li>Then \(U\) is a subspace of \(V\) with dimension 1 or 2 .&nbsp;</li><li>The equations above show that \(U\) is invariant under \(T\), completing the proof.</li></ul>
-
-    After:
-      <img src="paste-9eb0c523b8c723672c35b0444259780a6274209c.jpg"><br>Proof:<br>For real vector spaces (since we know the complex case is guaranteed to have an eigenvalue and a 1-dim invariant subspace):<br><ul><li>Hence assume \(V\) is a real vector space and \(T \in \mathcal{L}(V)\).&nbsp;</li><li>The complexification \(T_{\mathbf{C}}\) has an eigenvalue \(a+b i\) (by 5.21), where \(a, b \in \mathbf{R}\).&nbsp;</li><li>Thus there exist \(u, v \in V\), not both 0 , such that:</li><ul><li>&nbsp;\(T_{\mathbf{C}}\) ( \(u+i v\) )= \((a+b i)\) \((u+i v)\)</li></ul><li>Using the definition of \(T_{\mathbf{C}}\), the last equation can be rewritten as</li><ul><li>\(T u\) + \(i T v\)=\((a u-b v)\) + \((a v+b u) i \)</li></ul><li>Thus</li><ul><li>\(T u\) = \(a u-b v\)&nbsp;</li><li>and&nbsp;</li><li>\(T v\) = \(a v+b u .\)</li></ul><li>Let \(U\) equal the span in \(V\) of the list \(u, v\).&nbsp;</li><li>Then \(U\) is a subspace of \(V\) with dimension 1 or 2 .&nbsp;</li><li>The equations above show that \(U\) is invariant under \(T\), completing the proof.</li></ul>
-
-============================================================
-
-Note ID: 1707723430293
-  Field: Text
-    Before:
-      Minimal polynomial of \(T_{\mathbf{C} }\) equals minimal polynomial of \(T\)<br><br>Suppose \(V\) is a real vector space and \(T \in \mathcal{L}(V)\). Then the minimal polynomial of \(T_{\mathbf{C} }\) equals the minimal polynomial of \(T\).
-
-    After:
-      Minimal polynomial of \(T_{\mathbf{C} }\) equals minimal polynomial of \(T\)<br><br>Suppose \(V\) is a real vector space and \(T \in \mathcal{L}(V)\). Then the minimal polynomial of \(T_{\mathbf{C} }\) equals the minimal polynomial of \(T\).
-
-============================================================
-
-Note ID: 1707723806458
-  Field: Text
-    Before:
-      <img src="paste-db663e2e48fddd04cd496ff7918e29dd3828c303.jpg"><br>Proof:<br>Let \(p \in \mathcal{P}(\mathbf{R})\) denote the minimal polynomial of \(T\). From 9.9 it is easy to see that:<br><ul><ul><li>&nbsp;\(p\left(T_{\mathbf{C} }\right)\) = \((p(T))_{\mathbf{C} }\),&nbsp;</li><li>and thus \(p\left(T_{\mathbf{C} }\right)\) = \(0\).</li></ul></ul><ul><li>Suppose \(q \in \mathcal{P}(\mathbf{C})\) is a monic polynomial such that \(q\left(T_{\mathbf{C}}\right)\) = \(0\). </li><li>Then \(\left(q\left(T_{\mathbf{C}}\right)\right)(u)\) = \(0\) for every \(u \in V\).&nbsp;</li><li>Letting \(r\) denote the polynomial whose \(j^{\text {th}&nbsp; &nbsp;}\) coefficient is the real part of the \(j^{\text {th } }\) coefficient of \(q\), we see that \(r\) is a monic polynomial and \(r(T)\) = \(0\).&nbsp;</li><li>Thus \(\operatorname{deg} q\) = \(\operatorname{deg} r\) \(\geq\) \(\operatorname{deg} p\).</li><li>The conclusions of the two previous paragraphs imply that \(p\) is the minimal polynomial of \(T_{\mathbf{C}}\), as desired.<br></li></ul>
-
-    After:
-      <img src="paste-db663e2e48fddd04cd496ff7918e29dd3828c303.jpg"><br>Proof:<br>Let \(p \in \mathcal{P}(\mathbf{R})\) denote the minimal polynomial of \(T\). From 9.9 it is easy to see that:<br><ul><ul><li>&nbsp;\(p\left(T_{\mathbf{C} }\right)\) = \((p(T))_{\mathbf{C} }\),&nbsp;</li><li>and thus \(p\left(T_{\mathbf{C} }\right)\) = \(0\).</li></ul></ul><ul><li>Suppose \(q \in \mathcal{P}(\mathbf{C})\) is a monic polynomial such that \(q\left(T_{\mathbf{C}}\right)\) = \(0\). </li><li>Then \(\left(q\left(T_{\mathbf{C}}\right)\right)(u)\) = \(0\) for every \(u \in V\).&nbsp;</li><li>Letting \(r\) denote the polynomial whose \(j^{\text {th}&nbsp; &nbsp;}\) coefficient is the real part of the \(j^{\text {th } }\) coefficient of \(q\), we see that \(r\) is a monic polynomial and \(r(T)\) = \(0\).&nbsp;</li><li>Thus \(\operatorname{deg} q\) = \(\operatorname{deg} r\) \(\geq\) \(\operatorname{deg} p\).</li><li>The conclusions of the two previous paragraphs imply that \(p\) is the minimal polynomial of \(T_{\mathbf{C}}\), as desired.<br></li></ul>
-
-============================================================
-
-Note ID: 1707723918734
-  Field: Text
-    Before:
-      Real eigenvalues of \(T_{\mathbf{C} }\)<br><br>Suppose \(V\) is a real vector space, \(T \in \mathcal{L}(V)\), and \(\lambda\) \(\in\) \(\mathbf{R}\). Then \(\lambda\) is an eigenvalue of \(T_{\mathbf{C} }\) if and only if \(\lambda\) is an eigenvalue of \(T\).
-
-    After:
-      Real eigenvalues of \(T_{\mathbf{C} }\)<br><br>Suppose \(V\) is a real vector space, \(T \in \mathcal{L}(V)\), and \(\lambda\) \(\in\) \(\mathbf{R}\). Then \(\lambda\) is an eigenvalue of \(T_{\mathbf{C} }\) if and only if \(\lambda\) is an eigenvalue of \(T\).
-
-============================================================
-
-Note ID: 1707724095904
-  Field: Text
-    Before:
-      <img src="paste-2ed8a0710d360c78a55de02592b68673ce530e6b.jpg"><br>Proof 1 First suppose \(\lambda\) is an eigenvalue of \(T\). Then there exists \(v \in V\) with \(v \neq 0\) such that \(T v\) = \(\lambda v\). Thus \(T_{\mathbf{C} } v\) = \(\lambda v\), which shows that \(\lambda\) is an eigenvalue of \(T_{\mathbf{C}}\), completing one direction of the proof.
-
-    After:
-      <img src="paste-2ed8a0710d360c78a55de02592b68673ce530e6b.jpg"><br>Proof 1 First suppose \(\lambda\) is an eigenvalue of \(T\). Then there exists \(v \in V\) with \(v \neq 0\) such that \(T v\) = \(\lambda v\). Thus \(T_{\mathbf{C} } v\) = \(\lambda v\), which shows that \(\lambda\) is an eigenvalue of \(T_{\mathbf{C}}\), completing one direction of the proof.
-
-============================================================
-
-Note ID: 1707724212571
-  Field: Text
-    Before:
-      <img src="paste-e111a18693d7de267dd56c7e3d0d8d6eabe3a3a2.jpg"><br>To prove the other direction, suppose now that \(\lambda\) is an eigenvalue of \(T_{\mathbf{C} }\). Then there exist \(u, v \in V\) with \(u+i v \neq 0\) such that<br><br><ul><li>\(T_{\mathbf{C} }\) \((\) \(u+i v\) \()\)=\( \lambda(u+i v) .\)</li></ul><br>The equation above implies that:<br><ul><li>&nbsp;\(T u\) = \(\lambda u\)&nbsp;</li><li>and&nbsp;</li><li>\(T v\) = \(\lambda v\).&nbsp;</li></ul>Because \(u \neq 0\) or \(v \neq 0\), this implies that \(\lambda\) is an eigenvalue of \(T\), completing the proof.<br>
-
-    After:
-      <img src="paste-e111a18693d7de267dd56c7e3d0d8d6eabe3a3a2.jpg"><br>To prove the other direction, suppose now that \(\lambda\) is an eigenvalue of \(T_{\mathbf{C} }\). Then there exist \(u, v \in V\) with \(u+i v \neq 0\) such that<br><br><ul><li>\(T_{\mathbf{C} }\) \((\) \(u+i v\) \()\)=\( \lambda(u+i v) .\)</li></ul><br>The equation above implies that:<br><ul><li>&nbsp;\(T u\) = \(\lambda u\)&nbsp;</li><li>and&nbsp;</li><li>\(T v\) = \(\lambda v\).&nbsp;</li></ul>Because \(u \neq 0\) or \(v \neq 0\), this implies that \(\lambda\) is an eigenvalue of \(T\), completing the proof.<br>
-
-============================================================
-
-Note ID: 1707724335663
-  Field: Text
-    Before:
-      <img src="paste-ae66688317a74162cc0c0752164d11d60ca8e611.jpg"><br>Proof 2 <br><ul><li>The (real) eigenvalues of \(T\) are the (real) zeros of the minimal polynomial of \(T\)&nbsp;</li><li>The real eigenvalues of \(T_{\mathbf{C}}\) are the real zeros of the minimal polynomial of \(T_{\mathbf{C} }\) .&nbsp;</li><li>These two minimal polynomials are the same.&nbsp;</li><li>Thus the eigenvalues of \(T\) are precisely the real eigenvalues of \(T_{\mathbf{C} }\), as desired.</li></ul><br>
-
-    After:
-      <img src="paste-ae66688317a74162cc0c0752164d11d60ca8e611.jpg"><br>Proof 2 <br><ul><li>The (real) eigenvalues of \(T\) are the (real) zeros of the minimal polynomial of \(T\)&nbsp;</li><li>The real eigenvalues of \(T_{\mathbf{C}}\) are the real zeros of the minimal polynomial of \(T_{\mathbf{C} }\) .&nbsp;</li><li>These two minimal polynomials are the same.&nbsp;</li><li>Thus the eigenvalues of \(T\) are precisely the real eigenvalues of \(T_{\mathbf{C} }\), as desired.</li></ul><br>
-
-============================================================
-
-Note ID: 1707724633729
-  Field: Text
-    Before:
-      Suppose \(V\) is a real vector space, \(T \in \mathcal{L}(V)\), and \(\lambda\)&nbsp; \(\in \mathbf{C}\). Then \(\lambda\) is an eigenvalue of \(T_{\mathbf{C} }\) if and only if \(\bar{\lambda}\) is an eigenvalue of \(T_{\mathbf{C} }\).
-
-    After:
-      Suppose \(V\) is a real vector space, \(T \in \mathcal{L}(V)\), and \(\lambda\)&nbsp; \(\in \mathbf{C}\). Then \(\lambda\) is an eigenvalue of \(T_{\mathbf{C} }\) if and only if \(\bar{\lambda}\) is an eigenvalue of \(T_{\mathbf{C} }\).
-
-============================================================
-
-Note ID: 1707724895413
-  Field: Text
-    Before:
-      Suppose \(V\) is a real vector space, \(T \in \mathcal{L}(V)\), and \(\lambda \in \mathbf{C}\) is an eigenvalue of \(T_{\mathbf{C}}\). Then the multiplicity of \(\lambda\) as an eigenvalue of \(T_{\mathbf{C}}\) equals the multiplicity of \(\bar{\lambda}\) as an eigenvalue of \(T_{\mathbf{C} }\).
-
-    After:
-      Suppose \(V\) is a real vector space, \(T \in \mathcal{L}(V)\), and \(\lambda \in \mathbf{C}\) is an eigenvalue of \(T_{\mathbf{C}}\). Then the multiplicity of \(\lambda\) as an eigenvalue of \(T_{\mathbf{C}}\) equals the multiplicity of \(\bar{\lambda}\) as an eigenvalue of \(T_{\mathbf{C} }\).
-
-============================================================
-
-Note ID: 1707809966802
-  Field: Text
-    Before:
-      Suppose \(V\) is a real vector space and \(T \in \mathcal{L}(V)\). Then the coefficients of the characteristic polynomial of \(T_{\mathbf{C} }\) are all real.
-
-    After:
-      Suppose \(V\) is a real vector space and \(T \in \mathcal{L}(V)\). Then the coefficients of the characteristic polynomial of \(T_{\mathbf{C} }\) are all real.
-
-============================================================
-
-Note ID: 1707810168652
-  Field: Text
-    Before:
-      <img src="paste-29f0dab45ab6209f206d88b81643b2c70028a006.jpg"><br>Proof Suppose \(\lambda\) is a nonreal eigenvalue of \(T_{\mathbf{C}}\) with multiplicity \(m\). Then \(\bar{\lambda}\) is also an eigenvalue of \(T_{\mathbf{C} }\) with multiplicity \(m\) (by 9.17). Thus the characteristic polynomial of \(T_{\mathbf{C}}\) includes factors of \((z-\lambda)^{m}\) and \((z-\bar{\lambda})^{m}\). Multiplying together these two factors, we have<br><br><ul><li>\( (z-\lambda)^{m}\) \((z-\bar{\lambda})^{m}\)= \( \left(z^{2}-2(\operatorname{Re} \lambda) z+|\lambda|^{2}\right)^{m} \)</li></ul><br>The polynomial above on the right has real coefficients.<br>
-
-    After:
-      <img src="paste-29f0dab45ab6209f206d88b81643b2c70028a006.jpg"><br>Proof Suppose \(\lambda\) is a nonreal eigenvalue of \(T_{\mathbf{C}}\) with multiplicity \(m\). Then \(\bar{\lambda}\) is also an eigenvalue of \(T_{\mathbf{C} }\) with multiplicity \(m\) (by 9.17). Thus the characteristic polynomial of \(T_{\mathbf{C}}\) includes factors of \((z-\lambda)^{m}\) and \((z-\bar{\lambda})^{m}\). Multiplying together these two factors, we have<br><br><ul><li>\( (z-\lambda)^{m}\) \((z-\bar{\lambda})^{m}\)= \( \left(z^{2}-2(\operatorname{Re} \lambda) z+|\lambda|^{2}\right)^{m} \)</li></ul><br>The polynomial above on the right has real coefficients.<br>
-
-============================================================
-
-Note ID: 1707810585374
-  Field: Text
-    Before:
-      Suppose \(V\) is a real vector space and \(T \in \mathcal{L}(V)\). Then<br><br><ul><li>(a) the coefficients of the characteristic polynomial of \(T\) are all real;</li><li>(b) the characteristic polynomial of \(T\) has degree \(\operatorname{dim} V\);</li><li>(c) the eigenvalues of \(T\) are precisely the real zeros of the characteristic polynomial of \(T\).</li></ul>
-
-    After:
-      Suppose \(V\) is a real vector space and \(T \in \mathcal{L}(V)\). Then<br><br><ul><li>(a) the coefficients of the characteristic polynomial of \(T\) are all real;</li><li>(b) the characteristic polynomial of \(T\) has degree \(\operatorname{dim} V\);</li><li>(c) the eigenvalues of \(T\) are precisely the real zeros of the characteristic polynomial of \(T\).</li></ul>
-
-============================================================
-
-Note ID: 1707810829830
-  Field: Text
-    Before:
-      Suppose \(T\) \(\in\) \(\mathcal{L}(V)\). Then<br><br><ul><li>(a) the degree of the minimal polynomial of \(T\) is at most \(\operatorname{dim} V\);</li><li>(b) the characteristic polynomial of \(T\) is a polynomial multiple of the minimal polynomial of \(T\).</li></ul>
-
-    After:
-      Suppose \(T\) \(\in\) \(\mathcal{L}(V)\). Then<br><br><ul><li>(a) the degree of the minimal polynomial of \(T\) is at most \(\operatorname{dim} V\);</li><li>(b) the characteristic polynomial of \(T\) is a polynomial multiple of the minimal polynomial of \(T\).</li></ul>
-
-============================================================
-
-Note ID: 1707810971294
-  Field: Text
-    Before:
-      3 Suppose \(V\) is a real vector space and \(v_{1}, \ldots, v_{m} \in V\). Prove that \(v_{1}, \ldots, v_{m}\) is linearly independent in \(V_{\mathbf{C} }\) if and only if \(v_{1}, \ldots, v_{m}\) is linearly independent in \(V\).
-
-    After:
-      3 Suppose \(V\) is a real vector space and \(v_{1}, \ldots, v_{m} \in V\). Prove that \(v_{1}, \ldots, v_{m}\) is linearly independent in \(V_{\mathbf{C} }\) if and only if \(v_{1}, \ldots, v_{m}\) is linearly independent in \(V\).
-
-============================================================
-
-Note ID: 1707811020056
-  Field: Text
-    Before:
-      4 Suppose \(V\) is a real vector space and \(v_{1}, \ldots, v_{m} \in V\). Prove that \(v_{1}, \ldots, v_{m}\) spans \(V_{\mathbf{C} }\) if and only if \(v_{1}, \ldots, v_{m}\) spans \(V\).
-
-    After:
-      4 Suppose \(V\) is a real vector space and \(v_{1}, \ldots, v_{m} \in V\). Prove that \(v_{1}, \ldots, v_{m}\) spans \(V_{\mathbf{C} }\) if and only if \(v_{1}, \ldots, v_{m}\) spans \(V\).
-
-============================================================
-
-Note ID: 1707811137962
-  Field: Text
-    Before:
-      6 Suppose \(V\) is a real vector space and \(T \in \mathcal{L}(V)\). Prove that \(T_{\mathbf{C} }\) is invertible if and only if \(T\) is invertible.
-
-    After:
-      6 Suppose \(V\) is a real vector space and \(T \in \mathcal{L}(V)\). Prove that \(T_{\mathbf{C} }\) is invertible if and only if \(T\) is invertible.
-
-============================================================
-
-Note ID: 1707811157462
-  Field: Text
-    Before:
-      7 Suppose \(V\) is a real vector space and \(N \in \mathcal{L}(V)\). Prove that \(N_{\mathbf{C} }\) is nilpotent if and only if \(N\) is nilpotent.
-
-    After:
-      7 Suppose \(V\) is a real vector space and \(N \in \mathcal{L}(V)\). Prove that \(N_{\mathbf{C} }\) is nilpotent if and only if \(N\) is nilpotent.
-
-============================================================
-
-Note ID: 1707811927900
-  Field: Text
-    Before:
-      17 Suppose \(V\) is a real vector space and \(T \in \mathcal{L}(V)\) satisfies \(T^{2}\) = \(-I\). Define complex scalar multiplication on \(V\) as follows: if \(a, b \in \mathbf{R}\), then<br><ul><ul><li>\((a+b i)\) \(v\) = \(a v+b T v \)</li></ul><li>(a) Show that the complex scalar multiplication on \(V\) defined above and the addition on \(V\) makes \(V\) into a complex vector space.</li><li>(b) Show that the dimension of \(V\) as a complex vector space is half the dimension of \(V\) as a real vector space.</li></ul>
-
-    After:
-      17 Suppose \(V\) is a real vector space and \(T \in \mathcal{L}(V)\) satisfies \(T^{2}\) = \(-I\). Define complex scalar multiplication on \(V\) as follows: if \(a, b \in \mathbf{R}\), then<br><ul><ul><li>\((a+b i)\) \(v\) = \(a v+b T v \)</li></ul><li>(a) Show that the complex scalar multiplication on \(V\) defined above and the addition on \(V\) makes \(V\) into a complex vector space.</li><li>(b) Show that the dimension of \(V\) as a complex vector space is half the dimension of \(V\) as a real vector space.</li></ul>
-
-============================================================
-
-Note ID: 1707812009806
-  Field: Text
-    Before:
-      18 Suppose \(V\) is a real vector space and \(T \in \mathcal{L}(V)\). Prove that the following are equivalent:<br><br><ul><li>(a) All the eigenvalues of \(T_{\mathbf{C} }\) are real.</li><li>(b) There exists a basis of \(V\) with respect to which \(T\) has an uppertriangular matrix.</li><li>(c) There exists a basis of \(V\) consisting of generalized eigenvectors of \(T\).</li></ul>
-
-    After:
-      18 Suppose \(V\) is a real vector space and \(T \in \mathcal{L}(V)\). Prove that the following are equivalent:<br><br><ul><li>(a) All the eigenvalues of \(T_{\mathbf{C} }\) are real.</li><li>(b) There exists a basis of \(V\) with respect to which \(T\) has an uppertriangular matrix.</li><li>(c) There exists a basis of \(V\) consisting of generalized eigenvectors of \(T\).</li></ul>
-
-============================================================
-
-Note ID: 1707812256160
-  Field: Text
-    Before:
-      19 Suppose \(V\) is a real vector space with \(\operatorname{dim} V\) = \(n\) and \(T \in \mathcal{L}(V)\) is such that null \(T^{n-2}\) \(\neq\) \(\operatorname{null} T^{n-1}\). Prove that \(T\) has at most two distinct eigenvalues and that \(T_{\mathbf{C}}\) has no nonreal eigenvalues.
-
-    After:
-      19 Suppose \(V\) is a real vector space with \(\operatorname{dim} V\) = \(n\) and \(T \in \mathcal{L}(V)\) is such that null \(T^{n-2}\) \(\neq\) \(\operatorname{null} T^{n-1}\). Prove that \(T\) has at most two distinct eigenvalues and that \(T_{\mathbf{C}}\) has no nonreal eigenvalues.
-
-============================================================
-
-Note ID: 1708414868804
-  Field: Text
-    Before:
-      Suppose \(V\) is a 2-dimensional real inner product space and \(T \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(T\) is normal but not self-adjoint.</li><li>(b) The matrix of \(T\) with respect to every orthonormal basis of \(V\) has the form</li><ul><li>\[\left(\begin{array}{cc}a &amp; -b \\b &amp; a\end{array}\right)\]</li><li>with \(b\) \(\neq\) \(0\).</li></ul><li>(c) The matrix of \(T\) with respect to some orthonormal basis of \(V\) has the form</li><ul><li>\[\left(\begin{array}{cc}a &amp; -b \\b &amp; a\end{array}\right)\]</li><li>with \(b\) \(&gt;\) \(0\).</li></ul></ul>
-
-    After:
-      Suppose \(V\) is a 2-dimensional real inner product space and \(T \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(T\) is normal but not self-adjoint.</li><li>(b) The matrix of \(T\) with respect to every orthonormal basis of \(V\) has the form</li><ul><li>\[\left(\begin{array}{cc}a &amp; -b \\b &amp; a\end{array}\right)\]</li><li>with \(b\) \(\neq\) \(0\).</li></ul><li>(c) The matrix of \(T\) with respect to some orthonormal basis of \(V\) has the form</li><ul><li>\[\left(\begin{array}{cc}a &amp; -b \\b &amp; a\end{array}\right)\]</li><li>with \(b\) \(&gt;\) \(0\).</li></ul></ul>
-
-============================================================
-
-Note ID: 1708414996353
-  Field: Text
-    Before:
-      A normal operator restricted to an invariant subspace is normal
-
-    After:
-      A normal operator restricted to an invariant subspace is normal
-
-============================================================
-
-Note ID: 1708415208144
-  Field: Text
-    Before:
-      Suppose \(V\) is an inner product space, \(T \in \mathcal{L}(V)\) is normal, and \(U\) is a subspace of \(V\) that is invariant under \(T\). Then<br><br><ul><li>(a) \(U^{\perp}\) is invariant under \(T\);</li><li>(b) \(U\) is invariant under \(T^{*}\);</li></ul>
-
-    After:
-      Suppose \(V\) is an inner product space, \(T \in \mathcal{L}(V)\) is normal, and \(U\) is a subspace of \(V\) that is invariant under \(T\). Then<br><br><ul><li>(a) \(U^{\perp}\) is invariant under \(T\);</li><li>(b) \(U\) is invariant under \(T^{*}\);</li></ul>
-
-============================================================
-
-Note ID: 1708499217609
-  Field: Text
-    Before:
-      3 Suppose \(V\) is a real inner product space. Show that<br><br><ul><li>\(\langle\) \(u+i v, x+i y\)&nbsp;\(\rangle\) = \(\langle u, x\rangle\) + \(\langle v, y\rangle\) + \((\langle v, x\rangle-\langle u, y\rangle)\) \(i\)</li></ul><ul><li>for \(u, v, x, y \in V\) defines a complex inner product on \(V_{\mathbf{C}}\).</li></ul>
-
-    After:
-      3 Suppose \(V\) is a real inner product space. Show that<br><br><ul><li>\(\langle\) \(u+i v, x+i y\)&nbsp;\(\rangle\) = \(\langle u, x\rangle\) + \(\langle v, y\rangle\) + \((\langle v, x\rangle-\langle u, y\rangle)\) \(i\)</li></ul><ul><li>for \(u, v, x, y \in V\) defines a complex inner product on \(V_{\mathbf{C}}\).</li></ul>
-
-============================================================
-
-Note ID: 1708499270351
-  Field: Text
-    Before:
-      4 Suppose \(V\) is a real inner product space and \(T \in \mathcal{L}(V)\) is self-adjoint. Show that \(T_{\mathbf{C}}\) is a self-adjoint operator on the inner product space \(V_{\mathbf{C}}\) defined by the previous exercise.<br><br>\[<br>\langle u+i v, x+i y\rangle=\langle u, x\rangle+\langle v, y\rangle+(\langle v, x\rangle-\langle u, y\rangle) i<br>\]
-
-    After:
-      4 Suppose \(V\) is a real inner product space and \(T \in \mathcal{L}(V)\) is self-adjoint. Show that \(T_{\mathbf{C}}\) is a self-adjoint operator on the inner product space \(V_{\mathbf{C}}\) defined by the previous exercise.<br><br>\[<br>\langle u+i v, x+i y\rangle=\langle u, x\rangle+\langle v, y\rangle+(\langle v, x\rangle-\langle u, y\rangle) i<br>\]
-
-============================================================
-
-Note ID: 1708499305931
-  Field: Text
-    Before:
-      6 Give an example of an operator \(T\) on an inner product space such that \(T\) has an invariant subspace whose orthogonal complement is not invariant under \(T\).<br><br>[The exercise above shows that 9.30 can fail without the hypothesis that \(T\) is normal.]
-
-    After:
-      6 Give an example of an operator \(T\) on an inner product space such that \(T\) has an invariant subspace whose orthogonal complement is not invariant under \(T\).<br><br>[The exercise above shows that 9.30 can fail without the hypothesis that \(T\) is normal.]
-
-============================================================
-
-Note ID: 1708502861097
-  Field: Text
-    Before:
-      Every isometry is normal<br><ul><li>Suppose \(S \in \mathcal{L}(V)\). Then the following are equivalent:</li><li>- \(S\) is an isometry.</li><li>- \(S^* S\) = \(I\).</li><li>- \(S S^*\) = \(I\).</li></ul>
-
-    After:
-      Every isometry is normal<br><ul><li>Suppose \(S \in \mathcal{L}(V)\). Then the following are equivalent:</li><li>- \(S\) is an isometry.</li><li>- \(S^* S\) = \(I\).</li><li>- \(S S^*\) = \(I\).</li></ul>
-
-============================================================
-
-Note ID: 1708541394466
-  Field: Text
-    Before:
-      <img src="paste-b640ed7d09b8eea0b2f067bceb71c9a39c266f85.jpg"><br>Proof:<br><ul><li>Suppose a holds</li><li>Since S is a normal operator there is an orthonormal basis such that S has a block diagonal matrix such that each block is a 1-by-1 matrix or a 2-by-2 matrix of the form<br>\[<br>\left(\begin{array}{cc}<br>a &amp; -b \\<br>b &amp; a<br>\end{array}\right),<br>\]<br>with \(b&gt;0\).</li><li>If&nbsp;\(\lambda\) is an entry in a 1-by-1 matrix, because S is an isometry&nbsp;\(\lambda\) must have absolute value 1 and thus be -1 or 1</li><li>For 2-by-2 matrices, since S is an isometry it must map every orthonormal vector to a vector with norm 1 as well, thus&nbsp;\(a^2+b^2\) =&nbsp; 1&nbsp; as they are the coefficients of an orthonormal vector and must thus have norm equal 1</li><li>Since&nbsp;\(b&gt;0\) we can by definition pick&nbsp;\(\theta \in (0,\pi)\) such that&nbsp;\(a\)&nbsp;\(= \)&nbsp;\(\cos \theta\) and&nbsp;\(b\) =&nbsp;\(\sin \theta\)</li></ul>
-
-    After:
-      <img src="paste-b640ed7d09b8eea0b2f067bceb71c9a39c266f85.jpg"><br>Proof:<br><ul><li>Suppose a holds</li><li>Since S is a normal operator there is an orthonormal basis such that S has a block diagonal matrix such that each block is a 1-by-1 matrix or a 2-by-2 matrix of the form<br>\[<br>\left(\begin{array}{cc}<br>a &amp; -b \\<br>b &amp; a<br>\end{array}\right),<br>\]<br>with \(b&gt;0\).</li><li>If&nbsp;\(\lambda\) is an entry in a 1-by-1 matrix, because S is an isometry&nbsp;\(\lambda\) must have absolute value 1 and thus be -1 or 1</li><li>For 2-by-2 matrices, since S is an isometry it must map every orthonormal vector to a vector with norm 1 as well, thus&nbsp;\(a^2+b^2\) =&nbsp; 1&nbsp; as they are the coefficients of an orthonormal vector and must thus have norm equal 1</li><li>Since&nbsp;\(b&gt;0\) we can by definition pick&nbsp;\(\theta \in (0,\pi)\) such that&nbsp;\(a\)&nbsp;\(= \)&nbsp;\(\cos \theta\) and&nbsp;\(b\) =&nbsp;\(\sin \theta\)</li></ul>
-
-============================================================
-
-Note ID: 1708586672970
-  Field: Text
-    Before:
-      Definition invertible, inverse, \(A^{-1}\)<br><br>A square matrix \(A\) is called invertible if there is a square matrix \(B\) of the same size such that \(A B\) = \(B A\) = \(I\); we call \(B\) the inverse of \(A\) and denote it by \(A^{-1}\).
-
-    After:
-      Definition invertible, inverse, \(A^{-1}\)<br><br>A square matrix \(A\) is called invertible if there is a square matrix \(B\) of the same size such that \(A B\) = \(B A\) = \(I\); we call \(B\) the inverse of \(A\) and denote it by \(A^{-1}\).
-
-============================================================
-
-Note ID: 1708586691758
-  Field: Text
-    Before:
-      Some mathematicians use the terms nonsingular, which means the same as invertible, and singular, which means the same as noninvertible.
-
-    After:
-      Some mathematicians use the terms nonsingular, which means the same as invertible, and singular, which means the same as noninvertible.
-
-============================================================
-
-Note ID: 1708587591550
-  Field: Text
-    Before:
-      Definition trace of an operator<br><br>Suppose \(T \in \mathcal{L}(V)\).<br><br>- If \(\mathbf{F}=\mathbf{C}\), then the trace of \(T\) is the sum of the eigenvalues of \(T\), with each eigenvalue repeated according to its multiplicity.<br><br>- If \(\mathbf{F}=\mathbf{R}\), then the trace of \(T\) is the sum of the eigenvalues of \(T_{\mathbf{C} }\), with each eigenvalue repeated according to its multiplicity.<br><br>The trace of \(T\) is denoted by trace \(T\).
-
-    After:
-      Definition trace of an operator<br><br>Suppose \(T \in \mathcal{L}(V)\).<br><br>- If \(\mathbf{F}=\mathbf{C}\), then the trace of \(T\) is the sum of the eigenvalues of \(T\), with each eigenvalue repeated according to its multiplicity.<br><br>- If \(\mathbf{F}=\mathbf{R}\), then the trace of \(T\) is the sum of the eigenvalues of \(T_{\mathbf{C} }\), with each eigenvalue repeated according to its multiplicity.<br><br>The trace of \(T\) is denoted by trace \(T\).
-
-============================================================
-
-Note ID: 1708591187968
-  Field: Text
-    Before:
-      Definition trace of a matrix<br><br>The trace of a square matrix \(A\), denoted trace \(A\), is defined to be the sum of the diagonal entries of \(A\).
-
-    After:
-      Definition trace of a matrix<br><br>The trace of a square matrix \(A\), denoted trace \(A\), is defined to be the sum of the diagonal entries of \(A\).
-
-============================================================
-
-Note ID: 1708595896096
-  Field: Text
-    Before:
-      Let \(T \in \mathcal{L}(V)\). Suppose \(u_{1}, \ldots, u_{n}\) and \(v_{1}, \ldots, v_{n}\) are bases of \(V\). Then<br><br><ul><li>\(\operatorname{trace}\) \(\mathcal{M}\left(T,\left(u_{1}, \ldots, u_{n}\right)\right)\) = \(\operatorname{trace}\) \( \mathcal{M}\left(T,\left(v_{1}, \ldots, v_{n}\right)\right) \text {. }\)</li></ul>
-
-    After:
-      Let \(T \in \mathcal{L}(V)\). Suppose \(u_{1}, \ldots, u_{n}\) and \(v_{1}, \ldots, v_{n}\) are bases of \(V\). Then<br><br><ul><li>\(\operatorname{trace}\) \(\mathcal{M}\left(T,\left(u_{1}, \ldots, u_{n}\right)\right)\) = \(\operatorname{trace}\) \( \mathcal{M}\left(T,\left(v_{1}, \ldots, v_{n}\right)\right) \text {. }\)</li></ul>
-
-============================================================
-
-Note ID: 1708596402629
-  Field: Text
-    Before:
-      The identity is not the difference of \(S T\) and \(T S\)<br><br>There do not exist operators \(S, T \in \mathcal{L}(V)\) such that \(S T\) - \(T S\) = \(I\).
-
-    After:
-      The identity is not the difference of \(S T\) and \(T S\)<br><br>There do not exist operators \(S, T \in \mathcal{L}(V)\) such that \(S T\) - \(T S\) = \(I\).
-
-============================================================
-
-Note ID: 1708596475022
-  Field: Text
-    Before:
-      Suppose \(T \in \mathcal{L}(V)\) and \(v_{1}, \ldots, v_{n}\) is a basis of \(V\). Prove that the matrix \(\mathcal{M}\left(T,\left(v_{1}, \ldots, v_{n}\right)\right)\) is invertible if and only if \(T\) is invertible.
-
-    After:
-      Suppose \(T \in \mathcal{L}(V)\) and \(v_{1}, \ldots, v_{n}\) is a basis of \(V\). Prove that the matrix \(\mathcal{M}\left(T,\left(v_{1}, \ldots, v_{n}\right)\right)\) is invertible if and only if \(T\) is invertible.
-
-============================================================
-
-Note ID: 1708596697054
-  Field: Text
-    Before:
-      5 Suppose \(B\) is a square matrix with complex entries. Prove that there exists an invertible square matrix \(A\) with complex entries such that \(A^{-1}\) \(B\)&nbsp;\(A\) is an upper-triangular matrix.
-
-    After:
-      5 Suppose \(B\) is a square matrix with complex entries. Prove that there exists an invertible square matrix \(A\) with complex entries such that \(A^{-1}\) \(B\)&nbsp;\(A\) is an upper-triangular matrix.
-
-============================================================
-
-Note ID: 1708597522029
-  Field: Text
-    Before:
-      9 Suppose \(P \in \mathcal{L}(V)\) satisfies \(P^{2}\) = \(P\). Prove that<br><br><ul><li>\(\text { trace }\) \(P\) = \(\text { dim range } P \text {. }\)</li></ul>
-
-    After:
-      9 Suppose \(P \in \mathcal{L}(V)\) satisfies \(P^{2}\) = \(P\). Prove that<br><br><ul><li>\(\text { trace }\) \(P\) = \(\text { dim range } P \text {. }\)</li></ul>
-
-============================================================
-
-Note ID: 1708598002532
-  Field: Text
-    Before:
-      15 Suppose \(S, T \in \mathcal{L}(V)\). Prove that \(\operatorname{trace}\) \((S T)\) = \(\operatorname{trace}\) \((T S)\).
-
-    After:
-      15 Suppose \(S, T \in \mathcal{L}(V)\). Prove that \(\operatorname{trace}\) \((S T)\) = \(\operatorname{trace}\) \((T S)\).
-
-============================================================
-
-Note ID: 1708598329821
-  Field: Text
-    Before:
-      18 Suppose \(V\) is an inner product space with orthonormal basis \(e_{1}, \ldots, e_{n}\) and \(T \in \mathcal{L}(V)\). Prove that<br><br><ul><li>\(\operatorname{trace}\) \(\left(T^{*} T\right)\) = \(\left\|T e_{1}\right\|^{2}\) + \(\cdots\) + \(\left\|T e_{n}\right\|^{2} .\)</li></ul><br>Conclude that the right side of the equation above is independent of which orthonormal basis \(e_{1}, \ldots, e_{n}\) is chosen for \(V\).<br>
-
-    After:
-      18 Suppose \(V\) is an inner product space with orthonormal basis \(e_{1}, \ldots, e_{n}\) and \(T \in \mathcal{L}(V)\). Prove that<br><br><ul><li>\(\operatorname{trace}\) \(\left(T^{*} T\right)\) = \(\left\|T e_{1}\right\|^{2}\) + \(\cdots\) + \(\left\|T e_{n}\right\|^{2} .\)</li></ul><br>Conclude that the right side of the equation above is independent of which orthonormal basis \(e_{1}, \ldots, e_{n}\) is chosen for \(V\).<br>
-
-============================================================
-
-Note ID: 1708935664270
-  Field: Text
-    Before:
-      - Suppose \(P \in \mathcal{L}(V)\) satisfies \(P^2\) = \(P\). <br>Prove that trace \(P\) = \(\operatorname{dim}\) range \(P\).<br><br>This implies that if \(P^2\) = \(P\) then the trace is a nonnegative integer
-
-    After:
-      - Suppose \(P \in \mathcal{L}(V)\) satisfies \(P^2\) = \(P\). <br>Prove that trace \(P\) = \(\operatorname{dim}\) range \(P\).<br><br>This implies that if \(P^2\) = \(P\) then the trace is a nonnegative integer
-
-============================================================
-
-Note ID: 1708986194191
-  Field: Text
-    Before:
-      Definition determinant of an operator<br><br>Suppose \(T \in \mathcal{L}(V)\).<br><br>- If \(\mathbf{F}=\mathbf{C}\), then the determinant of \(T\) is the product of the eigenvalues of \(T\), with each eigenvalue&nbsp;&nbsp;repeated according to its multiplicity.<br><br>- If \(\mathbf{F}=\mathbf{R}\), then the determinant of \(T\) is the product of the eigenvalues of \(T_{\mathbf{C} }\), with each eigenvalue repeated according to its multiplicity.<br>
-
-    After:
-      Definition determinant of an operator<br><br>Suppose \(T \in \mathcal{L}(V)\).<br><br>- If \(\mathbf{F}=\mathbf{C}\), then the determinant of \(T\) is the product of the eigenvalues of \(T\), with each eigenvalue&nbsp;&nbsp;repeated according to its multiplicity.<br><br>- If \(\mathbf{F}=\mathbf{R}\), then the determinant of \(T\) is the product of the eigenvalues of \(T_{\mathbf{C} }\), with each eigenvalue repeated according to its multiplicity.<br>
-
-============================================================
-
-Note ID: 1708987132541
-  Field: Text
-    Before:
-      Suppose \(T \in \mathcal{L}(V)\). Then the characteristic polynomial of \(T\) can be written as<br><br><ul><li>\(z^{n}\) - \((\operatorname{trace} T)\) \(z^{n-1}\) +\(\cdots\)+\((-1)^{n}\) \((\operatorname{det} T)\)&nbsp;</li></ul>
-
-    After:
-      Suppose \(T \in \mathcal{L}(V)\). Then the characteristic polynomial of \(T\) can be written as<br><br><ul><li>\(z^{n}\) - \((\operatorname{trace} T)\) \(z^{n-1}\) +\(\cdots\)+\((-1)^{n}\) \((\operatorname{det} T)\)&nbsp;</li></ul>
-
-============================================================
-
-Note ID: 1709022341039
-  Field: Text
-    Before:
-      Invertible is equivalent to nonzero determinant<br><br>An operator on \(V\) is invertible if and only if its determinant is nonzero.
-
-    After:
-      Invertible is equivalent to nonzero determinant<br><br>An operator on \(V\) is invertible if and only if its determinant is nonzero.
-
-============================================================
-
-Note ID: 1709022540268
-  Field: Text
-    Before:
-      Characteristic polynomial of \(T\) equals \(\operatorname{det}\) \((z I-T)\)<br><br>Suppose \(T \in \mathcal{L}(V)\). Then the characteristic polynomial of \(T\) equals \(\operatorname{det}\) \((z I-T)\).
-
-    After:
-      Characteristic polynomial of \(T\) equals \(\operatorname{det}\) \((z I-T)\)<br><br>Suppose \(T \in \mathcal{L}(V)\). Then the characteristic polynomial of \(T\) equals \(\operatorname{det}\) \((z I-T)\).
-
-============================================================
-
-Note ID: 1709023136690
-  Field: Text
-    Before:
-      10.26 Example Suppose \(a_{1}, \ldots, a_{n} \in \mathbf{F}\). Let<br><br>\[<br>A=\left(\begin{array}{ccccc}<br>0 &amp; &amp; &amp; &amp; a_{n} \\<br>a_{1} &amp; 0 &amp; &amp; &amp; \\<br>&amp; a_{2} &amp; 0 &amp; &amp; \\<br>&amp; &amp; \ddots &amp; \ddots &amp; \\<br>&amp; &amp; &amp; a_{n-1} &amp; 0<br>\end{array}\right) ;<br>\]<br><br>here all entries of the matrix are 0 except for the upper-right corner and along the line just below the diagonal. Suppose \(v_{1}, \ldots, v_{n}\) is a basis of \(V\) and \(T \in \mathcal{L}(V)\) is such that \(\mathcal{M}\left(T,\left(v_{1}, \ldots, v_{n}\right)\right)=A\). Find the determinant of \(T\).<br><br>Solution First assume \(a_{j} \neq 0\) for each \(j=1, \ldots, n-1\). Note that the list \(v_{1}, T v_{1}, T^{2} v_{1}, \ldots, T^{n-1} v_{1}\) equals \(v_{1}, a_{1} v_{2}, a_{1} a_{2} v_{3}, \ldots, a_{1} \cdots a_{n-1} v_{n}\).<br><br>Thus \(v_{1}, T v_{1}, \ldots, T^{n-1} v_{1}\) is linearly independent (because the \(a\) 's are all nonzero). Hence if \(p\) is a monic polynomial with degree at most \(n-1\), then \(p(T) v_{1}\) \(\neq 0\). Thus the minimal polynomial of \(T\) cannot have degree less than \(n\).<br><br>As you should verify, \(T^{n} v_{j}\) = \(a_{1} \cdots a_{n} v_{j}\) for each \(j\). Thus we have \(T^{n}\) = \(a_{1} \cdots a_{n} I\). Hence \(z^{n}-a_{1} \cdots a_{n}\) is the minimal polynomial of \(T\). Because \(n=\operatorname{dim} V\) and the characteristic polynomial is a polynomial multiple of the minimal polynomial (9.26), this implies that \(z^{n}-a_{1} \cdots a_{n}\) is also the characteristic polynomial of \(T\).<br><br>Thus 10.22 implies that<br><br><ul><li>\(\operatorname{det} T\) = \((-1)^{n-1} a_{1} \cdots a_{n} .\)</li></ul><br>If some \(a_{j}\) equals 0 , then \(T v_{j}=0\) for some \(j\), which implies that 0 is an eigenvalue of \(T\) and hence \(\operatorname{det} T=0\). In other words, the formula above also holds if some \(a_{j}\) equals 0 .<br><br>Thus in order to have \(\operatorname{det} T=\operatorname{det} \mathcal{M}(T)\), we will have to make the determinant of the matrix in Example 10.26 equal to \((-1)^{n-1} a_{1} \cdots a_{n}\). However, we do not yet have enough evidence to make a reasonable guess about the proper definition of the determinant of an arbitrary square matrix.<br>
-
-    After:
-      10.26 Example Suppose \(a_{1}, \ldots, a_{n} \in \mathbf{F}\). Let<br><br>\[<br>A=\left(\begin{array}{ccccc}<br>0 &amp; &amp; &amp; &amp; a_{n} \\<br>a_{1} &amp; 0 &amp; &amp; &amp; \\<br>&amp; a_{2} &amp; 0 &amp; &amp; \\<br>&amp; &amp; \ddots &amp; \ddots &amp; \\<br>&amp; &amp; &amp; a_{n-1} &amp; 0<br>\end{array}\right) ;<br>\]<br><br>here all entries of the matrix are 0 except for the upper-right corner and along the line just below the diagonal. Suppose \(v_{1}, \ldots, v_{n}\) is a basis of \(V\) and \(T \in \mathcal{L}(V)\) is such that \(\mathcal{M}\left(T,\left(v_{1}, \ldots, v_{n}\right)\right)=A\). Find the determinant of \(T\).<br><br>Solution First assume \(a_{j} \neq 0\) for each \(j=1, \ldots, n-1\). Note that the list \(v_{1}, T v_{1}, T^{2} v_{1}, \ldots, T^{n-1} v_{1}\) equals \(v_{1}, a_{1} v_{2}, a_{1} a_{2} v_{3}, \ldots, a_{1} \cdots a_{n-1} v_{n}\).<br><br>Thus \(v_{1}, T v_{1}, \ldots, T^{n-1} v_{1}\) is linearly independent (because the \(a\) 's are all nonzero). Hence if \(p\) is a monic polynomial with degree at most \(n-1\), then \(p(T) v_{1}\) \(\neq 0\). Thus the minimal polynomial of \(T\) cannot have degree less than \(n\).<br><br>As you should verify, \(T^{n} v_{j}\) = \(a_{1} \cdots a_{n} v_{j}\) for each \(j\). Thus we have \(T^{n}\) = \(a_{1} \cdots a_{n} I\). Hence \(z^{n}-a_{1} \cdots a_{n}\) is the minimal polynomial of \(T\). Because \(n=\operatorname{dim} V\) and the characteristic polynomial is a polynomial multiple of the minimal polynomial (9.26), this implies that \(z^{n}-a_{1} \cdots a_{n}\) is also the characteristic polynomial of \(T\).<br><br>Thus 10.22 implies that<br><br><ul><li>\(\operatorname{det} T\) = \((-1)^{n-1} a_{1} \cdots a_{n} .\)</li></ul><br>If some \(a_{j}\) equals 0 , then \(T v_{j}=0\) for some \(j\), which implies that 0 is an eigenvalue of \(T\) and hence \(\operatorname{det} T=0\). In other words, the formula above also holds if some \(a_{j}\) equals 0 .<br><br>Thus in order to have \(\operatorname{det} T=\operatorname{det} \mathcal{M}(T)\), we will have to make the determinant of the matrix in Example 10.26 equal to \((-1)^{n-1} a_{1} \cdots a_{n}\). However, we do not yet have enough evidence to make a reasonable guess about the proper definition of the determinant of an arbitrary square matrix.<br>
-
-============================================================
-
-Note ID: 1709023260623
-  Field: Text
-    Before:
-      Definition permutation, perm \(n\)<br><br>- A permutation of \((1, \ldots, n)\) is a list \(\left(m_{1}, \ldots, m_{n}\right)\) that contains each of the numbers \(1, \ldots, n\) exactly once.<br><br>- The set of all permutations of \((1, \ldots, n)\) is denoted perm \(n\).
-
-    After:
-      Definition permutation, perm \(n\)<br><br>- A permutation of \((1, \ldots, n)\) is a list \(\left(m_{1}, \ldots, m_{n}\right)\) that contains each of the numbers \(1, \ldots, n\) exactly once.<br><br>- The set of all permutations of \((1, \ldots, n)\) is denoted perm \(n\).
-
-============================================================
-
-Note ID: 1709068654662
-  Field: Text
-    Before:
-      - The sign of a permutation \(\left(m_{1}, \ldots, m_{n}\right)\) is defined to be 1 if the number of pairs of integers \((j, k)\) with \(1 \leq j&lt;k \leq n\) such that \(j\) appears after \(k\) in the list \(\left(m_{1}, \ldots, m_{n}\right)\) is even and -1 if the number of such pairs is odd.<br><br>- In other words, the sign of a permutation equals 1 if the natural order has been changed an even number of times and equals -1 if the natural order has been changed an odd number of times.
-
-    After:
-      - The sign of a permutation \(\left(m_{1}, \ldots, m_{n}\right)\) is defined to be 1 if the number of pairs of integers \((j, k)\) with \(1 \leq j&lt;k \leq n\) such that \(j\) appears after \(k\) in the list \(\left(m_{1}, \ldots, m_{n}\right)\) is even and -1 if the number of such pairs is odd.<br><br>- In other words, the sign of a permutation equals 1 if the natural order has been changed an even number of times and equals -1 if the natural order has been changed an odd number of times.
-
-============================================================
-
-Note ID: 1709106730218
-  Field: Text
-    Before:
-      Determinant of an operator equals determinant of its matrix<br><br>Suppose \(T \in \mathcal{L}(V)\). Then \(\operatorname{det}\) \(T\) = \(\operatorname{det}\) \(\mathcal{M}(T)\).
-
-    After:
-      Determinant of an operator equals determinant of its matrix<br><br>Suppose \(T \in \mathcal{L}(V)\). Then \(\operatorname{det}\) \(T\) = \(\operatorname{det}\) \(\mathcal{M}(T)\).
-
-============================================================
-
-Note ID: 1709107314665
-  Field: Text
-    Before:
-      Isometries have determinant with absolute value 1<br><br>Suppose \(V\) is an inner product space and \(S \in \mathcal{L}(V)\) is an isometry. Then \(|\) \(\operatorname{det} S\) \(|\) = \(1\).
-
-    After:
-      Isometries have determinant with absolute value 1<br><br>Suppose \(V\) is an inner product space and \(S \in \mathcal{L}(V)\) is an isometry. Then \(|\) \(\operatorname{det} S\) \(|\) = \(1\).
-
-============================================================
-
-Note ID: 1709192865635
-  Field: Text
-    Before:
-      Recall that if \(V\) is an inner product space and \(T \in \mathcal{L}(V)\), then \(T^{*} T\) is a positive operator and hence has a unique positive square root, denoted \(\sqrt{T^{*} T}\) . Because \(\sqrt{T^{*} T}\) is positive, all its eigenvalues are nonnegative, and hence det \(\sqrt{T^{*} T}\) \(\geq\) \( 0\).
-
-    After:
-      Recall that if \(V\) is an inner product space and \(T \in \mathcal{L}(V)\), then \(T^{*} T\) is a positive operator and hence has a unique positive square root, denoted \(\sqrt{T^{*} T}\) . Because \(\sqrt{T^{*} T}\) is positive, all its eigenvalues are nonnegative, and hence det \(\sqrt{T^{*} T}\) \(\geq\) \( 0\).
-
-============================================================
-
-Note ID: 1709242452341
-  Field: Text
-    Before:
-      10.46 Example Suppose \(V\) is a real inner product space and \(T \in \mathcal{L}(V)\) is invertible (and thus det \(T\) is either positive or negative). Attach a geometric meaning to the sign of \(\operatorname{det} T\).<br><br>Solution First we consider an isometry \(S \in \mathcal{L}(V)\). By 10.45, the determinant of \(S\) equals 1 or -1 . Note that<br><br>\[<br>\{v \in V: S v=-v\}<br>\]<br><br>is the eigenspace \(E(-1, S)\). Thinking geometrically, we could say that this is the subspace on which \(S\) reverses direction. An examination of proof 2 of 10.45 shows that \(\operatorname{det} S\) = \(1\) if this subspace has even dimension and \(\operatorname{det} S\) = \(-1\) if this subspace has odd dimension.<br><br>Returning to our arbitrary invertible operator \(T \in \mathcal{L}(V)\), by the Polar Decomposition (7.45) there is an isometry \(S \in \mathcal{L}(V)\) such that<br><br>\[<br>T=S \sqrt{T^{*} T}<br>\]<br><br>Now 10.44 tells us that<br><br>\[<br>\operatorname{det} T=(\operatorname{det} S)\left(\operatorname{det} \sqrt{T^{*} T}\right) .<br>\]<br><br>The remarks just before this example pointed out that det \(\sqrt{T^{*} T} \geq 0\). Thus whether \(\operatorname{det} T\) is positive or negative depends on whether \(\operatorname{det} S\) is positive or negative. As we saw in the paragraph above, this depends on whether the subspace on which \(S\) reverses direction has even or odd dimension.<br><br>Because \(T\) is the product of \(S\) and an operator that never reverses direction (namely, \(\sqrt{T^{*} T}\) ), we can reasonably say that whether \(\operatorname{det} T\) is positive or negative depends on whether \(T\) reverses vectors an even or an odd number of times.
-
-    After:
-      10.46 Example Suppose \(V\) is a real inner product space and \(T \in \mathcal{L}(V)\) is invertible (and thus det \(T\) is either positive or negative). Attach a geometric meaning to the sign of \(\operatorname{det} T\).<br><br>Solution First we consider an isometry \(S \in \mathcal{L}(V)\). By 10.45, the determinant of \(S\) equals 1 or -1 . Note that<br><br>\[<br>\{v \in V: S v=-v\}<br>\]<br><br>is the eigenspace \(E(-1, S)\). Thinking geometrically, we could say that this is the subspace on which \(S\) reverses direction. An examination of proof 2 of 10.45 shows that \(\operatorname{det} S\) = \(1\) if this subspace has even dimension and \(\operatorname{det} S\) = \(-1\) if this subspace has odd dimension.<br><br>Returning to our arbitrary invertible operator \(T \in \mathcal{L}(V)\), by the Polar Decomposition (7.45) there is an isometry \(S \in \mathcal{L}(V)\) such that<br><br>\[<br>T=S \sqrt{T^{*} T}<br>\]<br><br>Now 10.44 tells us that<br><br>\[<br>\operatorname{det} T=(\operatorname{det} S)\left(\operatorname{det} \sqrt{T^{*} T}\right) .<br>\]<br><br>The remarks just before this example pointed out that det \(\sqrt{T^{*} T} \geq 0\). Thus whether \(\operatorname{det} T\) is positive or negative depends on whether \(\operatorname{det} S\) is positive or negative. As we saw in the paragraph above, this depends on whether the subspace on which \(S\) reverses direction has even or odd dimension.<br><br>Because \(T\) is the product of \(S\) and an operator that never reverses direction (namely, \(\sqrt{T^{*} T}\) ), we can reasonably say that whether \(\operatorname{det} T\) is positive or negative depends on whether \(T\) reverses vectors an even or an odd number of times.
-
-============================================================
-
-Note ID: 1709242528682
-  Field: Text
-    Before:
-      Suppose \(V\) is an inner product space and \(T \in \mathcal{L}(V)\). Then<br><br><ul><li>\(|\) \(\operatorname{det} T\) \(|\) = \(\operatorname{det} \sqrt{T^{*} T} .\)</li></ul>
-
-    After:
-      Suppose \(V\) is an inner product space and \(T \in \mathcal{L}(V)\). Then<br><br><ul><li>\(|\) \(\operatorname{det} T\) \(|\) = \(\operatorname{det} \sqrt{T^{*} T} .\)</li></ul>
-
-============================================================
-
-Note ID: 1709242703281
-  Field: Text
-    Before:
-      A box in \(\mathbf{R}^{n}\) is a set of the form<br><br>\[<br>\left\{\left(y_{1}, \ldots, y_{n}\right) \in \mathbf{R}^{n}: x_{j}&lt;y_{j}&lt;x_{j}+r_{j} \text { for } j=1, \ldots, n\right\},<br>\]<br><br>where \(r_{1}, \ldots, r_{n}\) are positive numbers and \(\left(x_{1}, \ldots, x_{n}\right) \in \mathbf{R}^{n}\). The numbers \(r_{1}, \ldots, r_{n}\) are called the side lengths of the box.
-
-    After:
-      A box in \(\mathbf{R}^{n}\) is a set of the form<br><br>\[<br>\left\{\left(y_{1}, \ldots, y_{n}\right) \in \mathbf{R}^{n}: x_{j}&lt;y_{j}&lt;x_{j}+r_{j} \text { for } j=1, \ldots, n\right\},<br>\]<br><br>where \(r_{1}, \ldots, r_{n}\) are positive numbers and \(\left(x_{1}, \ldots, x_{n}\right) \in \mathbf{R}^{n}\). The numbers \(r_{1}, \ldots, r_{n}\) are called the side lengths of the box.
-
-============================================================
-
-Note ID: 1709242786058
-  Field: Text
-    Before:
-      Definition volume of a box<br><br>The volume of a box \(B\) in \(\mathbf{R}^{n}\) with side lengths \(r_{1}, \ldots, r_{n}\) is defined to be \(r_{1} \cdots r_{n}\) and is denoted by volume \(B\).
-
-    After:
-      Definition volume of a box<br><br>The volume of a box \(B\) in \(\mathbf{R}^{n}\) with side lengths \(r_{1}, \ldots, r_{n}\) is defined to be \(r_{1} \cdots r_{n}\) and is denoted by volume \(B\).
-
-============================================================
-
-Note ID: 1709243005829
-  Field: Text
-    Before:
-      Definition volume<br><br>Suppose \(\Omega \subset \mathbf{R}^{n}\). Then the volume of \(\Omega\), denoted volume \(\Omega\), is defined to be the infimum of<br><br><ul><li>\(\text { volume } B_{1}+\text { volume } B_{2}+\cdots \text {, }\)</li></ul><br>where the infimum is taken over all sequences \(B_{1}, B_{2}, \ldots\) of boxes in \(\mathbf{R}^{n}\) whose union contains \(\Omega\).<br>
-
-    After:
-      Definition volume<br><br>Suppose \(\Omega \subset \mathbf{R}^{n}\). Then the volume of \(\Omega\), denoted volume \(\Omega\), is defined to be the infimum of<br><br><ul><li>\(\text { volume } B_{1}+\text { volume } B_{2}+\cdots \text {, }\)</li></ul><br>where the infimum is taken over all sequences \(B_{1}, B_{2}, \ldots\) of boxes in \(\mathbf{R}^{n}\) whose union contains \(\Omega\).<br>
-
-============================================================
-
-Note ID: 1709245083939
-  Field: Text
-    Before:
-      Suppose \(S \in \mathcal{L}\left(\mathbf{R}^{n}\right)\) is an isometry and \(\Omega \subset \mathbf{R}^{n}\). Then<br><br><ul><li>\(\text { volume }\)\(S(\Omega)\) = \(\text { volume }\)\( \Omega \text {. }\)</li></ul><br>
-
-    After:
-      Suppose \(S \in \mathcal{L}\left(\mathbf{R}^{n}\right)\) is an isometry and \(\Omega \subset \mathbf{R}^{n}\). Then<br><br><ul><li>\(\text { volume }\)\(S(\Omega)\) = \(\text { volume }\)\( \Omega \text {. }\)</li></ul><br>
-
-============================================================
-
-Note ID: 1709245486942
-  Field: Text
-    Before:
-      Suppose \(T \in \mathcal{L}\left(\mathbf{R}^{n}\right)\) and \(\Omega \subset \mathbf{R}^{n}\). Then<br><ul><li>\(\text { volume }\) \(T(\Omega)\) = \(|\) \(\operatorname{det} T\) \(|\) \((\text { volume } \Omega) .\)<br></li></ul>
-
-    After:
-      Suppose \(T \in \mathcal{L}\left(\mathbf{R}^{n}\right)\) and \(\Omega \subset \mathbf{R}^{n}\). Then<br><ul><li>\(\text { volume }\) \(T(\Omega)\) = \(|\) \(\operatorname{det} T\) \(|\) \((\text { volume } \Omega) .\)<br></li></ul>
-
-============================================================
-
-Note ID: 1709276287009
-  Field: Text
-    Before:
-      Suppose \(\Omega\) is an open subset of \(\mathbf{R}^{n}\) and \(\sigma\) is a function from \(\Omega\) to \(\mathbf{R}^{n}\). We can write<br><br><ul><li>\(\sigma(x)\) = \(\left(\sigma_{1}(x), \ldots, \sigma_{n}(x)\right),\)</li></ul><br>where each \(\sigma_{j}\) is a function from \(\Omega\) to \(\mathbf{R}\). The partial derivative of \(\sigma_{j}\) with respect to the \(k^{\text {th } } \) coordinate is denoted \(D_{k} \sigma_{j}\). Evaluating this partial derivative at a point \(x \in \Omega\) gives \(D_{k} \sigma_{j}(x)\). If \(\sigma\) is differentiable at \(x\), then the matrix of \(\sigma^{\prime}(x)\) with respect to the standard basis of \(\mathbf{R}^{n}\) contains \(D_{k} \sigma_{j}(x)\) in row \(j\), column \(k\) (this is left as an exercise). In other words,<br><br>\(\mathcal{M}\left(\sigma^{\prime}(x)\right)\) = \(\left(\begin{array}{ccc}D_{1} \sigma_{1}(x) &amp; \ldots &amp; D_{n} \sigma_{1}(x) \\ \vdots &amp; &amp; \vdots \\ D_{1} \sigma_{n}(x) &amp; \ldots &amp; D_{n} \sigma_{n}(x)\end{array}\right)\).<br>
-
-    After:
-      Suppose \(\Omega\) is an open subset of \(\mathbf{R}^{n}\) and \(\sigma\) is a function from \(\Omega\) to \(\mathbf{R}^{n}\). We can write<br><br><ul><li>\(\sigma(x)\) = \(\left(\sigma_{1}(x), \ldots, \sigma_{n}(x)\right),\)</li></ul><br>where each \(\sigma_{j}\) is a function from \(\Omega\) to \(\mathbf{R}\). The partial derivative of \(\sigma_{j}\) with respect to the \(k^{\text {th } } \) coordinate is denoted \(D_{k} \sigma_{j}\). Evaluating this partial derivative at a point \(x \in \Omega\) gives \(D_{k} \sigma_{j}(x)\). If \(\sigma\) is differentiable at \(x\), then the matrix of \(\sigma^{\prime}(x)\) with respect to the standard basis of \(\mathbf{R}^{n}\) contains \(D_{k} \sigma_{j}(x)\) in row \(j\), column \(k\) (this is left as an exercise). In other words,<br><br>\(\mathcal{M}\left(\sigma^{\prime}(x)\right)\) = \(\left(\begin{array}{ccc}D_{1} \sigma_{1}(x) &amp; \ldots &amp; D_{n} \sigma_{1}(x) \\ \vdots &amp; &amp; \vdots \\ D_{1} \sigma_{n}(x) &amp; \ldots &amp; D_{n} \sigma_{n}(x)\end{array}\right)\).<br>
-
-============================================================
-
-Note ID: 1709276550489
-  Field: Text
-    Before:
-      <b>Change of variables in an integral</b><br><br>Suppose \(\Omega\) is an open subset of \(\mathbf{R}^{n}\) and \(\sigma: \Omega \rightarrow \mathbf{R}^{n}\) is differentiable at every point of \(\Omega\). If \(f\) is a real-valued function defined on \(\sigma(\Omega)\), then<br><ul><li>\(\int_{\sigma(\Omega)} f(y)\) \(d y\) = \(\int_{\Omega}\) \(f(\sigma(x))\) \(|\) \(\operatorname{det}\) \(\sigma^{\prime}(x)\) \(|\) \(d x\)<br></li></ul>
-
-    After:
-      <b>Change of variables in an integral</b><br><br>Suppose \(\Omega\) is an open subset of \(\mathbf{R}^{n}\) and \(\sigma: \Omega \rightarrow \mathbf{R}^{n}\) is differentiable at every point of \(\Omega\). If \(f\) is a real-valued function defined on \(\sigma(\Omega)\), then<br><ul><li>\(\int_{\sigma(\Omega)} f(y)\) \(d y\) = \(\int_{\Omega}\) \(f(\sigma(x))\) \(|\) \(\operatorname{det}\) \(\sigma^{\prime}(x)\) \(|\) \(d x\)<br></li></ul>
-
-============================================================
-
-Note ID: 1709277181425
-  Field: Text
-    Before:
-      7 Suppose \(A\) is an \(n\)-by- \(n\) matrix with real entries. Let \(S \in \mathcal{L}\left(\mathbf{C}^{n}\right)\) denote the operator on \(\mathbf{C}^{n}\) whose matrix equals \(A\), and let \(T \in \mathcal{L}\left(\mathbf{R}^{n}\right)\) denote the operator on \(\mathbf{R}^{n}\) whose matrix equals \(A\). <br><br>Prove that:<br><ul><li>&nbsp;\(\operatorname{trace}\) \(S\) = \(\operatorname{trace}\) \(T\)</li><li>\(\operatorname{det}\) \(S\)= \(\operatorname{det}\) \(T\).</li></ul>
-
-    After:
-      7 Suppose \(A\) is an \(n\)-by- \(n\) matrix with real entries. Let \(S \in \mathcal{L}\left(\mathbf{C}^{n}\right)\) denote the operator on \(\mathbf{C}^{n}\) whose matrix equals \(A\), and let \(T \in \mathcal{L}\left(\mathbf{R}^{n}\right)\) denote the operator on \(\mathbf{R}^{n}\) whose matrix equals \(A\). <br><br>Prove that:<br><ul><li>&nbsp;\(\operatorname{trace}\) \(S\) = \(\operatorname{trace}\) \(T\)</li><li>\(\operatorname{det}\) \(S\)= \(\operatorname{det}\) \(T\).</li></ul>
-
-============================================================
-
-Note ID: 1709369198956
-  Field: Text
-    Before:
-      For an integral to make sense:<br>Actually, \(\Omega\) in the definition needs to be a reasonable set (for example, open or measurable) and \(f\) needs to be a reasonable function (for example, continuous or measurable).
-
-    After:
-      For an integral to make sense:<br>Actually, \(\Omega\) in the definition needs to be a reasonable set (for example, open or measurable) and \(f\) needs to be a reasonable function (for example, continuous or measurable).
-
-============================================================
-
-Note ID: 1709377028987
-  Field: Text
-    Before:
-      Theorem 1.1.1. There is no rational number whose square is 2.<br><br>Proof:<br><br>And so assume, for contradiction, that there exist integers \(p\) and \(q\) satisfying<br><br>\(\left(\frac{p}{q}\right)^2\) = 2<br><br>We may also assume that \(p\) and \(q\) have no common factor, because, if they had one, we could simply cancel it out and rewrite the fraction in lowest terms. Now, equation (1) implies<br><br>\(p^2\) =&nbsp;\(2 q^2\)<br><br>From this, we can see that the integer \(p^{2}\) is an even number (it is divisible by 2 ), and hence \(p\) must be even as well because the square of an odd number is odd. This allows us to write \(p\) = \(2 r\), where \(r\) is also an integer. If we substitute \(2 r\) for \(p\) in equation (2), then a little algebra yields the relationship<br><br>\(2 r^{2}\) = \(q^{2}\)<br>But now the absurdity is at hand. This last equation implies that \(q^{2}\) is even, and hence \(q\) must also be even. Thus, we have shown that \(p\) and \(q\) are both even&nbsp; when they were originally assumed to have no common factor.&nbsp;
-
-    After:
-      Theorem 1.1.1. There is no rational number whose square is 2.<br><br>Proof:<br><br>And so assume, for contradiction, that there exist integers \(p\) and \(q\) satisfying<br><br>\(\left(\frac{p}{q}\right)^2\) = 2<br><br>We may also assume that \(p\) and \(q\) have no common factor, because, if they had one, we could simply cancel it out and rewrite the fraction in lowest terms. Now, equation (1) implies<br><br>\(p^2\) =&nbsp;\(2 q^2\)<br><br>From this, we can see that the integer \(p^{2}\) is an even number (it is divisible by 2 ), and hence \(p\) must be even as well because the square of an odd number is odd. This allows us to write \(p\) = \(2 r\), where \(r\) is also an integer. If we substitute \(2 r\) for \(p\) in equation (2), then a little algebra yields the relationship<br><br>\(2 r^{2}\) = \(q^{2}\)<br>But now the absurdity is at hand. This last equation implies that \(q^{2}\) is even, and hence \(q\) must also be even. Thus, we have shown that \(p\) and \(q\) are both even&nbsp; when they were originally assumed to have no common factor.&nbsp;
-
-============================================================
-
-Note ID: 1709377429997
-  Field: Text
-    Before:
-      The next issue after addtion (what the integers get us) is multiplication and division. The number 1 acts as the multiplicative identity, but in order to define division we need to have multiplicative inverses. Thus, we extend our system again to the rational numbers<br><br><ul><li>\(\mathbf{Q}\) = <ul><li>\(\left\{\text { all fractions } \frac{p}{q} \text { where } p \text { and } q \text { are integers with } q \neq 0\right\}\)</li></ul></li></ul>
-
-    After:
-      The next issue after addtion (what the integers get us) is multiplication and division. The number 1 acts as the multiplicative identity, but in order to define division we need to have multiplicative inverses. Thus, we extend our system again to the rational numbers<br><br><ul><li>\(\mathbf{Q}\) = <ul><li>\(\left\{\text { all fractions } \frac{p}{q} \text { where } p \text { and } q \text { are integers with } q \neq 0\right\}\)</li></ul></li></ul>
-
-============================================================
-
-Note ID: 1709377526864
-  Field: Text
-    Before:
-      Taken together, the properties of Q:<br><ul><li>Closed under addtion</li><li>Addivite identiy</li><li>Addivitve inverse</li><li>Closed under multiplication</li><li>Multiplicative identity</li><li>Multiplicative inverse</li></ul><div>make Q into a field</div>
-
-    After:
-      Taken together, the properties of Q:<br><ul><li>Closed under addtion</li><li>Addivite identiy</li><li>Addivitve inverse</li><li>Closed under multiplication</li><li>Multiplicative identity</li><li>Multiplicative inverse</li></ul><div>make Q into a field</div>
-
-============================================================
-
-Note ID: 1709378592558
-  Field: Text
-    Before:
-      Intuitively speaking, a set is any collection of objects. These objects are referred to as the elements of the set. For our purposes, the sets in question will most often be sets of real numbers, although we will also encounter sets of functions and, on a few occasions, sets whose elements are other sets.
-
-    After:
-      Intuitively speaking, a set is any collection of objects. These objects are referred to as the elements of the set. For our purposes, the sets in question will most often be sets of real numbers, although we will also encounter sets of functions and, on a few occasions, sets whose elements are other sets.
-
-============================================================
-
-Note ID: 1709378809948
-  Field: Text
-    Before:
-      The set \(\emptyset\) is called the empty set and is understood to be the set that contains no elements. An equivalent statement would be to say that two sets whose intersection is&nbsp; \(\emptyset\)&nbsp; are disjoint.
-
-    After:
-      The set \(\emptyset\) is called the empty set and is understood to be the set that contains no elements. An equivalent statement would be to say that two sets whose intersection is&nbsp; \(\emptyset\)&nbsp; are disjoint.
-
-============================================================
-
-Note ID: 1709378929963
-  Field: Text
-    Before:
-      <ul><li>The inclusion relationship \(A \subseteq B\) or \(B \supseteq A\) is used to indicate that every element of \(A\) is also an element of \(B\).&nbsp;<br></li><li>In this case, we say \(A\) is a subset of \(B\), or \(B\) contains \(A\). To assert that \(A=B\) means that \(A \subseteq B\) and \(B \subseteq A\).&nbsp;</li><li>Put another way, \(A\) and \(B\) have exactly the same elements.</li></ul>
-
-    After:
-      <ul><li>The inclusion relationship \(A \subseteq B\) or \(B \supseteq A\) is used to indicate that every element of \(A\) is also an element of \(B\).&nbsp;<br></li><li>In this case, we say \(A\) is a subset of \(B\), or \(B\) contains \(A\). To assert that \(A=B\) means that \(A \subseteq B\) and \(B \subseteq A\).&nbsp;</li><li>Put another way, \(A\) and \(B\) have exactly the same elements.</li></ul>
-
-============================================================
-
-Note ID: 1709379261842
-  Field: Text
-    Before:
-      Given \(A \subseteq \mathbf{R}\), the complement of \(A\), written \(A^{c}\), refers to the set of all elements of \(\mathbf{R}\) not in \(A\). Thus, for \(A \subseteq \mathbf{R}\),<br><br><ul><li>\(A^{c}\) = \(\{x \in \mathbf{R}: x \notin A\} .\)</li></ul>
-
-    After:
-      Given \(A \subseteq \mathbf{R}\), the complement of \(A\), written \(A^{c}\), refers to the set of all elements of \(\mathbf{R}\) not in \(A\). Thus, for \(A \subseteq \mathbf{R}\),<br><br><ul><li>\(A^{c}\) = \(\{x \in \mathbf{R}: x \notin A\} .\)</li></ul>
-
-============================================================
-
-Note ID: 1709379813335
-  Field: Text
-    Before:
-      &nbsp;The absolute value function is so important that it merits the special notation \(|x|\) in place of the usual \(f(x)\) or \(g(x)\). It is defined for every real number via the piecewise definition<br><br><ul><li>\(|x|\) =&nbsp; \(\begin{cases}x &amp; \text { if } x \geq 0 \\ -x &amp; \text { if } x&lt;0\end{cases}\)</li></ul><br>With respect to multiplication and division, the absolute value function satisfies<br><br><ul><li>(i) \(|a b|\) = \(|a||b|\) and</li><li>(ii) \(|a+b|\)&nbsp; \(\leq\) \(|a|+|b|\)</li></ul><br>for all choices of \(a\) and \(b\).&nbsp;<br>
-
-    After:
-      &nbsp;The absolute value function is so important that it merits the special notation \(|x|\) in place of the usual \(f(x)\) or \(g(x)\). It is defined for every real number via the piecewise definition<br><br><ul><li>\(|x|\) =&nbsp; \(\begin{cases}x &amp; \text { if } x \geq 0 \\ -x &amp; \text { if } x&lt;0\end{cases}\)</li></ul><br>With respect to multiplication and division, the absolute value function satisfies<br><br><ul><li>(i) \(|a b|\) = \(|a||b|\) and</li><li>(ii) \(|a+b|\)&nbsp; \(\leq\) \(|a|+|b|\)</li></ul><br>for all choices of \(a\) and \(b\).&nbsp;<br>
-
-============================================================
-
-Note ID: 1709380980907
-  Field: Text
-    Before:
-      <b>Theorem 1.2.6. </b>Two real numbers \(a\) and \(b\) are equal if and only if for every real number \(\epsilon&gt;0\) it follows that \(|a-b|&lt;\epsilon\).<br><br><ul><li>For the second statement, we give a proof by contradiction. The conclusion of the proposition in this direction states that \(a=b\), so we assume that \(a \neq b\).&nbsp;</li><li>Heading off in search of a contradiction brings us to a consideration of the phrase "for every \(\epsilon&gt;0\)." Some equivalent ways to state the hypothesis would be to say that "for all possible choices of \(\epsilon&gt;0\) " or "no matter how \(\epsilon&gt;0\) is selected, it is always the case that \(|a-b|&lt;\epsilon\)."&nbsp;</li><li>But assuming \(a \neq b\) (as we are doing at the moment), the choice of</li><ul><li>\(\epsilon_{0}\) = \(|a-b|&gt;0\)</li><li>poses a serious problem.&nbsp;</li></ul><li>We are assuming that \(|a-b|&lt;\epsilon\) is true for every \(\epsilon&gt;0\), so this must certainly be true of the particular \(\epsilon_{0}\) just defined.&nbsp;</li><li>However, the statements:</li><ul><li>\(|a-b|\) \(&lt;\) \(\epsilon_{0}\)&nbsp;</li><li>\(|a-b|\) = \(\epsilon_{0}\)</li></ul><li>cannot both be true.&nbsp;</li><li>This contradiction means that our initial assumption that \(a \neq b\) is unacceptable.&nbsp;</li><li>Therefore, \(a=b\), and the indirect proof is complete.<br></li></ul>
-
-    After:
-      <b>Theorem 1.2.6. </b>Two real numbers \(a\) and \(b\) are equal if and only if for every real number \(\epsilon&gt;0\) it follows that \(|a-b|&lt;\epsilon\).<br><br><ul><li>For the second statement, we give a proof by contradiction. The conclusion of the proposition in this direction states that \(a=b\), so we assume that \(a \neq b\).&nbsp;</li><li>Heading off in search of a contradiction brings us to a consideration of the phrase "for every \(\epsilon&gt;0\)." Some equivalent ways to state the hypothesis would be to say that "for all possible choices of \(\epsilon&gt;0\) " or "no matter how \(\epsilon&gt;0\) is selected, it is always the case that \(|a-b|&lt;\epsilon\)."&nbsp;</li><li>But assuming \(a \neq b\) (as we are doing at the moment), the choice of</li><ul><li>\(\epsilon_{0}\) = \(|a-b|&gt;0\)</li><li>poses a serious problem.&nbsp;</li></ul><li>We are assuming that \(|a-b|&lt;\epsilon\) is true for every \(\epsilon&gt;0\), so this must certainly be true of the particular \(\epsilon_{0}\) just defined.&nbsp;</li><li>However, the statements:</li><ul><li>\(|a-b|\) \(&lt;\) \(\epsilon_{0}\)&nbsp;</li><li>\(|a-b|\) = \(\epsilon_{0}\)</li></ul><li>cannot both be true.&nbsp;</li><li>This contradiction means that our initial assumption that \(a \neq b\) is unacceptable.&nbsp;</li><li>Therefore, \(a=b\), and the indirect proof is complete.<br></li></ul>
-
-============================================================
-
-Note ID: 1709381303587
-  Field: Text
-    Before:
-      Exercise 1.2.5 (De Morgan's Laws). Let \(A\) and \(B\) be subsets of \(\mathbf{R}\).<br><br>(a) If \(x\)&nbsp; \(\in\) \((A \cap B)^{c}\), explain why \(x\) \(\in\) \(A^{c} \cup B^{c}\). This shows that \((A \cap B)^{c}\) \(\subseteq\) \(A^{c} \cup B^{c}\).
-
-    After:
-      Exercise 1.2.5 (De Morgan's Laws). Let \(A\) and \(B\) be subsets of \(\mathbf{R}\).<br><br>(a) If \(x\)&nbsp; \(\in\) \((A \cap B)^{c}\), explain why \(x\) \(\in\) \(A^{c} \cup B^{c}\). This shows that \((A \cap B)^{c}\) \(\subseteq\) \(A^{c} \cup B^{c}\).
-
-============================================================
-
-Note ID: 1709381359204
-  Field: Text
-    Before:
-      <br>(b) Prove the reverse inclusion \((A \cap B)^{c}\) \(\supseteq\) \(A^{c} \cup B^{c}\), and conclude that \((A \cap B)^{c}\) = \(A^{c} \cup B^{c}\).
-
-    After:
-      <br>(b) Prove the reverse inclusion \((A \cap B)^{c}\) \(\supseteq\) \(A^{c} \cup B^{c}\), and conclude that \((A \cap B)^{c}\) = \(A^{c} \cup B^{c}\).
-
-============================================================
-
-Note ID: 1709381535816
-  Field: Text
-    Before:
-      Exercise 1.2.6. (a) Verify the triangle inequality in the special case where \(a\) and \(b\) have the same sign.<br><br><ul><li>(b) Find an efficient proof for all the cases at once by first demonstrating \((a+b)^{2}\) \( \leq\) \((|a|+|b|)^{2}\).</li><li>(c) Prove \(|a-b|\)&nbsp; \(\leq\) \(|a-c|\)+\(|c-d|\) + \(|d-b|\) for all \(a, b\), \(c\), and \(d\).</li><li>(d) Prove ||\(a|-| b||\) \(\leq\) \(|a-b|\).</li></ul>
-
-    After:
-      Exercise 1.2.6. (a) Verify the triangle inequality in the special case where \(a\) and \(b\) have the same sign.<br><br><ul><li>(b) Find an efficient proof for all the cases at once by first demonstrating \((a+b)^{2}\) \( \leq\) \((|a|+|b|)^{2}\).</li><li>(c) Prove \(|a-b|\)&nbsp; \(\leq\) \(|a-c|\)+\(|c-d|\) + \(|d-b|\) for all \(a, b\), \(c\), and \(d\).</li><li>(d) Prove ||\(a|-| b||\) \(\leq\) \(|a-b|\).</li></ul>
-
-============================================================
-
-Note ID: 1709382149715
-  Field: Text
-    Before:
-      Exercise 1.2.9. Given a function \(f: D \rightarrow \mathbf{R}\) and a subset \(B \subseteq \mathbf{R}\):<br><ul><li>Let \(f^{-1}(B)\) be the set of all points from the domain \(D\) that get mapped into \(B\)</li><li>That is, \(f^{-1}(B)\) = \(\{x \in D: f(x) \in B\}\). This set is called the preimage of \(B\).</li></ul>
-
-    After:
-      Exercise 1.2.9. Given a function \(f: D \rightarrow \mathbf{R}\) and a subset \(B \subseteq \mathbf{R}\):<br><ul><li>Let \(f^{-1}(B)\) be the set of all points from the domain \(D\) that get mapped into \(B\)</li><li>That is, \(f^{-1}(B)\) = \(\{x \in D: f(x) \in B\}\). This set is called the preimage of \(B\).</li></ul>
-
-============================================================
-
-Note ID: 1709384732387
-  Field: Text
-    Before:
-      (a) Show how induction can be used to conclude that<br><br><ul><li>\((\) \(A_{1} \cup A_{2} \cup \cdots \cup A_{n}\) \()^{c}\) = \(A_{1}^{c}\) \(\cap\) \(A_{2}^{c}\) \(\cap\) \(\cdots\) \(\cap A_{n}^{c}\)</li></ul><br>for any finite \(n \in \mathbf{N}\).<br>
-
-    After:
-      (a) Show how induction can be used to conclude that<br><br><ul><li>\((\) \(A_{1} \cup A_{2} \cup \cdots \cup A_{n}\) \()^{c}\) = \(A_{1}^{c}\) \(\cap\) \(A_{2}^{c}\) \(\cap\) \(\cdots\) \(\cap A_{n}^{c}\)</li></ul><br>for any finite \(n \in \mathbf{N}\).<br>
-
-============================================================
-
-Note ID: 1709385325335
-  Field: Text
-    Before:
-      <b>An Initial Definition for \(\mathrm{R}\)<br></b><br><ul><li>First, \(\mathbf{R}\) is a set containing \(\mathbf{Q}\).&nbsp;</li><li>The operations of addition and multiplication on \(\mathbf{Q}\) extend to all of \(\mathbf{R}\) in such a way that every element of \(\mathbf{R}\) has an additive inverse and every nonzero element of \(\mathbf{R}\) has a multiplicative inverse. Echoing the discussion in Section 1.1, we assume \(\mathbf{R}\) is a field, meaning that addition and multiplication of real numbers are commutative, associative, and the distributive property holds.&nbsp;</li><li>We also assume that the familiar properties of the ordering on \(\mathbf{Q}\) extend to all of \(\mathbf{R}\).&nbsp;</li><li>To summarize the situation in the official terminology&nbsp;of the subject, we assume that \(\mathbf{R}\) is an ordered field, which contains \(\mathbf{Q}\) as a subfield</li><li>This brings us to the final, and most distinctive, assumption about the real number system. We must find some way to clearly articulate what we mean by insisting that \(\mathbf{R}\) does not contain the gaps that permeate \(\mathbf{Q}\). Because this is the defining difference between the rational numbers and the real numbers, we will be excessively precise about how we phrase this assumption, hereafter referred to as the <b>Axiom of&nbsp;</b>Completeness.</li><ul><li>Every nonempty set of real numbers that is bounded above has a least upper bound.</li></ul></ul>
-
-    After:
-      <b>An Initial Definition for \(\mathrm{R}\)<br></b><br><ul><li>First, \(\mathbf{R}\) is a set containing \(\mathbf{Q}\).&nbsp;</li><li>The operations of addition and multiplication on \(\mathbf{Q}\) extend to all of \(\mathbf{R}\) in such a way that every element of \(\mathbf{R}\) has an additive inverse and every nonzero element of \(\mathbf{R}\) has a multiplicative inverse. Echoing the discussion in Section 1.1, we assume \(\mathbf{R}\) is a field, meaning that addition and multiplication of real numbers are commutative, associative, and the distributive property holds.&nbsp;</li><li>We also assume that the familiar properties of the ordering on \(\mathbf{Q}\) extend to all of \(\mathbf{R}\).&nbsp;</li><li>To summarize the situation in the official terminology&nbsp;of the subject, we assume that \(\mathbf{R}\) is an ordered field, which contains \(\mathbf{Q}\) as a subfield</li><li>This brings us to the final, and most distinctive, assumption about the real number system. We must find some way to clearly articulate what we mean by insisting that \(\mathbf{R}\) does not contain the gaps that permeate \(\mathbf{Q}\). Because this is the defining difference between the rational numbers and the real numbers, we will be excessively precise about how we phrase this assumption, hereafter referred to as the <b>Axiom of&nbsp;</b>Completeness.</li><ul><li>Every nonempty set of real numbers that is bounded above has a least upper bound.</li></ul></ul>
-
-============================================================
-
-Note ID: 1709385650858
-  Field: Text
-    Before:
-      <b>Definition 1.3.1.</b>&nbsp;<br><ul><li>A set \(A \subseteq \mathbf{R}\) is bounded above if there exists a number \(b \in \mathbf{R}\) such that \(a\) \(\leq\) \(b\) for all \(a \in A\). The number \(b\) is called an upper bound for \(A\).</li><li>Similarly, the set \(A\) is bounded below if there exists a lower bound \(l \in \mathbf{R}\) satisfying \(l\) \(\leq\) \(a\) for every \(a \in A\).</li></ul>
-
-    After:
-      <b>Definition 1.3.1.</b>&nbsp;<br><ul><li>A set \(A \subseteq \mathbf{R}\) is bounded above if there exists a number \(b \in \mathbf{R}\) such that \(a\) \(\leq\) \(b\) for all \(a \in A\). The number \(b\) is called an upper bound for \(A\).</li><li>Similarly, the set \(A\) is bounded below if there exists a lower bound \(l \in \mathbf{R}\) satisfying \(l\) \(\leq\) \(a\) for every \(a \in A\).</li></ul>
-
-============================================================
-
-Note ID: 1709385727199
-  Field: Text
-    Before:
-      Definition 1.3.1. A set \(A \subseteq \mathbf{R}\) is bounded above if there exists a number \(b\) \(\in\) \(\mathbf{R}\) such that \(a\)&nbsp;&nbsp;\(\leq\) \(b\) for all \(a \in A\). The number \(b\) is called an upper bound for \(A\).
-
-    After:
-      Definition 1.3.1. A set \(A \subseteq \mathbf{R}\) is bounded above if there exists a number \(b\) \(\in\) \(\mathbf{R}\) such that \(a\)&nbsp;&nbsp;\(\leq\) \(b\) for all \(a \in A\). The number \(b\) is called an upper bound for \(A\).
-
-============================================================
-
-Note ID: 1709386890930
-  Field: Text
-    Before:
-      <b>Definition 1.3.2.<br></b><br><ul><li>A real number \(s\) is the least upper bound for a set \(A \subseteq \mathbf{R}\) if it meets the following two criteria:</li><ul><li>(i) \(s\) is an upper bound for \(A\);</li><li>(ii) if \(b\) is any upper bound for \(A\), then \(s\) \(\leq\) \(b\).</li></ul><li>The least upper bound is also frequently called the supremum of the set \(A\).&nbsp;</li><ul><li>Although the notation \(s\) = \(\operatorname{lub} A\) is sometimes used, we will always write \(s=\) \(\sup A\) for the least upper bound.</li></ul></ul>
-
-    After:
-      <b>Definition 1.3.2.<br></b><br><ul><li>A real number \(s\) is the least upper bound for a set \(A \subseteq \mathbf{R}\) if it meets the following two criteria:</li><ul><li>(i) \(s\) is an upper bound for \(A\);</li><li>(ii) if \(b\) is any upper bound for \(A\), then \(s\) \(\leq\) \(b\).</li></ul><li>The least upper bound is also frequently called the supremum of the set \(A\).&nbsp;</li><ul><li>Although the notation \(s\) = \(\operatorname{lub} A\) is sometimes used, we will always write \(s=\) \(\sup A\) for the least upper bound.</li></ul></ul>
-
-============================================================
-
-Note ID: 1709387099580
-  Field: Text
-    Before:
-      <ul><li>Definition 1.3.2. A real number \(s\) is the least upper bound for a set \(A \subseteq \mathbf{R}\) if it meets the following two criteria:</li><li>(i) \(s\) is an upper bound for \(A\);</li><li>(ii) if \(b\) is any upper bound for \(A\), then \(s \leq b\).</li></ul><br>Although a set can have a host of upper bounds, it can have only one least upper bound. If \(s_{1}\) and \(s_{2}\) are both least upper bounds for a set \(A\), then by property (ii) in Definition 1.3 .2 we can assert \(s_{1}\) \(\leq\) \(s_{2}\) and \(s_{2}\) \(\leq\) \(s_{1}\). The conclusion is that \(s_{1}=s_{2}\) and least upper bounds are unique.<br>
-
-    After:
-      <ul><li>Definition 1.3.2. A real number \(s\) is the least upper bound for a set \(A \subseteq \mathbf{R}\) if it meets the following two criteria:</li><li>(i) \(s\) is an upper bound for \(A\);</li><li>(ii) if \(b\) is any upper bound for \(A\), then \(s \leq b\).</li></ul><br>Although a set can have a host of upper bounds, it can have only one least upper bound. If \(s_{1}\) and \(s_{2}\) are both least upper bounds for a set \(A\), then by property (ii) in Definition 1.3 .2 we can assert \(s_{1}\) \(\leq\) \(s_{2}\) and \(s_{2}\) \(\leq\) \(s_{1}\). The conclusion is that \(s_{1}=s_{2}\) and least upper bounds are unique.<br>
-
-============================================================
-
-Note ID: 1709388407297
-  Field: Text
-    Before:
-      <b>Definition 1.3.4.</b><br><ul><li>&nbsp;A real number \(a_{0}\) is a maximum of the set \(A\) if \(a_{0}\) is an element of \(A\) and \(a_{0}\) \(\geq\) \(a\) for all \(a \in A\).&nbsp;</li><li>Similarly, a number \(a_{1}\) is a minimum of \(A\) if \(a_{1}\) \(\in\) \(A\) and \(a_{1}\) \(\leq\) \(a\) for every \(a \in A\).</li></ul>
-
-    After:
-      <b>Definition 1.3.4.</b><br><ul><li>&nbsp;A real number \(a_{0}\) is a maximum of the set \(A\) if \(a_{0}\) is an element of \(A\) and \(a_{0}\) \(\geq\) \(a\) for all \(a \in A\).&nbsp;</li><li>Similarly, a number \(a_{1}\) is a minimum of \(A\) if \(a_{1}\) \(\in\) \(A\) and \(a_{1}\) \(\leq\) \(a\) for every \(a \in A\).</li></ul>
-
-============================================================
-
-Note ID: 1709388499675
-  Field: Text
-    Before:
-      Example 1.3.5. To belabor the point, consider the open interval<br><br>\[<br>(0,2)=\{x \in \mathbf{R}: 0&lt;x&lt;2\},<br>\]<br><br>and the closed interval<br><br>\[<br>[0,2]=\{x \in \mathbf{R}: 0 \leq x \leq 2\} .<br>\]<br><br>Both sets are bounded above (and below), and both have the same least upper bound, namely 2 . It is not the case, however, that both sets have a maximum. A maximum is a specific type of upper bound that is required to be an element of the set in question, and the open interval \((0,2)\) does not possess such an element. Thus, the supremum can exist and not be a maximum, but when a maximum exists, then it is also the supremum.
-
-    After:
-      Example 1.3.5. To belabor the point, consider the open interval<br><br>\[<br>(0,2)=\{x \in \mathbf{R}: 0&lt;x&lt;2\},<br>\]<br><br>and the closed interval<br><br>\[<br>[0,2]=\{x \in \mathbf{R}: 0 \leq x \leq 2\} .<br>\]<br><br>Both sets are bounded above (and below), and both have the same least upper bound, namely 2 . It is not the case, however, that both sets have a maximum. A maximum is a specific type of upper bound that is required to be an element of the set in question, and the open interval \((0,2)\) does not possess such an element. Thus, the supremum can exist and not be a maximum, but when a maximum exists, then it is also the supremum.
-
-============================================================
-
-Note ID: 1709388868517
-  Field: Text
-    Before:
-      Example 1.3.6. Consider again the set<br><br>\[<br>S=\left\{r \in \mathbf{Q}: r^{2}&lt;2\right\},<br>\]<br><br><ul><li>and pretend for the moment that our world consists only of rational numbers.&nbsp;</li><li>The set \(S\) is certainly bounded above. Taking \(b=2\) works, as does \(b=3 / 2\). But notice what happens as we go in search of the least upper bound. (It may be useful here to know that the decimal expansion for \(\sqrt{2}\) begins \(1.4142 \ldots\). .)&nbsp;</li><li>We might try \(b=142 / 100\), which is indeed an upper bound, but then we discover that \(b=1415 / 1000\) is an upper bound that is smaller still. Is there a smallest one?</li><li>In the rational numbers, there is not. In the real numbers, there is.&nbsp;</li><li>Back in \(\mathbf{R}\), the Axiom of Completeness states that we may set \(\alpha=\sup S\) and be confident that such a number exists.&nbsp;</li><li>In the next section, we will prove that \(\alpha^{2}=2\). But according to Theorem 1.1.1, this implies \(\alpha\) is not a rational number.&nbsp;</li><li>If we are restricting our attention to only rational numbers, then \(\alpha\) is not an allowable option for \(\sup S\), and the search for a least upper bound goes on indefinitely.&nbsp;</li><li>Whatever rational upper bound is discovered, it is always possible to find one smaller.</li></ul>
-
-    After:
-      Example 1.3.6. Consider again the set<br><br>\[<br>S=\left\{r \in \mathbf{Q}: r^{2}&lt;2\right\},<br>\]<br><br><ul><li>and pretend for the moment that our world consists only of rational numbers.&nbsp;</li><li>The set \(S\) is certainly bounded above. Taking \(b=2\) works, as does \(b=3 / 2\). But notice what happens as we go in search of the least upper bound. (It may be useful here to know that the decimal expansion for \(\sqrt{2}\) begins \(1.4142 \ldots\). .)&nbsp;</li><li>We might try \(b=142 / 100\), which is indeed an upper bound, but then we discover that \(b=1415 / 1000\) is an upper bound that is smaller still. Is there a smallest one?</li><li>In the rational numbers, there is not. In the real numbers, there is.&nbsp;</li><li>Back in \(\mathbf{R}\), the Axiom of Completeness states that we may set \(\alpha=\sup S\) and be confident that such a number exists.&nbsp;</li><li>In the next section, we will prove that \(\alpha^{2}=2\). But according to Theorem 1.1.1, this implies \(\alpha\) is not a rational number.&nbsp;</li><li>If we are restricting our attention to only rational numbers, then \(\alpha\) is not an allowable option for \(\sup S\), and the search for a least upper bound goes on indefinitely.&nbsp;</li><li>Whatever rational upper bound is discovered, it is always possible to find one smaller.</li></ul>
-
-============================================================
-
-Note ID: 1709388987297
-  Field: Text
-    Before:
-      Example 1.3.7. Let \(A \subseteq \mathbf{R}\) be nonempty and bounded above, and let \(c \in \mathbf{R}\). Define the set \(c+A\) by<br><br><ul><li>\(c+A\) = \(\{c+a: a \in A\} .\)</li><li>Then \(\sup\) \((c+A)\) = \(c+\sup A\).</li></ul>
-
-    After:
-      Example 1.3.7. Let \(A \subseteq \mathbf{R}\) be nonempty and bounded above, and let \(c \in \mathbf{R}\). Define the set \(c+A\) by<br><br><ul><li>\(c+A\) = \(\{c+a: a \in A\} .\)</li><li>Then \(\sup\) \((c+A)\) = \(c+\sup A\).</li></ul>
-
-============================================================
-
-Note ID: 1709389479039
-  Field: Text
-    Before:
-      It is certainly the case that all of our conclusions to this point about least upper bounds have analogous versions for greatest lower bounds. The Axiom of Completeness does not explicitly assert that a nonempty set bounded below has an infimum, but this is because we do not need to assume this fact as part of the axiom. Using the Axiom of Completeness, there are several ways to prove that greatest lower bounds exist for nonempty bounded sets.&nbsp;
-
-    After:
-      It is certainly the case that all of our conclusions to this point about least upper bounds have analogous versions for greatest lower bounds. The Axiom of Completeness does not explicitly assert that a nonempty set bounded below has an infimum, but this is because we do not need to assume this fact as part of the axiom. Using the Axiom of Completeness, there are several ways to prove that greatest lower bounds exist for nonempty bounded sets.&nbsp;
-
-============================================================
-
-Note ID: 1709389717900
-  Field: Text
-    Before:
-      <b>Definition 1.3.2</b>. A real number \(i\) is the greatest lower bound or infimum for a set \(A\) \(\subseteq\) \( \mathbf{R}\) if it meets the following two criteria:<ul><li>(i) \(i\) is a lower bound for \(A\);</li><li>(ii) if \(b\) is any lower bound for \(A\), then \(i\) \(\geq\) \(b\).</li></ul>
-
-    After:
-      <b>Definition 1.3.2</b>. A real number \(i\) is the greatest lower bound or infimum for a set \(A\) \(\subseteq\) \( \mathbf{R}\) if it meets the following two criteria:<ul><li>(i) \(i\) is a lower bound for \(A\);</li><li>(ii) if \(b\) is any lower bound for \(A\), then \(i\) \(\geq\) \(b\).</li></ul>
-
-============================================================
-
-Note ID: 1709393724882
-  Field: Text
-    Before:
-      Exercise 1.3.9. <br><ul><li>(a) If \(\sup A\) &lt; \(\sup B\), show that there exists an element \(b \in B\) that is an upper bound for \(A\).<br></li><li>(b) Give an example to show that this is not always the case if we only assume \(\sup A\) \(\leq\) \(\sup B\).</li></ul>
-
-    After:
-      Exercise 1.3.9. <br><ul><li>(a) If \(\sup A\) &lt; \(\sup B\), show that there exists an element \(b \in B\) that is an upper bound for \(A\).<br></li><li>(b) Give an example to show that this is not always the case if we only assume \(\sup A\) \(\leq\) \(\sup B\).</li></ul>
-
-============================================================
-
-Note ID: 1709394068033
-  Field: Text
-    Before:
-      Exercise 1.3.10<br><br>The Cut Property of the real numbers is the following:<ul><li>If \(A\) and \(B\) are nonempty, disjoint sets with \(A \cup B\) = \(\mathbf{R}\) and \(a\) &lt; \(b\) for all \(a \in A\) and \(b \in B\), then there exists \(c\)&nbsp; \(\in\) \(\mathbf{R}\) such that \(x\) \(\leq\) \(c\) whenever \(x\) \(\in\) \(A\) and \(x\) \(\geq\) \(c\) whenever \(x\) \(\in\) \(B\).</li></ul>
-
-    After:
-      Exercise 1.3.10<br><br>The Cut Property of the real numbers is the following:<ul><li>If \(A\) and \(B\) are nonempty, disjoint sets with \(A \cup B\) = \(\mathbf{R}\) and \(a\) &lt; \(b\) for all \(a \in A\) and \(b \in B\), then there exists \(c\)&nbsp; \(\in\) \(\mathbf{R}\) such that \(x\) \(\leq\) \(c\) whenever \(x\) \(\in\) \(A\) and \(x\) \(\geq\) \(c\) whenever \(x\) \(\in\) \(B\).</li></ul>
-
-============================================================
-
-Note ID: 1709394153090
-  Field: Text
-    Before:
-      Exercise 1.3.10 (Cut Property). The Cut Property of the real numbers is the following:<br><br>If \(A\) and \(B\) are nonempty, disjoint sets with \(A \cup B=\mathbf{R}\) and \(a&lt;b\) for all \(a \in A\) and \(b \in B\), then there exists \(c \in \mathbf{R}\) such that \(x \leq c\) whenever \(x \in A\) and \(x \geq c\) whenever \(x \in B\).<br><br><ul><li>(a) Use the Axiom of Completeness to prove the Cut Property.</li><li>(b) Show that the implication goes the other way; that is, assume \(\mathbf{R}\) possesses the Cut Property and let \(E\) be a nonempty set that is bounded above. Prove \(\sup E\) exists.</li><li>(c) The punchline of parts (a) and (b) is that the Cut Property could be used in place of the Axiom of Completeness as the fundamental axiom that distinguishes the real numbers from the rational numbers. To drive this point home, give a concrete example showing that the Cut Property is not a valid statement when \(\mathbf{R}\) is replaced by \(\mathbf{Q}\).</li></ul>
-
-    After:
-      Exercise 1.3.10 (Cut Property). The Cut Property of the real numbers is the following:<br><br>If \(A\) and \(B\) are nonempty, disjoint sets with \(A \cup B=\mathbf{R}\) and \(a&lt;b\) for all \(a \in A\) and \(b \in B\), then there exists \(c \in \mathbf{R}\) such that \(x \leq c\) whenever \(x \in A\) and \(x \geq c\) whenever \(x \in B\).<br><br><ul><li>(a) Use the Axiom of Completeness to prove the Cut Property.</li><li>(b) Show that the implication goes the other way; that is, assume \(\mathbf{R}\) possesses the Cut Property and let \(E\) be a nonempty set that is bounded above. Prove \(\sup E\) exists.</li><li>(c) The punchline of parts (a) and (b) is that the Cut Property could be used in place of the Axiom of Completeness as the fundamental axiom that distinguishes the real numbers from the rational numbers. To drive this point home, give a concrete example showing that the Cut Property is not a valid statement when \(\mathbf{R}\) is replaced by \(\mathbf{Q}\).</li></ul>
-
-============================================================
-
-Note ID: 1709394528937
-  Field: Text
-    Before:
-      Theorem 1.4.1 (Nested Interval<b>&nbsp;Property</b>). <br><br><ul><li>For each \(n \in \mathbf{N}\), assume we are given a closed interval \(I_{n}\) = \(\left[a_{n}, b_{n}\right] \) = \(\left\{x \in \mathbf{R}: a_{n} \leq x \leq b_{n}\right\}\).&nbsp;</li><li>Assume also that each \(I_{n}\) contains \(I_{n+1}\).&nbsp;</li><li>Then, the resulting nested sequence of closed intervals</li><ul><li>\(I_{1}\) \(\supseteq\) \(I_{2}\) \(\supseteq\) \(I_{3}\) \(\supseteq\) \(I_{4}\) \(\supseteq\) \(\cdots\)</li></ul><li>has a nonempty intersection; that is, \(\bigcap_{n=1}^{\infty}\) \(I_{n}\)&nbsp; \(\neq\) \(\emptyset\).</li></ul>
-
-    After:
-      Theorem 1.4.1 (Nested Interval<b>&nbsp;Property</b>). <br><br><ul><li>For each \(n \in \mathbf{N}\), assume we are given a closed interval \(I_{n}\) = \(\left[a_{n}, b_{n}\right] \) = \(\left\{x \in \mathbf{R}: a_{n} \leq x \leq b_{n}\right\}\).&nbsp;</li><li>Assume also that each \(I_{n}\) contains \(I_{n+1}\).&nbsp;</li><li>Then, the resulting nested sequence of closed intervals</li><ul><li>\(I_{1}\) \(\supseteq\) \(I_{2}\) \(\supseteq\) \(I_{3}\) \(\supseteq\) \(I_{4}\) \(\supseteq\) \(\cdots\)</li></ul><li>has a nonempty intersection; that is, \(\bigcap_{n=1}^{\infty}\) \(I_{n}\)&nbsp; \(\neq\) \(\emptyset\).</li></ul>
-
-============================================================
-
-Note ID: 1709394909450
-  Field: Text
-    Before:
-      <img src="paste-82b591d2788564fcb184d3545fa186cb89d3a466.jpg"><br>Proof. In order to show that \(\bigcap_{n=1}^{\infty} I_{n}\) is not empty, we are going to use the Axiom of Completeness (AoC) to produce a single real number \(x\) satisfying \(x \in I_{n}\) for every \(n \in \mathbf{N}\). Now, AoC is a statement about bounded sets, and the one we want to consider is the set<br><br><ul><li>\(A\) = \(\left\{a_{n}: n \in \mathbf{N}\right\}\)</li></ul><br>of left-hand endpoints of the intervals. as seen in the figure:<br><img src="paste-8125e3d4b152b61d72dd0cc79b9e550fcab212ce.jpg"><br><br>Because the intervals are nested, we see that every \(b_{n}\) serves as an uper bound for \(A\). Thus, we are justified in setting<br><ul><li>\(x\) = \(\sup A \text {. }\)<br></li></ul><br>Now, consider a particular \(I_{n}=\left[a_{n}, b_{n}\right]\). Because \(x\) is an upper bound for \(A\), we have \(a_{n}\) \(\leq\) \(x\). The fact that each \(b_{n}\) is an upper bound for \(A\) and that \(x\) is the least upper bound implies \(x\) \(\leq\) \(b_{n}\).<br><br>Altogether then, we have \(a_{n} \leq x \leq b_{n}\), which means \(x \in I_{n}\) for every choice of \(n \in \mathbf{N}\). Hence, \(x \in \bigcap_{n=1}^{\infty} I_{n}\), and the intersection is not empty.<br>
-
-    After:
-      <img src="paste-82b591d2788564fcb184d3545fa186cb89d3a466.jpg"><br>Proof. In order to show that \(\bigcap_{n=1}^{\infty} I_{n}\) is not empty, we are going to use the Axiom of Completeness (AoC) to produce a single real number \(x\) satisfying \(x \in I_{n}\) for every \(n \in \mathbf{N}\). Now, AoC is a statement about bounded sets, and the one we want to consider is the set<br><br><ul><li>\(A\) = \(\left\{a_{n}: n \in \mathbf{N}\right\}\)</li></ul><br>of left-hand endpoints of the intervals. as seen in the figure:<br><img src="paste-8125e3d4b152b61d72dd0cc79b9e550fcab212ce.jpg"><br><br>Because the intervals are nested, we see that every \(b_{n}\) serves as an uper bound for \(A\). Thus, we are justified in setting<br><ul><li>\(x\) = \(\sup A \text {. }\)<br></li></ul><br>Now, consider a particular \(I_{n}=\left[a_{n}, b_{n}\right]\). Because \(x\) is an upper bound for \(A\), we have \(a_{n}\) \(\leq\) \(x\). The fact that each \(b_{n}\) is an upper bound for \(A\) and that \(x\) is the least upper bound implies \(x\) \(\leq\) \(b_{n}\).<br><br>Altogether then, we have \(a_{n} \leq x \leq b_{n}\), which means \(x \in I_{n}\) for every choice of \(n \in \mathbf{N}\). Hence, \(x \in \bigcap_{n=1}^{\infty} I_{n}\), and the intersection is not empty.<br>
-
-============================================================
-
-Note ID: 1709395050204
-  Field: Text
-    Before:
-      Theorem 1.4.2 (Archimedean<b>&nbsp;Property</b>). <br><br><ul><li>(i) Given any number \(x\)&nbsp; \(\in\) \(\mathbf{R}\), there exists an \(n\) \(\in\) \(\mathbf{N}\) satisfying \(n\) \(&gt;\) \(x\).</li><li>(ii) Given any real number \(y\) \(&gt;0\), there exists an \(n\) \(\in\) \(\mathbf{N}\) satisfying \(1 / n\) \(&lt;\) \(y\).</li></ul>
-
-    After:
-      Theorem 1.4.2 (Archimedean<b>&nbsp;Property</b>). <br><br><ul><li>(i) Given any number \(x\)&nbsp; \(\in\) \(\mathbf{R}\), there exists an \(n\) \(\in\) \(\mathbf{N}\) satisfying \(n\) \(&gt;\) \(x\).</li><li>(ii) Given any real number \(y\) \(&gt;0\), there exists an \(n\) \(\in\) \(\mathbf{N}\) satisfying \(1 / n\) \(&lt;\) \(y\).</li></ul>
-
-============================================================
-
-Note ID: 1709395313481
-  Field: Text
-    Before:
-      Theorem 1.4.2 (Archimedean Property). (i) Given any number \(x \in \mathbf{R}\), there exists an \(n \in \mathbf{N}\) satisfying \(n&gt;x\).<br><br>(ii) Given any real number \(y&gt;0\), there exists an \(n \in \mathbf{N}\) satisfying \(1 / n&lt;y\).<br><br><b>Proof:</b><br><ul><li>Part I: And so to the proof. Assume, for contradiction, that \(\mathbf{N}\) is bounded above. By the Axiom of Completeness (AoC), \(\mathbf{N}\) should then have a least upper bound, and we can set \(\alpha\) = \(\sup \mathbf{N}\). If we consider \(\alpha-1\), then we no longer have an upper bound, and therefore there exists an \(n \in \mathbf{N}\) satisfying \(\alpha-1&lt;n\). But this is equivalent to \(\alpha\) &lt; \(n+1\). Because \(n+1\)&nbsp; \(\in \mathbf{N}\), we have a contradiction to the fact that \(\alpha\) is supposed to be an upper bound for \(\mathbf{N}\). (Notice that the contradiction here depends only on AoC and the fact that \(\mathbf{N}\) is closed under addition.)</li><li>Part (ii) follows from (i) by letting \(x\) = \(1 / y\).<br></li></ul>
-
-    After:
-      Theorem 1.4.2 (Archimedean Property). (i) Given any number \(x \in \mathbf{R}\), there exists an \(n \in \mathbf{N}\) satisfying \(n&gt;x\).<br><br>(ii) Given any real number \(y&gt;0\), there exists an \(n \in \mathbf{N}\) satisfying \(1 / n&lt;y\).<br><br><b>Proof:</b><br><ul><li>Part I: And so to the proof. Assume, for contradiction, that \(\mathbf{N}\) is bounded above. By the Axiom of Completeness (AoC), \(\mathbf{N}\) should then have a least upper bound, and we can set \(\alpha\) = \(\sup \mathbf{N}\). If we consider \(\alpha-1\), then we no longer have an upper bound, and therefore there exists an \(n \in \mathbf{N}\) satisfying \(\alpha-1&lt;n\). But this is equivalent to \(\alpha\) &lt; \(n+1\). Because \(n+1\)&nbsp; \(\in \mathbf{N}\), we have a contradiction to the fact that \(\alpha\) is supposed to be an upper bound for \(\mathbf{N}\). (Notice that the contradiction here depends only on AoC and the fact that \(\mathbf{N}\) is closed under addition.)</li><li>Part (ii) follows from (i) by letting \(x\) = \(1 / y\).<br></li></ul>
-
-============================================================
-
-Note ID: 1709395425250
-  Field: Text
-    Before:
-      Theorem 1.4.3 (Density&nbsp;of \(\mathbf{Q}\) in \(\mathbf{R}\) ). <br><br>For every two real numbers \(a\) and \(b\) with \(a\) &lt; \(b\), there exists a rational&nbsp;number \(r\) satisfying \(a\) &lt; r &lt; \(b\).
-
-    After:
-      Theorem 1.4.3 (Density&nbsp;of \(\mathbf{Q}\) in \(\mathbf{R}\) ). <br><br>For every two real numbers \(a\) and \(b\) with \(a\) &lt; \(b\), there exists a rational&nbsp;number \(r\) satisfying \(a\) &lt; r &lt; \(b\).
-
-============================================================
-
-Note ID: 1709395588018
-  Field: Text
-    Before:
-      <b>Corollary</b> 1.4.4. Given any two real numbers \(a\) \(&lt;\) \(b\), there exists an irrational number \(t\) satisfying \(a\) &lt; \(t\) &lt; \(b\).
-
-    After:
-      <b>Corollary</b> 1.4.4. Given any two real numbers \(a\) \(&lt;\) \(b\), there exists an irrational number \(t\) satisfying \(a\) &lt; \(t\) &lt; \(b\).
-
-============================================================
-
-Note ID: 1709396201315
-  Field: Text
-    Before:
-      Theorem 1.4.5. There exists a real number \(\alpha \in \mathbf{R}\) satisfying \(\alpha^{2}=2\).<br><br>Proof. After reviewing Example 1.3.6, consider the set<br><br>\[<br>T=\left\{t \in \mathbf{R}: t^{2}&lt;2\right\}<br>\]<br>and set \(\alpha=\sup T\).&nbsp;<br><br><br>Let's first see what happens if we assume \(\alpha^{2}&lt;2\). In search of an element of \(T\) that is larger than \(\alpha\), write<br><br><ul><li>\(\left(\alpha+\frac{1}{n}\right)^2\) =&nbsp;\(\alpha^2+\frac{2 \alpha}{n}+\frac{1}{n^2}\)</li><li>&lt;&nbsp;\(\alpha^2+\frac{2 \alpha}{n}+\frac{1}{n}\)</li><li>=&nbsp;\(\alpha^2+\frac{2 \alpha+1}{n}\).</li></ul><div><br></div><div>But now assuming \(\alpha^{2}&lt;2\) gives us a little space in which to fit the \((2 \alpha+1) / n\) term and keep the total less than 2 . Specifically, choose \(n_{0} \in \mathbf{N}\) large enough so that<br><br><ul><li>\(\frac{1}{n_{0}}\) \(&lt;\) \(\frac{2-\alpha^{2} }{2 \alpha+1}\)</li></ul><br>This implies \((2 \alpha+1) / n_{0}\) &lt; \(2-\alpha^{2}\), and consequently that</div><div><br><ul><li>\(\left(\alpha+\frac{1}{n_{0}}\right)^{2}\) &lt; \(\alpha^{2}+\left(2-\alpha^{2}\right)\) = \(2 .\)</li></ul><div><br></div><div>Now, what about the case \(\alpha^{2}&gt;2\) ? This time, write<br></div></div><div><ul><li>\(\left(\alpha+\frac{1}{n}\right)^2\) =&nbsp;\(\alpha^2+\frac{2 \alpha}{n}+\frac{1}{n^2}\)</li><li>&gt;&nbsp;\(\alpha^2-\frac{2 \alpha}{n}\).</li></ul><div><br></div></div>
-
-    After:
-      Theorem 1.4.5. There exists a real number \(\alpha \in \mathbf{R}\) satisfying \(\alpha^{2}=2\).<br><br>Proof. After reviewing Example 1.3.6, consider the set<br><br>\[<br>T=\left\{t \in \mathbf{R}: t^{2}&lt;2\right\}<br>\]<br>and set \(\alpha=\sup T\).&nbsp;<br><br><br>Let's first see what happens if we assume \(\alpha^{2}&lt;2\). In search of an element of \(T\) that is larger than \(\alpha\), write<br><br><ul><li>\(\left(\alpha+\frac{1}{n}\right)^2\) =&nbsp;\(\alpha^2+\frac{2 \alpha}{n}+\frac{1}{n^2}\)</li><li>&lt;&nbsp;\(\alpha^2+\frac{2 \alpha}{n}+\frac{1}{n}\)</li><li>=&nbsp;\(\alpha^2+\frac{2 \alpha+1}{n}\).</li></ul><div><br></div><div>But now assuming \(\alpha^{2}&lt;2\) gives us a little space in which to fit the \((2 \alpha+1) / n\) term and keep the total less than 2 . Specifically, choose \(n_{0} \in \mathbf{N}\) large enough so that<br><br><ul><li>\(\frac{1}{n_{0}}\) \(&lt;\) \(\frac{2-\alpha^{2} }{2 \alpha+1}\)</li></ul><br>This implies \((2 \alpha+1) / n_{0}\) &lt; \(2-\alpha^{2}\), and consequently that</div><div><br><ul><li>\(\left(\alpha+\frac{1}{n_{0}}\right)^{2}\) &lt; \(\alpha^{2}+\left(2-\alpha^{2}\right)\) = \(2 .\)</li></ul><div><br></div><div>Now, what about the case \(\alpha^{2}&gt;2\) ? This time, write<br></div></div><div><ul><li>\(\left(\alpha+\frac{1}{n}\right)^2\) =&nbsp;\(\alpha^2+\frac{2 \alpha}{n}+\frac{1}{n^2}\)</li><li>&gt;&nbsp;\(\alpha^2-\frac{2 \alpha}{n}\).</li></ul><div><br></div></div>
-
-============================================================
-
-Note ID: 1709397193027
-  Field: Text
-    Before:
-      Exercise 1.4.1. Recall that I stands for the set of irrational numbers.<br><ul><li>(a) Show that if \(a, b \in \mathbf{Q}\), then \(a b\) and \(a+b\) are elements of \(\mathbf{Q}\) as well<br></li><li>(b) Show that if \(a \in \mathbf{Q}\) and \(t \in \mathbf{I}\), then \(a+t\) \(\in\) \(\mathbf{I}\) and \(a t\) \(\in\) \(\mathbf{I}\) as long as \(a\) \(\neq\) \(0\).</li><li>(c) Part (a) can be summarized by saying that \(\mathbf{Q}\) is closed under addition and multiplication. Is I closed under addition and multiplication?&nbsp;</li><ul><li>Answer: no</li></ul></ul>
-
-    After:
-      Exercise 1.4.1. Recall that I stands for the set of irrational numbers.<br><ul><li>(a) Show that if \(a, b \in \mathbf{Q}\), then \(a b\) and \(a+b\) are elements of \(\mathbf{Q}\) as well<br></li><li>(b) Show that if \(a \in \mathbf{Q}\) and \(t \in \mathbf{I}\), then \(a+t\) \(\in\) \(\mathbf{I}\) and \(a t\) \(\in\) \(\mathbf{I}\) as long as \(a\) \(\neq\) \(0\).</li><li>(c) Part (a) can be summarized by saying that \(\mathbf{Q}\) is closed under addition and multiplication. Is I closed under addition and multiplication?&nbsp;</li><ul><li>Answer: no</li></ul></ul>
-
-============================================================
-
-Note ID: 1709399857200
-  Field: Text
-    Before:
-      <b>Definition 1.5.1</b>. <br><br><ul><li>A function \(f: A \rightarrow B\) is one-to-one (1-1) if \(a_{1}\) \(\neq\) \(a_{2}\) in \(A\) implies that \(f\left(a_{1}\right)\) \(\neq\) \(f\left(a_{2}\right)\) in \(B\).&nbsp;</li><li>The function \(f\) is onto if, given any \(b \in B\), it is possible to find an element \(a \in A\) for which \(f(a)\)= \(b\).</li></ul>
-
-    After:
-      <b>Definition 1.5.1</b>. <br><br><ul><li>A function \(f: A \rightarrow B\) is one-to-one (1-1) if \(a_{1}\) \(\neq\) \(a_{2}\) in \(A\) implies that \(f\left(a_{1}\right)\) \(\neq\) \(f\left(a_{2}\right)\) in \(B\).&nbsp;</li><li>The function \(f\) is onto if, given any \(b \in B\), it is possible to find an element \(a \in A\) for which \(f(a)\)= \(b\).</li></ul>
-
-============================================================
-
-Note ID: 1709399913310
-  Field: Text
-    Before:
-      A function \(f: A \rightarrow B\) that is both 1-1 and onto provides us with exactly what we mean by a \(1-1\) correspondence between two sets. The property of being 1-1 means that no two elements of \(A\) correspond to the same element of \(B\) , and the property of being onto ensures that every element of \(B\) corresponds to something in \(A\)&nbsp;
-
-    After:
-      A function \(f: A \rightarrow B\) that is both 1-1 and onto provides us with exactly what we mean by a \(1-1\) correspondence between two sets. The property of being 1-1 means that no two elements of \(A\) correspond to the same element of \(B\) , and the property of being onto ensures that every element of \(B\) corresponds to something in \(A\)&nbsp;
-
-============================================================
-
-Note ID: 1709403075504
-  Field: Text
-    Before:
-      <b>Definition 1.5.5</b>. A set \(A\) is countable if \(\mathbf{N} \sim A\). An infinite set that is not countable is called an uncountable set.
-
-    After:
-      <b>Definition 1.5.5</b>. A set \(A\) is countable if \(\mathbf{N} \sim A\). An infinite set that is not countable is called an uncountable set.
-
-============================================================
-
-Note ID: 1709403971804
-  Field: Text
-    Before:
-      &nbsp;It is an important exercise to show that any subset of a countable set must be either countable or finite.
-
-    After:
-      &nbsp;It is an important exercise to show that any subset of a countable set must be either countable or finite.
-
-============================================================
-
-Note ID: 1709403993743
-  Field: Text
-    Before:
-      This means that countable sets are the smallest type of infinite set. Anything smaller is either still countable or finite.
-
-    After:
-      This means that countable sets are the smallest type of infinite set. Anything smaller is either still countable or finite.
-
-============================================================
-
-Note ID: 1709404121335
-  Field: Text
-    Before:
-      <ul><li>&nbsp;The set \(\mathbf{Q}\), on the other hand, is countable.&nbsp;</li><li>As far as infinite sets are concerned, this is as small as it gets.&nbsp;</li><li>What does this imply about the set I of irrational numbers?&nbsp;</li><li>By imitating the demonstration that \(\mathbf{N} \sim \mathbf{Z}\), we can prove that the union of two countable sets must be countable.&nbsp;</li><li>Because \(\mathbf{R}\) = \(\mathbf{Q} \cup \mathbf{I}\), it follows that \(\mathbf{I}\) cannot be countable because otherwise \(\mathbf{R}\) would be.&nbsp;</li><li>The inescapable conclusion is that, the irrational numbers form a far greater subset of \(\mathbf{R}\) than \(\mathbf{Q}\).</li></ul>
-
-    After:
-      <ul><li>&nbsp;The set \(\mathbf{Q}\), on the other hand, is countable.&nbsp;</li><li>As far as infinite sets are concerned, this is as small as it gets.&nbsp;</li><li>What does this imply about the set I of irrational numbers?&nbsp;</li><li>By imitating the demonstration that \(\mathbf{N} \sim \mathbf{Z}\), we can prove that the union of two countable sets must be countable.&nbsp;</li><li>Because \(\mathbf{R}\) = \(\mathbf{Q} \cup \mathbf{I}\), it follows that \(\mathbf{I}\) cannot be countable because otherwise \(\mathbf{R}\) would be.&nbsp;</li><li>The inescapable conclusion is that, the irrational numbers form a far greater subset of \(\mathbf{R}\) than \(\mathbf{Q}\).</li></ul>
-
-============================================================
-
-Note ID: 1709404152106
-  Field: Text
-    Before:
-      <b>Theorem</b> 1.5.7. If \(A\) \(\subseteq\) \(B\) and \(B\) is countable, then \(A\) is either countable or finite.
-
-    After:
-      <b>Theorem</b> 1.5.7. If \(A\) \(\subseteq\) \(B\) and \(B\) is countable, then \(A\) is either countable or finite.
-
-============================================================
-
-Note ID: 1709410042987
-  Field: Text
-    Before:
-      Theorem 1.5.8. <br><ul><li><br></li><li>(i) If \(A_{1}, A_{2}, \ldots A_{m}\) are each countable sets, then the union \(A_{1} \cup A_{2} \cup \cdots \cup A_{m}\) is countable.</li><li>(ii) If \(A_{n}\) is a countable set for each \(n \in \mathbf{N}\), then \(\bigcup_{n=1}^{\infty}\) \(A_{n}\) is countable.</li></ul>
-
-    After:
-      Theorem 1.5.8. <br><ul><li><br></li><li>(i) If \(A_{1}, A_{2}, \ldots A_{m}\) are each countable sets, then the union \(A_{1} \cup A_{2} \cup \cdots \cup A_{m}\) is countable.</li><li>(ii) If \(A_{n}\) is a countable set for each \(n \in \mathbf{N}\), then \(\bigcup_{n=1}^{\infty}\) \(A_{n}\) is countable.</li></ul>
-
-============================================================
-
-Note ID: 1709410126365
-  Field: Text
-    Before:
-      Assume \(B\) is a countable set. Thus, there exists \(f: \mathbf{N} \rightarrow B\), which is \(1-1\) and onto. Let \(A \subseteq B\) be an infinite subset of \(B\). We must show that \(A\) is countable.
-
-    After:
-      Assume \(B\) is a countable set. Thus, there exists \(f: \mathbf{N} \rightarrow B\), which is \(1-1\) and onto. Let \(A \subseteq B\) be an infinite subset of \(B\). We must show that \(A\) is countable.
-
-============================================================
-
-Note ID: 1709410411951
-  Field: Text
-    Before:
-      (c) Using open intervals makes it more convenient to produce the required \(1-1\), onto functions, but it is not really necessary. Show that \([0,1)\) \(\sim\) \((0,1)\) by exhibiting a 1-1 onto function between the two sets.
-
-    After:
-      (c) Using open intervals makes it more convenient to produce the required \(1-1\), onto functions, but it is not really necessary. Show that \([0,1)\) \(\sim\) \((0,1)\) by exhibiting a 1-1 onto function between the two sets.
-
-============================================================
-
-Note ID: 1709410441376
-  Field: Text
-    Before:
-      Exercise 1.5.5. (a) Why is \(A\) \(\sim\) \(A\) for every set \(A\) ?
-
-    After:
-      Exercise 1.5.5. (a) Why is \(A\) \(\sim\) \(A\) for every set \(A\) ?
-
-============================================================
-
-Note ID: 1709410486498
-  Field: Text
-    Before:
-      (b) Given sets \(A\) and \(B\), explain why \(A\) \(\sim\) \(B\) is equivalent to asserting \(B\) \(\sim\) \(A\).
-
-    After:
-      (b) Given sets \(A\) and \(B\), explain why \(A\) \(\sim\) \(B\) is equivalent to asserting \(B\) \(\sim\) \(A\).
-
-============================================================
-
-Note ID: 1709410555711
-  Field: Text
-    Before:
-      (c) For three sets \(A, B\), and \(C\), show that \(A\) \(\sim\) \(B\) and \(B\) \(\sim\) \(C\) implies \(A\) \(\sim\) \(C\).
-
-    After:
-      (c) For three sets \(A, B\), and \(C\), show that \(A\) \(\sim\) \(B\) and \(B\) \(\sim\) \(C\) implies \(A\) \(\sim\) \(C\).
-
-============================================================
-
-Note ID: 1709410805033
-  Field: Text
-    Before:
-      Exercise 1.5.9. A real number \(x \in \mathbf{R}\) is called algebraic if there exist integers \(a_{0}, a_{1}, a_{2}, \ldots, a_{n} \in \mathbf{Z}\), not all zero, such that<br><br><ul><li>\(\sum_{i=0}^{n}\) \(a_i x^i\)&nbsp;= \(0 .\)</li></ul><br>Said another way, a real number is algebraic if it is the root of a polynomial with integer coefficients. Real numbers that are not algebraic are called transcendental numbers. Reread the last paragraph of Section 1.1. The final question posed here is closely related to the question of whether or not transcendental numbers exist.<br>
-
-    After:
-      Exercise 1.5.9. A real number \(x \in \mathbf{R}\) is called algebraic if there exist integers \(a_{0}, a_{1}, a_{2}, \ldots, a_{n} \in \mathbf{Z}\), not all zero, such that<br><br><ul><li>\(\sum_{i=0}^{n}\) \(a_i x^i\)&nbsp;= \(0 .\)</li></ul><br>Said another way, a real number is algebraic if it is the root of a polynomial with integer coefficients. Real numbers that are not algebraic are called transcendental numbers. Reread the last paragraph of Section 1.1. The final question posed here is closely related to the question of whether or not transcendental numbers exist.<br>
-
-============================================================
-
-Note ID: 1709410995011
-  Field: Text
-    Before:
-      Exercise 1.5.9. A real number \(x \in \mathbf{R}\) is called algebraic if there exist integers \(a_{0}, a_{1}, a_{2}, \ldots, a_{n} \in \mathbf{Z}\), not all zero, such that<br><br>\[<br>a_{n} x^{n}+a_{n-1} x^{n-1}+\cdots+a_{1} x+a_{0}=0 .<br>\]<br><ul><li>(a) Show that \(\sqrt{2}, \sqrt[3]{2}\), and \(\sqrt{3}+\sqrt{2}\) are::are/not::are/not::are/not::are/not::are/not algebraic.</li><li>(b) Fix \(n \in \mathbf{N}\), and let \(A_{n}\) be the algebraic numbers obtained as roots of polynomials with integer coefficients that have degree \(n\). Using the fact that every polynomial has a finite number of roots, show that \(A_{n}\) is countable.</li><li>(c) Now, argue that the set of all algebraic numbers is countable.&nbsp;</li><ul><li>Thus we may conclude transcendental numebers are uncountable</li></ul></ul>
-
-    After:
-      Exercise 1.5.9. A real number \(x \in \mathbf{R}\) is called algebraic if there exist integers \(a_{0}, a_{1}, a_{2}, \ldots, a_{n} \in \mathbf{Z}\), not all zero, such that<br><br>\[<br>a_{n} x^{n}+a_{n-1} x^{n-1}+\cdots+a_{1} x+a_{0}=0 .<br>\]<br><ul><li>(a) Show that \(\sqrt{2}, \sqrt[3]{2}\), and \(\sqrt{3}+\sqrt{2}\) are::are/not::are/not::are/not::are/not::are/not algebraic.</li><li>(b) Fix \(n \in \mathbf{N}\), and let \(A_{n}\) be the algebraic numbers obtained as roots of polynomials with integer coefficients that have degree \(n\). Using the fact that every polynomial has a finite number of roots, show that \(A_{n}\) is countable.</li><li>(c) Now, argue that the set of all algebraic numbers is countable.&nbsp;</li><ul><li>Thus we may conclude transcendental numebers are uncountable</li></ul></ul>
-
-============================================================
-
-Note ID: 1709411083805
-  Field: Text
-    Before:
-      Exercise 1.5.10. (a) Let \(C \subseteq\) \([0,1]\) be uncountable. Show that there exists \(a\) \(\in\) \((0,1)\) such that \(C \) \(\cap\) \([a, 1]\) is uncountable.
-
-    After:
-      Exercise 1.5.10. (a) Let \(C \subseteq\) \([0,1]\) be uncountable. Show that there exists \(a\) \(\in\) \((0,1)\) such that \(C \) \(\cap\) \([a, 1]\) is uncountable.
-
-============================================================
-
-Note ID: 1709411194326
-  Field: Text
-    Before:
-      Exercise 1.5.11 (Schröder-Bernstein Theorem). Assume there exists a 1-1 function \(f: X \rightarrow Y\) and another 1-1 function \(g: Y \rightarrow X\). Follow the steps to show that there exists a 1-1, onto function \(h:\) \(X \rightarrow Y\) and hence \(X\)&nbsp; \(\sim\) \(Y\).
-
-    After:
-      Exercise 1.5.11 (Schröder-Bernstein Theorem). Assume there exists a 1-1 function \(f: X \rightarrow Y\) and another 1-1 function \(g: Y \rightarrow X\). Follow the steps to show that there exists a 1-1, onto function \(h:\) \(X \rightarrow Y\) and hence \(X\)&nbsp; \(\sim\) \(Y\).
-
-============================================================
-
-Note ID: 1709411294840
-  Field: Text
-    Before:
-      Exercise 1.5.11 (Schröder-Bernstein Theorem). Assume there exists a 1-1 function \(f: X \rightarrow Y\) and another 1-1 function \(g: Y \rightarrow X\). Follow the steps to show that there exists a 1-1, onto function \(h: X \rightarrow Y\) and hence \(X \sim Y\).<br><br>The strategy is to partition \(X\) and \(Y\) into components<br><br><ul><li>\(X\) = \(A \cup A^{\prime} \)</li><li>\(Y\) = \(B \cup B^{\prime}\)</li></ul><br>with \(A \cap A^{\prime}\) = \(\emptyset\) and \(B \cap B^{\prime}\) = \(\emptyset\), in such a way that \(f\) maps \(A\) onto \(B\), and \(g\) maps \(B^{\prime}\) onto \(A^{\prime}\).<br>
-
-    After:
-      Exercise 1.5.11 (Schröder-Bernstein Theorem). Assume there exists a 1-1 function \(f: X \rightarrow Y\) and another 1-1 function \(g: Y \rightarrow X\). Follow the steps to show that there exists a 1-1, onto function \(h: X \rightarrow Y\) and hence \(X \sim Y\).<br><br>The strategy is to partition \(X\) and \(Y\) into components<br><br><ul><li>\(X\) = \(A \cup A^{\prime} \)</li><li>\(Y\) = \(B \cup B^{\prime}\)</li></ul><br>with \(A \cap A^{\prime}\) = \(\emptyset\) and \(B \cap B^{\prime}\) = \(\emptyset\), in such a way that \(f\) maps \(A\) onto \(B\), and \(g\) maps \(B^{\prime}\) onto \(A^{\prime}\).<br>
-
-============================================================
-
-Note ID: 1709411607147
-  Field: Text
-    Before:
-      Exercise 1.6.1. Show that \((0,1)\) is uncountable if and only if \(\mathbf{R}\) is uncountable.&nbsp;
-
-    After:
-      Exercise 1.6.1. Show that \((0,1)\) is uncountable if and only if \(\mathbf{R}\) is uncountable.&nbsp;
-
-============================================================
-
-Note ID: 1709460323398
-  Field: Text
-    Before:
-      Given a set \(A\), the power set \(P(A)\) refers to the collection of all subsets of \(A\). It is important to understand that \(P(A)\) is itself considered a set whose elements are the different possible subsets of \(A\).
-
-    After:
-      Given a set \(A\), the power set \(P(A)\) refers to the collection of all subsets of \(A\). It is important to understand that \(P(A)\) is itself considered a set whose elements are the different possible subsets of \(A\).
-
-============================================================
-
-Note ID: 1709461189125
-  Field: Text
-    Before:
-      <b>Theorem 1.6.2 (Cantor's Theorem). Given any set \(A\), there does not exist a function \(f: A \rightarrow P(A)\) that is onto.</b><br><br>Proof. This proof, like the others of its kind, is indirect. Thus, assume, for contradiction, that \(f: A \rightarrow P(A)\) is onto. Unlike the usual situation in which we have sets of numbers for the domain and range, \(f\) is a correspondence between a set and its power set. For each element \(a \in A, f(a)\) is a particular subset of \(A\).<br><br><br>Construct \(B\) using the following rule:<br><ul><li>\(B\) = \(\{a \in A: a \notin f(a)\} .\)<br></li></ul><div>We now focus on the general argument. Because we have assumed that our function \(f: A \rightarrow P(A)\) is onto, it must be that \(B\) = \(f\left(a^{\prime}\right)\) for some \(a^{\prime} \in A\). The contradiction arises when we consider whether or not \(a^{\prime}\) is an element of \(B\).</div><div><br></div><div>Exercise 1.6.8. (a) First, show that the case \(a^{\prime}\)&nbsp; \(\in\)&nbsp; \(B\) leads to a contradiction.<br><br>(b) Now, finish the argument by showing that the case \(a^{\prime}\)&nbsp; \(\notin\) \(B\) is equally unacceptable.<br></div>
-
-    After:
-      <b>Theorem 1.6.2 (Cantor's Theorem). Given any set \(A\), there does not exist a function \(f: A \rightarrow P(A)\) that is onto.</b><br><br>Proof. This proof, like the others of its kind, is indirect. Thus, assume, for contradiction, that \(f: A \rightarrow P(A)\) is onto. Unlike the usual situation in which we have sets of numbers for the domain and range, \(f\) is a correspondence between a set and its power set. For each element \(a \in A, f(a)\) is a particular subset of \(A\).<br><br><br>Construct \(B\) using the following rule:<br><ul><li>\(B\) = \(\{a \in A: a \notin f(a)\} .\)<br></li></ul><div>We now focus on the general argument. Because we have assumed that our function \(f: A \rightarrow P(A)\) is onto, it must be that \(B\) = \(f\left(a^{\prime}\right)\) for some \(a^{\prime} \in A\). The contradiction arises when we consider whether or not \(a^{\prime}\) is an element of \(B\).</div><div><br></div><div>Exercise 1.6.8. (a) First, show that the case \(a^{\prime}\)&nbsp; \(\in\)&nbsp; \(B\) leads to a contradiction.<br><br>(b) Now, finish the argument by showing that the case \(a^{\prime}\)&nbsp; \(\notin\) \(B\) is equally unacceptable.<br></div>
-
-============================================================
-
-Note ID: 1709461481795
-  Field: Text
-    Before:
-      Two sets appear in the same group, or equivalence class, if and only if they have the same cardinality. Thus, \(\mathbf{N}, \mathbf{Z}\), and \(\mathbf{Q}\) are grouped together in one class with all of the&nbsp;other countable sets, whereas \(\mathbf{R}\) is in another class that includes the intervals \((a, b)\) as well as \(P(\mathbf{N})\).&nbsp;
-
-    After:
-      Two sets appear in the same group, or equivalence class, if and only if they have the same cardinality. Thus, \(\mathbf{N}, \mathbf{Z}\), and \(\mathbf{Q}\) are grouped together in one class with all of the&nbsp;other countable sets, whereas \(\mathbf{R}\) is in another class that includes the intervals \((a, b)\) as well as \(P(\mathbf{N})\).&nbsp;
-
-============================================================
-
-Note ID: 1709461539740
-  Field: Text
-    Before:
-      &nbsp;One implication of Cantor's Theorem is that \(P(\mathbf{R})\) - the set of all subsets of \(\mathbf{R}\) - is in a different class from \(\mathbf{R}\), and there is no reason to stop here. The set of subsets of \(P(\mathbf{R})\) - namely \(P(P(\mathbf{R}))\) - is in yet another class, and this process continues indefinitely.
-
-    After:
-      &nbsp;One implication of Cantor's Theorem is that \(P(\mathbf{R})\) - the set of all subsets of \(\mathbf{R}\) - is in a different class from \(\mathbf{R}\), and there is no reason to stop here. The set of subsets of \(P(\mathbf{R})\) - namely \(P(P(\mathbf{R}))\) - is in yet another class, and this process continues indefinitely.
-
-============================================================
-
-Note ID: 1709461870515
-  Field: Text
-    Before:
-      A more downto-earth problem in need of attention is demonstrating that our definition of " \(\leq\) " between cardinal numbers really is an ordering. This involves showing that cardinal numbers possess a property analogous to real numbers, which states that if \(\operatorname{card} X \leq \operatorname{card} Y\) and \(\operatorname{card} Y \leq \operatorname{card} X\), then \(\operatorname{card} X=\operatorname{card} Y\). In the end, this boils down to proving that if there exists \(f: X \rightarrow Y\) that is \(1-1\), and if there exists \(g: Y \rightarrow X\) that is \(1-1\), then it is possible to find a function \(h: X \rightarrow Y\) that is both 1-1 and onto.&nbsp;
-
-    After:
-      A more downto-earth problem in need of attention is demonstrating that our definition of " \(\leq\) " between cardinal numbers really is an ordering. This involves showing that cardinal numbers possess a property analogous to real numbers, which states that if \(\operatorname{card} X \leq \operatorname{card} Y\) and \(\operatorname{card} Y \leq \operatorname{card} X\), then \(\operatorname{card} X=\operatorname{card} Y\). In the end, this boils down to proving that if there exists \(f: X \rightarrow Y\) that is \(1-1\), and if there exists \(g: Y \rightarrow X\) that is \(1-1\), then it is possible to find a function \(h: X \rightarrow Y\) that is both 1-1 and onto.&nbsp;
-
-============================================================
-
-Note ID: 1709534610935
-  Field: Text
-    Before:
-      Consider the infinite series<br><ul><li>\(\sum_{n=1}^{\infty}\) \(\frac{(-1)^{n+1} }{n}\)&nbsp;= \(1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\frac{1}{5}-\frac{1}{6}+\frac{1}{7}-\frac{1}{8}+\cdots\)</li></ul><br>If we naively begin adding from the left-hand side, we get a sequence of what are called partial sums. In other words, let \(s_{n}\) equal the sum of the first \(n\) terms of the series, so that \(s_{1}=1, s_{2}=1 / 2, s_{3}=5 / 6, s_{4}=7 / 12\), and so on. One immediate observation is that the successive sums oscillate in a progressively narrower space. The odd sums decrease \(\left(s_{1}&gt;s_{3}&gt;s_{5}&gt;\ldots\right)\) while the even sums increase \(\left(s_{2}&lt;s_{4}&lt;s_{6}&lt;\ldots\right.\) ). As shown in the following figure<br><br><img src="paste-79643f453b463079d965aae31704eba344f8b31e.jpg"><br><br>It seems reasonable - and we will soon prove - that the sequence \(\left(s_{n}\right)\) eventually hones in on a value, call it \(S\), where the odd and even partial sums "meet."<br>
-
-    After:
-      Consider the infinite series<br><ul><li>\(\sum_{n=1}^{\infty}\) \(\frac{(-1)^{n+1} }{n}\)&nbsp;= \(1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\frac{1}{5}-\frac{1}{6}+\frac{1}{7}-\frac{1}{8}+\cdots\)</li></ul><br>If we naively begin adding from the left-hand side, we get a sequence of what are called partial sums. In other words, let \(s_{n}\) equal the sum of the first \(n\) terms of the series, so that \(s_{1}=1, s_{2}=1 / 2, s_{3}=5 / 6, s_{4}=7 / 12\), and so on. One immediate observation is that the successive sums oscillate in a progressively narrower space. The odd sums decrease \(\left(s_{1}&gt;s_{3}&gt;s_{5}&gt;\ldots\right)\) while the even sums increase \(\left(s_{2}&lt;s_{4}&lt;s_{6}&lt;\ldots\right.\) ). As shown in the following figure<br><br><img src="paste-79643f453b463079d965aae31704eba344f8b31e.jpg"><br><br>It seems reasonable - and we will soon prove - that the sequence \(\left(s_{n}\right)\) eventually hones in on a value, call it \(S\), where the odd and even partial sums "meet."<br>
-
-============================================================
-
-Note ID: 1709536784899
-  Field: Text
-    Before:
-      Definition 2.2.3 (Convergence&nbsp;of a Sequence):<br><ul><li>A sequence \(\left(a_{n}\right)\) converges to a real number \(a\) if, for every positive number \(\epsilon\), there exists an \(N \in \mathbf{N}\) such that whenever \(n\) \(\geq\) \(N\) it follows that:</li><ul><li>&nbsp;\(|\) \(a_{n}-a\) \(|\) &lt; \(\epsilon\).</li></ul></ul>
-
-    After:
-      Definition 2.2.3 (Convergence&nbsp;of a Sequence):<br><ul><li>A sequence \(\left(a_{n}\right)\) converges to a real number \(a\) if, for every positive number \(\epsilon\), there exists an \(N \in \mathbf{N}\) such that whenever \(n\) \(\geq\) \(N\) it follows that:</li><ul><li>&nbsp;\(|\) \(a_{n}-a\) \(|\) &lt; \(\epsilon\).</li></ul></ul>
-
-============================================================
-
-Note ID: 1709537103479
-  Field: Text
-    Before:
-      <b>Definition</b> 2.2.4. Given a real number \(a \in \mathbf{R}\) and a positive number \(\epsilon\) \(&gt;\) \(0\), the set<br><br><ul><li>\(V_{\epsilon}(a)\) = \(\{\) \(x \in \mathbf{R}\) : \(|x-a|\) \(&lt;\) \(\epsilon\) \(\}\)</li></ul><br>is called the \(\epsilon\)-neighborhood of \(a\).<br>
-
-    After:
-      <b>Definition</b> 2.2.4. Given a real number \(a \in \mathbf{R}\) and a positive number \(\epsilon\) \(&gt;\) \(0\), the set<br><br><ul><li>\(V_{\epsilon}(a)\) = \(\{\) \(x \in \mathbf{R}\) : \(|x-a|\) \(&lt;\) \(\epsilon\) \(\}\)</li></ul><br>is called the \(\epsilon\)-neighborhood of \(a\).<br>
-
-============================================================
-
-Note ID: 1709537244871
-  Field: Text
-    Before:
-      Notice that \(V_{\epsilon}(a)\) consists of all of those points whose distance from \(a\) is less than \(\epsilon\). Said another way, \(V_{\epsilon}(a)\) is an interval, centered at \(a\), with radius \(\epsilon\) as shown in the following figure<br><br><img src="2bJsgJdaSiTgiXeKSRq_TdOQ1Ln2IU5lkv_OapXjnDw.original.fullsize.png"><br><br>Recasting the definition of convergence in terms of \(\epsilon\)-neighborhoods gives a more geometric impression of what is being described.
-
-    After:
-      Notice that \(V_{\epsilon}(a)\) consists of all of those points whose distance from \(a\) is less than \(\epsilon\). Said another way, \(V_{\epsilon}(a)\) is an interval, centered at \(a\), with radius \(\epsilon\) as shown in the following figure<br><br><img src="2bJsgJdaSiTgiXeKSRq_TdOQ1Ln2IU5lkv_OapXjnDw.original.fullsize.png"><br><br>Recasting the definition of convergence in terms of \(\epsilon\)-neighborhoods gives a more geometric impression of what is being described.
-
-============================================================
-
-Note ID: 1709537445781
-  Field: Text
-    Before:
-      Definition 2.2.3B (Convergence of a Sequence: Topological Version):<br><ul><li>&nbsp;A sequence \(\left(a_{n}\right)\) converges to \(a\) if, given any \(\epsilon\)-neighborhood \(V_{\epsilon}(a)\) of \(a\), there exists a point in the sequence after which all of the terms are in \(V_{\epsilon}(a)\).&nbsp;</li><li>In other words, every \(\epsilon\)-neighborhood contains all but a finite number of the terms of \(\left(a_{n}\right)\).</li></ul><br><br>As shown in the following figure:<br><img src="paste-17aeb92339aa6b54792faef3af6093ad37bac36f.jpg"><br>
-
-    After:
-      Definition 2.2.3B (Convergence of a Sequence: Topological Version):<br><ul><li>&nbsp;A sequence \(\left(a_{n}\right)\) converges to \(a\) if, given any \(\epsilon\)-neighborhood \(V_{\epsilon}(a)\) of \(a\), there exists a point in the sequence after which all of the terms are in \(V_{\epsilon}(a)\).&nbsp;</li><li>In other words, every \(\epsilon\)-neighborhood contains all but a finite number of the terms of \(\left(a_{n}\right)\).</li></ul><br><br>As shown in the following figure:<br><img src="paste-17aeb92339aa6b54792faef3af6093ad37bac36f.jpg"><br>
-
-============================================================
-
-Note ID: 1709537508915
-  Field: Text
-    Before:
-      The natural number \(N\) in the original version of the convergence definition is the point where the sequence \(\left(a_{n}\right)\) enters \(V_{\epsilon}(a)\), never to leave. It should be apparent that the value of \(N\) depends on the choice of \(\epsilon\). The smaller the \(\epsilon\)-neighborhood, the larger \(N\) may have to be.
-
-    After:
-      The natural number \(N\) in the original version of the convergence definition is the point where the sequence \(\left(a_{n}\right)\) enters \(V_{\epsilon}(a)\), never to leave. It should be apparent that the value of \(N\) depends on the choice of \(\epsilon\). The smaller the \(\epsilon\)-neighborhood, the larger \(N\) may have to be.
-
-============================================================
-
-Note ID: 1709707878449
-  Field: Text
-    Before:
-      TEMplate for A PROOF THAT \(\left(x_{n}\right) \rightarrow x\) :<br><ul><li>- "Let \(\epsilon\) \(&gt;\) \(0\) be arbitrary."</li><li>- Demonstrate a choice for \(N\)&nbsp; \(\in \mathbf{N}\).&nbsp;</li><li>- Now, show that \(N\) actually works.</li><li>- "Assume \(n\) \(\geq\) \(N\)."</li><li>- With \(N\) well chosen, it should be possible to derive the inequality \(\left|x_{n}-x\right|\) \(&lt;\) \(\epsilon\).</li></ul>
-
-    After:
-      TEMplate for A PROOF THAT \(\left(x_{n}\right) \rightarrow x\) :<br><ul><li>- "Let \(\epsilon\) \(&gt;\) \(0\) be arbitrary."</li><li>- Demonstrate a choice for \(N\)&nbsp; \(\in \mathbf{N}\).&nbsp;</li><li>- Now, show that \(N\) actually works.</li><li>- "Assume \(n\) \(\geq\) \(N\)."</li><li>- With \(N\) well chosen, it should be possible to derive the inequality \(\left|x_{n}-x\right|\) \(&lt;\) \(\epsilon\).</li></ul>
-
-============================================================
-
-Note ID: 1709709299812
-  Field: Text
-    Before:
-      Exercise 2.2.7. Here are two useful definitions:<br><br><ul><li>(i) A sequence \(\left(a_{n}\right)\) is eventually in a set \(A \subseteq \mathbf{R}\) if there exists an \(N \in \mathbf{N}\) such that \(a_{n}\) \(\in A\) for all \(n \geq N\).</li><li>(ii) A sequence \(\left(a_{n}\right)\) is frequently in a set \(A \subseteq \mathbf{R}\) if, for every \(N \in \mathbf{N}\), there exists an \(n \geq N\) such that \(a_{n}\) \(\in A\).</li></ul>
-
-    After:
-      Exercise 2.2.7. Here are two useful definitions:<br><br><ul><li>(i) A sequence \(\left(a_{n}\right)\) is eventually in a set \(A \subseteq \mathbf{R}\) if there exists an \(N \in \mathbf{N}\) such that \(a_{n}\) \(\in A\) for all \(n \geq N\).</li><li>(ii) A sequence \(\left(a_{n}\right)\) is frequently in a set \(A \subseteq \mathbf{R}\) if, for every \(N \in \mathbf{N}\), there exists an \(n \geq N\) such that \(a_{n}\) \(\in A\).</li></ul>
-
-============================================================
-
-Note ID: 1709709652708
-  Field: Text
-    Before:
-      Definition 2.3.1. A sequence \(\left(x_{n}\right)\) is bounded if there exists a number \(M\) &gt; \(0\) such that \(\left|x_{n}\right|\) \(\leq\) \(M\) for all \(n \in \mathbf{N}\).<br>
-
-    After:
-      Definition 2.3.1. A sequence \(\left(x_{n}\right)\) is bounded if there exists a number \(M\) &gt; \(0\) such that \(\left|x_{n}\right|\) \(\leq\) \(M\) for all \(n \in \mathbf{N}\).<br>
-
-============================================================
-
-Note ID: 1709710317865
-  Field: Text
-    Before:
-      Theorem 2.3.2. Every convergent sequence is bounded.<br><br>Proof. Assume \(\left(x_{n}\right)\) converges to a limit \(l\). This means that given a particular value of \(\epsilon\), say \(\epsilon=1\), we know there must exist an \(N \in \mathbf{N}\) such that if \(n \geq N\), then \(x_{n}\) is in the interval \((l-1, l+1)\). Not knowing whether \(l\) is positive or negative, we can certainly conclude that<br><ul><li>\(\left|x_{n}\right|\) &lt; \(|l|+1\)<br></li></ul><br>for all \(n \geq N\) as shown in the figure<br><br><img src="paste-515c094bd5b761e5a75a79ed5446007b6f07cc0f.jpg"><br>We still need to worry (slightly) about the terms in the sequence that come before the \(N\) th term. Because there are only a finite number of these, we let<br><br><ul><li>\(M\) = \(\max \left\{\left|x_{1}\right|,\left|x_{2}\right|,\left|x_{3}\right|, \ldots,\left|x_{N-1}\right|,|l|+1\right\} .\)</li></ul><br>It follows that \(\left|x_{n}\right|\)&nbsp; \(\leq M\) for all \(n \in \mathbf{N}\), as desired.<br><br>
-
-    After:
-      Theorem 2.3.2. Every convergent sequence is bounded.<br><br>Proof. Assume \(\left(x_{n}\right)\) converges to a limit \(l\). This means that given a particular value of \(\epsilon\), say \(\epsilon=1\), we know there must exist an \(N \in \mathbf{N}\) such that if \(n \geq N\), then \(x_{n}\) is in the interval \((l-1, l+1)\). Not knowing whether \(l\) is positive or negative, we can certainly conclude that<br><ul><li>\(\left|x_{n}\right|\) &lt; \(|l|+1\)<br></li></ul><br>for all \(n \geq N\) as shown in the figure<br><br><img src="paste-515c094bd5b761e5a75a79ed5446007b6f07cc0f.jpg"><br>We still need to worry (slightly) about the terms in the sequence that come before the \(N\) th term. Because there are only a finite number of these, we let<br><br><ul><li>\(M\) = \(\max \left\{\left|x_{1}\right|,\left|x_{2}\right|,\left|x_{3}\right|, \ldots,\left|x_{N-1}\right|,|l|+1\right\} .\)</li></ul><br>It follows that \(\left|x_{n}\right|\)&nbsp; \(\leq M\) for all \(n \in \mathbf{N}\), as desired.<br><br>
-
-============================================================
-
-Note ID: 1709710815979
-  Field: Text
-    Before:
-      Theorem 2.3.3 (Algebraic Limit Theorem). Let \(\lim a_{n}=a\), and \(\lim b_{n}=\) b. Then,<br><ol><li>(i) \(\lim \left(c a_{n}\right)=c a\), for all \(c \in \mathbf{R}\);</li></ol>Proof. (i) Consider the case where \(c \neq 0\). We want to show that the sequence \(\left(c a_{n}\right)\) converges to \(c a\), so the structure of the proof follows the template we described in Section 2.2. First, we let \(\epsilon\) be some arbitrary positive number. Our goal is to find some point in the sequence \(\left(c a_{n}\right)\) after which we have<br><br><ul><li>\(\left|c a_{n}-c a\right|\) &lt; \(\epsilon\)</li></ul><br>Now,<br><br><ul><li>\(\left|c a_{n}-c a\right|\) = \(|c|\left|a_{n}-a\right| .\)</li></ul><br>We are given that \(\left(a_{n}\right) \rightarrow a\), so we know we can make \(\left|a_{n}-a\right|\) as small as we like. In particular, we can choose an \(N\) such that<br><br><ul><li>\(\left|a_{n}-a\right|\) \(&lt;\) \(\frac{\epsilon}{|c|}\)</li></ul><br>whenever \(n \geq N\). To see that this \(N\) indeed works, observe that, for all \(n \geq N\),<br><br><ul><li>\(\left|c a_{n}-c a\right|\) = \(|c|\left|a_{n}-a\right|\) &lt; \(|c| \frac{\epsilon}{|c|}\) = \(\epsilon\)</li></ul><br>The case \(c=0\) reduces to showing that the constant sequence \((0,0,0, \ldots)\) converges to 0 , which is easily verified.<br>
-
-    After:
-      Theorem 2.3.3 (Algebraic Limit Theorem). Let \(\lim a_{n}=a\), and \(\lim b_{n}=\) b. Then,<br><ol><li>(i) \(\lim \left(c a_{n}\right)=c a\), for all \(c \in \mathbf{R}\);</li></ol>Proof. (i) Consider the case where \(c \neq 0\). We want to show that the sequence \(\left(c a_{n}\right)\) converges to \(c a\), so the structure of the proof follows the template we described in Section 2.2. First, we let \(\epsilon\) be some arbitrary positive number. Our goal is to find some point in the sequence \(\left(c a_{n}\right)\) after which we have<br><br><ul><li>\(\left|c a_{n}-c a\right|\) &lt; \(\epsilon\)</li></ul><br>Now,<br><br><ul><li>\(\left|c a_{n}-c a\right|\) = \(|c|\left|a_{n}-a\right| .\)</li></ul><br>We are given that \(\left(a_{n}\right) \rightarrow a\), so we know we can make \(\left|a_{n}-a\right|\) as small as we like. In particular, we can choose an \(N\) such that<br><br><ul><li>\(\left|a_{n}-a\right|\) \(&lt;\) \(\frac{\epsilon}{|c|}\)</li></ul><br>whenever \(n \geq N\). To see that this \(N\) indeed works, observe that, for all \(n \geq N\),<br><br><ul><li>\(\left|c a_{n}-c a\right|\) = \(|c|\left|a_{n}-a\right|\) &lt; \(|c| \frac{\epsilon}{|c|}\) = \(\epsilon\)</li></ul><br>The case \(c=0\) reduces to showing that the constant sequence \((0,0,0, \ldots)\) converges to 0 , which is easily verified.<br>
-
-============================================================
-
-Note ID: 1709713087895
-  Field: Text
-    Before:
-      &nbsp;Limits can be computed from the individual component sequences provided that each component limit exists.
-
-    After:
-      &nbsp;Limits can be computed from the individual component sequences provided that each component limit exists.
-
-============================================================
-
-Note ID: 1709760036708
-  Field: Text
-    Before:
-      Theorem 2.3.4 (Order Limit Theorem). Assume \(\lim\) \(a_{n}\) = \(a\) and \(\lim b_{n}\) = \(b\).<br><ul><li>(i) If \(a_{n}\)&nbsp; \(\geq\) \(0\) for all \(n\) \(\in\) \(\mathbf{N}\), then \(a\) \(\geq\) \(0\).</li><li>(ii) If \(a_{n}\) \(\leq\) \(b_{n}\) for all \(n\) \(\in\) \(\mathbf{N}\), then \(a\) \(\leq\) \(b\).</li><li>(iii) If there exists \(c\) \(\in\) \(\mathbf{R}\) for which \(c\) \(\leq\) \(b_{n}\) for all \(n \in \mathbf{N}\), then \(c\) \(\leq\) \(b\).&nbsp;</li><ul><li>Similarly, if \(a_{n}\) \(\leq\) \(c\) for all \(n\) \(\in\) \(\mathbf{N}\), then \(a\) \(\leq\) \(c\).</li></ul></ul>
-
-    After:
-      Theorem 2.3.4 (Order Limit Theorem). Assume \(\lim\) \(a_{n}\) = \(a\) and \(\lim b_{n}\) = \(b\).<br><ul><li>(i) If \(a_{n}\)&nbsp; \(\geq\) \(0\) for all \(n\) \(\in\) \(\mathbf{N}\), then \(a\) \(\geq\) \(0\).</li><li>(ii) If \(a_{n}\) \(\leq\) \(b_{n}\) for all \(n\) \(\in\) \(\mathbf{N}\), then \(a\) \(\leq\) \(b\).</li><li>(iii) If there exists \(c\) \(\in\) \(\mathbf{R}\) for which \(c\) \(\leq\) \(b_{n}\) for all \(n \in \mathbf{N}\), then \(c\) \(\leq\) \(b\).&nbsp;</li><ul><li>Similarly, if \(a_{n}\) \(\leq\) \(c\) for all \(n\) \(\in\) \(\mathbf{N}\), then \(a\) \(\leq\) \(c\).</li></ul></ul>
-
-============================================================
-
-Note ID: 1709760218730
-  Field: Text
-    Before:
-      Theorem 2.3.4 (Order Limit Theorem). Assume \(\lim a_{n}=a\) and \(\lim b_{n}=b\).<br><br>(i) If \(a_{n} \geq 0\) for all \(n \in \mathbf{N}\), then \(a \geq 0\).<br><br><br>Proof. (i) We will prove this by contradiction; thus, let's assume \(a&lt;0\). The idea is to produce a term in the sequence \(\left(a_{n}\right)\) that is also less than zero. To do this, we consider the particular value \(\epsilon\) = \(|a|\). The definition of convergence guarantees that we can find an \(N\) such that \(\left|a_{n}-a\right|\) &lt; \(|a|\) for all \(n \geq N\). In particular, this would mean that \(\left|a_{N}-a\right|\) \(&lt;\) \(|a|\), which implies \(a_{N}\) &lt; \(0\). This contradicts our hypothesis that \(a_{N} \geq 0\). We therefore conclude that \(a \geq 0\).<br><br>Ass seen in the figure:<br><img src="paste-fb03c153b4593cc1e8ec890a70d3c024cb470ec3.jpg">
-
-    After:
-      Theorem 2.3.4 (Order Limit Theorem). Assume \(\lim a_{n}=a\) and \(\lim b_{n}=b\).<br><br>(i) If \(a_{n} \geq 0\) for all \(n \in \mathbf{N}\), then \(a \geq 0\).<br><br><br>Proof. (i) We will prove this by contradiction; thus, let's assume \(a&lt;0\). The idea is to produce a term in the sequence \(\left(a_{n}\right)\) that is also less than zero. To do this, we consider the particular value \(\epsilon\) = \(|a|\). The definition of convergence guarantees that we can find an \(N\) such that \(\left|a_{n}-a\right|\) &lt; \(|a|\) for all \(n \geq N\). In particular, this would mean that \(\left|a_{N}-a\right|\) \(&lt;\) \(|a|\), which implies \(a_{N}\) &lt; \(0\). This contradicts our hypothesis that \(a_{N} \geq 0\). We therefore conclude that \(a \geq 0\).<br><br>Ass seen in the figure:<br><img src="paste-fb03c153b4593cc1e8ec890a70d3c024cb470ec3.jpg">
-
-============================================================
-
-Note ID: 1709760447743
-  Field: Text
-    Before:
-      In the language of analysis, when a property (such as non-negativity) is not necessarily possessed by some finite number of initial terms but is possessed&nbsp; by all terms in the sequence after some point \(N\), we say that the sequence eventually has this property. (See Exercise 2.2.7.) Theorem 2.3.4, part (i), could be restated, "Convergent sequences that are eventually nonnegative converge to nonnegative limits."
-
-    After:
-      In the language of analysis, when a property (such as non-negativity) is not necessarily possessed by some finite number of initial terms but is possessed&nbsp; by all terms in the sequence after some point \(N\), we say that the sequence eventually has this property. (See Exercise 2.2.7.) Theorem 2.3.4, part (i), could be restated, "Convergent sequences that are eventually nonnegative converge to nonnegative limits."
-
-============================================================
-
-Note ID: 1709760689022
-  Field: Text
-    Before:
-      Exercise 2.3.3 (Squeeze Theorem):<br><ul><li>&nbsp;Show that if \(x_{n}\) \(\leq\) \(y_{n}\) \(\leq\) \(z_{n}\) for all \(n \in \mathbf{N}\)</li><li>And if \(\lim x_{n}\) = \(\lim z_{n}\) = \(l\)</li><li>Then \(\lim y_{n}\) = \(l\)&nbsp;</li></ul>
-
-    After:
-      Exercise 2.3.3 (Squeeze Theorem):<br><ul><li>&nbsp;Show that if \(x_{n}\) \(\leq\) \(y_{n}\) \(\leq\) \(z_{n}\) for all \(n \in \mathbf{N}\)</li><li>And if \(\lim x_{n}\) = \(\lim z_{n}\) = \(l\)</li><li>Then \(\lim y_{n}\) = \(l\)&nbsp;</li></ul>
-
-============================================================
-
-Note ID: 1709761070348
-  Field: Text
-    Before:
-      Exercise 2.3.11 (Cesaro Means). <br><br><ul><li>(a) Show that if \(\left(x_{n}\right)\) is a convergent sequence, then the sequence given by the averages</li><ul><li>\(y_{n}\) = \(\frac{x_{1}+x_{2}+\cdots+x_{n} }{n}\)</li><li>converges to the same limit.</li></ul><li>(b) Give an example to show that it is possible for the sequence \(\left(y_{n}\right)\) of averages to converge even if \(\left(x_{n}\right)\) does not.</li></ul>
-
-    After:
-      Exercise 2.3.11 (Cesaro Means). <br><br><ul><li>(a) Show that if \(\left(x_{n}\right)\) is a convergent sequence, then the sequence given by the averages</li><ul><li>\(y_{n}\) = \(\frac{x_{1}+x_{2}+\cdots+x_{n} }{n}\)</li><li>converges to the same limit.</li></ul><li>(b) Give an example to show that it is possible for the sequence \(\left(y_{n}\right)\) of averages to converge even if \(\left(x_{n}\right)\) does not.</li></ul>
-
-============================================================
-
-Note ID: 1709761668787
-  Field: Text
-    Before:
-      Exercise 2.3.13 (Iterated Limits). Given a doubly indexed array \(a_{m n}\) where \(m, n \in \mathbf{N}\), what should \(\lim _{m, n \rightarrow \infty} a_{m n}\) represent?<br><br><ul><li>(a) Let \(a_{m n}=m /(m+n)\) and compute the iterated limits</li><ul><ul><li>\(\lim _{n \rightarrow \infty}\left(\lim _{m \rightarrow \infty} a_{m n}\right)\)&nbsp;&nbsp;</li><li>\( \lim _{m \rightarrow \infty}\left(\lim _{n \rightarrow \infty} a_{m n}\right)\)</li></ul><li>Define \(\lim _{m, n \rightarrow \infty} a_{m n}\)=\(a\) to mean that for all \(\epsilon\) &gt; \(0\) there exists an \(N \in \mathbf{N}\) such that if both \(m, n \geq N\), then \(\left|a_{m n}-a\right|\) &lt; \(\epsilon\).</li></ul><li>(c) Let \(a_{m n}=1 /(m+n)\), Produce an example where \(\lim _{m, n \rightarrow \infty} a_{m n}\) exists but where neither iterated limit can be computed.</li><li>(d) Assume \(\lim _{m, n \rightarrow \infty} a_{m n}=a\), and assume that for each fixed \(m \in \mathbf{N}\), \(\lim _{n \rightarrow \infty}\left(a_{m n}\right) \rightarrow b_{m}\). Show \(\lim _{m \rightarrow \infty} b_{m}\) = \(a\).</li><li>(e) Prove that if \(\lim _{m, n \rightarrow \infty} a_{m n}\) exists and the iterated limits both exist, then all three limits must be equal.</li></ul>
-
-    After:
-      Exercise 2.3.13 (Iterated Limits). Given a doubly indexed array \(a_{m n}\) where \(m, n \in \mathbf{N}\), what should \(\lim _{m, n \rightarrow \infty} a_{m n}\) represent?<br><br><ul><li>(a) Let \(a_{m n}=m /(m+n)\) and compute the iterated limits</li><ul><ul><li>\(\lim _{n \rightarrow \infty}\left(\lim _{m \rightarrow \infty} a_{m n}\right)\)&nbsp;&nbsp;</li><li>\( \lim _{m \rightarrow \infty}\left(\lim _{n \rightarrow \infty} a_{m n}\right)\)</li></ul><li>Define \(\lim _{m, n \rightarrow \infty} a_{m n}\)=\(a\) to mean that for all \(\epsilon\) &gt; \(0\) there exists an \(N \in \mathbf{N}\) such that if both \(m, n \geq N\), then \(\left|a_{m n}-a\right|\) &lt; \(\epsilon\).</li></ul><li>(c) Let \(a_{m n}=1 /(m+n)\), Produce an example where \(\lim _{m, n \rightarrow \infty} a_{m n}\) exists but where neither iterated limit can be computed.</li><li>(d) Assume \(\lim _{m, n \rightarrow \infty} a_{m n}=a\), and assume that for each fixed \(m \in \mathbf{N}\), \(\lim _{n \rightarrow \infty}\left(a_{m n}\right) \rightarrow b_{m}\). Show \(\lim _{m \rightarrow \infty} b_{m}\) = \(a\).</li><li>(e) Prove that if \(\lim _{m, n \rightarrow \infty} a_{m n}\) exists and the iterated limits both exist, then all three limits must be equal.</li></ul>
-
-============================================================
-
-Note ID: 1709761826977
-  Field: Text
-    Before:
-      Definition 2.4.1. <br><ul><li>A sequence \(\left(a_{n}\right)\) is increasing if \(a_{n}\) \(\leq a_{n+1}\) for all \(n \in \mathbf{N}\) and decreasing if \(a_{n}\)&nbsp; \(\geq a_{n+1}\) for all \(n \in \mathbf{N}\).&nbsp;</li><li>A sequence is monotone if it is either increasing or decreasing.</li></ul>
-
-    After:
-      Definition 2.4.1. <br><ul><li>A sequence \(\left(a_{n}\right)\) is increasing if \(a_{n}\) \(\leq a_{n+1}\) for all \(n \in \mathbf{N}\) and decreasing if \(a_{n}\)&nbsp; \(\geq a_{n+1}\) for all \(n \in \mathbf{N}\).&nbsp;</li><li>A sequence is monotone if it is either increasing or decreasing.</li></ul>
-
-============================================================
-
-Note ID: 1709762152626
-  Field: Text
-    Before:
-      Theorem 2.4.2 (Monotone Convergence Theorem). If a sequence is monotone and bounded, then it converges.<br><br>Proof. Let \(\left(a_{n}\right)\) be monotone and bounded. To prove \(\left(a_{n}\right)\) converges using the definition of convergence, we are going to need a candidate for the limit. <br><ul><li>Let's assume the sequence is increasing (the decreasing case is handled similarly), and consider the set of points \(\left\{a_{n}: n \in \mathbf{N}\right\}\). By assumption, this set is bounded, so we can let</li></ul><ul><ul><li>\(s\) = \(\sup \left\{a_{n}: n \in \mathbf{N}\right\}\)</li></ul></ul><br>It seems reasonable to claim that \(\lim a_{n}\) = \(s\) as shown in the figure:<br><img src="paste-d9580e49717308ae667f3f6a572fa5e73f93c8c7.jpg"><br><br>To prove this, let \(\epsilon&gt;0\). Because \(s\) is the least upper bound for \(\left\{a_{n}: n \in \mathbf{N}\right\}\), \(s-\epsilon\) is not an upper bound, so there exists a point in the sequence \(a_{N}\) such that \(s-\epsilon\) &lt; \(a_{N}\). Now, the fact that \(\left(a_{n}\right)\) is increasing implies that if \(n \geq N\), then \(a_{N}\)&nbsp; \(\leq a_{n}\). Hence,<br><br><ul><li>\(s-\epsilon\) &lt; \(a_{N}\) \(\leq\) \(a_{n}\) \(\leq\) \(s\)&lt; \(s+\epsilon\)</li></ul><br>which implies \(\left|a_{n}-s\right|\) &lt; \(\epsilon\), as desired.<br>
-
-    After:
-      Theorem 2.4.2 (Monotone Convergence Theorem). If a sequence is monotone and bounded, then it converges.<br><br>Proof. Let \(\left(a_{n}\right)\) be monotone and bounded. To prove \(\left(a_{n}\right)\) converges using the definition of convergence, we are going to need a candidate for the limit. <br><ul><li>Let's assume the sequence is increasing (the decreasing case is handled similarly), and consider the set of points \(\left\{a_{n}: n \in \mathbf{N}\right\}\). By assumption, this set is bounded, so we can let</li></ul><ul><ul><li>\(s\) = \(\sup \left\{a_{n}: n \in \mathbf{N}\right\}\)</li></ul></ul><br>It seems reasonable to claim that \(\lim a_{n}\) = \(s\) as shown in the figure:<br><img src="paste-d9580e49717308ae667f3f6a572fa5e73f93c8c7.jpg"><br><br>To prove this, let \(\epsilon&gt;0\). Because \(s\) is the least upper bound for \(\left\{a_{n}: n \in \mathbf{N}\right\}\), \(s-\epsilon\) is not an upper bound, so there exists a point in the sequence \(a_{N}\) such that \(s-\epsilon\) &lt; \(a_{N}\). Now, the fact that \(\left(a_{n}\right)\) is increasing implies that if \(n \geq N\), then \(a_{N}\)&nbsp; \(\leq a_{n}\). Hence,<br><br><ul><li>\(s-\epsilon\) &lt; \(a_{N}\) \(\leq\) \(a_{n}\) \(\leq\) \(s\)&lt; \(s+\epsilon\)</li></ul><br>which implies \(\left|a_{n}-s\right|\) &lt; \(\epsilon\), as desired.<br>
-
-============================================================
-
-Note ID: 1709762376279
-  Field: Text
-    Before:
-      <b>Definition</b> 2.4.3 (Convergence of a Series). Let \(\left(b_{n}\right)\) be a sequence. An infinite series is a formal expression of the form<br><br><ul><li>\(\sum_{n=1}^{\infty}\) \( b_{n}\) = \(b_{1}+b_{2}+b_{3}+b_{4}+b_{5}+\cdots\)</li></ul><br>We define the corresponding sequence of partial sums \(\left(s_{m}\right)\) by<br><br><ul><li>\(s_{m}\) = \(b_{1}+b_{2}+b_{3}+\cdots+b_{m},\)</li></ul><br>and say that the series \(\sum_{n=1}^{\infty}\) \(b_{n}\) converges to \(B\) if the sequence \(\left(s_{m}\right)\) converges to \(B\). In this case, we write \(\sum_{n=1}^{\infty}\) \(b_{n}\) = \(B\).<br>
-
-    After:
-      <b>Definition</b> 2.4.3 (Convergence of a Series). Let \(\left(b_{n}\right)\) be a sequence. An infinite series is a formal expression of the form<br><br><ul><li>\(\sum_{n=1}^{\infty}\) \( b_{n}\) = \(b_{1}+b_{2}+b_{3}+b_{4}+b_{5}+\cdots\)</li></ul><br>We define the corresponding sequence of partial sums \(\left(s_{m}\right)\) by<br><br><ul><li>\(s_{m}\) = \(b_{1}+b_{2}+b_{3}+\cdots+b_{m},\)</li></ul><br>and say that the series \(\sum_{n=1}^{\infty}\) \(b_{n}\) converges to \(B\) if the sequence \(\left(s_{m}\right)\) converges to \(B\). In this case, we write \(\sum_{n=1}^{\infty}\) \(b_{n}\) = \(B\).<br>
-
-============================================================
-
-Note ID: 1709762435681
-  Field: Text
-    Before:
-      Definition 2.4.3 (Convergence of a Series). Let \(\left(b_{n}\right)\) be a sequence. An infinite series is a formal expression of the form<br><br>\[<br>\sum_{n=1}^{\infty} b_{n}=b_{1}+b_{2}+b_{3}+b_{4}+b_{5}+\cdots<br>\]<br><br>We define the corresponding sequence of partial sums \(\left(s_{m}\right)\) by<br><br>\[<br>s_{m}=b_{1}+b_{2}+b_{3}+\cdots+b_{m},<br>\]<br><br>and say that the series \(\sum_{n=1}^{\infty} b_{n}\) converges to \(B\) if the sequence \(\left(s_{m}\right)\) converges to \(B\). In this case, we write \(\sum_{n=1}^{\infty} b_{n}\) = \(B\).
-
-    After:
-      Definition 2.4.3 (Convergence of a Series). Let \(\left(b_{n}\right)\) be a sequence. An infinite series is a formal expression of the form<br><br>\[<br>\sum_{n=1}^{\infty} b_{n}=b_{1}+b_{2}+b_{3}+b_{4}+b_{5}+\cdots<br>\]<br><br>We define the corresponding sequence of partial sums \(\left(s_{m}\right)\) by<br><br>\[<br>s_{m}=b_{1}+b_{2}+b_{3}+\cdots+b_{m},<br>\]<br><br>and say that the series \(\sum_{n=1}^{\infty} b_{n}\) converges to \(B\) if the sequence \(\left(s_{m}\right)\) converges to \(B\). In this case, we write \(\sum_{n=1}^{\infty} b_{n}\) = \(B\).
-
-============================================================
-
-Note ID: 1709794331551
-  Field: Text
-    Before:
-      Exercise 2.4.4. (a) In Section 1.4 we used the Axiom of Completeness (AoC) to prove the Archimedean Property of \(\mathbf{R}\) (Theorem 1.4.2). Show that the Monotone Convergence Theorem can also be used to prove the Archimedean Property without making any use of AoC.
-
-    After:
-      Exercise 2.4.4. (a) In Section 1.4 we used the Axiom of Completeness (AoC) to prove the Archimedean Property of \(\mathbf{R}\) (Theorem 1.4.2). Show that the Monotone Convergence Theorem can also be used to prove the Archimedean Property without making any use of AoC.
-
-============================================================
-
-Note ID: 1709795042914
-  Field: Text
-    Before:
-      Exercise 2.4.7 (Limit Superior). Let \(\left(a_{n}\right)\) be a bounded sequence.<br><br><ul><li>(a) Prove that the sequence defined by \(y_{n}=\sup \left\{a_{k}: k \geq n\right\}\) converges.</li><li>(b) The limit superior of \(\left(a_{n}\right)\), or \(\lim \sup a_{n}\), is defined by</li><ul><ul><li>\[\limsup a_{n}=\lim y_{n},\]</li></ul><li>where \(y_{n}\) is the sequence from part (a) of this exercise. Provide a reasonable definition for \(\lim \inf a_{n}\) and briefly explain why it always exists for any bounded sequence.</li></ul><li>(c) Prove that \(\liminf a_{n}\) \(\leq\) \(\lim \sup a_{n}\) for every bounded sequence.</li><li>(d) Show that \(\lim \inf a_{n}\) = \(\lim \sup a_{n}\) if and only if \(\lim a_{n}\) exists. In this case, all three share the same value.</li></ul>
-
-    After:
-      Exercise 2.4.7 (Limit Superior). Let \(\left(a_{n}\right)\) be a bounded sequence.<br><br><ul><li>(a) Prove that the sequence defined by \(y_{n}=\sup \left\{a_{k}: k \geq n\right\}\) converges.</li><li>(b) The limit superior of \(\left(a_{n}\right)\), or \(\lim \sup a_{n}\), is defined by</li><ul><ul><li>\[\limsup a_{n}=\lim y_{n},\]</li></ul><li>where \(y_{n}\) is the sequence from part (a) of this exercise. Provide a reasonable definition for \(\lim \inf a_{n}\) and briefly explain why it always exists for any bounded sequence.</li></ul><li>(c) Prove that \(\liminf a_{n}\) \(\leq\) \(\lim \sup a_{n}\) for every bounded sequence.</li><li>(d) Show that \(\lim \inf a_{n}\) = \(\lim \sup a_{n}\) if and only if \(\lim a_{n}\) exists. In this case, all three share the same value.</li></ul>
-
-============================================================
-
-Note ID: 1709795255396
-  Field: Text
-    Before:
-      Consider the special class of infinite products of the form<br><br><ul><li>\(\prod_{n=1}^{\infty}\) \(\left(1+a_{n}\right)\)= \(\left(1+a_{1}\right)\left(1+a_{2}\right)\left(1+a_{3}\right) \cdots\)</li><li>\(\text { where } a_{n} \geq 0\)</li></ul><br>(b) Show, in general, that the sequence of partial products converges if and only if \(\sum_{n=1}^{\infty} a_{n}\) converges. (The inequality \(1+x\) \(\leq\) \(3^{x}\) for positive \(x\) will be useful in one direction.)<br>
-
-    After:
-      Consider the special class of infinite products of the form<br><br><ul><li>\(\prod_{n=1}^{\infty}\) \(\left(1+a_{n}\right)\)= \(\left(1+a_{1}\right)\left(1+a_{2}\right)\left(1+a_{3}\right) \cdots\)</li><li>\(\text { where } a_{n} \geq 0\)</li></ul><br>(b) Show, in general, that the sequence of partial products converges if and only if \(\sum_{n=1}^{\infty} a_{n}\) converges. (The inequality \(1+x\) \(\leq\) \(3^{x}\) for positive \(x\) will be useful in one direction.)<br>
-
-============================================================
-
-Note ID: 1709796216831
-  Field: Text
-    Before:
-      Example 2.5.3. Let \(0&lt;b&lt;1\). Because<br><br>\[<br>b&gt;b^{2}&gt;b^{3}&gt;b^{4}&gt;\cdots&gt;0<br>\]<br><br>the sequence \(\left(b^{n}\right)\) is decreasing and bounded below. <br><br><ul><li>The Monotone Convergence Theorem allows us to conclude that \(\left(b^{n}\right)\) converges to some \(l\) satisfying \(b&gt;l \geq 0\).&nbsp;</li><li>To compute \(l\), notice that \(\left(b^{2 n}\right)\) is a subsequence, so \(\left(b^{2 n}\right) \rightarrow\) \(l\) by Theorem 2.5.2.&nbsp;</li><li>But \(b^{2 n}\)= \(b^{n} \cdot b^{n}\), so by the Algebraic Limit Theorem, \(\left(b^{2 n}\right) \rightarrow l \cdot l\) = \(l^{2}\). Because limits are unique (Theorem 2.2.7), \(l^{2}\) = \(l\), and thus \(l\) = \(0\).</li></ul>
-
-    After:
-      Example 2.5.3. Let \(0&lt;b&lt;1\). Because<br><br>\[<br>b&gt;b^{2}&gt;b^{3}&gt;b^{4}&gt;\cdots&gt;0<br>\]<br><br>the sequence \(\left(b^{n}\right)\) is decreasing and bounded below. <br><br><ul><li>The Monotone Convergence Theorem allows us to conclude that \(\left(b^{n}\right)\) converges to some \(l\) satisfying \(b&gt;l \geq 0\).&nbsp;</li><li>To compute \(l\), notice that \(\left(b^{2 n}\right)\) is a subsequence, so \(\left(b^{2 n}\right) \rightarrow\) \(l\) by Theorem 2.5.2.&nbsp;</li><li>But \(b^{2 n}\)= \(b^{n} \cdot b^{n}\), so by the Algebraic Limit Theorem, \(\left(b^{2 n}\right) \rightarrow l \cdot l\) = \(l^{2}\). Because limits are unique (Theorem 2.2.7), \(l^{2}\) = \(l\), and thus \(l\) = \(0\).</li></ul>
-
-============================================================
-
-Note ID: 1709796859581
-  Field: Text
-    Before:
-      Exercise 2.5.3. (a) Prove that if an infinite series converges, then the associative property holds. Assume \(a_{1}+a_{2}+a_{3}+a_{4}+a_{5}+\cdots\) converges to a limit \(L\) . Show that any regrouping of the terms:<br><br><ul><li>\(\left(a_{1}+a_{2}+\cdots+a_{n_{1}&nbsp; }\right)+\left(a_{n_{1}+1}+\cdots+a_{n_{2} }\right)+\left(a_{n_{2}+1}+\cdots+a_{n_{3} }\right)+\cdots\)</li></ul><br>leads to a series that also converges to \(L\).<br>
-
-    After:
-      Exercise 2.5.3. (a) Prove that if an infinite series converges, then the associative property holds. Assume \(a_{1}+a_{2}+a_{3}+a_{4}+a_{5}+\cdots\) converges to a limit \(L\) . Show that any regrouping of the terms:<br><br><ul><li>\(\left(a_{1}+a_{2}+\cdots+a_{n_{1}&nbsp; }\right)+\left(a_{n_{1}+1}+\cdots+a_{n_{2} }\right)+\left(a_{n_{2}+1}+\cdots+a_{n_{3} }\right)+\cdots\)</li></ul><br>leads to a series that also converges to \(L\).<br>
-
-============================================================
-
-Note ID: 1709797092655
-  Field: Text
-    Before:
-      Exercise 2.5.8. Another way to prove the Bolzano-Weierstrass Theorem is to show that every sequence contains a monotone subsequence. A useful device in this endeavor is the notion of a peak term. Given a sequence \(\left(x_{n}\right)\), a particular term \(x_{m}\) is a peak term if no later term in the sequence exceeds it; i.e., if \(x_{m} \) \(\geq x_{n}\) for all \(n \geq m\).
-
-    After:
-      Exercise 2.5.8. Another way to prove the Bolzano-Weierstrass Theorem is to show that every sequence contains a monotone subsequence. A useful device in this endeavor is the notion of a peak term. Given a sequence \(\left(x_{n}\right)\), a particular term \(x_{m}\) is a peak term if no later term in the sequence exceeds it; i.e., if \(x_{m} \) \(\geq x_{n}\) for all \(n \geq m\).
-
-============================================================
-
-Note ID: 1709797392230
-  Field: Text
-    Before:
-      To spoil the surprise, we will argue in this section that in fact these two definitions are equivalent: Convergent sequences are Cauchy sequences, and Cauchy sequences converge. The significance of the definition of a Cauchy sequence is that there is no mention of a limit.&nbsp;
-
-    After:
-      To spoil the surprise, we will argue in this section that in fact these two definitions are equivalent: Convergent sequences are Cauchy sequences, and Cauchy sequences converge. The significance of the definition of a Cauchy sequence is that there is no mention of a limit.&nbsp;
-
-============================================================
-
-Note ID: 1709797591616
-  Field: Text
-    Before:
-      Lemma 2.6.3. Cauchy sequences are bounded.<br><br>Proof. Given \(\epsilon\) = \(1\), there exists an \(N\) such that \(\left|x_{m}-x_{n}\right|\) &lt; \(1\) for all \(m, n \geq N\). Thus, we must have \(\left|x_{n}\right|\) \(&lt;\) \(\left|x_{N}\right|+1\) for all \(n \geq N\). It follows that<br><ul><li>\(M\) = \(\max \left\{\left|x_{1}\right|,\left|x_{2}\right|,\left|x_{3}\right|, \ldots,\left|x_{N-1}\right|,\left|x_{N}\right|+1\right\}\)</li></ul><br>is a bound for the sequence \(\left(x_{n}\right)\).<br>
-
-    After:
-      Lemma 2.6.3. Cauchy sequences are bounded.<br><br>Proof. Given \(\epsilon\) = \(1\), there exists an \(N\) such that \(\left|x_{m}-x_{n}\right|\) &lt; \(1\) for all \(m, n \geq N\). Thus, we must have \(\left|x_{n}\right|\) \(&lt;\) \(\left|x_{N}\right|+1\) for all \(n \geq N\). It follows that<br><ul><li>\(M\) = \(\max \left\{\left|x_{1}\right|,\left|x_{2}\right|,\left|x_{3}\right|, \ldots,\left|x_{N-1}\right|,\left|x_{N}\right|+1\right\}\)</li></ul><br>is a bound for the sequence \(\left(x_{n}\right)\).<br>
-
-============================================================
-
-Note ID: 1709797647785
-  Field: Text
-    Before:
-      Theorem 2.6.4 (Cauchy Criterion). A sequence converges if and only if it is a Cauchy sequence.
-
-    After:
-      Theorem 2.6.4 (Cauchy Criterion). A sequence converges if and only if it is a Cauchy sequence.
-
-============================================================
-
-Note ID: 1709798381005
-  Field: Text
-    Before:
-      Theorem 2.7.1 (Algebraic Limit Theorem for Series). If \(\sum_{k=1}^{\infty} a_{k}\) = \(A\) and \(\sum_{k=1}^{\infty} b_{k}\) = \(B\), then<br><br><ul><li>(i) \(\sum_{k=1}^{\infty}\) \(c a_{k}\) = \(c A\) for all \(c \in \mathbf{R}\) and</li><li>(ii) \(\sum_{k=1}^{\infty}\) \(\left(a_{k}+b_{k}\right)\) = \(A+B\).</li></ul>
-
-    After:
-      Theorem 2.7.1 (Algebraic Limit Theorem for Series). If \(\sum_{k=1}^{\infty} a_{k}\) = \(A\) and \(\sum_{k=1}^{\infty} b_{k}\) = \(B\), then<br><br><ul><li>(i) \(\sum_{k=1}^{\infty}\) \(c a_{k}\) = \(c A\) for all \(c \in \mathbf{R}\) and</li><li>(ii) \(\sum_{k=1}^{\infty}\) \(\left(a_{k}+b_{k}\right)\) = \(A+B\).</li></ul>
-
-============================================================
-
-Note ID: 1709798626220
-  Field: Text
-    Before:
-      Theorem 2.7.2 (Cauchy Criterion for Series). The series \(\sum_{k=1}^{\infty} a_{k}\) converges if and only if, given \(\epsilon\) \(&gt;0\), there exists an \(N \in \mathbf{N}\) such that whenever \(n&gt;m\) \(\geq N\) it follows that<br><br><ul><li>\(\left|a_{m+1}+a_{m+2}+\cdots+a_{n}\right|\) &lt; \(\epsilon\)</li></ul>
-
-    After:
-      Theorem 2.7.2 (Cauchy Criterion for Series). The series \(\sum_{k=1}^{\infty} a_{k}\) converges if and only if, given \(\epsilon\) \(&gt;0\), there exists an \(N \in \mathbf{N}\) such that whenever \(n&gt;m\) \(\geq N\) it follows that<br><br><ul><li>\(\left|a_{m+1}+a_{m+2}+\cdots+a_{n}\right|\) &lt; \(\epsilon\)</li></ul>
-
-============================================================
-
-Note ID: 1709817670288
-  Field: Text
-    Before:
-      &nbsp;Theorem 2.7.4 (Comparison Test). Assume \(\left(a_{k}\right)\) and \(\left(b_{k}\right)\) are sequences satisfying \(0\) \(\leq\) \(a_{k}\) \(\leq\) \(b_{k}\) for all \(k \in \mathbf{N}\).<br><br><ul><li>(i) If \(\sum_{k=1}^{\infty}\) \(b_{k}\) converges, then \(\sum_{k=1}^{\infty}\) \(a_{k}\) converges.</li><li>(ii) If \(\sum_{k=1}^{\infty}\) \(a_{k}\) diverges, then \(\sum_{k=1}^{\infty}\) \(b_{k}\) diverges.</li></ul>
-
-    After:
-      &nbsp;Theorem 2.7.4 (Comparison Test). Assume \(\left(a_{k}\right)\) and \(\left(b_{k}\right)\) are sequences satisfying \(0\) \(\leq\) \(a_{k}\) \(\leq\) \(b_{k}\) for all \(k \in \mathbf{N}\).<br><br><ul><li>(i) If \(\sum_{k=1}^{\infty}\) \(b_{k}\) converges, then \(\sum_{k=1}^{\infty}\) \(a_{k}\) converges.</li><li>(ii) If \(\sum_{k=1}^{\infty}\) \(a_{k}\) diverges, then \(\sum_{k=1}^{\infty}\) \(b_{k}\) diverges.</li></ul>
-
-============================================================
-
-Note ID: 1709818085570
-  Field: Text
-    Before:
-      Theorem 2.7.6 (Absolute Convergence Test). If the series \(\sum_{n=1}^{\infty}\)\(\left|a_{n}\right|\) converges, then \(\sum_{n=1}^{\infty}\) \(a_{n}\) converges as well.
-
-    After:
-      Theorem 2.7.6 (Absolute Convergence Test). If the series \(\sum_{n=1}^{\infty}\)\(\left|a_{n}\right|\) converges, then \(\sum_{n=1}^{\infty}\) \(a_{n}\) converges as well.
-
-============================================================
-
-Note ID: 1710095993312
-  Field: Text
-    Before:
-      <b>Definition</b> 2.7.8. If \(\sum_{n=1}^{\infty}\) \(\left|a_{n}\right|\) converges, then we say that the original series \(\sum_{n=1}^{\infty}\) \(a_{n}\) converges absolutely. If, on the other hand, the series \(\sum_{n=1}^{\infty}\) \( a_{n}\) converges but the series \(\sum_{n=1}^{\infty}\) \(\left|a_{n}\right|\) does not converge, then we say that the original series \(\sum_{n=1}^{\infty}\) \(a_{n}\) converges conditionally.
-
-    After:
-      <b>Definition</b> 2.7.8. If \(\sum_{n=1}^{\infty}\) \(\left|a_{n}\right|\) converges, then we say that the original series \(\sum_{n=1}^{\infty}\) \(a_{n}\) converges absolutely. If, on the other hand, the series \(\sum_{n=1}^{\infty}\) \( a_{n}\) converges but the series \(\sum_{n=1}^{\infty}\) \(\left|a_{n}\right|\) does not converge, then we say that the original series \(\sum_{n=1}^{\infty}\) \(a_{n}\) converges conditionally.
-
-============================================================
-
-Note ID: 1710096372026
-  Field: Text
-    Before:
-      <b>Definition</b> 2.7.9. Let \(\sum_{k=1}^{\infty}\) \(a_{k}\) be a series. A series \(\sum_{k=1}^{\infty}\) \(b_{k}\) is called a rearrangement of \(\sum_{k=1}^{\infty}\) \(a_{k}\) if there exists a one-to-one, onto function \(f:\) \(\mathbf{N}\) \(\rightarrow \) \(\mathbf{N}\) such that \(b_{f(k)}\) = \(a_{k}\) for all \(k \in \mathbf{N}\).
-
-    After:
-      <b>Definition</b> 2.7.9. Let \(\sum_{k=1}^{\infty}\) \(a_{k}\) be a series. A series \(\sum_{k=1}^{\infty}\) \(b_{k}\) is called a rearrangement of \(\sum_{k=1}^{\infty}\) \(a_{k}\) if there exists a one-to-one, onto function \(f:\) \(\mathbf{N}\) \(\rightarrow \) \(\mathbf{N}\) such that \(b_{f(k)}\) = \(a_{k}\) for all \(k \in \mathbf{N}\).
-
-============================================================
-
-Note ID: 1710099139055
-  Field: Text
-    Before:
-      Theorem 2.7.10. If a series converges absolutely, then any rearrangement of this series converges to the same limit.<br><br>Proof. Assume \(\sum_{k=1}^{\infty} a_{k}\) converges absolutely to \(A\), and let \(\sum_{k=1}^{\infty} b_{k}\) be a rearrangement of \(\sum_{k=1}^{\infty} a_{k}\). Let's use<br><br><ul><li>\(s_{n}\) = \(\sum_{k=1}^{n} a_{k}\) = \(a_{1}+a_{2}+\cdots+a_{n}\)</li></ul><br>for the partial sums of the original series and use<br><br><ul><li>\(t_{m}\) = \(\sum_{k=1}^{m} b_{k}\) = \(b_{1}+b_{2}+\cdots+b_{m}\)</li></ul><br>for the partial sums of the rearranged series. Thus we want to show that \(\left(t_{m}\right) \rightarrow A\).<br><br>Let \(\epsilon&gt;0\). By hypothesis, \(\left(s_{n}\right) \rightarrow A\), so choose \(N_{1}\) such that<br><br><ul><li>\(\left|s_{n}-A\right|\) &lt; \(\frac{\epsilon}{2}\)</li></ul>for all \(n \geq N_{1}\). <br><br>Because the convergence is absolute, we can choose \(N_{2}\) so that<br><br><ul><li>\(\sum_{k=m+1}^{n}\) \(\left|a_{k}\right|\) &lt; \(\frac{\epsilon}{2}\)</li></ul><br>for all \(n&gt;m \geq N_{2}\). Now, take \(N\) = \(\max \left\{N_{1}, N_{2}\right\}\). We know that the finite set of terms \(\left\{a_{1}, a_{2}, a_{3}, \ldots, a_{N}\right\}\) must all appear in the rearranged series, and we want to move far enough out in the series \(\sum_{n=1}^{\infty} b_{n}\) so that we have included all of these terms. Thus, choose<br><br><ul><li>\(M\) = \(\max \{f(k): 1 \leq k \leq N\}\)</li></ul><br>It should now be evident that if \(m \geq M\), then \(\left(t_{m}-s_{N}\right)\) consists of a finite set of terms, the absolute values of which appear in the tail \(\sum_{k=N+1}^{\infty}\left|a_{k}\right|\). Our choice of \(N_{2}\) earlier then guarantees \(\left|t_{m}-s_{N}\right|\) &lt; \(\epsilon / 2\), and so<br><br><ul><li>\(\left|t_{m}-A\right| \) =&nbsp;</li><li>= \(\left|t_{m}-s_{N}+s_{N}-A\right| \)</li><li>\( \leq\) \(\left|t_{m}-s_{N}\right|+\left|s_{N}-A\right| \)</li><li>\( &lt;\) \(\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon\)</li></ul><br>whenever \(m \geq M\).<br>
-
-    After:
-      Theorem 2.7.10. If a series converges absolutely, then any rearrangement of this series converges to the same limit.<br><br>Proof. Assume \(\sum_{k=1}^{\infty} a_{k}\) converges absolutely to \(A\), and let \(\sum_{k=1}^{\infty} b_{k}\) be a rearrangement of \(\sum_{k=1}^{\infty} a_{k}\). Let's use<br><br><ul><li>\(s_{n}\) = \(\sum_{k=1}^{n} a_{k}\) = \(a_{1}+a_{2}+\cdots+a_{n}\)</li></ul><br>for the partial sums of the original series and use<br><br><ul><li>\(t_{m}\) = \(\sum_{k=1}^{m} b_{k}\) = \(b_{1}+b_{2}+\cdots+b_{m}\)</li></ul><br>for the partial sums of the rearranged series. Thus we want to show that \(\left(t_{m}\right) \rightarrow A\).<br><br>Let \(\epsilon&gt;0\). By hypothesis, \(\left(s_{n}\right) \rightarrow A\), so choose \(N_{1}\) such that<br><br><ul><li>\(\left|s_{n}-A\right|\) &lt; \(\frac{\epsilon}{2}\)</li></ul>for all \(n \geq N_{1}\). <br><br>Because the convergence is absolute, we can choose \(N_{2}\) so that<br><br><ul><li>\(\sum_{k=m+1}^{n}\) \(\left|a_{k}\right|\) &lt; \(\frac{\epsilon}{2}\)</li></ul><br>for all \(n&gt;m \geq N_{2}\). Now, take \(N\) = \(\max \left\{N_{1}, N_{2}\right\}\). We know that the finite set of terms \(\left\{a_{1}, a_{2}, a_{3}, \ldots, a_{N}\right\}\) must all appear in the rearranged series, and we want to move far enough out in the series \(\sum_{n=1}^{\infty} b_{n}\) so that we have included all of these terms. Thus, choose<br><br><ul><li>\(M\) = \(\max \{f(k): 1 \leq k \leq N\}\)</li></ul><br>It should now be evident that if \(m \geq M\), then \(\left(t_{m}-s_{N}\right)\) consists of a finite set of terms, the absolute values of which appear in the tail \(\sum_{k=N+1}^{\infty}\left|a_{k}\right|\). Our choice of \(N_{2}\) earlier then guarantees \(\left|t_{m}-s_{N}\right|\) &lt; \(\epsilon / 2\), and so<br><br><ul><li>\(\left|t_{m}-A\right| \) =&nbsp;</li><li>= \(\left|t_{m}-s_{N}+s_{N}-A\right| \)</li><li>\( \leq\) \(\left|t_{m}-s_{N}\right|+\left|s_{N}-A\right| \)</li><li>\( &lt;\) \(\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon\)</li></ul><br>whenever \(m \geq M\).<br>
-
-============================================================
-
-Note ID: 1710100093282
-  Field: Text
-    Before:
-      <ul><li>Exercise 2.7.7.&nbsp;</li><li>(a) Show that if \(a_{n}\) \(&gt;0\) and \(\lim\) \(\left(n a_{n}\right)\) = \(l\) with \(l\) \(\neq 0\), then the series \(\sum\) \(a_{n}\) diverges.</li><li>(b) Assume \(a_{n}\) \(&gt;0\) and \(\lim\) \(\left(n^{2} a_{n}\right)\) exists. Show that \(\sum\) \(a_{n}\) converges.</li></ul>
-
-    After:
-      <ul><li>Exercise 2.7.7.&nbsp;</li><li>(a) Show that if \(a_{n}\) \(&gt;0\) and \(\lim\) \(\left(n a_{n}\right)\) = \(l\) with \(l\) \(\neq 0\), then the series \(\sum\) \(a_{n}\) diverges.</li><li>(b) Assume \(a_{n}\) \(&gt;0\) and \(\lim\) \(\left(n^{2} a_{n}\right)\) exists. Show that \(\sum\) \(a_{n}\) converges.</li></ul>
-
-============================================================
-
-Note ID: 1710100256259
-  Field: Text
-    Before:
-      Exercise 2.7.9 (Ratio Test). Given a series \(\sum_{n=1}^{\infty} a_{n}\) with \(a_{n}\) \(\neq 0\), the Ratio Test states that if \(\left(a_{n}\right)\) satisfies<br><br><ul><li>\(\lim\) \(\left|\frac{a_{n+1} }{a_{n} }\right|\) = \(r\) \(&lt;\) \(1\)</li></ul><br>then the series converges absolutely.<br>
-
-    After:
-      Exercise 2.7.9 (Ratio Test). Given a series \(\sum_{n=1}^{\infty} a_{n}\) with \(a_{n}\) \(\neq 0\), the Ratio Test states that if \(\left(a_{n}\right)\) satisfies<br><br><ul><li>\(\lim\) \(\left|\frac{a_{n+1} }{a_{n} }\right|\) = \(r\) \(&lt;\) \(1\)</li></ul><br>then the series converges absolutely.<br>
-
-============================================================
-
-Note ID: 1710101735370
-  Field: Text
-    Before:
-      Exercise 2.7.12 (Summation-by-parts). Let \(\left(x_{n}\right)\) and \(\left(y_{n}\right)\) be sequences, let \(s_{n}\) = \(x_{1}+x_{2}+\cdots+x_{n}\) and set \(s_{0}\) = \(0\). Use the observation that \(x_{j}\) = \(s_{j}-s_{j-1}\) to verify the formula<br><ul><li>\(\sum_{j=m}^{n}\)&nbsp; \(x_{j} y_{j}\) = \(s_{n} y_{n+1}\) - \(s_{m-1} y_{m}\) + \(\sum_{j=m}^{n}\) \(s_{j}\) (\(y_{j}-y_{j+1}\))<br></li></ul>
-
-    After:
-      Exercise 2.7.12 (Summation-by-parts). Let \(\left(x_{n}\right)\) and \(\left(y_{n}\right)\) be sequences, let \(s_{n}\) = \(x_{1}+x_{2}+\cdots+x_{n}\) and set \(s_{0}\) = \(0\). Use the observation that \(x_{j}\) = \(s_{j}-s_{j-1}\) to verify the formula<br><ul><li>\(\sum_{j=m}^{n}\)&nbsp; \(x_{j} y_{j}\) = \(s_{n} y_{n+1}\) - \(s_{m-1} y_{m}\) + \(\sum_{j=m}^{n}\) \(s_{j}\) (\(y_{j}-y_{j+1}\))<br></li></ul>
-
-============================================================
-
-Note ID: 1710101872457
-  Field: Text
-    Before:
-      <b>Exercise</b> 2.7.13 (Abel's Test). Abel's Test for convergence states that if the series \(\sum_{k=1}^{\infty}\) \(x_{k}\) converges, and if \(\left(y_{k}\right)\) is a sequence satisfying<br><ul><li>\(y_{1}\) \(\geq\) \(y_{2}\) \(\geq\) \(y_{3}\) \(\geq\) \(\cdots\) \(\geq\) \(0\)<br></li></ul><br>then the series \(\sum_{k=1}^{\infty}\) \(x_{k} y_{k}\) converges.<br>
-
-    After:
-      <b>Exercise</b> 2.7.13 (Abel's Test). Abel's Test for convergence states that if the series \(\sum_{k=1}^{\infty}\) \(x_{k}\) converges, and if \(\left(y_{k}\right)\) is a sequence satisfying<br><ul><li>\(y_{1}\) \(\geq\) \(y_{2}\) \(\geq\) \(y_{3}\) \(\geq\) \(\cdots\) \(\geq\) \(0\)<br></li></ul><br>then the series \(\sum_{k=1}^{\infty}\) \(x_{k} y_{k}\) converges.<br>
-
-============================================================
-
-Note ID: 1710102009664
-  Field: Text
-    Before:
-      Exercise 2.7.13 (Abel's Test). Abel's Test for convergence states that if the series \(\sum_{k=1}^{\infty} x_{k}\) converges, and if \(\left(y_{k}\right)\) is a sequence satisfying<br><br>\[<br>y_{1} \geq y_{2} \geq y_{3} \geq \cdots \geq 0<br>\]<br><br>then the series \(\sum_{k=1}^{\infty} x_{k} y_{k}\) converges.<br><br><ul><li>(a) Use Exercise 2.7.12 to show that</li><ul><li>\(\sum_{k=1}^{n}\) \(x_{k} y_{k}\) = \(s_{n} y_{n+1}\) + \(\sum_{k=1}^{n}\)&nbsp; \(s_{k}\) ( \(y_{k}-y_{k+1}\) )</li><li>where \(s_{n}\) = \(\sum_{i=1}^n x_i\).</li></ul></ul>
-
-    After:
-      Exercise 2.7.13 (Abel's Test). Abel's Test for convergence states that if the series \(\sum_{k=1}^{\infty} x_{k}\) converges, and if \(\left(y_{k}\right)\) is a sequence satisfying<br><br>\[<br>y_{1} \geq y_{2} \geq y_{3} \geq \cdots \geq 0<br>\]<br><br>then the series \(\sum_{k=1}^{\infty} x_{k} y_{k}\) converges.<br><br><ul><li>(a) Use Exercise 2.7.12 to show that</li><ul><li>\(\sum_{k=1}^{n}\) \(x_{k} y_{k}\) = \(s_{n} y_{n+1}\) + \(\sum_{k=1}^{n}\)&nbsp; \(s_{k}\) ( \(y_{k}-y_{k+1}\) )</li><li>where \(s_{n}\) = \(\sum_{i=1}^n x_i\).</li></ul></ul>
-
-============================================================
-
-Note ID: 1710102304544
-  Field: Text
-    Before:
-      Exercise 2.7.14 (Dirichlet's Test). Dirichlet's Test for convergence states that:<br><ul><li>If the partial sums of \(\sum_{k=1}^{\infty}\) \(x_{k}\) are bounded (but not necessarily convergent),&nbsp;</li><li>And if ( \(\left.y_{k}\right)\) is a sequence satisfying \(y_{1}\) \(\geq\) \(y_{2}\) \(\geq\) \(y_{3}\) \(\geq\) \(\cdots\) \(\geq\) \(0\) with \(\lim y_{k}\) = \(0\)</li><li>Then the series \(\sum_{k=1}^{\infty}\) \(x_{k} y_{k}\) converges.</li></ul>
-
-    After:
-      Exercise 2.7.14 (Dirichlet's Test). Dirichlet's Test for convergence states that:<br><ul><li>If the partial sums of \(\sum_{k=1}^{\infty}\) \(x_{k}\) are bounded (but not necessarily convergent),&nbsp;</li><li>And if ( \(\left.y_{k}\right)\) is a sequence satisfying \(y_{1}\) \(\geq\) \(y_{2}\) \(\geq\) \(y_{3}\) \(\geq\) \(\cdots\) \(\geq\) \(0\) with \(\lim y_{k}\) = \(0\)</li><li>Then the series \(\sum_{k=1}^{\infty}\) \(x_{k} y_{k}\) converges.</li></ul>
-
-============================================================
-
-Note ID: 1710102693834
-  Field: Text
-    Before:
-      There are still other ways to reasonably define \(\sum_{i, j=1}^{\infty} a_{i j}\). One natural idea is to calculate a kind of partial sum by adding together finite numbers of terms in larger and larger "rectangles" in the array; that is, for \(m, n \in \mathbf{N}\), set<br><br>\[<br>\begin{equation*}<br>s_{m n}=\sum_{i=1}^{m} \sum_{j=1}^{n} a_{i j} \tag{1}<br>\end{equation*}<br>\]<br><br>The order of the sum here is irrelevant because the sum is finite. Of particular interest to our discussion are the sums \(s_{n n}\) (sums over "squares"), which form a legitimate sequence indexed by \(n\) and thus can be subjected to our arsenal of theorems and definitions. If the sequence \(\left(s_{n n}\right)\) converges, for instance, we might wish to define<br><br><ul><li>\(\sum_{i, j=1}^{\infty} a_{i j}\) = \(\lim _{n \rightarrow \infty}\) \(s_{n n}\)</li></ul>
-
-    After:
-      There are still other ways to reasonably define \(\sum_{i, j=1}^{\infty} a_{i j}\). One natural idea is to calculate a kind of partial sum by adding together finite numbers of terms in larger and larger "rectangles" in the array; that is, for \(m, n \in \mathbf{N}\), set<br><br>\[<br>\begin{equation*}<br>s_{m n}=\sum_{i=1}^{m} \sum_{j=1}^{n} a_{i j} \tag{1}<br>\end{equation*}<br>\]<br><br>The order of the sum here is irrelevant because the sum is finite. Of particular interest to our discussion are the sums \(s_{n n}\) (sums over "squares"), which form a legitimate sequence indexed by \(n\) and thus can be subjected to our arsenal of theorems and definitions. If the sequence \(\left(s_{n n}\right)\) converges, for instance, we might wish to define<br><br><ul><li>\(\sum_{i, j=1}^{\infty} a_{i j}\) = \(\lim _{n \rightarrow \infty}\) \(s_{n n}\)</li></ul>
-
-============================================================
-
-Note ID: 1710102813119
-  Field: Text
-    Before:
-      There is a deep similarity between the issue of how to define a double summation and the topic of rearrangements. Both relate to the commutativity of addition in an infinite setting. For rearrangements, the resolution came with the added hypothesis of absolute convergence, and it is not surprising that the same remedy applies for double summations.
-
-    After:
-      There is a deep similarity between the issue of how to define a double summation and the topic of rearrangements. Both relate to the commutativity of addition in an infinite setting. For rearrangements, the resolution came with the added hypothesis of absolute convergence, and it is not surprising that the same remedy applies for double summations.
-
-============================================================
-
-Note ID: 1710102886436
-  Field: Text
-    Before:
-      Under the assumption of absolute convergence taking the limit over rows then column, columns then rows or over squares gives the same result for double summation
-
-    After:
-      Under the assumption of absolute convergence taking the limit over rows then column, columns then rows or over squares gives the same result for double summation
-
-============================================================
-
-Note ID: 1710103305623
-  Field: Text
-    Before:
-      Theorem 2.8.1. Let \(\left\{a_{i j}: i, j \in \mathbf{N}\right\}\) be a doubly indexed array of real numbers. If<br><br><ul><li>\(\sum_{i=1}^{\infty}\) \(\sum_{j=1}^{\infty}\) \(\left|a_{i j}\right|\)</li></ul><br>converges, then both \(\sum_{i=1}^{\infty}\) \(\sum_{j=1}^{\infty}\) \(a_{i j}\) and \(\sum_{j=1}^{\infty}\) \(\sum_{i=1}^{\infty}\) \(a_{i j}\) converge to the same value. Moreover,<br><br><ul><li>\(\lim _{n \rightarrow \infty}\) \(s_{n n}\)&nbsp; =</li><li>= \(\sum_{i=1}^{\infty} \sum_{j=1}^{\infty}\) \(a_{i j}\)&nbsp;</li><li>= \(\sum_{j=1}^{\infty} \sum_{i=1}^{\infty}\) \(a_{i j}\)</li></ul><br>where \(s_{n n}\) = \(\sum_{i=1}^{n} \sum_{j=1}^{n} a_{i j}\).<br>
-
-    After:
-      Theorem 2.8.1. Let \(\left\{a_{i j}: i, j \in \mathbf{N}\right\}\) be a doubly indexed array of real numbers. If<br><br><ul><li>\(\sum_{i=1}^{\infty}\) \(\sum_{j=1}^{\infty}\) \(\left|a_{i j}\right|\)</li></ul><br>converges, then both \(\sum_{i=1}^{\infty}\) \(\sum_{j=1}^{\infty}\) \(a_{i j}\) and \(\sum_{j=1}^{\infty}\) \(\sum_{i=1}^{\infty}\) \(a_{i j}\) converge to the same value. Moreover,<br><br><ul><li>\(\lim _{n \rightarrow \infty}\) \(s_{n n}\)&nbsp; =</li><li>= \(\sum_{i=1}^{\infty} \sum_{j=1}^{\infty}\) \(a_{i j}\)&nbsp;</li><li>= \(\sum_{j=1}^{\infty} \sum_{i=1}^{\infty}\) \(a_{i j}\)</li></ul><br>where \(s_{n n}\) = \(\sum_{i=1}^{n} \sum_{j=1}^{n} a_{i j}\).<br>
-
-============================================================
-
-Note ID: 1710227413350
-  Field: Text
-    Before:
-      One final common way of computing a double summation is to sum along diagonals where \(i+j\) equals a constant. Given a doubly indexed array \(\left\{a_{i j}\right.\) : \(i, j \in \mathbf{N}\}\), let<br><br><ul><li>\(d_{k}\) = \(a_{1, k-1}\) + \(a_{2, k-2}\) +\(\cdots\)+ \(a_{k-1,1} .\)</li></ul><br>Then, \(\sum_{k=2}^{\infty}\) \(d_{k}\) represents another reasonable way of summing over every \(a_{i j}\) in the array.<br>
-
-    After:
-      One final common way of computing a double summation is to sum along diagonals where \(i+j\) equals a constant. Given a doubly indexed array \(\left\{a_{i j}\right.\) : \(i, j \in \mathbf{N}\}\), let<br><br><ul><li>\(d_{k}\) = \(a_{1, k-1}\) + \(a_{2, k-2}\) +\(\cdots\)+ \(a_{k-1,1} .\)</li></ul><br>Then, \(\sum_{k=2}^{\infty}\) \(d_{k}\) represents another reasonable way of summing over every \(a_{i j}\) in the array.<br>
-
-============================================================
-
-Note ID: 1710228396841
-  Field: Text
-    Before:
-      Exercise 2.8.7. Assume that \(\sum_{i=1}^{\infty} a_{i}\) converges absolutely to \(A\), and \(\sum_{j=1}^{\infty} b_{j}\) converges absolutely to \(B\).<br><br><br>(a) Show that the iterated sum \(\sum_{i=1}^{\infty} \sum_{j=1}^{\infty}\) \(\left|a_{i} b_{j}\right|\) converges so that we may apply Theorem 2.8.1.<br><br>(b) Let \(s_{n n}\) =\(\sum_{i=1}^{n}\) \(\sum_{j=1}^{n}\) \(a_{i} b_{j}\), and prove that \(\lim _{n \rightarrow \infty} s_{n n}\) = \(A B\). Conclude that<br><br><ul><li>\(\sum_{i=1}^{\infty} \sum_{j=1}^{\infty}\) \(a_{i} b_{j}\)&nbsp;</li><li>= \(\sum_{j=1}^{\infty} \sum_{i=1}^{\infty}\) \(a_{i} b_{j} \)</li><li>= \(\sum_{k=2}^{\infty}\) \(d_{k}\)&nbsp;</li><li>= \(A B\)</li></ul><br>where, as before, \(d_{k}\) = \(a_{1} b_{k-1}+a_{2} b_{k-2}+\cdots+a_{k-1} b_{1}\).<br>
-
-    After:
-      Exercise 2.8.7. Assume that \(\sum_{i=1}^{\infty} a_{i}\) converges absolutely to \(A\), and \(\sum_{j=1}^{\infty} b_{j}\) converges absolutely to \(B\).<br><br><br>(a) Show that the iterated sum \(\sum_{i=1}^{\infty} \sum_{j=1}^{\infty}\) \(\left|a_{i} b_{j}\right|\) converges so that we may apply Theorem 2.8.1.<br><br>(b) Let \(s_{n n}\) =\(\sum_{i=1}^{n}\) \(\sum_{j=1}^{n}\) \(a_{i} b_{j}\), and prove that \(\lim _{n \rightarrow \infty} s_{n n}\) = \(A B\). Conclude that<br><br><ul><li>\(\sum_{i=1}^{\infty} \sum_{j=1}^{\infty}\) \(a_{i} b_{j}\)&nbsp;</li><li>= \(\sum_{j=1}^{\infty} \sum_{i=1}^{\infty}\) \(a_{i} b_{j} \)</li><li>= \(\sum_{k=2}^{\infty}\) \(d_{k}\)&nbsp;</li><li>= \(A B\)</li></ul><br>where, as before, \(d_{k}\) = \(a_{1} b_{k-1}+a_{2} b_{k-2}+\cdots+a_{k-1} b_{1}\).<br>
-
-============================================================
-
-Note ID: 1710228584504
-  Field: Text
-    Before:
-      In the case of rearrangements, not only are conditionally convergent series no longer guaranteed to converge to the same limit, but in fact if \(\sum_{n=1}^{\infty} a_{n}\) converges conditionally, then for any \(r \in \mathbf{R}\) there exists a rearrangement of \(\sum_{n=1}^{\infty} a_{n}\) that converges to \(r\).&nbsp;
-
-    After:
-      In the case of rearrangements, not only are conditionally convergent series no longer guaranteed to converge to the same limit, but in fact if \(\sum_{n=1}^{\infty} a_{n}\) converges conditionally, then for any \(r \in \mathbf{R}\) there exists a rearrangement of \(\sum_{n=1}^{\infty} a_{n}\) that converges to \(r\).&nbsp;
-
-============================================================
-
-Note ID: 1710228871987
-  Field: Text
-    Before:
-      Perhaps the best way to summarize the situation is to say that the hypothesis of absolute convergence essentially allows us to treat infinite sums as though they were finite sums. This assessment extends to double sums as well
-
-    After:
-      Perhaps the best way to summarize the situation is to say that the hypothesis of absolute convergence essentially allows us to treat infinite sums as though they were finite sums. This assessment extends to double sums as well
-
-============================================================
-
-Note ID: 1710228980406
-  Field: Text
-    Before:
-      We showed in Exercise 2.8.7 that the Cauchy product of two absolutely convergent infinite series converges to the product of the two factors, but in fact the same conclusion follows if we only have absolute convergence in one of the two original series. In the notation of Exercise 2.8.7, if \(\sum a_{n}\) converges absolutely to \(A\), and if \(\sum b_{n}\) converges&nbsp; to \(B\), then the Cauchy product \(\sum d_{k}\) = \(A B\).
-
-    After:
-      We showed in Exercise 2.8.7 that the Cauchy product of two absolutely convergent infinite series converges to the product of the two factors, but in fact the same conclusion follows if we only have absolute convergence in one of the two original series. In the notation of Exercise 2.8.7, if \(\sum a_{n}\) converges absolutely to \(A\), and if \(\sum b_{n}\) converges&nbsp; to \(B\), then the Cauchy product \(\sum d_{k}\) = \(A B\).
-
-============================================================
-
-Note ID: 1710229072640
-  Field: Text
-    Before:
-      Of course, it is also possible to find \(\sum a_{n}\) = \(A\) conditionally and \(\sum b_{n}\) = \(B\) conditionally whose Cauchy product \(\sum d_{k}\) converges. If this is the case, then the convergence is to the right value, namely \(\sum d_{k}\) = \(A B\).
-
-    After:
-      Of course, it is also possible to find \(\sum a_{n}\) = \(A\) conditionally and \(\sum b_{n}\) = \(B\) conditionally whose Cauchy product \(\sum d_{k}\) converges. If this is the case, then the convergence is to the right value, namely \(\sum d_{k}\) = \(A B\).
-
-============================================================
-
-Note ID: 1710229470854
-  Field: Text
-    Before:
-      Let \(C_{0}\) be the closed interval \([0,1]\), and define \(C_{1}\) to be the set that results when the open middle third is removed; that is,<br><br><ul><li>\(C_{1}\) =</li><li>= \(C_{0} \backslash\left(\frac{1}{3}, \frac{2}{3}\right)\)&nbsp;</li><li>= \(\left[0, \frac{1}{3}\right] \cup\left[\frac{2}{3}, 1\right]\)</li></ul><br>Now, construct \(C_{2}\) in a similar way by removing the open middle third of each of the two components of \(C_{1}\) :<br><br><ul><li>\(C_{2}\) =&nbsp;</li><li>\(\left(\left[0, \frac{1}{9}\right] \cup\left[\frac{2}{9}, \frac{1}{3}\right]\right) \cup\left(\left[\frac{2}{3}, \frac{7}{9}\right] \cup\left[\frac{8}{9}, 1\right]\right) .\)</li></ul><br>If we continue this process inductively, then for each \(n=0,1,2, \ldots\) we get a set \(C_{n}\) consisting of \(2^{n}\)::how many?::how many?::how many? closed intervals each having length \(1 / 3^{n}\). Finally, we define the Cantor set \(C\) (Fig. 3.1) to be the intersection<br><ul><li>\(C\) = \(\bigcap_{n=0}^{\infty}\) \(C_{n}\)</li></ul>
-
-    After:
-      Let \(C_{0}\) be the closed interval \([0,1]\), and define \(C_{1}\) to be the set that results when the open middle third is removed; that is,<br><br><ul><li>\(C_{1}\) =</li><li>= \(C_{0} \backslash\left(\frac{1}{3}, \frac{2}{3}\right)\)&nbsp;</li><li>= \(\left[0, \frac{1}{3}\right] \cup\left[\frac{2}{3}, 1\right]\)</li></ul><br>Now, construct \(C_{2}\) in a similar way by removing the open middle third of each of the two components of \(C_{1}\) :<br><br><ul><li>\(C_{2}\) =&nbsp;</li><li>\(\left(\left[0, \frac{1}{9}\right] \cup\left[\frac{2}{9}, \frac{1}{3}\right]\right) \cup\left(\left[\frac{2}{3}, \frac{7}{9}\right] \cup\left[\frac{8}{9}, 1\right]\right) .\)</li></ul><br>If we continue this process inductively, then for each \(n=0,1,2, \ldots\) we get a set \(C_{n}\) consisting of \(2^{n}\)::how many?::how many?::how many? closed intervals each having length \(1 / 3^{n}\). Finally, we define the Cantor set \(C\) (Fig. 3.1) to be the intersection<br><ul><li>\(C\) = \(\bigcap_{n=0}^{\infty}\) \(C_{n}\)</li></ul>
-
-============================================================
-
-Note ID: 1710229660533
-  Field: Text
-    Before:
-      It may be useful to understand the cantor set \(C\) as the remainder of the interval \([0,1]\) after the iterative process of removing open middle thirds is taken to infinity:<br><br><ul><li>\(C\) = \([0,1]\) \(\backslash \) [\(\left(\frac{1}{3}, \frac{2}{3}\right)\)&nbsp; \(\cup\) \(\left(\frac{1}{9}, \frac{2}{9}\right)\)&nbsp; \(\cup\) \(\left(\frac{7}{9}, \frac{8}{9}\right)\) \(\cup\) \(\cdots\) ]</li></ul>
-
-    After:
-      It may be useful to understand the cantor set \(C\) as the remainder of the interval \([0,1]\) after the iterative process of removing open middle thirds is taken to infinity:<br><br><ul><li>\(C\) = \([0,1]\) \(\backslash \) [\(\left(\frac{1}{3}, \frac{2}{3}\right)\)&nbsp; \(\cup\) \(\left(\frac{1}{9}, \frac{2}{9}\right)\)&nbsp; \(\cup\) \(\left(\frac{7}{9}, \frac{8}{9}\right)\) \(\cup\) \(\cdots\) ]</li></ul>
-
-============================================================
-
-Note ID: 1710229944618
-  Field: Text
-    Before:
-      <img src="paste-5b106ae2d07c386ea9fa88da651c450b6caba5f6.jpg"><br>Figure 3.1: Defining the Cantor set; \(C=\bigcap_{n=0}^{\infty} C_{n}\).<br><br>It may be useful to understand \(C\) as the remainder of the interval \([0,1]\) after the iterative process of removing open middle thirds is taken to infinity:<br><br>\[<br>C=[0,1] \backslash\left[\left(\frac{1}{3}, \frac{2}{3}\right) \cup\left(\frac{1}{9}, \frac{2}{9}\right) \cup\left(\frac{7}{9}, \frac{8}{9}\right) \cup \cdots\right] .<br>\]<br><br><ul><li>There is some initial doubt whether anything remains at all,&nbsp;</li><li>But notice that because we are always removing open middle thirds, then for every \(n \in \mathbf{N}\), \(0\) \(\in C_{n}\) and hence \(0\) \(\in C\).&nbsp;</li><li>The same argument shows \(1\) \(\in C\).&nbsp;</li><li>In fact, if \(y\) is the endpoint of some closed interval of some particular set \(C_{n}\), then it is also an endpoint of one of the intervals of \(C_{n+1}\).&nbsp;</li><li>Because, at each stage, endpoints are never removed, it follows that \(y\) \(\in C_{n}\) for all \(n\). Thus, \(C\) at least contains the endpoints of all of the intervals that make up each of the sets \(C_{n}\).</li></ul>
-
-    After:
-      <img src="paste-5b106ae2d07c386ea9fa88da651c450b6caba5f6.jpg"><br>Figure 3.1: Defining the Cantor set; \(C=\bigcap_{n=0}^{\infty} C_{n}\).<br><br>It may be useful to understand \(C\) as the remainder of the interval \([0,1]\) after the iterative process of removing open middle thirds is taken to infinity:<br><br>\[<br>C=[0,1] \backslash\left[\left(\frac{1}{3}, \frac{2}{3}\right) \cup\left(\frac{1}{9}, \frac{2}{9}\right) \cup\left(\frac{7}{9}, \frac{8}{9}\right) \cup \cdots\right] .<br>\]<br><br><ul><li>There is some initial doubt whether anything remains at all,&nbsp;</li><li>But notice that because we are always removing open middle thirds, then for every \(n \in \mathbf{N}\), \(0\) \(\in C_{n}\) and hence \(0\) \(\in C\).&nbsp;</li><li>The same argument shows \(1\) \(\in C\).&nbsp;</li><li>In fact, if \(y\) is the endpoint of some closed interval of some particular set \(C_{n}\), then it is also an endpoint of one of the intervals of \(C_{n+1}\).&nbsp;</li><li>Because, at each stage, endpoints are never removed, it follows that \(y\) \(\in C_{n}\) for all \(n\). Thus, \(C\) at least contains the endpoints of all of the intervals that make up each of the sets \(C_{n}\).</li></ul>
-
-============================================================
-
-Note ID: 1710230413421
-  Field: Text
-    Before:
-      It may be useful to understand \(C\) as the remainder of the interval \([0,1]\) after the iterative process of removing open middle thirds is taken to infinity:<br><br>\[<br>C=[0,1] \backslash\left[\left(\frac{1}{3}, \frac{2}{3}\right) \cup\left(\frac{1}{9}, \frac{2}{9}\right) \cup\left(\frac{7}{9}, \frac{8}{9}\right) \cup \cdots\right] .<br>\]<br><br>There is some strong evidence that not much is left in \(C\) if we consider the total length of the intervals removed. To form \(C_{1}\), an open interval of length \(1 / 3\) was taken out. In the second step, we removed two intervals of length \(1 / 9\), and to construct \(C_{n}\) we removed \(2^{n-1}\) middle thirds of length \(1 / 3^{n}\). There is some logic, then, to defining the "length" of \(C\) to be 1 minus the total<br><br><ul><li>\(1\) - \(\sum_{I=1}^{n}\) \(2^{i-1} \frac{1}{3^i}\) = \(1\) - \(\frac{\frac{1}{3} }{1-\frac{2}{3} }\) = \(0\)</li></ul><br>The Cantor set has zero length.<br>
-
-    After:
-      It may be useful to understand \(C\) as the remainder of the interval \([0,1]\) after the iterative process of removing open middle thirds is taken to infinity:<br><br>\[<br>C=[0,1] \backslash\left[\left(\frac{1}{3}, \frac{2}{3}\right) \cup\left(\frac{1}{9}, \frac{2}{9}\right) \cup\left(\frac{7}{9}, \frac{8}{9}\right) \cup \cdots\right] .<br>\]<br><br>There is some strong evidence that not much is left in \(C\) if we consider the total length of the intervals removed. To form \(C_{1}\), an open interval of length \(1 / 3\) was taken out. In the second step, we removed two intervals of length \(1 / 9\), and to construct \(C_{n}\) we removed \(2^{n-1}\) middle thirds of length \(1 / 3^{n}\). There is some logic, then, to defining the "length" of \(C\) to be 1 minus the total<br><br><ul><li>\(1\) - \(\sum_{I=1}^{n}\) \(2^{i-1} \frac{1}{3^i}\) = \(1\) - \(\frac{\frac{1}{3} }{1-\frac{2}{3} }\) = \(0\)</li></ul><br>The Cantor set has zero length.<br>
-
-============================================================
-
-Note ID: 1710230523929
-  Field: Text
-    Before:
-      The cantor set:<br>\[<br>C=[0,1] \backslash\left[\left(\frac{1}{3}, \frac{2}{3}\right) \cup\left(\frac{1}{9}, \frac{2}{9}\right) \cup\left(\frac{7}{9}, \frac{8}{9}\right) \cup \cdots\right] .<br>\]<br><br>To this point, the information we have collected suggests a mental picture of \(C\) as a relatively small, thin set. For these reasons, the set \(C\) is often referred to as Cantor "dust." But there are some strong counterarguments that imply a very different picture. However, \(C\) is actually uncountable, with cardinality equal to the cardinality of \(\mathbf{R}\).
-
-    After:
-      The cantor set:<br>\[<br>C=[0,1] \backslash\left[\left(\frac{1}{3}, \frac{2}{3}\right) \cup\left(\frac{1}{9}, \frac{2}{9}\right) \cup\left(\frac{7}{9}, \frac{8}{9}\right) \cup \cdots\right] .<br>\]<br><br>To this point, the information we have collected suggests a mental picture of \(C\) as a relatively small, thin set. For these reasons, the set \(C\) is often referred to as Cantor "dust." But there are some strong counterarguments that imply a very different picture. However, \(C\) is actually uncountable, with cardinality equal to the cardinality of \(\mathbf{R}\).
-
-============================================================
-
-Note ID: 1710235274428
-  Field: Text
-    Before:
-      &nbsp;First, \(C\) is actually uncountable, with cardinality equal to the cardinality of \(\mathbf{R}\). One slightly intuitive but convincing way to see this is to create a 1-1 correspondence between \(C\) and sequences of the form \(\left(a_{n}\right)_{n=1}^{\infty}\), where \(a_{n}\) = (\(0\) or 1) . For each \(c \in C\), set \(a_{1}\) = \(0\) if \(c\) falls in the left-hand component of \(C_{1}\) and set \(a_{1}\) = \(1\) if \(c\) falls in the right-hand component. Having established where in \(C_{1}\) the point \(c\) is located, there are now two possible components of \(C_{2}\) that might contain \(c\). This time, we set \(a_{2}\) = \(0\) or 1 depending on whether \(c\) falls in the left or right half of these two components of \(C_{2}\). Continuing in this way, we come to see that every element \(c \in C\) yields a sequence \(\left(a_{1}, a_{2}, a_{3}, \ldots\right)\) of zeros and ones that acts as a set of directions for how to locate \(c\) within \(C\). Likewise, every such sequence corresponds to a point in the Cantor set. Because the set of sequences of zeros and ones is uncountable (Exercise 1.6.4), we must conclude that \(C\) is uncountable as well.
-
-    After:
-      &nbsp;First, \(C\) is actually uncountable, with cardinality equal to the cardinality of \(\mathbf{R}\). One slightly intuitive but convincing way to see this is to create a 1-1 correspondence between \(C\) and sequences of the form \(\left(a_{n}\right)_{n=1}^{\infty}\), where \(a_{n}\) = (\(0\) or 1) . For each \(c \in C\), set \(a_{1}\) = \(0\) if \(c\) falls in the left-hand component of \(C_{1}\) and set \(a_{1}\) = \(1\) if \(c\) falls in the right-hand component. Having established where in \(C_{1}\) the point \(c\) is located, there are now two possible components of \(C_{2}\) that might contain \(c\). This time, we set \(a_{2}\) = \(0\) or 1 depending on whether \(c\) falls in the left or right half of these two components of \(C_{2}\). Continuing in this way, we come to see that every element \(c \in C\) yields a sequence \(\left(a_{1}, a_{2}, a_{3}, \ldots\right)\) of zeros and ones that acts as a set of directions for how to locate \(c\) within \(C\). Likewise, every such sequence corresponds to a point in the Cantor set. Because the set of sequences of zeros and ones is uncountable (Exercise 1.6.4), we must conclude that \(C\) is uncountable as well.
-
-============================================================
-
-Note ID: 1710279502488
-  Field: Text
-    Before:
-      Given \(a \in \mathbf{R}\) and \(\epsilon&gt;0\), recall that the \(\epsilon\)-neighborhood of \(a\) is the set<br><br><ul><li>\(V_{\epsilon}(a)\) = \(\{x \in \mathbf{R}:|x-a|&lt;\epsilon\} .\)</li></ul><br>In other words, \(V_{\epsilon}(a)\) is the open interval \((a-\epsilon, a+\epsilon)\), centered at \(a\) with radius \(\epsilon\).<br>
-
-    After:
-      Given \(a \in \mathbf{R}\) and \(\epsilon&gt;0\), recall that the \(\epsilon\)-neighborhood of \(a\) is the set<br><br><ul><li>\(V_{\epsilon}(a)\) = \(\{x \in \mathbf{R}:|x-a|&lt;\epsilon\} .\)</li></ul><br>In other words, \(V_{\epsilon}(a)\) is the open interval \((a-\epsilon, a+\epsilon)\), centered at \(a\) with radius \(\epsilon\).<br>
-
-============================================================
-
-Note ID: 1710279643226
-  Field: Text
-    Before:
-      Example 3.2.2. (i) Perhaps the simplest example of an open set is \(\mathbf{R}\) itself. Given an arbitrary element \(a \in \mathbf{R}\), we are free to pick any \(\epsilon\)-neighborhood we like and it will always be true that \(V_{\epsilon}(a)\)&nbsp; \(\subseteq \mathbf{R}\). It is also the case that the logical structure of Definition 3.2.1 requires us to classify the empty set \(\emptyset\) as an open subset of the real line.
-
-    After:
-      Example 3.2.2. (i) Perhaps the simplest example of an open set is \(\mathbf{R}\) itself. Given an arbitrary element \(a \in \mathbf{R}\), we are free to pick any \(\epsilon\)-neighborhood we like and it will always be true that \(V_{\epsilon}(a)\)&nbsp; \(\subseteq \mathbf{R}\). It is also the case that the logical structure of Definition 3.2.1 requires us to classify the empty set \(\emptyset\) as an open subset of the real line.
-
-============================================================
-
-Note ID: 1710279861044
-  Field: Text
-    Before:
-      <b>Theorem</b> 3.2.3. <br><ul><li>(i) The union of an arbitrary collection of open sets is open.</li><li>(ii) The intersection of a finite collection of open sets is open.</li></ul>
-
-    After:
-      <b>Theorem</b> 3.2.3. <br><ul><li>(i) The union of an arbitrary collection of open sets is open.</li><li>(ii) The intersection of a finite collection of open sets is open.</li></ul>
-
-============================================================
-
-Note ID: 1710281096680
-  Field: Text
-    Before:
-      Theorem 3.2.5. \(A\) point \(x\) is a limit point of a set \(A\) if and only if \(x=\lim a_{n}\) for some sequence \(\left(a_{n}\right)\) contained in \(A\) satisfying \(a_{n} \neq x\) for all \(n \in \mathbf{N}\).<br><br>The restriction that \(a_{n} \neq x\) in Theorem 3.2.5 deserves a comment. Given a point \(a \in A\), it is always the case that \(a\) is the limit of a sequence in \(A\) if we are allowed to consider the constant sequence \((a, a, a, \ldots)\). There will be occasions where we will want to avoid this somewhat uninteresting situation, so it is important to have a vocabulary that can distinguish limit points of a set from isolated points.
-
-    After:
-      Theorem 3.2.5. \(A\) point \(x\) is a limit point of a set \(A\) if and only if \(x=\lim a_{n}\) for some sequence \(\left(a_{n}\right)\) contained in \(A\) satisfying \(a_{n} \neq x\) for all \(n \in \mathbf{N}\).<br><br>The restriction that \(a_{n} \neq x\) in Theorem 3.2.5 deserves a comment. Given a point \(a \in A\), it is always the case that \(a\) is the limit of a sequence in \(A\) if we are allowed to consider the constant sequence \((a, a, a, \ldots)\). There will be occasions where we will want to avoid this somewhat uninteresting situation, so it is important to have a vocabulary that can distinguish limit points of a set from isolated points.
-
-============================================================
-
-Note ID: 1710323391531
-  Field: Text
-    Before:
-      Example 3.2.9. (i) Consider<br><br>\[<br>A=\left\{\frac{1}{n}: n \in \mathbf{N}\right\}<br>\]<br><br>Let's show that each point of \(A\) is isolated. Given \(1 / n \in A\), choose \(\epsilon\) =\(1 / n-1 /(n+1)\). Then,<br><br><ul><li>\(V_{\epsilon}(1 / n) \cap A\) = \(\left\{\frac{1}{n}\right\}\)</li></ul><br>It follows from Definition 3.2.4 that \(1 / n\) is not a limit point and so is isolated. Although all of the points of \(A\) are isolated, the set does have&nbsp;<br>one limit point, namely 0 . This is because every neighborhood centered at zero, no matter how small, is going to contain points of \(A\). Because \(0\) \(\notin A, A\) is not closed. The set \(F\) = \(A \cup\{0\}\) is an example of a closed set and is called the closure of \(A\)<br><br>
-
-    After:
-      Example 3.2.9. (i) Consider<br><br>\[<br>A=\left\{\frac{1}{n}: n \in \mathbf{N}\right\}<br>\]<br><br>Let's show that each point of \(A\) is isolated. Given \(1 / n \in A\), choose \(\epsilon\) =\(1 / n-1 /(n+1)\). Then,<br><br><ul><li>\(V_{\epsilon}(1 / n) \cap A\) = \(\left\{\frac{1}{n}\right\}\)</li></ul><br>It follows from Definition 3.2.4 that \(1 / n\) is not a limit point and so is isolated. Although all of the points of \(A\) are isolated, the set does have&nbsp;<br>one limit point, namely 0 . This is because every neighborhood centered at zero, no matter how small, is going to contain points of \(A\). Because \(0\) \(\notin A, A\) is not closed. The set \(F\) = \(A \cup\{0\}\) is an example of a closed set and is called the closure of \(A\)<br><br>
-
-============================================================
-
-Note ID: 1710323659229
-  Field: Text
-    Before:
-      (ii) Let's prove that a closed interval<br><br>\[<br>[c, d]=\{x \in \mathbf{R}: c \leq x \leq d\}<br>\]<br><br>is a closed set using Definition 3.2.7. If \(x\) is a limit point of \([c, d]\), then by Theorem 3.2.5 there exists \(\left(x_{n}\right)\) \(\subseteq[c, d]\) with \(\left(x_{n}\right) \rightarrow x\). We need to prove that \(x \in[c, d]\).<br><br>The key to this argument is contained in the Order Limit Theorem (Theorem 2.3.4), which summarizes the relationship between inequalities and the limiting process. Because \(c\) \(\leq\) \(x_{n}\) \(\leq\) \(d\), it follows from Theorem 2.3.4 (iii) that \(c\) \(\leq\) \(x\) \(\leq\) \(d\).<div>Thus, \([c, d]\) is closed.</div>
-
-    After:
-      (ii) Let's prove that a closed interval<br><br>\[<br>[c, d]=\{x \in \mathbf{R}: c \leq x \leq d\}<br>\]<br><br>is a closed set using Definition 3.2.7. If \(x\) is a limit point of \([c, d]\), then by Theorem 3.2.5 there exists \(\left(x_{n}\right)\) \(\subseteq[c, d]\) with \(\left(x_{n}\right) \rightarrow x\). We need to prove that \(x \in[c, d]\).<br><br>The key to this argument is contained in the Order Limit Theorem (Theorem 2.3.4), which summarizes the relationship between inequalities and the limiting process. Because \(c\) \(\leq\) \(x_{n}\) \(\leq\) \(d\), it follows from Theorem 2.3.4 (iii) that \(c\) \(\leq\) \(x\) \(\leq\) \(d\).<div>Thus, \([c, d]\) is closed.</div>
-
-============================================================
-
-Note ID: 1710371071554
-  Field: Text
-    Before:
-      <b>Theorem</b> 3.2.12. For any \(A \subseteq \mathbf{R}\), the closure::name::name::name::name \(\bar{A}\)::notation::notation::notation::notation is a closed set and is the smallest closed set containing \(A\).
-
-    After:
-      <b>Theorem</b> 3.2.12. For any \(A \subseteq \mathbf{R}\), the closure::name::name::name::name \(\bar{A}\)::notation::notation::notation::notation is a closed set and is the smallest closed set containing \(A\).
-
-============================================================
-
-Note ID: 1710371185580
-  Field: Text
-    Before:
-      Theorem 3.2.12. For any \(A \subseteq \mathbf{R}\), the closure \(\bar{A}\) is a closed set and is the smallest closed set containing \(A\).<br><br>Proof. If \(L\) is the set of limit points of \(A\), then it is immediately clear that \(\bar{A}\) contains the limit points of \(A\). There is still something more to prove, however, because taking the union of \(L\) with \(A\) could potentially produce some new limit points of \(\bar{A}\). In Exercise 3.2.7, we outline the argument that this does not happen.<br><br>Now, any closed set containing \(A\) must contain \(L\) as well. This shows that \(\bar{A}\) = \(A \cup L\) is the smallest closed set containing \(A\).
-
-    After:
-      Theorem 3.2.12. For any \(A \subseteq \mathbf{R}\), the closure \(\bar{A}\) is a closed set and is the smallest closed set containing \(A\).<br><br>Proof. If \(L\) is the set of limit points of \(A\), then it is immediately clear that \(\bar{A}\) contains the limit points of \(A\). There is still something more to prove, however, because taking the union of \(L\) with \(A\) could potentially produce some new limit points of \(\bar{A}\). In Exercise 3.2.7, we outline the argument that this does not happen.<br><br>Now, any closed set containing \(A\) must contain \(L\) as well. This shows that \(\bar{A}\) = \(A \cup L\) is the smallest closed set containing \(A\).
-
-============================================================
-
-Note ID: 1710371287138
-  Field: Text
-    Before:
-      If a set is not open, that does not imply it must be closed. Many sets such as the half-open interval \((c, d]\) = \(\{x \in \mathbf{R}: c&lt;x \leq d \} \) are neither open nor closed.&nbsp;
-
-    After:
-      If a set is not open, that does not imply it must be closed. Many sets such as the half-open interval \((c, d]\) = \(\{x \in \mathbf{R}: c&lt;x \leq d \} \) are neither open nor closed.&nbsp;
-
-============================================================
-
-Note ID: 1710371399514
-  Field: Text
-    Before:
-      <b>Theorem</b> 3.2.13. A set \(O\) is open if and only if \(O^{c}\) is closed. Likewise, a set \(F\) is closed if and only if \(F^{c}\) is open.
-
-    After:
-      <b>Theorem</b> 3.2.13. A set \(O\) is open if and only if \(O^{c}\) is closed. Likewise, a set \(F\) is closed if and only if \(F^{c}\) is open.
-
-============================================================
-
-Note ID: 1710371495262
-  Field: Text
-    Before:
-      Theorem 3.2.13. A set \(O\) is open if and only if \(O^{c}\) is closed. Likewise, a set \(F\) is closed if and only if \(F^{c}\) is open.<br><br>Proof. Given an open set \(O \subseteq \mathbf{R}\), let's first prove that \(O^{c}\) is a closed set. To prove \(O^{c}\) is closed, we need to show that it contains all of its limit points. If \(x\) is a limit point of \(O^{c}\), then every neighborhood of \(x\) contains some point of \(O^{c}\). But that is enough to conclude that \(x\) cannot be in the open set \(O\) because \(x \in O\) would imply that there exists a neighborhood \(V_{\epsilon}(x)\) \(\subseteq O\). Thus, \(x \in O^{c}\), as desired.
-
-    After:
-      Theorem 3.2.13. A set \(O\) is open if and only if \(O^{c}\) is closed. Likewise, a set \(F\) is closed if and only if \(F^{c}\) is open.<br><br>Proof. Given an open set \(O \subseteq \mathbf{R}\), let's first prove that \(O^{c}\) is a closed set. To prove \(O^{c}\) is closed, we need to show that it contains all of its limit points. If \(x\) is a limit point of \(O^{c}\), then every neighborhood of \(x\) contains some point of \(O^{c}\). But that is enough to conclude that \(x\) cannot be in the open set \(O\) because \(x \in O\) would imply that there exists a neighborhood \(V_{\epsilon}(x)\) \(\subseteq O\). Thus, \(x \in O^{c}\), as desired.
-
-============================================================
-
-Note ID: 1710371722458
-  Field: Text
-    Before:
-      <ul><li><b>Theorem</b> 3.2.14. (i) The union of a finite collection of closed sets is closed.</li><li>(ii) The intersection of an arbitrary collection of closed sets is closed.</li></ul>
-
-    After:
-      <ul><li><b>Theorem</b> 3.2.14. (i) The union of a finite collection of closed sets is closed.</li><li>(ii) The intersection of an arbitrary collection of closed sets is closed.</li></ul>
-
-============================================================
-
-Note ID: 1710372066227
-  Field: Text
-    Before:
-      Exercise 3.2.7. Given \(A \subseteq \mathbf{R}\), let \(L\) be the set of all limit points of \(A\).<br><br><ul><li>(a) Show that the set \(L\) is closed.</li><li><br></li><li>(b) Argue that if \(x\) is a limit point of \(A \cup L\), then \(x\) is a limit point of \(A\).&nbsp;</li></ul>
-
-    After:
-      Exercise 3.2.7. Given \(A \subseteq \mathbf{R}\), let \(L\) be the set of all limit points of \(A\).<br><br><ul><li>(a) Show that the set \(L\) is closed.</li><li><br></li><li>(b) Argue that if \(x\) is a limit point of \(A \cup L\), then \(x\) is a limit point of \(A\).&nbsp;</li></ul>
-
-============================================================
-
-Note ID: 1710372254516
-  Field: Text
-    Before:
-      Exercise 3.2.12. Let \(A\) be an uncountable set and let \(B\) be the set of real numbers that divides \(A\) into two uncountable sets; that is, \(s \in B\) if both \(\{x\) : \(x \in A\) and \(x&lt;s\}\) and \(\{x: x \in A\) and \(x&gt;s\}\) are uncountable. Show \(B\) is nonempty and open.
-
-    After:
-      Exercise 3.2.12. Let \(A\) be an uncountable set and let \(B\) be the set of real numbers that divides \(A\) into two uncountable sets; that is, \(s \in B\) if both \(\{x\) : \(x \in A\) and \(x&lt;s\}\) and \(\{x: x \in A\) and \(x&gt;s\}\) are uncountable. Show \(B\) is nonempty and open.
-
-============================================================
-
-Note ID: 1710372339406
-  Field: Text
-    Before:
-      Exercise 3.2.14. A dual notion to the closure of a set is the interior of a set. The interior of \(E\) is denoted \(E^{\circ}\) and is defined as<br><br><ul><li>\(E^{\circ}\) = \(\left\{x \in E: \text { there exists } V_{\epsilon}(x) \subseteq E\right\}\)</li></ul>
-
-    After:
-      Exercise 3.2.14. A dual notion to the closure of a set is the interior of a set. The interior of \(E\) is denoted \(E^{\circ}\) and is defined as<br><br><ul><li>\(E^{\circ}\) = \(\left\{x \in E: \text { there exists } V_{\epsilon}(x) \subseteq E\right\}\)</li></ul>
-
-============================================================
-
-Note ID: 1710372484907
-  Field: Text
-    Before:
-      Exercise 3.2.15. A set \(A\) is called an \(F_{\sigma}\) set if it can be written as the countable union of closed sets. A set \(B\) is called a \(G_{\delta}\) set if it can be written as the countable intersection of open sets.
-
-    After:
-      Exercise 3.2.15. A set \(A\) is called an \(F_{\sigma}\) set if it can be written as the countable union of closed sets. A set \(B\) is called a \(G_{\delta}\) set if it can be written as the countable intersection of open sets.
-
-============================================================
-
-Note ID: 1710455150587
-  Field: Text
-    Before:
-      Example 3.3.2. The most basic example of a compact set is a closed interval. To see this, notice that if \(\left(a_{n}\right)\) is contained in an interval \([c, d]\), then the Bolzano Weierstrass Theorem guarantees that we can find a convergent subsequence \(\left(a_{n_{k} }\right)\). Because a closed interval is a closed set (Example 3.2.9, (ii)), we know that the limit of this subsequence is also in \([c, d]\).
-
-    After:
-      Example 3.3.2. The most basic example of a compact set is a closed interval. To see this, notice that if \(\left(a_{n}\right)\) is contained in an interval \([c, d]\), then the Bolzano Weierstrass Theorem guarantees that we can find a convergent subsequence \(\left(a_{n_{k} }\right)\). Because a closed interval is a closed set (Example 3.2.9, (ii)), we know that the limit of this subsequence is also in \([c, d]\).
-
-============================================================
-
-Note ID: 1710455700401
-  Field: Text
-    Before:
-      Theorem 3.3.4 (Characterization of Compactness in \(\mathbf{R}\) ). \(A\) set \(K \subseteq \mathbf{R}\) is compact if and only if it is closed and bounded.<br><br>Proof. Let \(K\) be compact. We will first prove that \(K\) must be bounded, so assume, for contradiction, that \(K\) is not a bounded set. The idea is to produce a sequence in \(K\) that marches off to infinity in such a way that it cannot have a convergent subsequence as the definition of compact requires. To do this, notice that because \(K\) is not bounded there must exist an element \(x_{1} \in K\) satisfying \(\left|x_{1}\right|\) \(&gt;1\). Likewise, there must exist \(x_{2} \in K\) with \(\left|x_{2}\right|\) \(&gt;2\), and in general, given any \(n \in \mathbf{N}\), we can produce \(x_{n} \in K\) such that \(\left|x_{n}\right|\) &gt; \(n\).<br><br>Now, because \(K\) is assumed to be compact, \(\left(x_{n}\right)\) should have a convergent subsequence \(\left(x_{n_{k} }\right)\). But the elements of the subsequence must satisfy \(\left|x_{n_{k} }\right|\) \(&gt;\) \(n_{k}\), and consequently \(\left(x_{n_{k} }\right)\) is unbounded. Because convergent sequences are bounded (Theorem 2.3.2), we have a contradiction. Thus, \(K\) must at least be a bounded set.<br>
-
-    After:
-      Theorem 3.3.4 (Characterization of Compactness in \(\mathbf{R}\) ). \(A\) set \(K \subseteq \mathbf{R}\) is compact if and only if it is closed and bounded.<br><br>Proof. Let \(K\) be compact. We will first prove that \(K\) must be bounded, so assume, for contradiction, that \(K\) is not a bounded set. The idea is to produce a sequence in \(K\) that marches off to infinity in such a way that it cannot have a convergent subsequence as the definition of compact requires. To do this, notice that because \(K\) is not bounded there must exist an element \(x_{1} \in K\) satisfying \(\left|x_{1}\right|\) \(&gt;1\). Likewise, there must exist \(x_{2} \in K\) with \(\left|x_{2}\right|\) \(&gt;2\), and in general, given any \(n \in \mathbf{N}\), we can produce \(x_{n} \in K\) such that \(\left|x_{n}\right|\) &gt; \(n\).<br><br>Now, because \(K\) is assumed to be compact, \(\left(x_{n}\right)\) should have a convergent subsequence \(\left(x_{n_{k} }\right)\). But the elements of the subsequence must satisfy \(\left|x_{n_{k} }\right|\) \(&gt;\) \(n_{k}\), and consequently \(\left(x_{n_{k} }\right)\) is unbounded. Because convergent sequences are bounded (Theorem 2.3.2), we have a contradiction. Thus, \(K\) must at least be a bounded set.<br>
-
-============================================================
-
-Note ID: 1710455798543
-  Field: Text
-    Before:
-      Theorem 3.3.4 (Characterization of Compactness in \(\mathbf{R}\) ). \(A\) set \(K \subseteq \mathbf{R}\) is compact if and only if it is closed and bounded.<br><br>Proof:<br>We will show that \(K\) is closed. To see that \(K\) contains its limit points, we let \(x\) = \(\lim x_{n}\), where \(\left(x_{n}\right)\) is contained in \(K\) and argue that \(x\) must be in \(K\) as well. By Definition 3.3.1, the sequence \(\left(x_{n}\right)\) has a convergent&nbsp;subsequence \(\left(x_{n_{k} }\right)\), and by Theorem 2.5.2, we know \(\left(x_{n_{k} }\right)\) converges to the same limit \(x\). Finally, Definition 3.3.1 requires that \(x\) \(\in K\). This proves that \(K\) is closed.
-
-    After:
-      Theorem 3.3.4 (Characterization of Compactness in \(\mathbf{R}\) ). \(A\) set \(K \subseteq \mathbf{R}\) is compact if and only if it is closed and bounded.<br><br>Proof:<br>We will show that \(K\) is closed. To see that \(K\) contains its limit points, we let \(x\) = \(\lim x_{n}\), where \(\left(x_{n}\right)\) is contained in \(K\) and argue that \(x\) must be in \(K\) as well. By Definition 3.3.1, the sequence \(\left(x_{n}\right)\) has a convergent&nbsp;subsequence \(\left(x_{n_{k} }\right)\), and by Theorem 2.5.2, we know \(\left(x_{n_{k} }\right)\) converges to the same limit \(x\). Finally, Definition 3.3.1 requires that \(x\) \(\in K\). This proves that \(K\) is closed.
-
-============================================================
-
-Note ID: 1710489677812
-  Field: Text
-    Before:
-      There may be a temptation to consider closed intervals as being a kind of standard archetype for compact sets, but this is misleading. The structure of compact sets can be much more intricate and interesting. For instance, one implication of Theorem 3.3.4 is that the Cantor set is compact. It is more useful to think of compact sets as generalizations of closed intervals. Whenever a fact involving closed intervals&nbsp; is true, it is often the case that the same result holds when we replace "closed interval" with "compact set."
-
-    After:
-      There may be a temptation to consider closed intervals as being a kind of standard archetype for compact sets, but this is misleading. The structure of compact sets can be much more intricate and interesting. For instance, one implication of Theorem 3.3.4 is that the Cantor set is compact. It is more useful to think of compact sets as generalizations of closed intervals. Whenever a fact involving closed intervals&nbsp; is true, it is often the case that the same result holds when we replace "closed interval" with "compact set."
-
-============================================================
-
-Note ID: 1710489857600
-  Field: Text
-    Before:
-      Theorem 3.3.5 (Nested&nbsp;Compact Set Property). If<br><br><ul><li>\(K_{1}\)&nbsp; \(\supseteq\) \(K_{2}\)&nbsp; \(\supseteq\)&nbsp; \(K_{3}\)&nbsp; \(\supseteq\)&nbsp; \(K_{4}\)&nbsp; \(\supseteq\)&nbsp; \(\cdots\)</li></ul><br>is a nested sequence of nonempty compact sets, then \(\bigcap_{n=1}^{\infty}\) \(K_{n}\) is not empty.<br>
-
-    After:
-      Theorem 3.3.5 (Nested&nbsp;Compact Set Property). If<br><br><ul><li>\(K_{1}\)&nbsp; \(\supseteq\) \(K_{2}\)&nbsp; \(\supseteq\)&nbsp; \(K_{3}\)&nbsp; \(\supseteq\)&nbsp; \(K_{4}\)&nbsp; \(\supseteq\)&nbsp; \(\cdots\)</li></ul><br>is a nested sequence of nonempty compact sets, then \(\bigcap_{n=1}^{\infty}\) \(K_{n}\) is not empty.<br>
-
-============================================================
-
-Note ID: 1710491610341
-  Field: Text
-    Before:
-      <b>Definition</b> 3.3.6. Let \(A \subseteq \mathbf{R}\). An open cover for \(A\) is a (possibly infinite) collection of open sets \(\left\{O_{\lambda}: \lambda \in \Lambda\right\}\) whose union contains the set \(A\); that is, \(A\) \(\subseteq\)\( \bigcup_{\lambda \in \Lambda} O_{\lambda}\). Given an open cover for \(A\), a finite subcover is a finite subcollection of open sets from the original open cover whose union manages to completely contain \(A\).
-
-    After:
-      <b>Definition</b> 3.3.6. Let \(A \subseteq \mathbf{R}\). An open cover for \(A\) is a (possibly infinite) collection of open sets \(\left\{O_{\lambda}: \lambda \in \Lambda\right\}\) whose union contains the set \(A\); that is, \(A\) \(\subseteq\)\( \bigcup_{\lambda \in \Lambda} O_{\lambda}\). Given an open cover for \(A\), a finite subcover is a finite subcollection of open sets from the original open cover whose union manages to completely contain \(A\).
-
-============================================================
-
-Note ID: 1710492034882
-  Field: Text
-    Before:
-      &nbsp;For \(x \in(0,1)\), the sets \(O_{x}=(x / 2,1)\) do a fine job covering \((0,1)\), but in order to have an open cover of the closed interval \([0,1]\), we must also cover the endpoints. To remedy this, we could fix \(\epsilon&gt;0\), and let \(O_{0}\) = \((-\epsilon, \epsilon)\) and \(O_{1}\) = \((1-\epsilon, 1+\epsilon)\). Then, the collection<br><br>\[<br>\left\{O_{0}, O_{1}, O_{x}: x \in(0,1)\right\}<br>\]<br><br>is an open cover for \([0,1]\).&nbsp;
-
-    After:
-      &nbsp;For \(x \in(0,1)\), the sets \(O_{x}=(x / 2,1)\) do a fine job covering \((0,1)\), but in order to have an open cover of the closed interval \([0,1]\), we must also cover the endpoints. To remedy this, we could fix \(\epsilon&gt;0\), and let \(O_{0}\) = \((-\epsilon, \epsilon)\) and \(O_{1}\) = \((1-\epsilon, 1+\epsilon)\). Then, the collection<br><br>\[<br>\left\{O_{0}, O_{1}, O_{x}: x \in(0,1)\right\}<br>\]<br><br>is an open cover for \([0,1]\).&nbsp;
-
-============================================================
-
-Note ID: 1710492418283
-  Field: Text
-    Before:
-      For \(x \in(0,1)\), the sets \(O_{x}=(x / 2,1)\) do a fine job covering \((0,1)\), but in order to have an open cover of the closed interval \([0,1]\), we must also cover the endpoints. To remedy this, we could fix \(\epsilon&gt;0\), and let \(O_{0}=(-\epsilon, \epsilon)\) and \(O_{1}=(1-\epsilon, 1+\epsilon)\). Then, the collection<br><br>\[<br>\left\{O_{0}, O_{1}, O_{x}: x \in(0,1)\right\}<br>\]<br><br>is an open cover for \([0,1]\). But this time, notice there is a finite subcover. Because of the addition of the set \(O_{0}\), we can choose \(x^{\prime}\) so that \(x^{\prime} /&nbsp;2\)&nbsp;&lt; \(\epsilon\). It follows that \(\left\{O_{0}, O_{x^{\prime} }, O_{1}\right\}\) is a finite subcover for the closed interval \([0,1]\).
-
-    After:
-      For \(x \in(0,1)\), the sets \(O_{x}=(x / 2,1)\) do a fine job covering \((0,1)\), but in order to have an open cover of the closed interval \([0,1]\), we must also cover the endpoints. To remedy this, we could fix \(\epsilon&gt;0\), and let \(O_{0}=(-\epsilon, \epsilon)\) and \(O_{1}=(1-\epsilon, 1+\epsilon)\). Then, the collection<br><br>\[<br>\left\{O_{0}, O_{1}, O_{x}: x \in(0,1)\right\}<br>\]<br><br>is an open cover for \([0,1]\). But this time, notice there is a finite subcover. Because of the addition of the set \(O_{0}\), we can choose \(x^{\prime}\) so that \(x^{\prime} /&nbsp;2\)&nbsp;&lt; \(\epsilon\). It follows that \(\left\{O_{0}, O_{x^{\prime} }, O_{1}\right\}\) is a finite subcover for the closed interval \([0,1]\).
-
-============================================================
-
-Note ID: 1710492899812
-  Field: Text
-    Before:
-      Theorem 3.3.8 (Heine-Borel Theorem). Let \(K\) be a subset of \(\mathbf{R}\). All of the following statements are equivalent in the sense that any one of them implies the two others:<br><ul><li>(i) \(K\) is compact.<br></li><li>(ii) \(K\) is closed and bounded.</li><li>(iii) Every open cover for \(K\) has a finite subcover.</li></ul><div>Proof: Let's first assume (iii), and prove that it implies (ii) (and thus (i) as well).</div><div><br></div><div><ul><li>To show that \(K\) is bounded, we construct an open cover for \(K\) by defining \(O_{x}\) to be an open interval of radius 1 around each point \(x \in K\). In the language of neighborhoods, \(O_{x}\) = \(V_{1}(x)\). The open cover \(\left\{O_{x}: x \in K\right\}\)::math def::math def::math def::math def::math def then must have a finite subcover \(\left\{O_{x_{1} }, O_{x_{2} }, \ldots, O_{x_{n} }\right\}\)::math def::math def::math def. Because \(K\) is contained in a finite union of bounded sets, \(K\) must be bounded.</li></ul></div>
-
-    After:
-      Theorem 3.3.8 (Heine-Borel Theorem). Let \(K\) be a subset of \(\mathbf{R}\). All of the following statements are equivalent in the sense that any one of them implies the two others:<br><ul><li>(i) \(K\) is compact.<br></li><li>(ii) \(K\) is closed and bounded.</li><li>(iii) Every open cover for \(K\) has a finite subcover.</li></ul><div>Proof: Let's first assume (iii), and prove that it implies (ii) (and thus (i) as well).</div><div><br></div><div><ul><li>To show that \(K\) is bounded, we construct an open cover for \(K\) by defining \(O_{x}\) to be an open interval of radius 1 around each point \(x \in K\). In the language of neighborhoods, \(O_{x}\) = \(V_{1}(x)\). The open cover \(\left\{O_{x}: x \in K\right\}\)::math def::math def::math def::math def::math def then must have a finite subcover \(\left\{O_{x_{1} }, O_{x_{2} }, \ldots, O_{x_{n} }\right\}\)::math def::math def::math def. Because \(K\) is contained in a finite union of bounded sets, \(K\) must be bounded.</li></ul></div>
-
-============================================================
-
-Note ID: 1710496579061
-  Field: Text
-    Before:
-      Exercise 3.3.8. Let \(K\) and \(L\) be nonempty compact sets, and define<br><br>\[<br>d=\inf \{|x-y|: x \in K \text { and } y \in L\} .<br>\]<br><br>This turns out to be a reasonable definition for the distance between \(K\) and \(L\).<br><br><ul><li>(a) If \(K\) and \(L\) are disjoint, show \(d\) \(&gt;0\) and that \(d\) = \(\left|x_{0}-y_{0}\right|\) for some \(x_{0} \in K\) and \(y_{0} \in L\).</li><li>(b) Show that it's possible to have \(d\) = \(0\) if we assume only that the disjoint sets \(K\) and \(L\) are closed.</li></ul>
-
-    After:
-      Exercise 3.3.8. Let \(K\) and \(L\) be nonempty compact sets, and define<br><br>\[<br>d=\inf \{|x-y|: x \in K \text { and } y \in L\} .<br>\]<br><br>This turns out to be a reasonable definition for the distance between \(K\) and \(L\).<br><br><ul><li>(a) If \(K\) and \(L\) are disjoint, show \(d\) \(&gt;0\) and that \(d\) = \(\left|x_{0}-y_{0}\right|\) for some \(x_{0} \in K\) and \(y_{0} \in L\).</li><li>(b) Show that it's possible to have \(d\) = \(0\) if we assume only that the disjoint sets \(K\) and \(L\) are closed.</li></ul>
-
-============================================================
-
-Note ID: 1710497102985
-  Field: Text
-    Before:
-      One of the underlying goals of topology is to strip away all of the extraneous information that comes with our intuitive picture of the real numbers and isolate just those properties that are responsible for the phenomenon we are studying. For example, we were quick to observe that any closed interval is a compact set. The content of Theorem 3.3.4, however, is that the compactness of a closed interval has nothing to do with the fact that the set is an interval but is a consequence of the set being bounded and closed. In Chapter 1, we argued that the set of real numbers between 0 and 1 is an uncountable set. This turns out to be the case for any nonempty closed set that does not contain isolated points.
-
-    After:
-      One of the underlying goals of topology is to strip away all of the extraneous information that comes with our intuitive picture of the real numbers and isolate just those properties that are responsible for the phenomenon we are studying. For example, we were quick to observe that any closed interval is a compact set. The content of Theorem 3.3.4, however, is that the compactness of a closed interval has nothing to do with the fact that the set is an interval but is a consequence of the set being bounded and closed. In Chapter 1, we argued that the set of real numbers between 0 and 1 is an uncountable set. This turns out to be the case for any nonempty closed set that does not contain isolated points.
-
-============================================================
-
-Note ID: 1710497323991
-  Field: Text
-    Before:
-      Example 3.4.2 (Cantor Set). It is not too hard to see that the Cantor set is perfect. In Section 3.1, we defined the Cantor set as the intersection<br><br><ul><li>\(C=\bigcap_{n=0}^{\infty} C_{n}\)</li></ul><br>where each \(C_{n}\) is a finite union of closed intervals. By Theorem 3.2.14, each \(C_{n}\) is closed, and by the same theorem, \(C\) is closed as well. It remains to show that no point in \(C\) is isolated.<br><br>Let \(x \in C\) be arbitrary. To convince ourselves that \(x\) is not isolated, we must construct a sequence \(\left(x_{n}\right.\) ) of points in \(C\), different from \(x\), that converges to \(x\). From our earlier discussion, we know that \(C\) at least contains the endpoints of the intervals that make up each \(C_{n}\). In Exercise 3.4.3, we sketch the argument that these are all that is needed to construct \(\left(x_{n}\right)\).<br>
-
-    After:
-      Example 3.4.2 (Cantor Set). It is not too hard to see that the Cantor set is perfect. In Section 3.1, we defined the Cantor set as the intersection<br><br><ul><li>\(C=\bigcap_{n=0}^{\infty} C_{n}\)</li></ul><br>where each \(C_{n}\) is a finite union of closed intervals. By Theorem 3.2.14, each \(C_{n}\) is closed, and by the same theorem, \(C\) is closed as well. It remains to show that no point in \(C\) is isolated.<br><br>Let \(x \in C\) be arbitrary. To convince ourselves that \(x\) is not isolated, we must construct a sequence \(\left(x_{n}\right.\) ) of points in \(C\), different from \(x\), that converges to \(x\). From our earlier discussion, we know that \(C\) at least contains the endpoints of the intervals that make up each \(C_{n}\). In Exercise 3.4.3, we sketch the argument that these are all that is needed to construct \(\left(x_{n}\right)\).<br>
-
-============================================================
-
-Note ID: 1710579128841
-  Field: Text
-    Before:
-      Theorem 3.4.3. A nonempty perfect set is uncountable.<br><br>Proof. If \(P\) is perfect and nonempty, then it must be infinite because otherwise it would consist only of isolated points. Let's assume, for contradiction, that \(P\) is countable. Thus, we can write<br><ul><li>\(P\)= \(\left\{x_{1}, x_{2}, x_{3}, \ldots\right\},\)</li></ul><br>where every element of \(P\) appears on this list. The idea is to construct a sequence of nested compact sets \(K_{n}\), all contained in \(P\), with the property that :<br><ul><li>\(x_{1}\) \(\notin\) \(K_{2}\), \(x_{2}\) \(\notin\) \(K_{3}\), \(x_{3}\) \(\notin\) \(K_{4}\), \(\ldots\).&nbsp;</li></ul><br>
-
-    After:
-      Theorem 3.4.3. A nonempty perfect set is uncountable.<br><br>Proof. If \(P\) is perfect and nonempty, then it must be infinite because otherwise it would consist only of isolated points. Let's assume, for contradiction, that \(P\) is countable. Thus, we can write<br><ul><li>\(P\)= \(\left\{x_{1}, x_{2}, x_{3}, \ldots\right\},\)</li></ul><br>where every element of \(P\) appears on this list. The idea is to construct a sequence of nested compact sets \(K_{n}\), all contained in \(P\), with the property that :<br><ul><li>\(x_{1}\) \(\notin\) \(K_{2}\), \(x_{2}\) \(\notin\) \(K_{3}\), \(x_{3}\) \(\notin\) \(K_{4}\), \(\ldots\).&nbsp;</li></ul><br>
-
-============================================================
-
-Note ID: 1710579443659
-  Field: Text
-    Before:
-      Theorem 3.4.3. A nonempty perfect set is uncountable.<br><br>Proof. If \(P\) is perfect and nonempty, then it must be infinite because otherwise it would consist only of isolated points. Let's assume, for contradiction, that \(P\) is countable. Thus, we can write<br><ul><li>\(P\)= \(\left\{x_{1}, x_{2}, x_{3}, \ldots\right\},\)</li></ul><br>where every element of \(P\) appears on this list. The idea is to construct a sequence of nested compact sets \(K_{n}\), all contained in \(P\), with the property that&nbsp;<br>\(x_{1} \notin K_{2}, x_{2} \notin K_{3}, x_{3} \notin K_{4}, \ldots\). Some care must be taken to ensure that each \(K_{n}\) is nonempty, for then we can use Theorem 3.3.5 to produce an<br><ul><li>\(x \in \bigcap_{n=1}^{\infty} K_{n} \subseteq P\)</li></ul><br>that cannot be on the list \(\left\{x_{1}, x_{2}, x_{3}, \ldots\right\}\).<br><br>Let \(I_{1}\) be a closed interval that contains \(x_{1}\) in its interior (i.e., \(x_{1}\) is not an endpoint of \(\left.I_{1}\right)\). Now, \(x_{1}\) is not isolated, so there exists some other point \(y_{2} \in P\) that is also in the interior of \(I_{1}\). Construct a closed interval \(I_{2}\), centered on \(y_{2}\), so that \(I_{2} \subseteq I_{1}\) but \(x_{1} \notin I_{2}\). More explicitly, if \(I_{1}=[a, b]\), let<br><br><ul><li>\(\epsilon\) = \(\min \left\{y_{2}-a, b-y_{2},\left|x_{1}-y_{2}\right|\right\} .\)</li></ul><br>Then, the interval \(I_{2}=\left[y_{2}-\epsilon / 2, y_{2}+\epsilon / 2\right]\) has the desired properties as shown in the figure<br><br><img src="paste-106414706d1a272ec1b5324859bc0a9cfaef91ac.jpg"><br><br>This process can be continued. Because \(y_{2} \in P\) is not isolated, there must exist another point \(y_{3} \in P\) in the interior of \(I_{2}\), and we may insist that \(y_{3} \neq x_{2}\). Now, construct \(I_{3}\) centered on \(y_{3}\) and small enough so that \(x_{2} \notin I_{3}\) and \(I_{3} \subseteq I_{2}\). Observe that \(I_{3} \cap P \neq \emptyset\) because this intersection contains at least \(y_{3}\).<br><br>If we carry out this construction inductively, the result is a sequence of closed intervals \(I_{n}\) satisfying<br><br><ul><li>(i) \(I_{n+1}\) \(\subseteq\) \(I_{n}\),</li><li>(ii) \(x_{n}\) \(\notin\) \(I_{n+1}\)</li><li>(iii) \(I_{n}\) \(\cap P\) \(\neq\) \(\emptyset\).</li></ul><br>To finish the proof, we let \(K_{n}\) = \(I_{n} \cap P\). For each \(n \in \mathbf{N}\), we have that \(K_{n}\) is closed because it is the intersection of closed sets, and bounded because it is contained in the bounded set \(I_{n}\). Hence, \(K_{n}\) is compact. By construction, \(K_{n}\) is not empty and \(K_{n+1} \subseteq K_{n}\). Thus, we can employ the Nested Compact Set Property (Theorem 3.3.5) to conclude that the intersection<br><br><ul><li>\(\bigcap_{n=1}^{\infty} K_{n} \) \(\neq\) \(\emptyset\)</li></ul><br>But each \(K_{n}\) is a subset of \(P\), and the fact that \(x_{n} \notin I_{n+1}\) leads to the conclusion that \(\bigcap_{n=1}^{\infty} K_{n}=\emptyset\), which is the sought-after contradiction.<br>
-
-    After:
-      Theorem 3.4.3. A nonempty perfect set is uncountable.<br><br>Proof. If \(P\) is perfect and nonempty, then it must be infinite because otherwise it would consist only of isolated points. Let's assume, for contradiction, that \(P\) is countable. Thus, we can write<br><ul><li>\(P\)= \(\left\{x_{1}, x_{2}, x_{3}, \ldots\right\},\)</li></ul><br>where every element of \(P\) appears on this list. The idea is to construct a sequence of nested compact sets \(K_{n}\), all contained in \(P\), with the property that&nbsp;<br>\(x_{1} \notin K_{2}, x_{2} \notin K_{3}, x_{3} \notin K_{4}, \ldots\). Some care must be taken to ensure that each \(K_{n}\) is nonempty, for then we can use Theorem 3.3.5 to produce an<br><ul><li>\(x \in \bigcap_{n=1}^{\infty} K_{n} \subseteq P\)</li></ul><br>that cannot be on the list \(\left\{x_{1}, x_{2}, x_{3}, \ldots\right\}\).<br><br>Let \(I_{1}\) be a closed interval that contains \(x_{1}\) in its interior (i.e., \(x_{1}\) is not an endpoint of \(\left.I_{1}\right)\). Now, \(x_{1}\) is not isolated, so there exists some other point \(y_{2} \in P\) that is also in the interior of \(I_{1}\). Construct a closed interval \(I_{2}\), centered on \(y_{2}\), so that \(I_{2} \subseteq I_{1}\) but \(x_{1} \notin I_{2}\). More explicitly, if \(I_{1}=[a, b]\), let<br><br><ul><li>\(\epsilon\) = \(\min \left\{y_{2}-a, b-y_{2},\left|x_{1}-y_{2}\right|\right\} .\)</li></ul><br>Then, the interval \(I_{2}=\left[y_{2}-\epsilon / 2, y_{2}+\epsilon / 2\right]\) has the desired properties as shown in the figure<br><br><img src="paste-106414706d1a272ec1b5324859bc0a9cfaef91ac.jpg"><br><br>This process can be continued. Because \(y_{2} \in P\) is not isolated, there must exist another point \(y_{3} \in P\) in the interior of \(I_{2}\), and we may insist that \(y_{3} \neq x_{2}\). Now, construct \(I_{3}\) centered on \(y_{3}\) and small enough so that \(x_{2} \notin I_{3}\) and \(I_{3} \subseteq I_{2}\). Observe that \(I_{3} \cap P \neq \emptyset\) because this intersection contains at least \(y_{3}\).<br><br>If we carry out this construction inductively, the result is a sequence of closed intervals \(I_{n}\) satisfying<br><br><ul><li>(i) \(I_{n+1}\) \(\subseteq\) \(I_{n}\),</li><li>(ii) \(x_{n}\) \(\notin\) \(I_{n+1}\)</li><li>(iii) \(I_{n}\) \(\cap P\) \(\neq\) \(\emptyset\).</li></ul><br>To finish the proof, we let \(K_{n}\) = \(I_{n} \cap P\). For each \(n \in \mathbf{N}\), we have that \(K_{n}\) is closed because it is the intersection of closed sets, and bounded because it is contained in the bounded set \(I_{n}\). Hence, \(K_{n}\) is compact. By construction, \(K_{n}\) is not empty and \(K_{n+1} \subseteq K_{n}\). Thus, we can employ the Nested Compact Set Property (Theorem 3.3.5) to conclude that the intersection<br><br><ul><li>\(\bigcap_{n=1}^{\infty} K_{n} \) \(\neq\) \(\emptyset\)</li></ul><br>But each \(K_{n}\) is a subset of \(P\), and the fact that \(x_{n} \notin I_{n+1}\) leads to the conclusion that \(\bigcap_{n=1}^{\infty} K_{n}=\emptyset\), which is the sought-after contradiction.<br>
-
-============================================================
-
-Note ID: 1710579483743
-  Field: Text
-    Before:
-      Theorem 3.4.3. A nonempty perfect set is uncountable.<br><br>Proof. If \(P\) is perfect and nonempty, then it must be infinite because otherwise it would consist only of isolated points. Let's assume, for contradiction, that \(P\) is countable. Thus, we can write<br><ul><li>\(P\)= \(\left\{x_{1}, x_{2}, x_{3}, \ldots\right\},\)</li></ul><br>where every element of \(P\) appears on this list. The idea is to construct a sequence of nested compact sets \(K_{n}\), all contained in \(P\), with the property that&nbsp;<br>\(x_{1} \notin K_{2}, x_{2} \notin K_{3}, x_{3} \notin K_{4}, \ldots\). Some care must be taken to ensure that each \(K_{n}\) is nonempty, for then we can use Theorem 3.3.5 to produce an<br><ul><li>\(x \in \bigcap_{n=1}^{\infty} K_{n} \subseteq P\)</li></ul><br>that cannot be on the list \(\left\{x_{1}, x_{2}, x_{3}, \ldots\right\}\).<br><br>Let \(I_{1}\) be a closed interval that contains \(x_{1}\) in its interior (i.e., \(x_{1}\) is not an endpoint of \(\left.I_{1}\right)\). Now, \(x_{1}\) is not isolated, so there exists some other point \(y_{2} \in P\) that is also in the interior of \(I_{1}\). Construct a closed interval \(I_{2}\), centered on \(y_{2}\), so that \(I_{2} \subseteq I_{1}\) but \(x_{1} \notin I_{2}\). More explicitly, if \(I_{1}=[a, b]\), let<br><br><ul><li>\(\epsilon\) = \(\min \left\{y_{2}-a, b-y_{2},\left|x_{1}-y_{2}\right|\right\} .\)</li></ul><br>Then, the interval \(I_{2}=\left[y_{2}-\epsilon / 2, y_{2}+\epsilon / 2\right]\) has the desired properties as shown in the figure<br><br><img src="paste-106414706d1a272ec1b5324859bc0a9cfaef91ac.jpg"><br><br>This process can be continued. Because \(y_{2} \in P\) is not isolated, there must exist another point \(y_{3} \in P\) in the interior of \(I_{2}\), and we may insist that \(y_{3} \neq x_{2}\). Now, construct \(I_{3}\) centered on \(y_{3}\) and small enough so that \(x_{2} \notin I_{3}\) and \(I_{3} \subseteq I_{2}\). Observe that \(I_{3} \cap P \neq \emptyset\) because this intersection contains at least \(y_{3}\).<br><br>If we carry out this construction inductively, the result is a sequence of closed intervals \(I_{n}\) satisfying<br><br><ul><li>(i) \(I_{n+1}\) \(\subseteq\) \(I_{n}\),</li><li>(ii) \(x_{n}\) \(\notin\) \(I_{n+1}\)</li><li>(iii) \(I_{n}\) \(\cap P\) \(\neq\) \(\emptyset\).</li></ul><br>To finish the proof, we let \(K_{n}\) = \(I_{n} \cap P\). For each \(n \in \mathbf{N}\), we have that \(K_{n}\) is closed because it is the intersection of closed sets, and bounded because it is contained in the bounded set \(I_{n}\). Hence, \(K_{n}\) is compact. By construction, \(K_{n}\) is not empty and \(K_{n+1} \subseteq K_{n}\). Thus, we can employ the Nested Compact Set Property (Theorem 3.3.5) to conclude that the intersection<br><br><ul><li>\(\bigcap_{n=1}^{\infty} K_{n} \) \(\neq\) \(\emptyset\)</li></ul><br>But each \(K_{n}\) is a subset of \(P\), and the fact that \(x_{n} \notin I_{n+1}\) leads to the conclusion that \(\bigcap_{n=1}^{\infty} K_{n}=\emptyset\), which is the sought-after contradiction.<br>
-
-    After:
-      Theorem 3.4.3. A nonempty perfect set is uncountable.<br><br>Proof. If \(P\) is perfect and nonempty, then it must be infinite because otherwise it would consist only of isolated points. Let's assume, for contradiction, that \(P\) is countable. Thus, we can write<br><ul><li>\(P\)= \(\left\{x_{1}, x_{2}, x_{3}, \ldots\right\},\)</li></ul><br>where every element of \(P\) appears on this list. The idea is to construct a sequence of nested compact sets \(K_{n}\), all contained in \(P\), with the property that&nbsp;<br>\(x_{1} \notin K_{2}, x_{2} \notin K_{3}, x_{3} \notin K_{4}, \ldots\). Some care must be taken to ensure that each \(K_{n}\) is nonempty, for then we can use Theorem 3.3.5 to produce an<br><ul><li>\(x \in \bigcap_{n=1}^{\infty} K_{n} \subseteq P\)</li></ul><br>that cannot be on the list \(\left\{x_{1}, x_{2}, x_{3}, \ldots\right\}\).<br><br>Let \(I_{1}\) be a closed interval that contains \(x_{1}\) in its interior (i.e., \(x_{1}\) is not an endpoint of \(\left.I_{1}\right)\). Now, \(x_{1}\) is not isolated, so there exists some other point \(y_{2} \in P\) that is also in the interior of \(I_{1}\). Construct a closed interval \(I_{2}\), centered on \(y_{2}\), so that \(I_{2} \subseteq I_{1}\) but \(x_{1} \notin I_{2}\). More explicitly, if \(I_{1}=[a, b]\), let<br><br><ul><li>\(\epsilon\) = \(\min \left\{y_{2}-a, b-y_{2},\left|x_{1}-y_{2}\right|\right\} .\)</li></ul><br>Then, the interval \(I_{2}=\left[y_{2}-\epsilon / 2, y_{2}+\epsilon / 2\right]\) has the desired properties as shown in the figure<br><br><img src="paste-106414706d1a272ec1b5324859bc0a9cfaef91ac.jpg"><br><br>This process can be continued. Because \(y_{2} \in P\) is not isolated, there must exist another point \(y_{3} \in P\) in the interior of \(I_{2}\), and we may insist that \(y_{3} \neq x_{2}\). Now, construct \(I_{3}\) centered on \(y_{3}\) and small enough so that \(x_{2} \notin I_{3}\) and \(I_{3} \subseteq I_{2}\). Observe that \(I_{3} \cap P \neq \emptyset\) because this intersection contains at least \(y_{3}\).<br><br>If we carry out this construction inductively, the result is a sequence of closed intervals \(I_{n}\) satisfying<br><br><ul><li>(i) \(I_{n+1}\) \(\subseteq\) \(I_{n}\),</li><li>(ii) \(x_{n}\) \(\notin\) \(I_{n+1}\)</li><li>(iii) \(I_{n}\) \(\cap P\) \(\neq\) \(\emptyset\).</li></ul><br>To finish the proof, we let \(K_{n}\) = \(I_{n} \cap P\). For each \(n \in \mathbf{N}\), we have that \(K_{n}\) is closed because it is the intersection of closed sets, and bounded because it is contained in the bounded set \(I_{n}\). Hence, \(K_{n}\) is compact. By construction, \(K_{n}\) is not empty and \(K_{n+1} \subseteq K_{n}\). Thus, we can employ the Nested Compact Set Property (Theorem 3.3.5) to conclude that the intersection<br><br><ul><li>\(\bigcap_{n=1}^{\infty} K_{n} \) \(\neq\) \(\emptyset\)</li></ul><br>But each \(K_{n}\) is a subset of \(P\), and the fact that \(x_{n} \notin I_{n+1}\) leads to the conclusion that \(\bigcap_{n=1}^{\infty} K_{n}=\emptyset\), which is the sought-after contradiction.<br>
-
-============================================================
-
-Note ID: 1710579563603
-  Field: Text
-    Before:
-      Theorem 3.4.3. A nonempty perfect set is uncountable.<br><br>Proof. If \(P\) is perfect and nonempty, then it must be infinite because otherwise it would consist only of isolated points. Let's assume, for contradiction, that \(P\) is countable. Thus, we can write<br><ul><li>\(P\)= \(\left\{x_{1}, x_{2}, x_{3}, \ldots\right\},\)</li></ul><br>where every element of \(P\) appears on this list. The idea is to construct a sequence of nested compact sets \(K_{n}\), all contained in \(P\), with the property that&nbsp;<br>\(x_{1} \notin K_{2}, x_{2} \notin K_{3}, x_{3} \notin K_{4}, \ldots\). Some care must be taken to ensure that each \(K_{n}\) is nonempty, for then we can use Theorem 3.3.5 to produce an<br><ul><li>\(x \in \bigcap_{n=1}^{\infty} K_{n} \subseteq P\)</li></ul><br>that cannot be on the list \(\left\{x_{1}, x_{2}, x_{3}, \ldots\right\}\).<br><br>Let \(I_{1}\) be a closed interval that contains \(x_{1}\) in its interior (i.e., \(x_{1}\) is not an endpoint of \(\left.I_{1}\right)\). Now, \(x_{1}\) is not isolated, so there exists some other point \(y_{2} \in P\) that is also in the interior of \(I_{1}\). Construct a closed interval \(I_{2}\), centered on \(y_{2}\), so that \(I_{2} \subseteq I_{1}\) but \(x_{1} \notin I_{2}\). More explicitly, if \(I_{1}=[a, b]\), let<br><br><ul><li>\(\epsilon\) = \(\min \left\{y_{2}-a, b-y_{2},\left|x_{1}-y_{2}\right|\right\} .\)</li></ul><br>Then, the interval \(I_{2}=\left[y_{2}-\epsilon / 2, y_{2}+\epsilon / 2\right]\) has the desired properties as shown in the figure<br><br><img src="paste-106414706d1a272ec1b5324859bc0a9cfaef91ac.jpg"><br><br>This process can be continued. Because \(y_{2} \in P\) is not isolated, there must exist another point \(y_{3} \in P\) in the interior of \(I_{2}\), and we may insist that \(y_{3} \neq x_{2}\). Now, construct \(I_{3}\) centered on \(y_{3}\) and small enough so that \(x_{2} \notin I_{3}\) and \(I_{3} \subseteq I_{2}\). Observe that \(I_{3} \cap P \neq \emptyset\) because this intersection contains at least \(y_{3}\).<br><br>If we carry out this construction inductively, the result is a sequence of closed intervals \(I_{n}\) satisfying<br><br><ul><li>(i) \(I_{n+1}\) \(\subseteq\) \(I_{n}\),</li><li>(ii) \(x_{n}\) \(\notin\) \(I_{n+1}\)</li><li>(iii) \(I_{n}\) \(\cap P\) \(\neq\) \(\emptyset\).</li></ul><br>To finish the proof, we let \(K_{n}\) = \(I_{n} \cap P\). For each \(n \in \mathbf{N}\), we have that \(K_{n}\) is closed because it is the intersection of closed sets, and bounded because it is contained in the bounded set \(I_{n}\). Hence, \(K_{n}\) is compact. <br><br>By construction, \(K_{n}\) is not empty and \(K_{n+1}\) \(\subseteq K_{n}\). Thus, we can employ the Nested Compact Set Property (Theorem 3.3.5) to conclude that<br><br><ul><li>\(\bigcap_{n=1}^{\infty}\) \(K_{n} \) \(\neq\) \(\emptyset\)</li></ul><br>But each \(K_{n}\) is a subset of \(P\), and the fact that \(x_{n}\) \(\notin I_{n+1}\) leads to the conclusion that \(\bigcap_{n=1}^{\infty} K_{n}\) =\(\emptyset\), which is the sought-after contradiction.<br>
-
-    After:
-      Theorem 3.4.3. A nonempty perfect set is uncountable.<br><br>Proof. If \(P\) is perfect and nonempty, then it must be infinite because otherwise it would consist only of isolated points. Let's assume, for contradiction, that \(P\) is countable. Thus, we can write<br><ul><li>\(P\)= \(\left\{x_{1}, x_{2}, x_{3}, \ldots\right\},\)</li></ul><br>where every element of \(P\) appears on this list. The idea is to construct a sequence of nested compact sets \(K_{n}\), all contained in \(P\), with the property that&nbsp;<br>\(x_{1} \notin K_{2}, x_{2} \notin K_{3}, x_{3} \notin K_{4}, \ldots\). Some care must be taken to ensure that each \(K_{n}\) is nonempty, for then we can use Theorem 3.3.5 to produce an<br><ul><li>\(x \in \bigcap_{n=1}^{\infty} K_{n} \subseteq P\)</li></ul><br>that cannot be on the list \(\left\{x_{1}, x_{2}, x_{3}, \ldots\right\}\).<br><br>Let \(I_{1}\) be a closed interval that contains \(x_{1}\) in its interior (i.e., \(x_{1}\) is not an endpoint of \(\left.I_{1}\right)\). Now, \(x_{1}\) is not isolated, so there exists some other point \(y_{2} \in P\) that is also in the interior of \(I_{1}\). Construct a closed interval \(I_{2}\), centered on \(y_{2}\), so that \(I_{2} \subseteq I_{1}\) but \(x_{1} \notin I_{2}\). More explicitly, if \(I_{1}=[a, b]\), let<br><br><ul><li>\(\epsilon\) = \(\min \left\{y_{2}-a, b-y_{2},\left|x_{1}-y_{2}\right|\right\} .\)</li></ul><br>Then, the interval \(I_{2}=\left[y_{2}-\epsilon / 2, y_{2}+\epsilon / 2\right]\) has the desired properties as shown in the figure<br><br><img src="paste-106414706d1a272ec1b5324859bc0a9cfaef91ac.jpg"><br><br>This process can be continued. Because \(y_{2} \in P\) is not isolated, there must exist another point \(y_{3} \in P\) in the interior of \(I_{2}\), and we may insist that \(y_{3} \neq x_{2}\). Now, construct \(I_{3}\) centered on \(y_{3}\) and small enough so that \(x_{2} \notin I_{3}\) and \(I_{3} \subseteq I_{2}\). Observe that \(I_{3} \cap P \neq \emptyset\) because this intersection contains at least \(y_{3}\).<br><br>If we carry out this construction inductively, the result is a sequence of closed intervals \(I_{n}\) satisfying<br><br><ul><li>(i) \(I_{n+1}\) \(\subseteq\) \(I_{n}\),</li><li>(ii) \(x_{n}\) \(\notin\) \(I_{n+1}\)</li><li>(iii) \(I_{n}\) \(\cap P\) \(\neq\) \(\emptyset\).</li></ul><br>To finish the proof, we let \(K_{n}\) = \(I_{n} \cap P\). For each \(n \in \mathbf{N}\), we have that \(K_{n}\) is closed because it is the intersection of closed sets, and bounded because it is contained in the bounded set \(I_{n}\). Hence, \(K_{n}\) is compact. <br><br>By construction, \(K_{n}\) is not empty and \(K_{n+1}\) \(\subseteq K_{n}\). Thus, we can employ the Nested Compact Set Property (Theorem 3.3.5) to conclude that<br><br><ul><li>\(\bigcap_{n=1}^{\infty}\) \(K_{n} \) \(\neq\) \(\emptyset\)</li></ul><br>But each \(K_{n}\) is a subset of \(P\), and the fact that \(x_{n}\) \(\notin I_{n+1}\) leads to the conclusion that \(\bigcap_{n=1}^{\infty} K_{n}\) =\(\emptyset\), which is the sought-after contradiction.<br>
-
-============================================================
-
-Note ID: 1710580112285
-  Field: Text
-    Before:
-      Definition 3.4.4. Two nonempty sets \(A, B \subseteq \mathbf{R}\) are separated if \(\bar{A}\) \(\cap\) \(B\) and \(A\) \(\cap\) \(\bar{B}\) are both empty.&nbsp;
-
-    After:
-      Definition 3.4.4. Two nonempty sets \(A, B \subseteq \mathbf{R}\) are separated if \(\bar{A}\) \(\cap\) \(B\) and \(A\) \(\cap\) \(\bar{B}\) are both empty.&nbsp;
-
-============================================================
-
-Note ID: 1710580193927
-  Field: Text
-    Before:
-      Definition 3.4.4. Two nonempty sets \(A, B \subseteq \mathbf{R}\) are separated if \(\bar{A} \cap B\) and \(A \cap \bar{B}\) are both empty. A set \(E \subseteq \mathbf{R}\) is disconnected if it can be written as \(E\)= \(A \cup B\), where \(A\) and \(B\) are nonempty separated sets.<br><br>A set that is not disconnected is called a connected set.
-
-    After:
-      Definition 3.4.4. Two nonempty sets \(A, B \subseteq \mathbf{R}\) are separated if \(\bar{A} \cap B\) and \(A \cap \bar{B}\) are both empty. A set \(E \subseteq \mathbf{R}\) is disconnected if it can be written as \(E\)= \(A \cup B\), where \(A\) and \(B\) are nonempty separated sets.<br><br>A set that is not disconnected is called a connected set.
-
-============================================================
-
-Note ID: 1710580325056
-  Field: Text
-    Before:
-      Definition 3.4.4. Two nonempty sets \(A, B \subseteq \mathbf{R}\) are separated if \(\bar{A} \cap B\) and \(A \cap \bar{B}\) are both empty. A set \(E \subseteq \mathbf{R}\) is disconnected if it can be written as \(E=A \cup B\), where \(A\) and \(B\) are nonempty separated sets.<br><br>A set that is not disconnected is called a connected set.<br><br>Example 3.4.5. (i) If we let \(A=(1,2)\) and \(B=(2,5)\), then it is not difficult to verify that \(E\) = \((1,2) \cup(2,5)\) is disconnected. Notice that the sets \(C\) = \((1,2]\) and \(D\) = \((2,5)\) are not separated because \(C \cap \bar{D}\) = \(\{2\}\) is not empty. This should be comforting. The union \(C \cup D\) is equal to the interval \((1,5)\), which better not qualify as a disconnected set.&nbsp;
-
-    After:
-      Definition 3.4.4. Two nonempty sets \(A, B \subseteq \mathbf{R}\) are separated if \(\bar{A} \cap B\) and \(A \cap \bar{B}\) are both empty. A set \(E \subseteq \mathbf{R}\) is disconnected if it can be written as \(E=A \cup B\), where \(A\) and \(B\) are nonempty separated sets.<br><br>A set that is not disconnected is called a connected set.<br><br>Example 3.4.5. (i) If we let \(A=(1,2)\) and \(B=(2,5)\), then it is not difficult to verify that \(E\) = \((1,2) \cup(2,5)\) is disconnected. Notice that the sets \(C\) = \((1,2]\) and \(D\) = \((2,5)\) are not separated because \(C \cap \bar{D}\) = \(\{2\}\) is not empty. This should be comforting. The union \(C \cup D\) is equal to the interval \((1,5)\), which better not qualify as a disconnected set.&nbsp;
-
-============================================================
-
-Note ID: 1710580345288
-  Field: Text
-    Before:
-      Definition 3.4.4. Two nonempty sets \(A, B \subseteq \mathbf{R}\) are separated if \(\bar{A} \cap B\) and \(A \cap \bar{B}\) are both empty. A set \(E \subseteq \mathbf{R}\) is disconnected if it can be written as \(E=A \cup B\), where \(A\) and \(B\) are nonempty separated sets.<br><br>A set that is not disconnected is called a connected set.<br><br>Example 3.4.5. (i) If we let \(A=(1,2)\) and \(B=(2,5)\), then it is not difficult to verify that \(E=(1,2) \cup(2,5)\) is disconnected. Notice that the sets \(C=(1,2]\) and \(D=(2,5)\) are not separated because \(C \cap \bar{D}=\{2\}\) is not empty. This should be comforting. The union \(C \cup D\) is equal to the interval \((1,5)\), which better not qualify as a disconnected set. We will prove in a moment that every interval is a connected subset of \(\mathbf{R}\) and vice versa.
-
-    After:
-      Definition 3.4.4. Two nonempty sets \(A, B \subseteq \mathbf{R}\) are separated if \(\bar{A} \cap B\) and \(A \cap \bar{B}\) are both empty. A set \(E \subseteq \mathbf{R}\) is disconnected if it can be written as \(E=A \cup B\), where \(A\) and \(B\) are nonempty separated sets.<br><br>A set that is not disconnected is called a connected set.<br><br>Example 3.4.5. (i) If we let \(A=(1,2)\) and \(B=(2,5)\), then it is not difficult to verify that \(E=(1,2) \cup(2,5)\) is disconnected. Notice that the sets \(C=(1,2]\) and \(D=(2,5)\) are not separated because \(C \cap \bar{D}=\{2\}\) is not empty. This should be comforting. The union \(C \cup D\) is equal to the interval \((1,5)\), which better not qualify as a disconnected set. We will prove in a moment that every interval is a connected subset of \(\mathbf{R}\) and vice versa.
-
-============================================================
-
-Note ID: 1710580866549
-  Field: Text
-    Before:
-      <b>Theorem</b> 3.4.7. A set \(E \subseteq \mathbf{R}\) is connected if and only if whenever \(a\) &lt; \(c\) &lt; \(b\) with \(a, b\) \(\in E\), it follows that \(c\) \(\in E\).
-
-    After:
-      <b>Theorem</b> 3.4.7. A set \(E \subseteq \mathbf{R}\) is connected if and only if whenever \(a\) &lt; \(c\) &lt; \(b\) with \(a, b\) \(\in E\), it follows that \(c\) \(\in E\).
-
-============================================================
-
-Note ID: 1710584481078
-  Field: Text
-    Before:
-      Theorem 3.4.7. A set \(E \subseteq \mathbf{R}\) is connected if and only if whenever \(a&lt;c&lt;b\) with \(a, b \in E\), it follows that \(c \in E\) as well.<br><br>Proof. Assume \(E\) is connected, and let \(a, b \in E\) and \(a\) &lt; \(c\) &lt; \(b\). Set<br><br><ul><li>\(A\) = \((-\infty, c)\) \(\cap E \)&nbsp;</li><li>\(B\) = \((c, \infty)\) \(\cap E\)&nbsp;</li></ul><br>Because \(a \in A\) and \(b \in B\), neither set is empty and, just as in Example 3.4.5 (ii), neither set contains a limit point of the other. If \(E\) = \(A \cup B\), then we would have that \(E\) is disconnected, which it is not. It must then be that \(A \cup B\) is missing some element of \(E\), and \(c\) is the only possibility. Thus, \(c \in E\).<br><br>
-
-    After:
-      Theorem 3.4.7. A set \(E \subseteq \mathbf{R}\) is connected if and only if whenever \(a&lt;c&lt;b\) with \(a, b \in E\), it follows that \(c \in E\) as well.<br><br>Proof. Assume \(E\) is connected, and let \(a, b \in E\) and \(a\) &lt; \(c\) &lt; \(b\). Set<br><br><ul><li>\(A\) = \((-\infty, c)\) \(\cap E \)&nbsp;</li><li>\(B\) = \((c, \infty)\) \(\cap E\)&nbsp;</li></ul><br>Because \(a \in A\) and \(b \in B\), neither set is empty and, just as in Example 3.4.5 (ii), neither set contains a limit point of the other. If \(E\) = \(A \cup B\), then we would have that \(E\) is disconnected, which it is not. It must then be that \(A \cup B\) is missing some element of \(E\), and \(c\) is the only possibility. Thus, \(c \in E\).<br><br>
-
-============================================================
-
-Note ID: 1710585206798
-  Field: Text
-    Before:
-      Exercise 3.4.7. A set \(E\) is totally disconnected if, given any two distinct points \(x, y \in E\), there exist separated sets \(A\) and \(B\) with \(x \in A, y \in B\), and \(E\) = \(A \cup B\).<br><br>(a) Show that \(\mathbf{Q}\) is totally disconnected.
-
-    After:
-      Exercise 3.4.7. A set \(E\) is totally disconnected if, given any two distinct points \(x, y \in E\), there exist separated sets \(A\) and \(B\) with \(x \in A, y \in B\), and \(E\) = \(A \cup B\).<br><br>(a) Show that \(\mathbf{Q}\) is totally disconnected.
-
-============================================================
-
-Note ID: 1710585306015
-  Field: Text
-    Before:
-      Exercise 3.4.7. A set \(E\) is totally disconnected if, given any two distinct points \(x, y \in E\), there exist separated sets \(A\) and \(B\) with \(x \in A, y \in B\), and \(E=A \cup B\).<br><br>Exercise 3.4.8. Follow these steps to show that the Cantor set is totally disconnected in the sense described in Exercise 3.4.7.<br><br>Let \(C=\bigcap_{n=0}^{\infty} C_{n}\), as defined in Section 3.1.<br><br><ul><li>(a) Given \(x, y \in C\), with \(x&lt;y\), set \(\epsilon\) = \(y-x\). For each \(n=0,1,2, \ldots\), the set \(C_{n}\) consists of a finite number of closed intervals. Explain why there must exist an \(N\) large enough so that it is impossible for \(x\) and \(y\) both to belong to the same closed interval of \(C_{N}\).</li><li>(b) Show that \(C\) is totally disconnected.</li></ul>
-
-    After:
-      Exercise 3.4.7. A set \(E\) is totally disconnected if, given any two distinct points \(x, y \in E\), there exist separated sets \(A\) and \(B\) with \(x \in A, y \in B\), and \(E=A \cup B\).<br><br>Exercise 3.4.8. Follow these steps to show that the Cantor set is totally disconnected in the sense described in Exercise 3.4.7.<br><br>Let \(C=\bigcap_{n=0}^{\infty} C_{n}\), as defined in Section 3.1.<br><br><ul><li>(a) Given \(x, y \in C\), with \(x&lt;y\), set \(\epsilon\) = \(y-x\). For each \(n=0,1,2, \ldots\), the set \(C_{n}\) consists of a finite number of closed intervals. Explain why there must exist an \(N\) large enough so that it is impossible for \(x\) and \(y\) both to belong to the same closed interval of \(C_{N}\).</li><li>(b) Show that \(C\) is totally disconnected.</li></ul>
-
-============================================================
-
-Note ID: 1710587305458
-  Field: Text
-    Before:
-      The structure of open sets is fairly straightforward. Every open set is either a finite or countable union of open intervals. Standing in opposition&nbsp; to this tidy description of all open sets is the Cantor set. The Cantor set is a closed, uncountable set that contains no intervals of any kind.&nbsp;
-
-    After:
-      The structure of open sets is fairly straightforward. Every open set is either a finite or countable union of open intervals. Standing in opposition&nbsp; to this tidy description of all open sets is the Cantor set. The Cantor set is a closed, uncountable set that contains no intervals of any kind.&nbsp;
-
-============================================================
-
-Note ID: 1710587523983
-  Field: Text
-    Before:
-      Definition 3.5.1:<br><ul><li>&nbsp;A set \(A \subseteq \mathbf{R}\) is called an \(F_{\sigma}\) set if it can be written as the countable union of closed sets.&nbsp;</li><li>A set \(B \subseteq \mathbf{R}\) is called a \(G_{\delta}\) set if it can be written as the countable intersection of open sets.</li></ul>
-
-    After:
-      Definition 3.5.1:<br><ul><li>&nbsp;A set \(A \subseteq \mathbf{R}\) is called an \(F_{\sigma}\) set if it can be written as the countable union of closed sets.&nbsp;</li><li>A set \(B \subseteq \mathbf{R}\) is called a \(G_{\delta}\) set if it can be written as the countable intersection of open sets.</li></ul>
-
-============================================================
-
-Note ID: 1710587960442
-  Field: Text
-    Before:
-      Exercise 3.5.3. (This exercise has already appeared as Exercise 3.2.15.)<ul><li>(a) Show that a closed interval \([a, b]\) is a \(G_{\delta}\) set.</li><li>(b) Show that the half-open interval \((a, b]\) is both a \(G_{\delta}\) and an \(F_{\sigma}\) set.</li><li>(c) Show that \(\mathbf{Q}\) is an \(F_{\sigma}\) set, and the set of irrationals \(\mathbf{I}\) forms a \(G_{\delta}\) set.</li></ul>
-
-    After:
-      Exercise 3.5.3. (This exercise has already appeared as Exercise 3.2.15.)<ul><li>(a) Show that a closed interval \([a, b]\) is a \(G_{\delta}\) set.</li><li>(b) Show that the half-open interval \((a, b]\) is both a \(G_{\delta}\) and an \(F_{\sigma}\) set.</li><li>(c) Show that \(\mathbf{Q}\) is an \(F_{\sigma}\) set, and the set of irrationals \(\mathbf{I}\) forms a \(G_{\delta}\) set.</li></ul>
-
-============================================================
-
-Note ID: 1710588028268
-  Field: Text
-    Before:
-      Definition 3.5.1. A set \(A \subseteq \mathbf{R}\) is called an \(F_{\sigma}\) set if it can be written as the countable union of closed sets. A set \(B \subseteq \mathbf{R}\) is called a \(G_{\delta}\) set if it can be written as the countable intersection of open sets.<br><br>It is not readily obvious that the class \(F_{\sigma}\) does not include every subset of \(\mathbf{R}\), but we are now ready to argue that \(\mathbf{I}\) is not an \(F_{\sigma}\) set (and consequently \(\mathbf{Q}\) is not a \(G_{\delta}\) set). This will follow from a theorem due to René Louis Baire (1874-1932).
-
-    After:
-      Definition 3.5.1. A set \(A \subseteq \mathbf{R}\) is called an \(F_{\sigma}\) set if it can be written as the countable union of closed sets. A set \(B \subseteq \mathbf{R}\) is called a \(G_{\delta}\) set if it can be written as the countable intersection of open sets.<br><br>It is not readily obvious that the class \(F_{\sigma}\) does not include every subset of \(\mathbf{R}\), but we are now ready to argue that \(\mathbf{I}\) is not an \(F_{\sigma}\) set (and consequently \(\mathbf{Q}\) is not a \(G_{\delta}\) set). This will follow from a theorem due to René Louis Baire (1874-1932).
-
-============================================================
-
-Note ID: 1710588301067
-  Field: Text
-    Before:
-      Exercise 3.5.5. Show that it is impossible to write<br><br><ul><li>\(\mathbf{R}\) = \(\bigcup_{n=1}^{\infty}\) \(F_{n}\)</li></ul><br>where for each \(n \in \mathbf{N}\), \(F_{n}\) is a closed set containing no nonempty open intervals.<br>
-
-    After:
-      Exercise 3.5.5. Show that it is impossible to write<br><br><ul><li>\(\mathbf{R}\) = \(\bigcup_{n=1}^{\infty}\) \(F_{n}\)</li></ul><br>where for each \(n \in \mathbf{N}\), \(F_{n}\) is a closed set containing no nonempty open intervals.<br>
-
-============================================================
-
-Note ID: 1710588606859
-  Field: Text
-    Before:
-      We have encountered several equivalent ways to assert that a particular set \(G\) is dense in \(\mathbf{R}\). In Section 3.2, we observed that \(G\) is dense in \(\mathbf{R}\) if and only if every point of \(\mathbf{R}\) is a limit point of \(G\). Because the closure of any set is obtained by taking the union of the set and its limit points, we have that<br><br><ul><li>\(G\) is dense in&nbsp; \(\mathbf{R}\) iff \(\bar{G}\)= \(\mathbf{R} \text {. }\)</li></ul><br>The set \(\mathbf{Q}\) is dense in \(\mathbf{R}\); the set \(\mathbf{Z}\) is clearly not. In fact, in the jargon of analysis, \(\mathbf{Z}\) is nowhere-dense in \(\mathbf{R}\).<br>
-
-    After:
-      We have encountered several equivalent ways to assert that a particular set \(G\) is dense in \(\mathbf{R}\). In Section 3.2, we observed that \(G\) is dense in \(\mathbf{R}\) if and only if every point of \(\mathbf{R}\) is a limit point of \(G\). Because the closure of any set is obtained by taking the union of the set and its limit points, we have that<br><br><ul><li>\(G\) is dense in&nbsp; \(\mathbf{R}\) iff \(\bar{G}\)= \(\mathbf{R} \text {. }\)</li></ul><br>The set \(\mathbf{Q}\) is dense in \(\mathbf{R}\); the set \(\mathbf{Z}\) is clearly not. In fact, in the jargon of analysis, \(\mathbf{Z}\) is nowhere-dense in \(\mathbf{R}\).<br>
-
-============================================================
-
-Note ID: 1710588651056
-  Field: Text
-    Before:
-      Exercise 3.5.8. Show that a set \(E\) is nowhere-dense in \(\mathbf{R}\) if and only if the complement of \(\bar{E}\) is dense in \(\mathbf{R}\).
-
-    After:
-      Exercise 3.5.8. Show that a set \(E\) is nowhere-dense in \(\mathbf{R}\) if and only if the complement of \(\bar{E}\) is dense in \(\mathbf{R}\).
-
-============================================================
-
-Note ID: 1710588864866
-  Field: Text
-    Before:
-      We also briefly discussed the concept of "length," or "measure," in Section 3.1. Baire's Theorem offers a third perspective. From this point of view, nowhere-dense sets are considered to be "thin" sets. Any set that is the countable union - i.e., a not very large union - of these small sets is called a "meager" set or a set of "first category." A set that is not of first category is of "second category." Intuitively, sets of the second category are the "fat" subsets. The Baire Category Theorem, as it is often called, states that \(\mathbf{R}\) is of second category.
-
-    After:
-      We also briefly discussed the concept of "length," or "measure," in Section 3.1. Baire's Theorem offers a third perspective. From this point of view, nowhere-dense sets are considered to be "thin" sets. Any set that is the countable union - i.e., a not very large union - of these small sets is called a "meager" set or a set of "first category." A set that is not of first category is of "second category." Intuitively, sets of the second category are the "fat" subsets. The Baire Category Theorem, as it is often called, states that \(\mathbf{R}\) is of second category.
-
-============================================================
-
-Note ID: 1710588977786
-  Field: Text
-    Before:
-      <ul><li>There is a significance to the Baire Category Theorem that is difficult to appreciate at the moment because we are only seeing a special case of this result.&nbsp;</li><li>The real numbers are an example of a complete metric space.&nbsp;</li><li>Metric spaces are discussed in some detail in Section 8.2, but here is the basic idea.&nbsp;</li><li>Given a set of mathematical objects such as real numbers, points in the plane or continuous functions defined on \([0,1]\), a "metric" is a rule that assigns a "distance" between two elements in the set. In \(\mathbf{R}\), we have been using \(|x-y|\) as the distance between the real numbers \(x\) and \(y\).&nbsp;</li><li>The point is that if we can create a satisfactory notion of "distance" on these other spaces (we will need the triangle inequality to hold, for instance), then the concepts of convergence, Cauchy sequences, and open sets, for example, can be naturally transferred over.&nbsp;</li></ul>
-
-    After:
-      <ul><li>There is a significance to the Baire Category Theorem that is difficult to appreciate at the moment because we are only seeing a special case of this result.&nbsp;</li><li>The real numbers are an example of a complete metric space.&nbsp;</li><li>Metric spaces are discussed in some detail in Section 8.2, but here is the basic idea.&nbsp;</li><li>Given a set of mathematical objects such as real numbers, points in the plane or continuous functions defined on \([0,1]\), a "metric" is a rule that assigns a "distance" between two elements in the set. In \(\mathbf{R}\), we have been using \(|x-y|\) as the distance between the real numbers \(x\) and \(y\).&nbsp;</li><li>The point is that if we can create a satisfactory notion of "distance" on these other spaces (we will need the triangle inequality to hold, for instance), then the concepts of convergence, Cauchy sequences, and open sets, for example, can be naturally transferred over.&nbsp;</li></ul>
-
-============================================================
-
-Note ID: 1710589021618
-  Field: Text
-    Before:
-      A complete metric space is any set with a suitably defined metric in which Cauchy sequences have limits. We have spent a good deal of time discussing the fact that \(\mathbf{R}\) is a complete metric space whereas \(\mathbf{Q}\) is not.
-
-    After:
-      A complete metric space is any set with a suitably defined metric in which Cauchy sequences have limits. We have spent a good deal of time discussing the fact that \(\mathbf{R}\) is a complete metric space whereas \(\mathbf{Q}\) is not.
-
-============================================================
-
-Note ID: 1710592121347
-  Field: Text
-    Before:
-      What is an equivalence relation \(R\) for a set \(S\)?<br><ul><li>\(R\) is a subset of \(S \times S\), formed of pairs \((a,b)\), such that it is:</li></ul><ol><ol><li>Reflexive: \(aRa\)</li><li>Symetric: \(aRb\) =&nbsp; \(bRa\)</li><li>Transitive:&nbsp;\(aRb\)&nbsp;\( \land \) \(bRc\)&nbsp;\(\implies\)&nbsp;\(aRc\)</li></ol></ol>
-
-    After:
-      What is an equivalence relation \(R\) for a set \(S\)?<br><ul><li>\(R\) is a subset of \(S \times S\), formed of pairs \((a,b)\), such that it is:</li></ul><ol><ol><li>Reflexive: \(aRa\)</li><li>Symetric: \(aRb\) =&nbsp; \(bRa\)</li><li>Transitive:&nbsp;\(aRb\)&nbsp;\( \land \) \(bRc\)&nbsp;\(\implies\)&nbsp;\(aRc\)</li></ol></ol>
-
-============================================================
-
-Note ID: 1710674057571
-  Field: Text
-    Before:
-      1. (Abbott 1.2.1(b)) Where does the proof in Theorem 1.1.1 (of the book) breakdown if we try to use it to prove \(\sqrt{4}\) is irrational.<br><br>Solution. We could start by assuming that \((p / q)^{2}=4\) where \(p, q \in \mathbb{Z}\) have no common factor. This implies that \(p^{2}=4 q^{2}\). If we proceed in the same manner of the book we would want to argue that if 4 divides \(p^{2}\) then 4 divides \(p\), but this is not true. So the same proof does not work.<br><br>We could try to deviate from the proof just slightly. Notice that \(p^{2}=4 q^{2}=2 \cdot 2 q^{2}\) implies that \(p\) = \(2 r\) is even. Thus, \((2 r)^{2}\) = \(4 q^{2} \Longrightarrow r^{2}=q^{2} \Longrightarrow r= \pm q\). We have shown that \(r\) divides \(p\) and \(r\) divides \(q\). We conclude \(r\) = \(1\) and therefore \(p\) = \(2\) and \(q\) = \(1\). Of course this is true: \((2 / 1)^{2}=4\).
-
-    After:
-      1. (Abbott 1.2.1(b)) Where does the proof in Theorem 1.1.1 (of the book) breakdown if we try to use it to prove \(\sqrt{4}\) is irrational.<br><br>Solution. We could start by assuming that \((p / q)^{2}=4\) where \(p, q \in \mathbb{Z}\) have no common factor. This implies that \(p^{2}=4 q^{2}\). If we proceed in the same manner of the book we would want to argue that if 4 divides \(p^{2}\) then 4 divides \(p\), but this is not true. So the same proof does not work.<br><br>We could try to deviate from the proof just slightly. Notice that \(p^{2}=4 q^{2}=2 \cdot 2 q^{2}\) implies that \(p\) = \(2 r\) is even. Thus, \((2 r)^{2}\) = \(4 q^{2} \Longrightarrow r^{2}=q^{2} \Longrightarrow r= \pm q\). We have shown that \(r\) divides \(p\) and \(r\) divides \(q\). We conclude \(r\) = \(1\) and therefore \(p\) = \(2\) and \(q\) = \(1\). Of course this is true: \((2 / 1)^{2}=4\).
-
-============================================================
-
-Note ID: 1710674750698
-  Field: Text
-    Before:
-      <br>4. (Abbott 1.2.8) Give an example of a function in each of the following cases.<br><br>(a) \(f: \mathbb{N} \rightarrow \mathbb{N}\) that is \(1-1\) but not onto.<br><br>(b) \(f: \mathbb{N} \rightarrow \mathbb{N}\) that is onto but not \(1-1\).<br><br>(c) \(f: \mathbb{N} \rightarrow \mathbb{Z}\) that is \(1-1\) and onto.<br><br><ul><li>For c:</li><ul><li>\[<br>f(n)=\left\{\begin{array}{ll}<br>-n / 2 &amp; \text { if } n \text { is even } \\<br>(n-1) / 2 &amp; \text { if } n \text { is odd }<br>\end{array} .\right.<br>\]</li><li>Notice that \(f(1)=-1, f(2)=0, f(3)=-2, f(4)=1\) and so forth. The idea is that \(f\) is mapping the even natural numbers to negative integers and the odd natural numbers to the positive integers including zero.</li><li>Let \(k \in \mathbb{Z}\). If \(k \geq 0\) then \(f(2 k+1)\) = \(((2 k+1)-1) / 2\) = \(k\).&nbsp;</li><li>If \(k&lt;0\) then \(-2 k \in \mathbb{N}\) and \(f(-2 k)\) = \(-(-2 k) / 2\) = \(k\). Thus, \(f\) is onto.</li><li>Suppose \(f(n)\) = \(f(m)\). Then, \(m\) and \(n\) must be of the same sign, otherwise their images wouldn't match parity. So either \(-m / 2=-n / 2\) or \((n-1) / 2=(m-1) / 2\). In both cases we find with a little algebra that \(m\) = \(n\). Thus \(f\) is one to one.<br></li></ul></ul>
-
-    After:
-      <br>4. (Abbott 1.2.8) Give an example of a function in each of the following cases.<br><br>(a) \(f: \mathbb{N} \rightarrow \mathbb{N}\) that is \(1-1\) but not onto.<br><br>(b) \(f: \mathbb{N} \rightarrow \mathbb{N}\) that is onto but not \(1-1\).<br><br>(c) \(f: \mathbb{N} \rightarrow \mathbb{Z}\) that is \(1-1\) and onto.<br><br><ul><li>For c:</li><ul><li>\[<br>f(n)=\left\{\begin{array}{ll}<br>-n / 2 &amp; \text { if } n \text { is even } \\<br>(n-1) / 2 &amp; \text { if } n \text { is odd }<br>\end{array} .\right.<br>\]</li><li>Notice that \(f(1)=-1, f(2)=0, f(3)=-2, f(4)=1\) and so forth. The idea is that \(f\) is mapping the even natural numbers to negative integers and the odd natural numbers to the positive integers including zero.</li><li>Let \(k \in \mathbb{Z}\). If \(k \geq 0\) then \(f(2 k+1)\) = \(((2 k+1)-1) / 2\) = \(k\).&nbsp;</li><li>If \(k&lt;0\) then \(-2 k \in \mathbb{N}\) and \(f(-2 k)\) = \(-(-2 k) / 2\) = \(k\). Thus, \(f\) is onto.</li><li>Suppose \(f(n)\) = \(f(m)\). Then, \(m\) and \(n\) must be of the same sign, otherwise their images wouldn't match parity. So either \(-m / 2=-n / 2\) or \((n-1) / 2=(m-1) / 2\). In both cases we find with a little algebra that \(m\) = \(n\). Thus \(f\) is one to one.<br></li></ul></ul>
-
-============================================================
-
-Note ID: 1710674940906
-  Field: Text
-    Before:
-      \section*{7. (Abbott 1.3.3)}<br><br>(a) Let \(A\) be non-empty and bounded below and define \(B=\{b \in \mathbb{R}: b\) is a lower bound for \(A\}\). Show that \(\sup B=\inf A\).<br><br>(a) Notice that by construction \(B\) is non-empty and bounded above by any element of the set \(A\). By completeness, we can let \(s=\sup B\), that is, \(s\) is an upper bound for \(B\) and if \(b\) is an upper bound then \(s \leq b\). (We want to show \(s=\inf A\) ).<br><br>First, we show \(s\) is a lower bound for \(A\). Let \(a \in A\). By definition of \(B\), we have that \(b\) \(\leq a\) for all \(b \in B\). Therefore, \(a\) is an upper bound for \(B\). Since \(s\) is the least upper bound for \(B\) we have that \(s\) \(\leq a\). This shows that \(s\) is a lower bound for \(A\).<br><br>Next, we show that \(s\) is the greatest lower bound for \(A\). Let \(l\) be a lower bound for \(A\). It follows that \(l \in B\). Since \(s\) is an upper bound for \(B\) we have that \(l\) \(\leq s\). Therefore, \(s\) is the greatest lower bound and \(s=\inf A\).
-
-    After:
-      \section*{7. (Abbott 1.3.3)}<br><br>(a) Let \(A\) be non-empty and bounded below and define \(B=\{b \in \mathbb{R}: b\) is a lower bound for \(A\}\). Show that \(\sup B=\inf A\).<br><br>(a) Notice that by construction \(B\) is non-empty and bounded above by any element of the set \(A\). By completeness, we can let \(s=\sup B\), that is, \(s\) is an upper bound for \(B\) and if \(b\) is an upper bound then \(s \leq b\). (We want to show \(s=\inf A\) ).<br><br>First, we show \(s\) is a lower bound for \(A\). Let \(a \in A\). By definition of \(B\), we have that \(b\) \(\leq a\) for all \(b \in B\). Therefore, \(a\) is an upper bound for \(B\). Since \(s\) is the least upper bound for \(B\) we have that \(s\) \(\leq a\). This shows that \(s\) is a lower bound for \(A\).<br><br>Next, we show that \(s\) is the greatest lower bound for \(A\). Let \(l\) be a lower bound for \(A\). It follows that \(l \in B\). Since \(s\) is an upper bound for \(B\) we have that \(l\) \(\leq s\). Therefore, \(s\) is the greatest lower bound and \(s=\inf A\).
-
-============================================================
-
-Note ID: 1710675323868
-  Field: Text
-    Before:
-      8. (Abbott 1.3.4) Let \(\left\{A_{n}\right\}\) be a collection of non-empty and bounded above sets.<br><br>(a) Find a formula for \(\sup \left(A_{1} \cup A_{2}\right)\) and extend this to \(\sup \left(\bigcup_{n=1}^{N} A_{n}\right)\).<br><ul><li>Fromula is \(\sup \left(\bigcup_{n=1}^{N} A_{n}\right)\) =&nbsp;\(\max(\sup A_1,\ldots, \sup A_n)\)</li><li>Prove base case:</li><ul><li>Let \(a \in A_{1} \cup A_{2}\). Then, \(a \in A_{1}\) or \(a \in A_{2}\) which implies that \(a\) \(\leq \sup A_{1}\) or \(a\) \(\leq \sup A_{2}\). Thus, \(a\) \(\leq \max \left(\sup A_{1}, \sup A_{2}\right)\) and \(\max \left(\sup A_{1}, \sup A_{2}\right)\) is an upper bound for \(A_{1} \cup A_{2}\).</li><li>Let \(b\) be an upper bound for \(A_{1} \cup A_{2}\). Then, \(b\) is an upper bound for \(A_{1}\) and \(A_{2}\). It follows that \(\sup A_{1} \) \(\leq b\) and \(\sup A_{2}\)&nbsp; \(\leq b\). Therefore, \(\max \left(\sup A_{1}, \sup A_{2}\right)\)&nbsp; \(\leq b\) and \(\max \left(\sup A_{1}, \sup A_{2}\right)\) is the least upper bound.<br></li></ul></ul>
-
-    After:
-      8. (Abbott 1.3.4) Let \(\left\{A_{n}\right\}\) be a collection of non-empty and bounded above sets.<br><br>(a) Find a formula for \(\sup \left(A_{1} \cup A_{2}\right)\) and extend this to \(\sup \left(\bigcup_{n=1}^{N} A_{n}\right)\).<br><ul><li>Fromula is \(\sup \left(\bigcup_{n=1}^{N} A_{n}\right)\) =&nbsp;\(\max(\sup A_1,\ldots, \sup A_n)\)</li><li>Prove base case:</li><ul><li>Let \(a \in A_{1} \cup A_{2}\). Then, \(a \in A_{1}\) or \(a \in A_{2}\) which implies that \(a\) \(\leq \sup A_{1}\) or \(a\) \(\leq \sup A_{2}\). Thus, \(a\) \(\leq \max \left(\sup A_{1}, \sup A_{2}\right)\) and \(\max \left(\sup A_{1}, \sup A_{2}\right)\) is an upper bound for \(A_{1} \cup A_{2}\).</li><li>Let \(b\) be an upper bound for \(A_{1} \cup A_{2}\). Then, \(b\) is an upper bound for \(A_{1}\) and \(A_{2}\). It follows that \(\sup A_{1} \) \(\leq b\) and \(\sup A_{2}\)&nbsp; \(\leq b\). Therefore, \(\max \left(\sup A_{1}, \sup A_{2}\right)\)&nbsp; \(\leq b\) and \(\max \left(\sup A_{1}, \sup A_{2}\right)\) is the least upper bound.<br></li></ul></ul>
-
-============================================================
-
-Note ID: 1710675802439
-  Field: Text
-    Before:
-      \section*{9. (Abbott 1.3.6)}<br><br>(a) Let \(s=\sup A\) and \(t=\sup B\). Show that \(s+t\) is an upper bound for \(A+B\).<br><br>(b) Let \(u\) be an upper bound for \(A+B\) and fix \(a \in A\). Show that \(t \leq u-a\).<br><br>(c) Show that \(\sup A+B=s+t\).<br><br>(c) From (b) we have that \(t\)&nbsp; \(\leq u-a\) \(\Longrightarrow\) \(a\) \(\leq u-t\). Since \(a\) was arbitrary we have, by the least upper bound definition for \(A\), that \(s\) \(\leq u-t\) \(\Longrightarrow\) \(s+t\)&nbsp; \(\leq u\), where \(u\) was an upper bound. Therefore, \(s+t=\sup A+B\) is the least upper bound.
-
-    After:
-      \section*{9. (Abbott 1.3.6)}<br><br>(a) Let \(s=\sup A\) and \(t=\sup B\). Show that \(s+t\) is an upper bound for \(A+B\).<br><br>(b) Let \(u\) be an upper bound for \(A+B\) and fix \(a \in A\). Show that \(t \leq u-a\).<br><br>(c) Show that \(\sup A+B=s+t\).<br><br>(c) From (b) we have that \(t\)&nbsp; \(\leq u-a\) \(\Longrightarrow\) \(a\) \(\leq u-t\). Since \(a\) was arbitrary we have, by the least upper bound definition for \(A\), that \(s\) \(\leq u-t\) \(\Longrightarrow\) \(s+t\)&nbsp; \(\leq u\), where \(u\) was an upper bound. Therefore, \(s+t=\sup A+B\) is the least upper bound.
-
-============================================================
-
-Note ID: 1710918095812
-  Field: Text
-    Before:
-      \subsection*{1.2.1 Classification}<br><br>In classification problems, the output space is a set of \(C\) unordered and mutually exclusive labels known as classes, \(\mathcal{Y}\) = \(\{1,2, \ldots, C\}\). The problem of predicting the class label given an input is also called pattern recognition. (If there are just two classes, often denoted by \(y \in\{0,1\}\) or \(y \in\{-1,+1\}\), it is called binary classification.)
-
-    After:
-      \subsection*{1.2.1 Classification}<br><br>In classification problems, the output space is a set of \(C\) unordered and mutually exclusive labels known as classes, \(\mathcal{Y}\) = \(\{1,2, \ldots, C\}\). The problem of predicting the class label given an input is also called pattern recognition. (If there are just two classes, often denoted by \(y \in\{0,1\}\) or \(y \in\{-1,+1\}\), it is called binary classification.)
-
-============================================================
-
-Note ID: 1710918403299
-  Field: Text
-    Before:
-      The Iris dataset is an example of tabular data. When the inputs are of variable size (e.g., sequences of words, or social networks), rather than fixed-length vectors, the data is usually stored&nbsp;in some other format rather than in a design matrix. However, such data is often converted to a fixed-sized feature representation (a process known as featurization), thus implicitly creating a design matrix for further processing.
-
-    After:
-      The Iris dataset is an example of tabular data. When the inputs are of variable size (e.g., sequences of words, or social networks), rather than fixed-length vectors, the data is usually stored&nbsp;in some other format rather than in a design matrix. However, such data is often converted to a fixed-sized feature representation (a process known as featurization), thus implicitly creating a design matrix for further processing.
-
-============================================================
-
-Note ID: 1710918873879
-  Field: Text
-    Before:
-      The goal of supervised learning is to automatically come up with classification models such as the one shown in Figure 1.4a, so as to reliably predict the labels for any given input. A common way to measure performance on this task is in terms of the misclassification rate on the training set:<br><br><ul><li>\(\mathcal{L}(\boldsymbol{\theta})\)&nbsp; \(\triangleq\) \(\frac{1}{N} \sum_{n=1}^{N}\) \( \mathbb{I}\) ( \((y_{n} \neq f\left(\boldsymbol{x}_{n} ; \boldsymbol{\theta}\right) \) )</li></ul><br><br>where \(\mathbb{I}(e)\) is the binary indicator function:<br><br><ul><li>\(\mathbb{I}(e)\) =&nbsp; \(\begin{cases}1 &amp; \text { if } e \text { is true }&nbsp; \tag{1.3}\\ 0 &amp; \text { if } e \text { is false }\end{cases}\)</li></ul>
-
-    After:
-      The goal of supervised learning is to automatically come up with classification models such as the one shown in Figure 1.4a, so as to reliably predict the labels for any given input. A common way to measure performance on this task is in terms of the misclassification rate on the training set:<br><br><ul><li>\(\mathcal{L}(\boldsymbol{\theta})\)&nbsp; \(\triangleq\) \(\frac{1}{N} \sum_{n=1}^{N}\) \( \mathbb{I}\) ( \((y_{n} \neq f\left(\boldsymbol{x}_{n} ; \boldsymbol{\theta}\right) \) )</li></ul><br><br>where \(\mathbb{I}(e)\) is the binary indicator function:<br><br><ul><li>\(\mathbb{I}(e)\) =&nbsp; \(\begin{cases}1 &amp; \text { if } e \text { is true }&nbsp; \tag{1.3}\\ 0 &amp; \text { if } e \text { is false }\end{cases}\)</li></ul>
-
-============================================================
-
-Note ID: 1711006361860
-  Field: Text
-    Before:
-      A common classification function&nbsp; arises when \(f\) is an affine function of the form<br><br>\[<br>\begin{equation*}<br>f(\boldsymbol{x} ; \boldsymbol{\theta})=b+\boldsymbol{w}^{\top} \boldsymbol{x}=b+w_{1} x_{1}+w_{2} x_{2}+\cdots+w_{D} x_{D} \tag{1.10}<br>\end{equation*}<br>\]<br><br>where \(\boldsymbol{\theta}=(b, \boldsymbol{w})\) are the parameters of the model. This model is called logistic regression, and will be discussed in more detail in Chapter 10.<br><br>In statistics:<br><ul><li>&nbsp;the \(\boldsymbol{w}\) parameters are usually called regression coefficients (and are typically denoted by \(\boldsymbol{\beta}\) )&nbsp;</li><li>and \(b\) is called the intercept.&nbsp;</li><li>In ML, the parameters \(\boldsymbol{w}\) are called the weights and \(b\) is called the bias.&nbsp;</li><li>This terminology arises from electrical engineering, where we view the function \(f\) as a circuit which takes in \(\boldsymbol{x}\) and returns \(f(\boldsymbol{x})\). Each input is fed to the circuit on "wires", which have weights \(\boldsymbol{w}\). The circuit computes the weighted sum of its inputs, and adds a constant bias or offset term \(b\).&nbsp;</li></ul>
-
-    After:
-      A common classification function&nbsp; arises when \(f\) is an affine function of the form<br><br>\[<br>\begin{equation*}<br>f(\boldsymbol{x} ; \boldsymbol{\theta})=b+\boldsymbol{w}^{\top} \boldsymbol{x}=b+w_{1} x_{1}+w_{2} x_{2}+\cdots+w_{D} x_{D} \tag{1.10}<br>\end{equation*}<br>\]<br><br>where \(\boldsymbol{\theta}=(b, \boldsymbol{w})\) are the parameters of the model. This model is called logistic regression, and will be discussed in more detail in Chapter 10.<br><br>In statistics:<br><ul><li>&nbsp;the \(\boldsymbol{w}\) parameters are usually called regression coefficients (and are typically denoted by \(\boldsymbol{\beta}\) )&nbsp;</li><li>and \(b\) is called the intercept.&nbsp;</li><li>In ML, the parameters \(\boldsymbol{w}\) are called the weights and \(b\) is called the bias.&nbsp;</li><li>This terminology arises from electrical engineering, where we view the function \(f\) as a circuit which takes in \(\boldsymbol{x}\) and returns \(f(\boldsymbol{x})\). Each input is fed to the circuit on "wires", which have weights \(\boldsymbol{w}\). The circuit computes the weighted sum of its inputs, and adds a constant bias or offset term \(b\).&nbsp;</li></ul>
-
-============================================================
-
-Note ID: 1711006938808
-  Field: Text
-    Before:
-      When fitting probabilistic models, it is common to use the negative log probability as our loss function:<br><br><ul><li>\(\ell(y, f(\boldsymbol{x} ; \boldsymbol{\theta}))\) = \(-\log p(y \mid f(\boldsymbol{x} ; \boldsymbol{\theta})) \)</li></ul><br>The reasons for this are explained in Section 5.1.6.1, but the intuition is that a good model (with low loss) is one that assigns a high probability to the true output \(y\) for each corresponding input \(\boldsymbol{x}\). The average negative log probability of the training set is given by<br><br><ul><li>\(\mathrm{NLL}(\boldsymbol{\theta})\) = \(-\frac{1}{N}\)&nbsp; \(\sum_{n=1}^{N}\) \(\log p\left(y_{n} \mid f\left(\boldsymbol{x}_{n} ; \boldsymbol{\theta}\right)\right)\))</li></ul><br>This is called the negative log likelihood. If we minimize this, we can compute the maximum likelihood estimate or MLE:<br><br><ul><li>\(\hat{\boldsymbol{\theta} }_{\mathrm{mle} }\) = \(\underset{\boldsymbol{\theta} }{\operatorname{argmin} } \mathrm{NLL}(\boldsymbol{\theta} ) \)</li></ul>
-
-    After:
-      When fitting probabilistic models, it is common to use the negative log probability as our loss function:<br><br><ul><li>\(\ell(y, f(\boldsymbol{x} ; \boldsymbol{\theta}))\) = \(-\log p(y \mid f(\boldsymbol{x} ; \boldsymbol{\theta})) \)</li></ul><br>The reasons for this are explained in Section 5.1.6.1, but the intuition is that a good model (with low loss) is one that assigns a high probability to the true output \(y\) for each corresponding input \(\boldsymbol{x}\). The average negative log probability of the training set is given by<br><br><ul><li>\(\mathrm{NLL}(\boldsymbol{\theta})\) = \(-\frac{1}{N}\)&nbsp; \(\sum_{n=1}^{N}\) \(\log p\left(y_{n} \mid f\left(\boldsymbol{x}_{n} ; \boldsymbol{\theta}\right)\right)\))</li></ul><br>This is called the negative log likelihood. If we minimize this, we can compute the maximum likelihood estimate or MLE:<br><br><ul><li>\(\hat{\boldsymbol{\theta} }_{\mathrm{mle} }\) = \(\underset{\boldsymbol{\theta} }{\operatorname{argmin} } \mathrm{NLL}(\boldsymbol{\theta} ) \)</li></ul>
-
-============================================================
-
-Note ID: 1711031275244
-  Field: Text
-    Before:
-      The linear model in Figure 1.5a is obviously not a very good fit to the data. We can improve the fit by using a polynomial regression model of degree \(D\). This has the form \(f(x ; \boldsymbol{w})=\boldsymbol{w}^{\top} \boldsymbol{\phi}(x)\), where \(\boldsymbol{\phi}(x)\) is a feature vector derived from the input, which has the following form:<br><br>\[<br>\begin{equation*}<br>\phi(x)=\left[1, x, x^{2}, \ldots, x^{D}\right] \tag{1.26}<br>\end{equation*}<br>\]<br><br>This is a simple example of feature preprocessing, also called feature engineering.
-
-    After:
-      The linear model in Figure 1.5a is obviously not a very good fit to the data. We can improve the fit by using a polynomial regression model of degree \(D\). This has the form \(f(x ; \boldsymbol{w})=\boldsymbol{w}^{\top} \boldsymbol{\phi}(x)\), where \(\boldsymbol{\phi}(x)\) is a feature vector derived from the input, which has the following form:<br><br>\[<br>\begin{equation*}<br>\phi(x)=\left[1, x, x^{2}, \ldots, x^{D}\right] \tag{1.26}<br>\end{equation*}<br>\]<br><br>This is a simple example of feature preprocessing, also called feature engineering.
-
-============================================================
-
-Note ID: 1711317250904
-  Field: Text
-    Before:
-      We can recursively decompose the feature extractor \(\phi(\boldsymbol{x} ; \mathbf{V})\) into a composition of simpler functions. The resulting model then becomes a stack of \(L\) nested functions:<br><br>\[<br>\begin{equation*}<br>f(\boldsymbol{x} ; \boldsymbol{\theta})=f_{L}\left(f_{L-1}\left(\cdots\left(f_{1}(\boldsymbol{x})\right) \cdots\right)\right) \tag{1.29}<br>\end{equation*}<br>\]<br><br><br><br><br>The final layer is linear and has the form:<br><ul><li>&nbsp;\(f_{L}(\boldsymbol{x})\) = \(\boldsymbol{w}_{L}^{\top} \boldsymbol{x}\),&nbsp;</li><li>so \(f(\boldsymbol{x} ; \boldsymbol{\theta})\) = \(\boldsymbol{w}_{L}^{\top}\) \(f_{1: L-1}(\boldsymbol{x})\)</li><li>where \(f_{1: L-1}(\boldsymbol{x})\) = \(f_{L-1}\left(\cdots\left(f_{1}(\boldsymbol{x})\right) \cdots\right)\) is the learned feature extractor</li></ul>
-
-    After:
-      We can recursively decompose the feature extractor \(\phi(\boldsymbol{x} ; \mathbf{V})\) into a composition of simpler functions. The resulting model then becomes a stack of \(L\) nested functions:<br><br>\[<br>\begin{equation*}<br>f(\boldsymbol{x} ; \boldsymbol{\theta})=f_{L}\left(f_{L-1}\left(\cdots\left(f_{1}(\boldsymbol{x})\right) \cdots\right)\right) \tag{1.29}<br>\end{equation*}<br>\]<br><br><br><br><br>The final layer is linear and has the form:<br><ul><li>&nbsp;\(f_{L}(\boldsymbol{x})\) = \(\boldsymbol{w}_{L}^{\top} \boldsymbol{x}\),&nbsp;</li><li>so \(f(\boldsymbol{x} ; \boldsymbol{\theta})\) = \(\boldsymbol{w}_{L}^{\top}\) \(f_{1: L-1}(\boldsymbol{x})\)</li><li>where \(f_{1: L-1}(\boldsymbol{x})\) = \(f_{L-1}\left(\cdots\left(f_{1}(\boldsymbol{x})\right) \cdots\right)\) is the learned feature extractor</li></ul>
-
-============================================================
-
-Note ID: 1711318901906
-  Field: Text
-    Before:
-      To detect if a model is overfitting, let us assume (for now) that we have access to the true (but unknown) distribution \(p^{*}(\boldsymbol{x}, \boldsymbol{y})\) used to generate the training set. Then, instead of computing the empirical risk we compute the theoretical expected loss or population risk<br><br>\[<br>\begin{equation*}<br>\mathcal{L}\left(\boldsymbol{\theta} ; p^{*}\right) \triangleq \mathbb{E}_{p^{*}(\boldsymbol{x}, \boldsymbol{y})}[\ell(\boldsymbol{y}, f(\boldsymbol{x} ; \boldsymbol{\theta}))] \tag{1.31}<br>\end{equation*}<br>\]<br><br>The difference \(\mathcal{L}\left(\boldsymbol{\theta} ; p^{*}\right)\) - \(\mathcal{L}\left(\boldsymbol{\theta} ; \mathcal{D}_{\text {train } }\right)\) is called the generalization gap. If a model has a large generalization gap (i.e., low empirical risk but high population risk), it is a sign that it is overfitting.
-
-    After:
-      To detect if a model is overfitting, let us assume (for now) that we have access to the true (but unknown) distribution \(p^{*}(\boldsymbol{x}, \boldsymbol{y})\) used to generate the training set. Then, instead of computing the empirical risk we compute the theoretical expected loss or population risk<br><br>\[<br>\begin{equation*}<br>\mathcal{L}\left(\boldsymbol{\theta} ; p^{*}\right) \triangleq \mathbb{E}_{p^{*}(\boldsymbol{x}, \boldsymbol{y})}[\ell(\boldsymbol{y}, f(\boldsymbol{x} ; \boldsymbol{\theta}))] \tag{1.31}<br>\end{equation*}<br>\]<br><br>The difference \(\mathcal{L}\left(\boldsymbol{\theta} ; p^{*}\right)\) - \(\mathcal{L}\left(\boldsymbol{\theta} ; \mathcal{D}_{\text {train } }\right)\) is called the generalization gap. If a model has a large generalization gap (i.e., low empirical risk but high population risk), it is a sign that it is overfitting.
-
-============================================================
-
-Note ID: 1711319351580
-  Field: Text
-    Before:
-      In practice, we need to partition the data into three sets, namely the training set, the test set and a validation set; the latter is used for model selection, and we just use the test set to estimate future performance (the population risk), i.e., the test set is not used for model fitting or model selection. See Section 4.5.4 for further details.
-
-    After:
-      In practice, we need to partition the data into three sets, namely the training set, the test set and a validation set; the latter is used for model selection, and we just use the test set to estimate future performance (the population risk), i.e., the test set is not used for model fitting or model selection. See Section 4.5.4 for further details.
-
-============================================================
-
-Note ID: 1711369559686
-  Field: Text
-    Before:
-      All models are wrong, but some models are useful. - George Box [BD87, p424]. \({ }^{5}\)<br>
-
-    After:
-      All models are wrong, but some models are useful. - George Box [BD87, p424]. \({ }^{5}\)<br>
-
-============================================================
-
-Note ID: 1711369830154
-  Field: Text
-    Before:
-      From a probabilistic perspective, we can view the task of unsupervised learning as fitting an unconditional model of the form \(p(\boldsymbol{x})\), which can generate new data \(\boldsymbol{x}\), whereas supervised learning involves fitting a conditional model of the form \(p(\boldsymbol{y} \mid \boldsymbol{x})\), which specifies (a distribution over) outputs given inputs. \({ }^{6}\)
-
-    After:
-      From a probabilistic perspective, we can view the task of unsupervised learning as fitting an unconditional model of the form \(p(\boldsymbol{x})\), which can generate new data \(\boldsymbol{x}\), whereas supervised learning involves fitting a conditional model of the form \(p(\boldsymbol{y} \mid \boldsymbol{x})\), which specifies (a distribution over) outputs given inputs. \({ }^{6}\)
-
-============================================================
-
-Note ID: 1711370013631
-  Field: Text
-    Before:
-      Finally, unsupervised learning forces the model to "explain" the high-dimensional inputs, rather than just the low-dimensional outputs. This allows us to learn richer models of "how the world works"
-
-    After:
-      Finally, unsupervised learning forces the model to "explain" the high-dimensional inputs, rather than just the low-dimensional outputs. This allows us to learn richer models of "how the world works"
-
-============================================================
-
-Note ID: 1711370448715
-  Field: Text
-    Before:
-      When dealing with high-dimensional data, it is often useful to reduce the dimensionality by projecting it to a lower dimensional subspace which captures the "essence" of the data.
-
-    After:
-      When dealing with high-dimensional data, it is often useful to reduce the dimensionality by projecting it to a lower dimensional subspace which captures the "essence" of the data.
-
-============================================================
-
-Note ID: 1711370970204
-  Field: Text
-    Before:
-      Of course, assuming a linear mapping in the style of PCA from \(\boldsymbol{z}_{n}\) to \(\boldsymbol{x}_{n}\) is very restrictive. <br><br>However, we can create nonlinear extensions by defining:<br><ul><li>&nbsp;\(p\left(\boldsymbol{x}_{n} \mid \boldsymbol{z}_{n} ; \boldsymbol{\theta}\right)\) = \(\mathcal{N}\) ( \(\boldsymbol{x}_{n} \mid f\left(\boldsymbol{z}_{n} ; \boldsymbol{\theta}\right), \sigma^{2} \mathbf{I}\))</li><ul><li>&nbsp;where \(f(\boldsymbol{z} ; \boldsymbol{\theta})\) is a nonlinear model, such as a deep neural network.</li></ul><li>It becomes much harder to fit such a model (i.e., to estimate the parameters \(\boldsymbol{\theta}\) ), because the inputs to the neural net have to be inferred, as well as the parameters of the model. However, there are various approximate methods, such as the variational autoencoder which can be applied (see Section 20.3.5).</li></ul>
-
-    After:
-      Of course, assuming a linear mapping in the style of PCA from \(\boldsymbol{z}_{n}\) to \(\boldsymbol{x}_{n}\) is very restrictive. <br><br>However, we can create nonlinear extensions by defining:<br><ul><li>&nbsp;\(p\left(\boldsymbol{x}_{n} \mid \boldsymbol{z}_{n} ; \boldsymbol{\theta}\right)\) = \(\mathcal{N}\) ( \(\boldsymbol{x}_{n} \mid f\left(\boldsymbol{z}_{n} ; \boldsymbol{\theta}\right), \sigma^{2} \mathbf{I}\))</li><ul><li>&nbsp;where \(f(\boldsymbol{z} ; \boldsymbol{\theta})\) is a nonlinear model, such as a deep neural network.</li></ul><li>It becomes much harder to fit such a model (i.e., to estimate the parameters \(\boldsymbol{\theta}\) ), because the inputs to the neural net have to be inferred, as well as the parameters of the model. However, there are various approximate methods, such as the variational autoencoder which can be applied (see Section 20.3.5).</li></ul>
-
-============================================================
-
-Note ID: 1711371602079
-  Field: Text
-    Before:
-      A common method for evaluating unsupervised models is to measure the probability assigned by the model to unseen test examples. We can do this by computing the (unconditional) negative log likelihood of the data:<br><br>\[<br>\begin{equation*}<br>\mathcal{L}(\boldsymbol{\theta} ; \mathcal{D})=-\frac{1}{|\mathcal{D}|} \sum_{\boldsymbol{x} \in \mathcal{D} } \log p(\boldsymbol{x} \mid \boldsymbol{\theta} ) \tag{1.33}<br>\end{equation*}<br>\]<br><br>This treats the problem of unsupervised learning as one of density estimation. The idea is that a good model will not be "surprised" by actual data samples. Furthermore, since probabilities must sum to 1.0, if the model assigns high probability to regions of data space where the data samples come from, it implicitly assigns low probability to the regions where the data does not come from. Thus the model has learned to capture the typical patterns in the data. This can be used inside of a data compression algorithm.
-
-    After:
-      A common method for evaluating unsupervised models is to measure the probability assigned by the model to unseen test examples. We can do this by computing the (unconditional) negative log likelihood of the data:<br><br>\[<br>\begin{equation*}<br>\mathcal{L}(\boldsymbol{\theta} ; \mathcal{D})=-\frac{1}{|\mathcal{D}|} \sum_{\boldsymbol{x} \in \mathcal{D} } \log p(\boldsymbol{x} \mid \boldsymbol{\theta} ) \tag{1.33}<br>\end{equation*}<br>\]<br><br>This treats the problem of unsupervised learning as one of density estimation. The idea is that a good model will not be "surprised" by actual data samples. Furthermore, since probabilities must sum to 1.0, if the model assigns high probability to regions of data space where the data samples come from, it implicitly assigns low probability to the regions where the data does not come from. Thus the model has learned to capture the typical patterns in the data. This can be used inside of a data compression algorithm.
-
-============================================================
-
-Note ID: 1711373590124
-  Field: Text
-    Before:
-      &nbsp;"If intelligence was a cake, unsupervised learning would be the chocolate sponge, supervised learning would be the icing, and reinforcement learning would be the cherry."&nbsp;
-
-    After:
-      &nbsp;"If intelligence was a cake, unsupervised learning would be the chocolate sponge, supervised learning would be the icing, and reinforcement learning would be the cherry."&nbsp;
-
-============================================================
-
-Note ID: 1711388267027
-  Field: Text
-    Before:
-      \subsection*{1.5.4.1 Bag of words model}<br><br>Let \(x_{n t}\) be the token at location \(t\) in the \(n^{\prime}\) th document. If there are \(D\) unique tokens in the vocabulary, then we can represent the \(n\) 'th document as a \(D\)-dimensional vector \(\tilde{\boldsymbol{x} }_{n}\), where \(\tilde{x}_{n v}\) is the number of times that word \(v\) occurs in document \(n\) :<br><br><ul><li>\(\tilde{x}_{n v}\) = \(\sum_{t=1}^{T}\) \(\mathbb{I}\left(x_{n t}=v\right) \)</li></ul>
-
-    After:
-      \subsection*{1.5.4.1 Bag of words model}<br><br>Let \(x_{n t}\) be the token at location \(t\) in the \(n^{\prime}\) th document. If there are \(D\) unique tokens in the vocabulary, then we can represent the \(n\) 'th document as a \(D\)-dimensional vector \(\tilde{\boldsymbol{x} }_{n}\), where \(\tilde{x}_{n v}\) is the number of times that word \(v\) occurs in document \(n\) :<br><br><ul><li>\(\tilde{x}_{n v}\) = \(\sum_{t=1}^{T}\) \(\mathbb{I}\left(x_{n t}=v\right) \)</li></ul>
-
-============================================================
-
-Note ID: 1711389122115
-  Field: Text
-    Before:
-      Although the TF-IDF transformation improves on raw count vectors by placing more weight on "informative" words and less on "uninformative" words, it does not solve the fundamental problem with the one-hot encoding (from which count vectors are derived), which is that that semantically similar words may not be any closer (in vector space) than semantically dissimilar words. Thus the assumption that points that are close in input space should have similar outputs, which is implicitly made by most prediction models, is invalid .
-
-    After:
-      Although the TF-IDF transformation improves on raw count vectors by placing more weight on "informative" words and less on "uninformative" words, it does not solve the fundamental problem with the one-hot encoding (from which count vectors are derived), which is that that semantically similar words may not be any closer (in vector space) than semantically dissimilar words. Thus the assumption that points that are close in input space should have similar outputs, which is implicitly made by most prediction models, is invalid .
-
-============================================================
-
-Note ID: 1711391136873
-  Field: Text
-    Before:
-      What does it mean for input data to be missing:<br>To model this:<br><ul><li>let \(\mathbf{M}\) be an \(N \times D\) matrix of binary variables:</li><ul><li>where \(M_{n d}\) = \(1\) if feature \(d\) in example \(n\) is missing</li><li>and \(M_{n d}\) = \(0\) otherwise.&nbsp;</li></ul><li>Let:</li><ul><li>&nbsp;\(\mathbf{X}_{v}\) be the visible parts of the input feature matrix, corresponding to \(M_{n d}\) = \(0\),&nbsp;</li><li>and \(\mathbf{X}_{h}\) be the missing parts, corresponding to \(M_{n d}\) = \(1\).&nbsp;</li></ul><li>Let \(\mathbf{Y}\) be the output label matrix, which we assume is fully observed.&nbsp;</li><li>If we assume: \(p\left(\mathbf{M} \mid \mathbf{X}_{v}, \mathbf{X}_{h}, \mathbf{Y}\right)\) = \(p(\mathbf{M})\)</li><ul><li>we say the data is missing completely at random or MCAR, since the missingness does not depend on the hidden or observed features.&nbsp;</li></ul><li>If we assume \(p\left(\mathbf{M} \mid \mathbf{X}_{v}, \mathbf{X}_{h}, \mathbf{Y}\right)\) = \(p\left(\mathbf{M} \mid \mathbf{X}_{v}, \mathbf{Y}\right)\)</li><ul><li>we say the data is missing at random or MAR, since the missingness does not depend on the hidden features, but may depend on the visible features. If neither of these assumptions hold, we say the data is not missing at random or NMAR.</li></ul></ul>
-
-    After:
-      What does it mean for input data to be missing:<br>To model this:<br><ul><li>let \(\mathbf{M}\) be an \(N \times D\) matrix of binary variables:</li><ul><li>where \(M_{n d}\) = \(1\) if feature \(d\) in example \(n\) is missing</li><li>and \(M_{n d}\) = \(0\) otherwise.&nbsp;</li></ul><li>Let:</li><ul><li>&nbsp;\(\mathbf{X}_{v}\) be the visible parts of the input feature matrix, corresponding to \(M_{n d}\) = \(0\),&nbsp;</li><li>and \(\mathbf{X}_{h}\) be the missing parts, corresponding to \(M_{n d}\) = \(1\).&nbsp;</li></ul><li>Let \(\mathbf{Y}\) be the output label matrix, which we assume is fully observed.&nbsp;</li><li>If we assume: \(p\left(\mathbf{M} \mid \mathbf{X}_{v}, \mathbf{X}_{h}, \mathbf{Y}\right)\) = \(p(\mathbf{M})\)</li><ul><li>we say the data is missing completely at random or MCAR, since the missingness does not depend on the hidden or observed features.&nbsp;</li></ul><li>If we assume \(p\left(\mathbf{M} \mid \mathbf{X}_{v}, \mathbf{X}_{h}, \mathbf{Y}\right)\) = \(p\left(\mathbf{M} \mid \mathbf{X}_{v}, \mathbf{Y}\right)\)</li><ul><li>we say the data is missing at random or MAR, since the missingness does not depend on the hidden features, but may depend on the visible features. If neither of these assumptions hold, we say the data is not missing at random or NMAR.</li></ul></ul>
-
-============================================================
-
-Note ID: 1711392097285
-  Field: Text
-    Before:
-      We are all comfortable saying that the probability that a (fair) coin will land heads is \(50 \%\). But what does this mean? There are actually two different interpretations of probability. One is called the frequentist interpretation. In this view, probabilities represent long run frequencies of events that can happen multiple times. For example, the above statement means that, if we flip the coin many times, we expect it to land heads about half the time. \({ }^{1}\)&nbsp;<br><br>The other interpretation is called the Bayesian interpretation of probability. In this view, probability is used to quantify our uncertainty or ignorance about something; hence it is fundamentally related to information rather than repeated trials. In the Bayesian view, the above statement means we believe the coin is equally likely to land heads or tails on the next toss.
-
-    After:
-      We are all comfortable saying that the probability that a (fair) coin will land heads is \(50 \%\). But what does this mean? There are actually two different interpretations of probability. One is called the frequentist interpretation. In this view, probabilities represent long run frequencies of events that can happen multiple times. For example, the above statement means that, if we flip the coin many times, we expect it to land heads about half the time. \({ }^{1}\)&nbsp;<br><br>The other interpretation is called the Bayesian interpretation of probability. In this view, probability is used to quantify our uncertainty or ignorance about something; hence it is fundamentally related to information rather than repeated trials. In the Bayesian view, the above statement means we believe the coin is equally likely to land heads or tails on the next toss.
-
-============================================================
-
-Note ID: 1711392154256
-  Field: Text
-    Before:
-      One big advantage of the Bayesian interpretation is that it can be used to model our uncertainty about one-off events that do not have long term frequencies.&nbsp; We shall therefore adopt the Bayesian interpretation in this book. Fortunately, the basic rules of probability theory are the same, no matter which interpretation is adopted.
-
-    After:
-      One big advantage of the Bayesian interpretation is that it can be used to model our uncertainty about one-off events that do not have long term frequencies.&nbsp; We shall therefore adopt the Bayesian interpretation in this book. Fortunately, the basic rules of probability theory are the same, no matter which interpretation is adopted.
-
-============================================================
-
-Note ID: 1711392255428
-  Field: Text
-    Before:
-      The uncertainty in our predictions can arise for two fundamentally different reasons. The first is due to our ignorance of the underlying hidden causes or mechanism generating our data. This is&nbsp;called epistemic uncertainty. However, a simpler term for this is model uncertainty. <br><br>The second kind of uncertainty arises from intrinsic variability, which cannot be reduced even if we collect more data. This is sometimes called aleatoric uncertainty [Hac75; KD09], although a simpler term would be data uncertainty.&nbsp;
-
-    After:
-      The uncertainty in our predictions can arise for two fundamentally different reasons. The first is due to our ignorance of the underlying hidden causes or mechanism generating our data. This is&nbsp;called epistemic uncertainty. However, a simpler term for this is model uncertainty. <br><br>The second kind of uncertainty arises from intrinsic variability, which cannot be reduced even if we collect more data. This is sometimes called aleatoric uncertainty [Hac75; KD09], although a simpler term would be data uncertainty.&nbsp;
-
-============================================================
-
-Note ID: 1711392466565
-  Field: Text
-    Before:
-      We define an event, denoted by the binary variable \(A\), as some state of the world that either holds or does not hold.&nbsp; The expression \(\operatorname{Pr}(A)\) denotes the probability with which you believe event \(A\) is true (or the long run fraction of times that \(A\) will occur::frequentist::frequentist).
-
-    After:
-      We define an event, denoted by the binary variable \(A\), as some state of the world that either holds or does not hold.&nbsp; The expression \(\operatorname{Pr}(A)\) denotes the probability with which you believe event \(A\) is true (or the long run fraction of times that \(A\) will occur::frequentist::frequentist).
-
-============================================================
-
-Note ID: 1711392608831
-  Field: Text
-    Before:
-      We require that:<br><ul><li>&nbsp;\(0\)&nbsp; \(\leq\) \(\operatorname{Pr}(A)\) \(\leq\) \(1\),&nbsp;</li><li>where \(\operatorname{Pr}(A)\) = \(0\) means the event definitely will not happen, and \(\operatorname{Pr}(A)\) = \(1\) means the event definitely will happen.&nbsp;</li><li>We write \(\operatorname{Pr}(\bar{A})\) to denote the probability of event \(A\) not happening; this is defined to be \(\operatorname{Pr}(\bar{A})\)&nbsp; =&nbsp; \(1-\operatorname{Pr}(A)\).</li></ul>
-
-    After:
-      We require that:<br><ul><li>&nbsp;\(0\)&nbsp; \(\leq\) \(\operatorname{Pr}(A)\) \(\leq\) \(1\),&nbsp;</li><li>where \(\operatorname{Pr}(A)\) = \(0\) means the event definitely will not happen, and \(\operatorname{Pr}(A)\) = \(1\) means the event definitely will happen.&nbsp;</li><li>We write \(\operatorname{Pr}(\bar{A})\) to denote the probability of event \(A\) not happening; this is defined to be \(\operatorname{Pr}(\bar{A})\)&nbsp; =&nbsp; \(1-\operatorname{Pr}(A)\).</li></ul>
-
-============================================================
-
-Note ID: 1711392895585
-  Field: Text
-    Before:
-      The probability of event \(A\) or \(B\) happening is given by<br><br><ul><li>\(\operatorname{Pr}(A \vee B)\) = \(\operatorname{Pr}(A)+\operatorname{Pr}(B)-\operatorname{Pr}(A \wedge B) \)</li></ul><br>If the events are mutually exclusive (so they cannot happen at the same time), we get<br><br><ul><li>\(\operatorname{Pr}(A \vee B)\) = \(\operatorname{Pr}(A)+\operatorname{Pr}(B) \)</li></ul>
-
-    After:
-      The probability of event \(A\) or \(B\) happening is given by<br><br><ul><li>\(\operatorname{Pr}(A \vee B)\) = \(\operatorname{Pr}(A)+\operatorname{Pr}(B)-\operatorname{Pr}(A \wedge B) \)</li></ul><br>If the events are mutually exclusive (so they cannot happen at the same time), we get<br><br><ul><li>\(\operatorname{Pr}(A \vee B)\) = \(\operatorname{Pr}(A)+\operatorname{Pr}(B) \)</li></ul>
-
-============================================================
-
-Note ID: 1711396221772
-  Field: Text
-    Before:
-      If the sample space \(\mathcal{X}\) is finite or countably infinite, then \(X\) is called a discrete random variable. In this case, we denote the probability of the event that \(X\) has value \(x\) by \(\operatorname{Pr}(X=x)\). We define the probability mass function or pmf as a function which computes the probability of events which correspond to setting the rv to each possible value:<br><br><ul><li>\(p(x)\) \(\triangleq\) \(\operatorname{Pr}(X=x) \)</li><li>The pmf satisfies the properties \(0 \leq p(x) \leq 1\) and \(\sum_{x \in \mathcal{X} } p(x)\) = \(1\).<br></li></ul>
-
-    After:
-      If the sample space \(\mathcal{X}\) is finite or countably infinite, then \(X\) is called a discrete random variable. In this case, we denote the probability of the event that \(X\) has value \(x\) by \(\operatorname{Pr}(X=x)\). We define the probability mass function or pmf as a function which computes the probability of events which correspond to setting the rv to each possible value:<br><br><ul><li>\(p(x)\) \(\triangleq\) \(\operatorname{Pr}(X=x) \)</li><li>The pmf satisfies the properties \(0 \leq p(x) \leq 1\) and \(\sum_{x \in \mathcal{X} } p(x)\) = \(1\).<br></li></ul>
-
-============================================================
-
-Note ID: 1711396476842
-  Field: Text
-    Before:
-      If \(X \in \mathbb{R}\) is a real-valued quantity, it is called a continuous random variable. In this case, we can no longer create a finite (or countable) set of distinct possible values it can take on. However, there are a countable number of intervals which we can partition the real line into. If we associate events with \(X\) being in each one of these intervals, we can use the methods discussed above for discrete random variables.&nbsp;
-
-    After:
-      If \(X \in \mathbb{R}\) is a real-valued quantity, it is called a continuous random variable. In this case, we can no longer create a finite (or countable) set of distinct possible values it can take on. However, there are a countable number of intervals which we can partition the real line into. If we associate events with \(X\) being in each one of these intervals, we can use the methods discussed above for discrete random variables.&nbsp;
-
-============================================================
-
-Note ID: 1711396653979
-  Field: Text
-    Before:
-      <img src="paste-0622bd44e77dcf18b6e1ca046692817edc392711.jpg"><img src="paste-d39f422d14230b5c93c6df8bdc29f88cfdff179c.jpg"><br>Figure 2.2: (a) Plot of the cdf for the standard normal, \(\mathcal{N}(0,1)\). Generated by gauss_plot.ipynb. (b) Corresponding pdf. The shaded regions each contain \(\alpha / 2\) of the probability mass. Therefore the nonshaded region contains \(1-\alpha\) of the probability mass. The leftmost cutoff point is \(\Phi^{-1}(\alpha / 2)\), where \(\Phi\) is the cdf of the Gaussian. By symmetry, the rightmost cutoff point is \(\Phi^{-1}(1-\alpha / 2)\) = \(-\Phi^{-1}(\alpha / 2)\). Generated by quantile plot.ipynb.
-
-    After:
-      <img src="paste-0622bd44e77dcf18b6e1ca046692817edc392711.jpg"><img src="paste-d39f422d14230b5c93c6df8bdc29f88cfdff179c.jpg"><br>Figure 2.2: (a) Plot of the cdf for the standard normal, \(\mathcal{N}(0,1)\). Generated by gauss_plot.ipynb. (b) Corresponding pdf. The shaded regions each contain \(\alpha / 2\) of the probability mass. Therefore the nonshaded region contains \(1-\alpha\) of the probability mass. The leftmost cutoff point is \(\Phi^{-1}(\alpha / 2)\), where \(\Phi\) is the cdf of the Gaussian. By symmetry, the rightmost cutoff point is \(\Phi^{-1}(1-\alpha / 2)\) = \(-\Phi^{-1}(\alpha / 2)\). Generated by quantile plot.ipynb.
-
-============================================================
-
-Note ID: 1711398350656
-  Field: Text
-    Before:
-      We define the probability density function or pdf as the derivative of the cdf:<br><br><ul><li>\(p(x)\)&nbsp; \(\triangleq\) \(\frac{d}{d x}\) \(P(x) \)</li></ul><br>(Note that this derivative does not always exist, in which case the pdf is not defined.)&nbsp;<br>
-
-    After:
-      We define the probability density function or pdf as the derivative of the cdf:<br><br><ul><li>\(p(x)\)&nbsp; \(\triangleq\) \(\frac{d}{d x}\) \(P(x) \)</li></ul><br>(Note that this derivative does not always exist, in which case the pdf is not defined.)&nbsp;<br>
-
-============================================================
-
-Note ID: 1711398937400
-  Field: Text
-    Before:
-      For example, let \(\Phi\) be the cdf of the Gaussian distribution \(\mathcal{N}(0,1)\), and \(\Phi^{-1}\) be the inverse cdf. Then points to the left of \(\Phi^{-1}(\alpha / 2)\) contain \(\alpha / 2\) of the probability mass, as illustrated in Figure \(2.2 \mathrm{~b}\). By symmetry, points to the right of \(\Phi^{-1}(1-\alpha / 2)\) also contain \(\alpha / 2\) of the mass. Hence the central interval \(\left(\Phi^{-1}(\alpha / 2), \Phi^{-1}(1-\alpha / 2)\right)\) contains \(1-\alpha\) of the mass. If we set \(\alpha=0.05\), the central \(95 \%\) interval is covered by the range<br><br>\[<br>\begin{equation*}<br>\left(\Phi^{-1}(0.025), \Phi^{-1}(0.975)\right)=(-1.96,1.96) \tag{2.16}<br>\end{equation*}<br>\]<br><br>If the distribution is \(\mathcal{N}\left(\mu, \sigma^{2}\right)\), then the \(95 \%\) interval becomes \((\mu-1.96 \sigma, \mu+1.96 \sigma)\). This is often approximated by writing \(\mu\) \(\pm 2 \sigma\).
-
-    After:
-      For example, let \(\Phi\) be the cdf of the Gaussian distribution \(\mathcal{N}(0,1)\), and \(\Phi^{-1}\) be the inverse cdf. Then points to the left of \(\Phi^{-1}(\alpha / 2)\) contain \(\alpha / 2\) of the probability mass, as illustrated in Figure \(2.2 \mathrm{~b}\). By symmetry, points to the right of \(\Phi^{-1}(1-\alpha / 2)\) also contain \(\alpha / 2\) of the mass. Hence the central interval \(\left(\Phi^{-1}(\alpha / 2), \Phi^{-1}(1-\alpha / 2)\right)\) contains \(1-\alpha\) of the mass. If we set \(\alpha=0.05\), the central \(95 \%\) interval is covered by the range<br><br>\[<br>\begin{equation*}<br>\left(\Phi^{-1}(0.025), \Phi^{-1}(0.975)\right)=(-1.96,1.96) \tag{2.16}<br>\end{equation*}<br>\]<br><br>If the distribution is \(\mathcal{N}\left(\mu, \sigma^{2}\right)\), then the \(95 \%\) interval becomes \((\mu-1.96 \sigma, \mu+1.96 \sigma)\). This is often approximated by writing \(\mu\) \(\pm 2 \sigma\).
-
-============================================================
-
-Note ID: 1711400010919
-  Field: Text
-    Before:
-      The most familiar property of a distribution is its mean, or expected value, often denoted by \(\mu\). For continuous rv's, the mean is defined as follows:<br><br><ul><li>\(\mathbb{E}[X]\) \(\triangleq\) \(\int_{\mathcal{X} } x p(x) d x \tag{2.24}\)</li></ul><br>If the integral is not finite, the mean is not defined.<br>
-
-    After:
-      The most familiar property of a distribution is its mean, or expected value, often denoted by \(\mu\). For continuous rv's, the mean is defined as follows:<br><br><ul><li>\(\mathbb{E}[X]\) \(\triangleq\) \(\int_{\mathcal{X} } x p(x) d x \tag{2.24}\)</li></ul><br>If the integral is not finite, the mean is not defined.<br>
-
-============================================================
-
-Note ID: 1711407154623
-  Field: Text
-    Before:
-      Bayes' rule itself is very simple: it is just a formula for computing the probability distribution over possible values of an unknown (or hidden) quantity \(H\) given some observed data \(Y=y\) :<br><br>\[<br>\begin{equation*}<br>p(H=h \mid Y=y)=\frac{p(H=h) p(Y=y \mid H=h)}{p(Y=y)} \tag{2.51}<br>\end{equation*}<br>\]<br><br><ul><li>In Equation (2.51), the term \(p(H)\) represents what we know about possible values of \(H\) before we see any data; this is called the prior distribution.&nbsp;</li><li>(If \(H\) has \(K\) possible values, then \(p(H)\) is a vector of \(K\) probabilities, that sum to 1.)&nbsp;</li><li>The term \(p(Y \mid H=h)\) represents the distribution over the possible outcomes \(Y\) we expect to see if \(H=h\); this is called the observation distribution.&nbsp;</li><li>When we evaluate this at a point corresponding to the actual observations, \(y\), we get the function \(p(Y=y \mid H=h)\), which is called the likelihood. (Note that this is a function of \(h\), since \(y\) is fixed, but it is not a probability distribution, since it does not sum to one.) Multiplying the prior distribution \(p(H=h)\) by the likelihood function \(p(Y=y \mid H=h)\) for each \(h\) gives the unnormalized joint distribution \(p(H=h, Y=y)\)::math::math.&nbsp;</li></ul>
-
-    After:
-      Bayes' rule itself is very simple: it is just a formula for computing the probability distribution over possible values of an unknown (or hidden) quantity \(H\) given some observed data \(Y=y\) :<br><br>\[<br>\begin{equation*}<br>p(H=h \mid Y=y)=\frac{p(H=h) p(Y=y \mid H=h)}{p(Y=y)} \tag{2.51}<br>\end{equation*}<br>\]<br><br><ul><li>In Equation (2.51), the term \(p(H)\) represents what we know about possible values of \(H\) before we see any data; this is called the prior distribution.&nbsp;</li><li>(If \(H\) has \(K\) possible values, then \(p(H)\) is a vector of \(K\) probabilities, that sum to 1.)&nbsp;</li><li>The term \(p(Y \mid H=h)\) represents the distribution over the possible outcomes \(Y\) we expect to see if \(H=h\); this is called the observation distribution.&nbsp;</li><li>When we evaluate this at a point corresponding to the actual observations, \(y\), we get the function \(p(Y=y \mid H=h)\), which is called the likelihood. (Note that this is a function of \(h\), since \(y\) is fixed, but it is not a probability distribution, since it does not sum to one.) Multiplying the prior distribution \(p(H=h)\) by the likelihood function \(p(Y=y \mid H=h)\) for each \(h\) gives the unnormalized joint distribution \(p(H=h, Y=y)\)::math::math.&nbsp;</li></ul>
-
-============================================================
-
-Note ID: 1711407334277
-  Field: Text
-    Before:
-      Bayes' rule itself is very simple: it is just a formula for computing the probability distribution over possible values of an unknown (or hidden) quantity \(H\) given some observed data \(Y=y\) :<br><br>\[<br>\begin{equation*}<br>p(H=h \mid Y=y)=\frac{p(H=h) p(Y=y \mid H=h)}{p(Y=y)} \tag{2.51}<br>\end{equation*}<br>\]<br><br>Normalizing the joint distribution by computing \(p(H=h, Y=y) / p(Y=y)\) for each \(h\) gives the posterior distribution \(p(H=h \mid Y=y)\); this represents our new belief state about the possible values of \(H\).<br><br>We can summarize Bayes rule in words as follows:<br><br>posterior \(\propto\) prior \(\times\) likelihood
-
-    After:
-      Bayes' rule itself is very simple: it is just a formula for computing the probability distribution over possible values of an unknown (or hidden) quantity \(H\) given some observed data \(Y=y\) :<br><br>\[<br>\begin{equation*}<br>p(H=h \mid Y=y)=\frac{p(H=h) p(Y=y \mid H=h)}{p(Y=y)} \tag{2.51}<br>\end{equation*}<br>\]<br><br>Normalizing the joint distribution by computing \(p(H=h, Y=y) / p(Y=y)\) for each \(h\) gives the posterior distribution \(p(H=h \mid Y=y)\); this represents our new belief state about the possible values of \(H\).<br><br>We can summarize Bayes rule in words as follows:<br><br>posterior \(\propto\) prior \(\times\) likelihood
-
-============================================================
-
-Note ID: 1711408493260
-  Field: Text
-    Before:
-      <ul><li>The probability mass function (pmf) of the bernouli distribution is defined as follows:<br></li><li>\(\operatorname{Ber}(y \mid \theta)\) =&nbsp;\(\begin{cases}1-\theta &amp; \text { if } y=0 \\ \theta &amp; \text { if } y=1\end{cases}\)</li></ul>We can write this in a more concise manner as follows:<br><ul><li>\(\operatorname{Ber}(y \mid \theta)\) \(\triangleq\) \(\theta^{y}\) \((1-\theta)^{1-y} \)<br></li></ul>
-
-    After:
-      <ul><li>The probability mass function (pmf) of the bernouli distribution is defined as follows:<br></li><li>\(\operatorname{Ber}(y \mid \theta)\) =&nbsp;\(\begin{cases}1-\theta &amp; \text { if } y=0 \\ \theta &amp; \text { if } y=1\end{cases}\)</li></ul>We can write this in a more concise manner as follows:<br><ul><li>\(\operatorname{Ber}(y \mid \theta)\) \(\triangleq\) \(\theta^{y}\) \((1-\theta)^{1-y} \)<br></li></ul>
-
-============================================================
-
-Note ID: 1711408673438
-  Field: Text
-    Before:
-      The Bernoulli distribution is a special case of the binomial distribution. To explain this, suppose we observe a set of \(N\) Bernoulli trials, denoted \(y_{n} \sim \operatorname{Ber}(\cdot \mid \theta)\), for \(n=1: N\). Concretely, think of tossing a coin \(N\) times. Let us define \(s\) to be the total number of heads, \(s \triangleq \sum_{n=1}^{N} \mathbb{I}\left(y_{n}=1\right)\). The distribution of \(s\) is given by the binomial distribution:<br><div><br></div><div><ul><li>\(\operatorname{Bin}(s \mid N, \theta)\) =&nbsp;\(\left(\begin{array}{c}N \\ s\end{array}\right) \theta^s(1-\theta)^{N-s}\)<br></li></ul></div>where<br><br>\[<br>\left(\begin{array}{c}<br>N&nbsp; \tag{2.70}\\<br>k<br>\end{array}\right) \triangleq \frac{N !}{(N-k) ! k !}<br>\]<br><br>is the number of ways to choose \(k\) items from \(N\) (this is known as the binomial coefficient, and is pronounced "N choose k"). See Figure 2.9 for some examples of the binomial distribution. If \(N=1\), the binomial distribution reduces to the Bernoulli distribution.<br>
-
-    After:
-      The Bernoulli distribution is a special case of the binomial distribution. To explain this, suppose we observe a set of \(N\) Bernoulli trials, denoted \(y_{n} \sim \operatorname{Ber}(\cdot \mid \theta)\), for \(n=1: N\). Concretely, think of tossing a coin \(N\) times. Let us define \(s\) to be the total number of heads, \(s \triangleq \sum_{n=1}^{N} \mathbb{I}\left(y_{n}=1\right)\). The distribution of \(s\) is given by the binomial distribution:<br><div><br></div><div><ul><li>\(\operatorname{Bin}(s \mid N, \theta)\) =&nbsp;\(\left(\begin{array}{c}N \\ s\end{array}\right) \theta^s(1-\theta)^{N-s}\)<br></li></ul></div>where<br><br>\[<br>\left(\begin{array}{c}<br>N&nbsp; \tag{2.70}\\<br>k<br>\end{array}\right) \triangleq \frac{N !}{(N-k) ! k !}<br>\]<br><br>is the number of ways to choose \(k\) items from \(N\) (this is known as the binomial coefficient, and is pronounced "N choose k"). See Figure 2.9 for some examples of the binomial distribution. If \(N=1\), the binomial distribution reduces to the Bernoulli distribution.<br>
-
-============================================================
-
-Note ID: 1711409169084
-  Field: Text
-    Before:
-      When we want to predict a binary variable \(y \in\{0,1\}\) given some inputs \(\boldsymbol{x} \in \mathcal{X}\), we need to use a conditional probability distribution of the form<br><br><ul><li>\(p(y \mid \boldsymbol{x}, \boldsymbol{\theta})\) =&nbsp;\(\operatorname{Ber}(y \mid f(\boldsymbol{x} ; \boldsymbol{\theta}))\)<br></li></ul><div>where \(f(\boldsymbol{x} ; \boldsymbol{\theta})\) is some function that predicts the mean parameter of the output distribution. We will consider many different kinds of function \(f\) in Part II-Part IV.<br></div><div><br></div><div>To avoid the requirement that \(0 \leq f(\boldsymbol{x} ; \boldsymbol{\theta}) \leq 1\), we can let \(f\) be an unconstrained function, and use the following model:<br><br><ul><li>\(\operatorname{Ber}(y \mid \sigma(f(\boldsymbol{x} ; \boldsymbol{\theta})))\)<br></li></ul><br>Here \(\sigma()\) is the sigmoid or logistic function, defined as follows:<br><br><ul><li>\(\sigma(a)\)&nbsp; \(\triangleq\)&nbsp; \(\frac{1}{1+e^{-a} } \tag{2.79}\)</li></ul><br>where \(a\) = \(f(\boldsymbol{x} ; \boldsymbol{\theta})\). The term "sigmoid" means S-shaped: see Figure 2.10a for a plot. We see that it<br></div>
-
-    After:
-      When we want to predict a binary variable \(y \in\{0,1\}\) given some inputs \(\boldsymbol{x} \in \mathcal{X}\), we need to use a conditional probability distribution of the form<br><br><ul><li>\(p(y \mid \boldsymbol{x}, \boldsymbol{\theta})\) =&nbsp;\(\operatorname{Ber}(y \mid f(\boldsymbol{x} ; \boldsymbol{\theta}))\)<br></li></ul><div>where \(f(\boldsymbol{x} ; \boldsymbol{\theta})\) is some function that predicts the mean parameter of the output distribution. We will consider many different kinds of function \(f\) in Part II-Part IV.<br></div><div><br></div><div>To avoid the requirement that \(0 \leq f(\boldsymbol{x} ; \boldsymbol{\theta}) \leq 1\), we can let \(f\) be an unconstrained function, and use the following model:<br><br><ul><li>\(\operatorname{Ber}(y \mid \sigma(f(\boldsymbol{x} ; \boldsymbol{\theta})))\)<br></li></ul><br>Here \(\sigma()\) is the sigmoid or logistic function, defined as follows:<br><br><ul><li>\(\sigma(a)\)&nbsp; \(\triangleq\)&nbsp; \(\frac{1}{1+e^{-a} } \tag{2.79}\)</li></ul><br>where \(a\) = \(f(\boldsymbol{x} ; \boldsymbol{\theta})\). The term "sigmoid" means S-shaped: see Figure 2.10a for a plot. We see that it<br></div>
-
-============================================================
-
-Note ID: 1711409874015
-  Field: Text
-    Before:
-      Plugging the definition of the sigmoid function into Equation (2.78) we get<br><br>\[<br>\begin{align*}<br>&amp; p(y=1 \mid \boldsymbol{x}, \boldsymbol{\theta})=\frac{1}{1+e^{-a}}=\frac{e^{a}}{1+e^{a}}=\sigma(a)&nbsp; \tag{2.81}\\<br>&amp; p(y=0 \mid \boldsymbol{x}, \boldsymbol{\theta})=1-\frac{1}{1+e^{-a}}=\frac{e^{-a}}{1+e^{-a}}=\frac{1}{1+e^{a}}=\sigma(-a) \tag{2.82}<br>\end{align*}<br>\]<br><br>The quantity \(a\) is equal to the \(\log\) odds, \(\log \left(\frac{p}{1-p}\right)\), where \(p\) = \(p(y=1 \mid \boldsymbol{x} ; \boldsymbol{\theta})\). To see this, note that<br><br><ul><li>\(\log \left(\frac{p}{1-p}\right)\) = \(\log \left(\frac{e^a}{1+e^a} \frac{1+e^a}{1}\right)\) = \(\log \left(e^a\right)\) = \(a\)</li></ul>
-
-    After:
-      Plugging the definition of the sigmoid function into Equation (2.78) we get<br><br>\[<br>\begin{align*}<br>&amp; p(y=1 \mid \boldsymbol{x}, \boldsymbol{\theta})=\frac{1}{1+e^{-a}}=\frac{e^{a}}{1+e^{a}}=\sigma(a)&nbsp; \tag{2.81}\\<br>&amp; p(y=0 \mid \boldsymbol{x}, \boldsymbol{\theta})=1-\frac{1}{1+e^{-a}}=\frac{e^{-a}}{1+e^{-a}}=\frac{1}{1+e^{a}}=\sigma(-a) \tag{2.82}<br>\end{align*}<br>\]<br><br>The quantity \(a\) is equal to the \(\log\) odds, \(\log \left(\frac{p}{1-p}\right)\), where \(p\) = \(p(y=1 \mid \boldsymbol{x} ; \boldsymbol{\theta})\). To see this, note that<br><br><ul><li>\(\log \left(\frac{p}{1-p}\right)\) = \(\log \left(\frac{e^a}{1+e^a} \frac{1+e^a}{1}\right)\) = \(\log \left(e^a\right)\) = \(a\)</li></ul>
-
-============================================================
-
-Note ID: 1711410013568
-  Field: Text
-    Before:
-      Plugging the definition of the sigmoid function into Equation (2.78) we get<br><br>\[<br>\begin{align*}<br>&amp; p(y=1 \mid \boldsymbol{x}, \boldsymbol{\theta})=\frac{1}{1+e^{-a}}=\frac{e^{a}}{1+e^{a}}=\sigma(a)&nbsp; \tag{2.81}\\<br>&amp; p(y=0 \mid \boldsymbol{x}, \boldsymbol{\theta})=1-\frac{1}{1+e^{-a}}=\frac{e^{-a}}{1+e^{-a}}=\frac{1}{1+e^{a}}=\sigma(-a) \tag{2.82}<br>\end{align*}<br>\]<br><br>The quantity \(a\) is equal to the \(\log\) odds, \(\log \left(\frac{p}{1-p}\right)\), where \(p=p(y=1 \mid \boldsymbol{x} ; \boldsymbol{\theta})\). To see this, note that<br><br>\[<br>\begin{equation*}<br>\log \left(\frac{p}{1-p}\right)=\log \left(\frac{e^{a}}{1+e^{a}} \frac{1+e^{a}}{1}\right)=\log \left(e^{a}\right)=a \tag{2.83}<br>\end{equation*}<br>\]<br><br>The logistic function or sigmoid function maps the \(\log\)-odds \(a\) to \(p\) :<br><br><ul><li>\(p\) = \(\operatorname{logistic}(a)\) = \(\sigma(a)\)&nbsp; \(\triangleq\) \(\frac{1}{1+e^{-a} }\) = \(\frac{e^a}{1+e^a}\)</li></ul><br>
-
-    After:
-      Plugging the definition of the sigmoid function into Equation (2.78) we get<br><br>\[<br>\begin{align*}<br>&amp; p(y=1 \mid \boldsymbol{x}, \boldsymbol{\theta})=\frac{1}{1+e^{-a}}=\frac{e^{a}}{1+e^{a}}=\sigma(a)&nbsp; \tag{2.81}\\<br>&amp; p(y=0 \mid \boldsymbol{x}, \boldsymbol{\theta})=1-\frac{1}{1+e^{-a}}=\frac{e^{-a}}{1+e^{-a}}=\frac{1}{1+e^{a}}=\sigma(-a) \tag{2.82}<br>\end{align*}<br>\]<br><br>The quantity \(a\) is equal to the \(\log\) odds, \(\log \left(\frac{p}{1-p}\right)\), where \(p=p(y=1 \mid \boldsymbol{x} ; \boldsymbol{\theta})\). To see this, note that<br><br>\[<br>\begin{equation*}<br>\log \left(\frac{p}{1-p}\right)=\log \left(\frac{e^{a}}{1+e^{a}} \frac{1+e^{a}}{1}\right)=\log \left(e^{a}\right)=a \tag{2.83}<br>\end{equation*}<br>\]<br><br>The logistic function or sigmoid function maps the \(\log\)-odds \(a\) to \(p\) :<br><br><ul><li>\(p\) = \(\operatorname{logistic}(a)\) = \(\sigma(a)\)&nbsp; \(\triangleq\) \(\frac{1}{1+e^{-a} }\) = \(\frac{e^a}{1+e^a}\)</li></ul><br>
-
-============================================================
-
-Note ID: 1711412515365
-  Field: Text
-    Before:
-      To avoid the requirement that \(f\) directly predict a probability vector in multilable classification, it is common to pass the output from \(f\) into the softmax function [Bri90], also called the multinomial logit. This is defined as follows:<br><br>\[<br>\begin{equation*}<br>\operatorname{softmax}(\boldsymbol{a}) \triangleq\left[\frac{e^{a_{1}{\sum_{c^{\prime}=1}^{C} e^{a_{c^{\prime}}}}, \ldots, \frac{e^{a_{C}}}{\sum_{c^{\prime}=1}^{C} e^{a_{c^{\prime}}}}\right] \tag{2.94}<br>\end{equation*}<br>\]}}<br><br>This maps \(\mathbb{R}^{C}\) to \([0,1]^{C}\), and satisfies the constraints that \(0 \leq \operatorname{softmax}(\boldsymbol{a})_{c} \leq 1\) and \(\sum_{c=1}^{C} \operatorname{softmax}(\boldsymbol{a})_{c}\)=&nbsp; 1. The inputs to the softmax, \(\boldsymbol{a}=f(\boldsymbol{x} ; \boldsymbol{\theta})\), are called logits, and are a generalization of the log odds.
-
-    After:
-      To avoid the requirement that \(f\) directly predict a probability vector in multilable classification, it is common to pass the output from \(f\) into the softmax function [Bri90], also called the multinomial logit. This is defined as follows:<br><br>\[<br>\begin{equation*}<br>\operatorname{softmax}(\boldsymbol{a}) \triangleq\left[\frac{e^{a_{1}{\sum_{c^{\prime}=1}^{C} e^{a_{c^{\prime}}}}, \ldots, \frac{e^{a_{C}}}{\sum_{c^{\prime}=1}^{C} e^{a_{c^{\prime}}}}\right] \tag{2.94}<br>\end{equation*}<br>\]}}<br><br>This maps \(\mathbb{R}^{C}\) to \([0,1]^{C}\), and satisfies the constraints that \(0 \leq \operatorname{softmax}(\boldsymbol{a})_{c} \leq 1\) and \(\sum_{c=1}^{C} \operatorname{softmax}(\boldsymbol{a})_{c}\)=&nbsp; 1. The inputs to the softmax, \(\boldsymbol{a}=f(\boldsymbol{x} ; \boldsymbol{\theta})\), are called logits, and are a generalization of the log odds.
-
-============================================================
-
-Note ID: 1711413651046
-  Field: Text
-    Before:
-      To avoid numerical problems in softmax, we can use the following identity:<br><br><ul><li>\(\log \sum_{c=1}^{C} \exp \left(a_{c}\right)\) = \(m\) + \(\log \sum_{c=1}^{C} \exp \left(a_{c}-m\right) \)</li></ul><br>This holds for any \(m\). It is common to use \(m\) = \(\max _{c} a_{c}\) which ensures that the largest value you exponentiate will be zero, so you will definitely not overflow, and even if you underflow, the answer will be sensible. This is known as the log-sum-exp trick. We use this trick when implementing the lse function:<br><br>\[<br>\begin{equation*}<br>\operatorname{lse}(\boldsymbol{a}) \triangleq \log \sum_{c=1}^{C} \exp \left(a_{c}\right) \tag{2.101}<br>\end{equation*}<br>\]<br>
-
-    After:
-      To avoid numerical problems in softmax, we can use the following identity:<br><br><ul><li>\(\log \sum_{c=1}^{C} \exp \left(a_{c}\right)\) = \(m\) + \(\log \sum_{c=1}^{C} \exp \left(a_{c}-m\right) \)</li></ul><br>This holds for any \(m\). It is common to use \(m\) = \(\max _{c} a_{c}\) which ensures that the largest value you exponentiate will be zero, so you will definitely not overflow, and even if you underflow, the answer will be sensible. This is known as the log-sum-exp trick. We use this trick when implementing the lse function:<br><br>\[<br>\begin{equation*}<br>\operatorname{lse}(\boldsymbol{a}) \triangleq \log \sum_{c=1}^{C} \exp \left(a_{c}\right) \tag{2.101}<br>\end{equation*}<br>\]<br>
-
-============================================================
-
-Note ID: 1711415753912
-  Field: Text
-    Before:
-      So far we have been considering the unconditional Gaussian distribution. In some cases, it is helpful to make the parameters of the Gaussian be functions of some input variables, i.e., we want to create a conditional density model of the form<br><br>\[<br>\begin{equation*}<br>p(y \mid \boldsymbol{x} ; \boldsymbol{\theta})=\mathcal{N}\left(y \mid f_{\mu}(\boldsymbol{x} ; \boldsymbol{\theta}), f_{\sigma}(\boldsymbol{x} ; \boldsymbol{\theta})^{2}\right) \tag{2.120}<br>\end{equation*}<br>\]<br><br>where \(f_{\mu}(\boldsymbol{x} ; \boldsymbol{\theta}) \in \mathbb{R}\) predicts the mean, and \(f_{\sigma}(\boldsymbol{x} ; \boldsymbol{\theta})^{2} \in \mathbb{R}_{+}\)predicts the variance.<br><br>It is common to assume that the variance is fixed, and is independent of the input. This is called homoscedastic regression. Furthermore it is common to assume the mean is a linear function of the input. The resulting model is called linear regression:
-
-    After:
-      So far we have been considering the unconditional Gaussian distribution. In some cases, it is helpful to make the parameters of the Gaussian be functions of some input variables, i.e., we want to create a conditional density model of the form<br><br>\[<br>\begin{equation*}<br>p(y \mid \boldsymbol{x} ; \boldsymbol{\theta})=\mathcal{N}\left(y \mid f_{\mu}(\boldsymbol{x} ; \boldsymbol{\theta}), f_{\sigma}(\boldsymbol{x} ; \boldsymbol{\theta})^{2}\right) \tag{2.120}<br>\end{equation*}<br>\]<br><br>where \(f_{\mu}(\boldsymbol{x} ; \boldsymbol{\theta}) \in \mathbb{R}\) predicts the mean, and \(f_{\sigma}(\boldsymbol{x} ; \boldsymbol{\theta})^{2} \in \mathbb{R}_{+}\)predicts the variance.<br><br>It is common to assume that the variance is fixed, and is independent of the input. This is called homoscedastic regression. Furthermore it is common to assume the mean is a linear function of the input. The resulting model is called linear regression:
-
-============================================================
-
-Note ID: 1711415871082
-  Field: Text
-    Before:
-      to make the parameters of the Gaussian be functions of some input variables, i.e., we want to create a conditional density model of the form<br><br>\[<br>\begin{equation*}<br>p(y \mid \boldsymbol{x} ; \boldsymbol{\theta})=\mathcal{N}\left(y \mid f_{\mu}(\boldsymbol{x} ; \boldsymbol{\theta}), f_{\sigma}(\boldsymbol{x} ; \boldsymbol{\theta})^{2}\right) \tag{2.120}<br>\end{equation*}<br>\]<br><br>where \(f_{\mu}(\boldsymbol{x} ; \boldsymbol{\theta}) \in \mathbb{R}\) predicts the mean, and \(f_{\sigma}(\boldsymbol{x} ; \boldsymbol{\theta})^{2} \in \mathbb{R}_{+}\)predicts the variance.<br><br>It is common to assume that the variance is fixed, and is independent of the input. This is called homoscedastic regression. Furthermore it is common to assume the mean is a linear function of the input. The resulting model is called linear regression:<br><br><ul><li>\(p(y \mid \boldsymbol{x} ; \boldsymbol{\theta})\) = \(\mathcal{N}\) \(\left(y \mid \boldsymbol{w}^{\top} \boldsymbol{x}+b, \sigma^{2}\right) \)</li></ul><br>where \(\boldsymbol{\theta}\) = \(\left(\boldsymbol{w}, b, \sigma^{2}\right)\).&nbsp;<br>
-
-    After:
-      to make the parameters of the Gaussian be functions of some input variables, i.e., we want to create a conditional density model of the form<br><br>\[<br>\begin{equation*}<br>p(y \mid \boldsymbol{x} ; \boldsymbol{\theta})=\mathcal{N}\left(y \mid f_{\mu}(\boldsymbol{x} ; \boldsymbol{\theta}), f_{\sigma}(\boldsymbol{x} ; \boldsymbol{\theta})^{2}\right) \tag{2.120}<br>\end{equation*}<br>\]<br><br>where \(f_{\mu}(\boldsymbol{x} ; \boldsymbol{\theta}) \in \mathbb{R}\) predicts the mean, and \(f_{\sigma}(\boldsymbol{x} ; \boldsymbol{\theta})^{2} \in \mathbb{R}_{+}\)predicts the variance.<br><br>It is common to assume that the variance is fixed, and is independent of the input. This is called homoscedastic regression. Furthermore it is common to assume the mean is a linear function of the input. The resulting model is called linear regression:<br><br><ul><li>\(p(y \mid \boldsymbol{x} ; \boldsymbol{\theta})\) = \(\mathcal{N}\) \(\left(y \mid \boldsymbol{w}^{\top} \boldsymbol{x}+b, \sigma^{2}\right) \)</li></ul><br>where \(\boldsymbol{\theta}\) = \(\left(\boldsymbol{w}, b, \sigma^{2}\right)\).&nbsp;<br>
-
-============================================================
-
-Note ID: 1711454115339
-  Field: Text
-    Before:
-      As the variance of a Gaussian goes to 0 , the distribution approaches an infinitely narrow, but infinitely tall, "spike" at the mean. We can write this as follows:<br><br><ul><li>\(\lim _{\sigma \rightarrow 0}\) \(\mathcal{N}\left(y \mid \mu, \sigma^{2}\right)\)&nbsp; \(\rightarrow\)&nbsp; \(\delta(y-\mu) \)</li></ul><div>where \(\delta\) is the Dirac delta function, defined by<br><br><ul><li>\(\delta(x)\) = \(\begin{cases}+\infty &amp; \text { if } x=0&nbsp; \tag{2.125}\\ 0 &amp; \text { if } x \neq 0\end{cases}\)</li></ul><br>where<br><br><ul><li>\(\int_{-\infty}^{\infty}\)&nbsp; \(\delta(x)\) \(d x\) = \(1 \)</li></ul></div>
-
-    After:
-      As the variance of a Gaussian goes to 0 , the distribution approaches an infinitely narrow, but infinitely tall, "spike" at the mean. We can write this as follows:<br><br><ul><li>\(\lim _{\sigma \rightarrow 0}\) \(\mathcal{N}\left(y \mid \mu, \sigma^{2}\right)\)&nbsp; \(\rightarrow\)&nbsp; \(\delta(y-\mu) \)</li></ul><div>where \(\delta\) is the Dirac delta function, defined by<br><br><ul><li>\(\delta(x)\) = \(\begin{cases}+\infty &amp; \text { if } x=0&nbsp; \tag{2.125}\\ 0 &amp; \text { if } x \neq 0\end{cases}\)</li></ul><br>where<br><br><ul><li>\(\int_{-\infty}^{\infty}\)&nbsp; \(\delta(x)\) \(d x\) = \(1 \)</li></ul></div>
-
-============================================================
-
-Note ID: 1711454289229
-  Field: Text
-    Before:
-      where \(\delta\) is the Dirac delta function, defined by<br><br>\[<br>\delta(x)= \begin{cases}+\infty &amp; \text { if } x=0&nbsp; \tag{2.125}\\ 0 &amp; \text { if } x \neq 0\end{cases}<br>\]<br><br>where<br><br>\[<br>\begin{equation*}<br>\int_{-\infty}^{\infty} \delta(x) d x=1 \tag{2.126}<br>\end{equation*}<br>\]<br><br>A slight variant of this is to define<br><ul><li>\(\delta_{y}(x)\) = \(\begin{cases}+\infty &amp; \text { if } x=y&nbsp; \tag{2.127}\\ 0 &amp; \text { if } x \neq y\end{cases}\)</li></ul><br>Note that we have<br><br><ul><li>\(\delta_{y}(x)\) = \(\delta(x-y) \tag{2.128}\)</li></ul><div>The delta function distribution satisfies the following sifting property, which we will use later on:<br><br><ul><li>\(\int_{-\infty}^{\infty} f(y) \delta(x-y) d y\) = \(f(x) \tag{2.129}\)</li></ul></div>
-
-    After:
-      where \(\delta\) is the Dirac delta function, defined by<br><br>\[<br>\delta(x)= \begin{cases}+\infty &amp; \text { if } x=0&nbsp; \tag{2.125}\\ 0 &amp; \text { if } x \neq 0\end{cases}<br>\]<br><br>where<br><br>\[<br>\begin{equation*}<br>\int_{-\infty}^{\infty} \delta(x) d x=1 \tag{2.126}<br>\end{equation*}<br>\]<br><br>A slight variant of this is to define<br><ul><li>\(\delta_{y}(x)\) = \(\begin{cases}+\infty &amp; \text { if } x=y&nbsp; \tag{2.127}\\ 0 &amp; \text { if } x \neq y\end{cases}\)</li></ul><br>Note that we have<br><br><ul><li>\(\delta_{y}(x)\) = \(\delta(x-y) \tag{2.128}\)</li></ul><div>The delta function distribution satisfies the following sifting property, which we will use later on:<br><br><ul><li>\(\int_{-\infty}^{\infty} f(y) \delta(x-y) d y\) = \(f(x) \tag{2.129}\)</li></ul></div>
-
-============================================================
-
-Note ID: 1711455570552
-  Field: Text
-    Before:
-      If \(\nu=1\), the Student distribution is known as the Cauchy or Lorentz distribution. Its pdf is defined by<br><br>\[<br>\begin{equation*}<br>\mathcal{C}(x \mid \mu, \gamma)=\frac{1}{\gamma \pi}\left[1+\left(\frac{x-\mu}{\gamma}\right)^{2}\right]^{-1} \tag{2.132}<br>\end{equation*}<br>\]<br><br>This distribution has very heavy tails compared to a Gaussian. For example, \(95 \%\) of the values from a standard normal are between -1.96 and 1.96 , but for a standard Cauchy they are between -12.7 and 12.7. In fact the tails are so heavy that the integral that defines the mean does not converge.
-
-    After:
-      If \(\nu=1\), the Student distribution is known as the Cauchy or Lorentz distribution. Its pdf is defined by<br><br>\[<br>\begin{equation*}<br>\mathcal{C}(x \mid \mu, \gamma)=\frac{1}{\gamma \pi}\left[1+\left(\frac{x-\mu}{\gamma}\right)^{2}\right]^{-1} \tag{2.132}<br>\end{equation*}<br>\]<br><br>This distribution has very heavy tails compared to a Gaussian. For example, \(95 \%\) of the values from a standard normal are between -1.96 and 1.96 , but for a standard Cauchy they are between -12.7 and 12.7. In fact the tails are so heavy that the integral that defines the mean does not converge.
-
-============================================================
-
-Note ID: 1711455985764
-  Field: Text
-    Before:
-      Another distribution with heavy tails is the Laplace distribution \({ }^{10}\), also known as the double sided exponential distribution. This has the following pdf:<br><br>\[<br>\begin{equation*}<br>\operatorname{Laplace}(y \mid \mu, b) \triangleq \frac{1}{2 b} \exp \left(-\frac{|y-\mu|}{b}\right) \tag{2.134}<br>\end{equation*}<br>\]<br><br>See Figure 2.15 for a plot. Here \(\mu\) is a location parameter and \(b&gt;0\) is a scale parameter. This distribution has the following properties:<br><br><ul><li>\(\text { mean }\) = \(\mu\)&nbsp;</li><li>\(\text { mode }\) = \(\mu\)&nbsp;</li><li>\(\text { var }\) = \(2 b^{2} \)</li></ul>
-
-    After:
-      Another distribution with heavy tails is the Laplace distribution \({ }^{10}\), also known as the double sided exponential distribution. This has the following pdf:<br><br>\[<br>\begin{equation*}<br>\operatorname{Laplace}(y \mid \mu, b) \triangleq \frac{1}{2 b} \exp \left(-\frac{|y-\mu|}{b}\right) \tag{2.134}<br>\end{equation*}<br>\]<br><br>See Figure 2.15 for a plot. Here \(\mu\) is a location parameter and \(b&gt;0\) is a scale parameter. This distribution has the following properties:<br><br><ul><li>\(\text { mean }\) = \(\mu\)&nbsp;</li><li>\(\text { mode }\) = \(\mu\)&nbsp;</li><li>\(\text { var }\) = \(2 b^{2} \)</li></ul>
-
-============================================================
-
-Note ID: 1711656212775
-  Field: Text
-    Before:
-      Many actions have outcomes which are largely unpredictable in advance-tossing a coin and throwing a dart are simple examples. Probability theory is about such actions and their consequences. The mathematical theory starts with the idea of an experiment (or trial), being a course of action whose consequence is not predetermined. This experiment is reformulated as a mathematical object called a probability space. In broad terms, the probability space corresponding to a given experiment comprises three items:<br><ul><li>(i) the set of all possible outcomes of the experiment,<br></li><li>(ii) a list of all the events which may possibly occur as consequences of the experiment,</li><li>(iii) an assessment of the likelihoods of these events.</li></ul>
-
-    After:
-      Many actions have outcomes which are largely unpredictable in advance-tossing a coin and throwing a dart are simple examples. Probability theory is about such actions and their consequences. The mathematical theory starts with the idea of an experiment (or trial), being a course of action whose consequence is not predetermined. This experiment is reformulated as a mathematical object called a probability space. In broad terms, the probability space corresponding to a given experiment comprises three items:<br><ul><li>(i) the set of all possible outcomes of the experiment,<br></li><li>(ii) a list of all the events which may possibly occur as consequences of the experiment,</li><li>(iii) an assessment of the likelihoods of these events.</li></ul>
-
-============================================================
-
-Note ID: 1711656402045
-  Field: Text
-    Before:
-      The set of all possible outcomes is called the sample space of \(\mathcal{E}\) and we usually denote it by \(\Omega\). The Greek letter \(\omega\) denotes a typical member of \(\Omega\), and we call each member \(\omega\) an elementary event.
-
-    After:
-      The set of all possible outcomes is called the sample space of \(\mathcal{E}\) and we usually denote it by \(\Omega\). The Greek letter \(\omega\) denotes a typical member of \(\Omega\), and we call each member \(\omega\) an elementary event.
-
-============================================================
-
-Note ID: 1711656534020
-  Field: Text
-    Before:
-      If, for example, \(\mathcal{E}\) is the experiment of throwing a fair die once, then<br><br>\[<br>\Omega=\{1,2,3,4,5,6\} .<br>\]<br><br>There are many questions which we may wish to ask about the actual outcome of this experiment (questions such as 'is the outcome a prime number?'), and all such questions may be rewritten in terms of subsets of \(\Omega\) (the is the number odd question becomes 'does the outcome lie in the subset \(\{2,3,5\}\) of \(\Omega\) ?').&nbsp;
-
-    After:
-      If, for example, \(\mathcal{E}\) is the experiment of throwing a fair die once, then<br><br>\[<br>\Omega=\{1,2,3,4,5,6\} .<br>\]<br><br>There are many questions which we may wish to ask about the actual outcome of this experiment (questions such as 'is the outcome a prime number?'), and all such questions may be rewritten in terms of subsets of \(\Omega\) (the is the number odd question becomes 'does the outcome lie in the subset \(\{2,3,5\}\) of \(\Omega\) ?').&nbsp;
-
-============================================================
-
-Note ID: 1711656668512
-  Field: Text
-    Before:
-      This relationship between events and subsets is very natural, especially because two or more events combine with each other in just the same way as the corresponding subsets combine.&nbsp;
-
-    After:
-      This relationship between events and subsets is very natural, especially because two or more events combine with each other in just the same way as the corresponding subsets combine.&nbsp;
-
-============================================================
-
-Note ID: 1711656905259
-  Field: Text
-    Before:
-      In a similar way, if \(A_{1}, A_{2}, \ldots\) are events, then the sets \(\bigcup_{i=1}^{\infty} A_{i}\) and \(\bigcap_{i=1}^{\infty} A_{i}\) represent the events ' \(A_{i}\) occurs, for some \(i\) ' and ' \(A_{i}\) occurs, for every \(i\) ', respectively.
-
-    After:
-      In a similar way, if \(A_{1}, A_{2}, \ldots\) are events, then the sets \(\bigcup_{i=1}^{\infty} A_{i}\) and \(\bigcap_{i=1}^{\infty} A_{i}\) represent the events ' \(A_{i}\) occurs, for some \(i\) ' and ' \(A_{i}\) occurs, for every \(i\) ', respectively.
-
-============================================================
-
-Note ID: 1711657096351
-  Field: Text
-    Before:
-      Thus we write down a collection \(\mathcal{F}\) = \(\left\{A_{i}: i \in I\right\}\) of subsets of \(\Omega\) which are interesting to us; each \(A \in \mathcal{F}\) is called an event. In simple cases, such as the die-throwing example above, we usually take \(\mathcal{F}\) to be the set of all subsets of \(\Omega\) (called the power set of \(\Omega\) ), but for reasons which may be appreciated later there are many circumstances in which we take \(\mathcal{F}\) to be a very much smaller collection than the entire power set. \({ }^{2}\) In all cases we demand a certain consistency of \(\mathcal{F}\), in the following sense. If \(A, B, C, \ldots \in \mathcal{F}\), we may reasonably be interested also in the events ' \(A\) does not occur' and 'at least one of \(A, B, C, \ldots\) occurs'.&nbsp;
-
-    After:
-      Thus we write down a collection \(\mathcal{F}\) = \(\left\{A_{i}: i \in I\right\}\) of subsets of \(\Omega\) which are interesting to us; each \(A \in \mathcal{F}\) is called an event. In simple cases, such as the die-throwing example above, we usually take \(\mathcal{F}\) to be the set of all subsets of \(\Omega\) (called the power set of \(\Omega\) ), but for reasons which may be appreciated later there are many circumstances in which we take \(\mathcal{F}\) to be a very much smaller collection than the entire power set. \({ }^{2}\) In all cases we demand a certain consistency of \(\mathcal{F}\), in the following sense. If \(A, B, C, \ldots \in \mathcal{F}\), we may reasonably be interested also in the events ' \(A\) does not occur' and 'at least one of \(A, B, C, \ldots\) occurs'.&nbsp;
-
-============================================================
-
-Note ID: 1711657720759
-  Field: Text
-    Before:
-      <img src="paste-c2be4bc8be7fc6778fe427eca426ccc862c96ffc.jpg"><br>(c) The third condition (1.4) is written in terms of unions. An event space is also closed under the operations of taking finite or countable intersections. This follows by the elementary formula \((A \cap B)^{\mathrm{c} }\) = \(A^{\mathrm{c} } \cup B^{\mathrm{c} }\), and its extension to finite and countable families.
-
-    After:
-      <img src="paste-c2be4bc8be7fc6778fe427eca426ccc862c96ffc.jpg"><br>(c) The third condition (1.4) is written in terms of unions. An event space is also closed under the operations of taking finite or countable intersections. This follows by the elementary formula \((A \cap B)^{\mathrm{c} }\) = \(A^{\mathrm{c} } \cup B^{\mathrm{c} }\), and its extension to finite and countable families.
-
-============================================================
-
-Note ID: 1711657808322
-  Field: Text
-    Before:
-      Here are some examples of pairs \((\Omega, \mathcal{F})\) of sample spaces and event spaces.<br><br>Example 1.5 \(\Omega\) is any non-empty set and \(\mathcal{F}\) is the power set of \(\Omega\).<br><br>Example 1.6 \(\Omega\) is any non-empty set and \(\mathcal{F}\) = \(\{\varnothing, A, \Omega \backslash A, \Omega\}\), where \(A\) is a given nontrivial subset of \(\Omega\).<br>
-
-    After:
-      Here are some examples of pairs \((\Omega, \mathcal{F})\) of sample spaces and event spaces.<br><br>Example 1.5 \(\Omega\) is any non-empty set and \(\mathcal{F}\) is the power set of \(\Omega\).<br><br>Example 1.6 \(\Omega\) is any non-empty set and \(\mathcal{F}\) = \(\{\varnothing, A, \Omega \backslash A, \Omega\}\), where \(A\) is a given nontrivial subset of \(\Omega\).<br>
-
-============================================================
-
-Note ID: 1711657928836
-  Field: Text
-    Before:
-      Exercise 1.9 The difference \(A \backslash B\) of two subsets \(A\) and \(B\) of \(\Omega\) is the set \(A \cap(\Omega \backslash B)\) of all points of \(\Omega\) which are in \(A\) but not in \(B\). Show that if \(A, B \in \mathcal{F}\), then \(A \backslash B\)&nbsp; \(\in \mathcal{F}\).
-
-    After:
-      Exercise 1.9 The difference \(A \backslash B\) of two subsets \(A\) and \(B\) of \(\Omega\) is the set \(A \cap(\Omega \backslash B)\) of all points of \(\Omega\) which are in \(A\) but not in \(B\). Show that if \(A, B \in \mathcal{F}\), then \(A \backslash B\)&nbsp; \(\in \mathcal{F}\).
-
-============================================================
-
-Note ID: 1711658380014
-  Field: Text
-    Before:
-      From our experiment \(\mathcal{E}\), we have so far constructed a sample space \(\Omega\) and an event space \(\mathcal{F}\) associated with \(\mathcal{E}\), but there has been no mention yet of probabilities. The third thing which we do is to allocate probabilities to each event in \(\mathcal{F}\), writing \(\mathbb{P}(A)\) for the probability of the event \(A\). We shall assume that this can be done in such a way that the probability function \(\mathbb{P}\) satisfies certain intuitively attractive conditions:
-
-    After:
-      From our experiment \(\mathcal{E}\), we have so far constructed a sample space \(\Omega\) and an event space \(\mathcal{F}\) associated with \(\mathcal{E}\), but there has been no mention yet of probabilities. The third thing which we do is to allocate probabilities to each event in \(\mathcal{F}\), writing \(\mathbb{P}(A)\) for the probability of the event \(A\). We shall assume that this can be done in such a way that the probability function \(\mathbb{P}\) satisfies certain intuitively attractive conditions:
-
-============================================================
-
-Note ID: 1711658557891
-  Field: Text
-    Before:
-      We shall assume that this can be done in such a way that the probability function \(\mathbb{P}\) satisfies certain intuitively attractive conditions:<br><br><ul><li>(a) each event \(A\) in the event space has a probability \(\mathbb{P}(A)\) satisfying \(0 \leq \mathbb{P}(A) \leq 1\),</li><li>(b) the event \(\Omega\), that 'something happens', has probability 1 , and the event \(\varnothing\), that 'nothing happens', has probability 0 ,</li><li>(c) if \(A\) and \(B\) are disjoint events (in that \(A \cap B\) = \(\varnothing\) ), then \(\mathbb{P}(A \cup B)\) = \(\mathbb{P}(A)+\mathbb{P}(B)\).</li></ul>
-
-    After:
-      We shall assume that this can be done in such a way that the probability function \(\mathbb{P}\) satisfies certain intuitively attractive conditions:<br><br><ul><li>(a) each event \(A\) in the event space has a probability \(\mathbb{P}(A)\) satisfying \(0 \leq \mathbb{P}(A) \leq 1\),</li><li>(b) the event \(\Omega\), that 'something happens', has probability 1 , and the event \(\varnothing\), that 'nothing happens', has probability 0 ,</li><li>(c) if \(A\) and \(B\) are disjoint events (in that \(A \cap B\) = \(\varnothing\) ), then \(\mathbb{P}(A \cup B)\) = \(\mathbb{P}(A)+\mathbb{P}(B)\).</li></ul>
-
-============================================================
-
-Note ID: 1711660993474
-  Field: Text
-    Before:
-      Exercise 1.17 Let \(p_{1}, p_{2}, \ldots, p_{N}\) be non-negative numbers such that \(p_{1}+p_{2}+\cdots+p_{N}\) = \(1\), and let \(\Omega\) = \(\left\{\omega_{1}, \omega_{2}, \ldots, \omega_{N}\right\}\), with \(\mathcal{F}\) the power set of \(\Omega\), as in Example 1.16. Show that the function \(\mathbb{Q}\) given by<br><ul><li>\(\mathbb{Q}(A)\) = \(\sum_{i: \omega_{i} \in A}\) \( p_{i}\)&nbsp; \(\quad \text { for } A \in \mathcal{F}\)</li></ul><br>is a probability measure on \((\Omega, \mathcal{F})\). Is \(\mathbb{Q}\) a probability measure on \((\Omega, \mathcal{F})\) if \(\mathcal{F}\) is not the power set of \(\Omega\) but merely some event space of subsets of \(\Omega\) ?<br>
-
-    After:
-      Exercise 1.17 Let \(p_{1}, p_{2}, \ldots, p_{N}\) be non-negative numbers such that \(p_{1}+p_{2}+\cdots+p_{N}\) = \(1\), and let \(\Omega\) = \(\left\{\omega_{1}, \omega_{2}, \ldots, \omega_{N}\right\}\), with \(\mathcal{F}\) the power set of \(\Omega\), as in Example 1.16. Show that the function \(\mathbb{Q}\) given by<br><ul><li>\(\mathbb{Q}(A)\) = \(\sum_{i: \omega_{i} \in A}\) \( p_{i}\)&nbsp; \(\quad \text { for } A \in \mathcal{F}\)</li></ul><br>is a probability measure on \((\Omega, \mathcal{F})\). Is \(\mathbb{Q}\) a probability measure on \((\Omega, \mathcal{F})\) if \(\mathcal{F}\) is not the power set of \(\Omega\) but merely some event space of subsets of \(\Omega\) ?<br>
-
-============================================================
-
-Note ID: 1711661220425
-  Field: Text
-    Before:
-      Property If \(A, B\) \(\in \mathcal{F}\), then \( A \backslash B\) \(\in \mathcal{F}\).<br><br>Proof The complement of \(A \backslash B\) equals \((\Omega \backslash A) \cup B\), which is the union of events and is therefore an event. Hence \(A \backslash B\) is an event, by (1.3).
-
-    After:
-      Property If \(A, B\) \(\in \mathcal{F}\), then \( A \backslash B\) \(\in \mathcal{F}\).<br><br>Proof The complement of \(A \backslash B\) equals \((\Omega \backslash A) \cup B\), which is the union of events and is therefore an event. Hence \(A \backslash B\) is an event, by (1.3).
-
-============================================================
-
-Note ID: 1711661281626
-  Field: Text
-    Before:
-      Property If \(A_{1}, A_{2}, \ldots\) \(\in \mathcal{F}\), then \(\bigcap_{i=1}^{\infty}\) \(A_{i}\) \(\in \mathcal{F}\).
-
-    After:
-      Property If \(A_{1}, A_{2}, \ldots\) \(\in \mathcal{F}\), then \(\bigcap_{i=1}^{\infty}\) \(A_{i}\) \(\in \mathcal{F}\).
-
-============================================================
-
-Note ID: 1711702801642
-  Field: Text
-    Before:
-      Property If \(A, B \in \mathcal{F}\) then \(\mathbb{P}(A \cup B)\) + \(\mathbb{P}(A \cap B)\) = \(\mathbb{P}(A)\) + \(\mathbb{P}(B)\).
-
-    After:
-      Property If \(A, B \in \mathcal{F}\) then \(\mathbb{P}(A \cup B)\) + \(\mathbb{P}(A \cap B)\) = \(\mathbb{P}(A)\) + \(\mathbb{P}(B)\).
-
-============================================================
-
-Note ID: 1711703680744
-  Field: Text
-    Before:
-      Exercise 1.20 If \(A, B, C \in \mathcal{F}\), show that<br><br><ul><li>\(\mathbb{P}(A \cup B \cup C)\) = \(\mathbb{P}(A)\) + \(\mathbb{P}(B)\) + \(\mathbb{P}(C)\) - \(\mathbb{P}(A \cap B)\) - \(\mathbb{P}(B \cap C)\) - \(\mathbb{P}(A \cap C)\) + \(\mathbb{P}(A \cap B \cap C)\)</li></ul>
-
-    After:
-      Exercise 1.20 If \(A, B, C \in \mathcal{F}\), show that<br><br><ul><li>\(\mathbb{P}(A \cup B \cup C)\) = \(\mathbb{P}(A)\) + \(\mathbb{P}(B)\) + \(\mathbb{P}(C)\) - \(\mathbb{P}(A \cap B)\) - \(\mathbb{P}(B \cap C)\) - \(\mathbb{P}(A \cap C)\) + \(\mathbb{P}(A \cap B \cap C)\)</li></ul>
-
-============================================================
-
-Note ID: 1711732422038
-  Field: Text
-    Before:
-      Let \(\mathcal{E}\) be an experiment with probability space \((\Omega, \mathcal{F}, \mathbb{P})\). The structure of this space depends greatly on whether \(\Omega\) is a countable set (that is, a finite or countably infinite set) or an uncountable set. If \(\Omega\) is a countable set, we normally take \(\mathcal{F}\) to be the set of all subsets of \(\Omega\).
-
-    After:
-      Let \(\mathcal{E}\) be an experiment with probability space \((\Omega, \mathcal{F}, \mathbb{P})\). The structure of this space depends greatly on whether \(\Omega\) is a countable set (that is, a finite or countably infinite set) or an uncountable set. If \(\Omega\) is a countable set, we normally take \(\mathcal{F}\) to be the set of all subsets of \(\Omega\).
-
-============================================================
-
-Note ID: 1711894749089
-  Field: Text
-    Before:
-      Exercise 1.26 We distribute \(r\) distinguishable balls into \(n\) cells at random, multiple occupancy being permitted. Show that<br><br><ul><li>(a) there are \(n^{r}\) possible arrangements,&nbsp;</li><li>(b) there are \(\left(\begin{array}{l}r \\ k\end{array}\right)\)\((n-1)^{r-k}\) arrangements in which the first cell contains exactly \(k\) balls,</li><li>(c) the probability that the first cell contains exactly \(k\) balls is</li><li>\(\left(\begin{array}{l}r \\ k\end{array}\right)\)\((\frac{1}{n})^k\)\(\left(1-\frac{1}{n}\right)^{r-k}\).<br></li></ul><br>
-
-    After:
-      Exercise 1.26 We distribute \(r\) distinguishable balls into \(n\) cells at random, multiple occupancy being permitted. Show that<br><br><ul><li>(a) there are \(n^{r}\) possible arrangements,&nbsp;</li><li>(b) there are \(\left(\begin{array}{l}r \\ k\end{array}\right)\)\((n-1)^{r-k}\) arrangements in which the first cell contains exactly \(k\) balls,</li><li>(c) the probability that the first cell contains exactly \(k\) balls is</li><li>\(\left(\begin{array}{l}r \\ k\end{array}\right)\)\((\frac{1}{n})^k\)\(\left(1-\frac{1}{n}\right)^{r-k}\).<br></li></ul><br>
-
-============================================================
-
-Note ID: 1711895227384
-  Field: Text
-    Before:
-      Definition 1.31 If \(A, B \in \mathcal{F}\) and \(\mathbb{P}(B)\) &gt; \(0\), the (conditional) probability of \(A\) given \(B\) is denoted by \(\mathbb{P}(A \mid B)\) and defined by<br><br><ul><li>\(\mathbb{P}(A \mid B)\) = \(\frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} \)</li></ul>
-
-    After:
-      Definition 1.31 If \(A, B \in \mathcal{F}\) and \(\mathbb{P}(B)\) &gt; \(0\), the (conditional) probability of \(A\) given \(B\) is denoted by \(\mathbb{P}(A \mid B)\) and defined by<br><br><ul><li>\(\mathbb{P}(A \mid B)\) = \(\frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} \)</li></ul>
-
-============================================================
-
-Note ID: 1711895287144
-  Field: Text
-    Before:
-      <img src="paste-2e507e919ad4c37c19bca56d57e1804ff58e6ae1.jpg"><br>Note that the constant of proportionality in (1.32) has been chosen so that the probability \(\mathbb{P}(B \mid B)\), that \(B\) occurs given that \(B\) occurs, satisfies \(\mathbb{P}(B \mid B)\) = \(1\).&nbsp;
-
-    After:
-      <img src="paste-2e507e919ad4c37c19bca56d57e1804ff58e6ae1.jpg"><br>Note that the constant of proportionality in (1.32) has been chosen so that the probability \(\mathbb{P}(B \mid B)\), that \(B\) occurs given that \(B\) occurs, satisfies \(\mathbb{P}(B \mid B)\) = \(1\).&nbsp;
-
-============================================================
-
-Note ID: 1711896071361
-  Field: Text
-    Before:
-      <img src="paste-cd3a30a7fb624abf70ba1aa076f602122ee5e4c8.jpg"><br>Proof We need only check that \(\mathbb{Q}\) is a probability measure on \((\Omega, \mathcal{F})\). Certainly \(\mathbb{Q}(A) \geq 0\) for \(A \in \mathcal{F}\) and<br><br><ul><li>\(\mathbb{Q}(\Omega)\) = \(\mathbb{P}(\Omega \mid B)\) = \(\frac{\mathbb{P}(\Omega \cap B)}{\mathbb{P}(B)}\) = \(1 \)</li></ul><br>and it remains to check that \(\mathbb{Q}\) satisfies (1.14). Suppose that \(A_{1}, A_{2}, \ldots\) are disjoint events in \(\mathcal{F}\). Then<br><ul><li>\(\mathbb{Q}\left(\bigcup_i A_i\right)\)&nbsp;</li><li>= \(\frac{1}{\mathbb{P}(B)}\) \( \mathbb{P}\left(\left(\bigcup_i A_i\right) \cap B\right)\)</li><li>=&nbsp;\(\frac{1}{\mathbb{P}(B)} \) \( \mathbb{P}\left(\bigcup_i\left(A_i \cap B\right)\right)\)</li><li>=&nbsp;\(\frac{1}{\mathbb{P}(B)}\) \(\sum_i\) \(\mathbb{P}\left(A_i \cap B\right)\)</li><li>=&nbsp;\(\sum_i\) \(\mathbb{Q}\left(A_i\right)\).</li></ul>
-
-    After:
-      <img src="paste-cd3a30a7fb624abf70ba1aa076f602122ee5e4c8.jpg"><br>Proof We need only check that \(\mathbb{Q}\) is a probability measure on \((\Omega, \mathcal{F})\). Certainly \(\mathbb{Q}(A) \geq 0\) for \(A \in \mathcal{F}\) and<br><br><ul><li>\(\mathbb{Q}(\Omega)\) = \(\mathbb{P}(\Omega \mid B)\) = \(\frac{\mathbb{P}(\Omega \cap B)}{\mathbb{P}(B)}\) = \(1 \)</li></ul><br>and it remains to check that \(\mathbb{Q}\) satisfies (1.14). Suppose that \(A_{1}, A_{2}, \ldots\) are disjoint events in \(\mathcal{F}\). Then<br><ul><li>\(\mathbb{Q}\left(\bigcup_i A_i\right)\)&nbsp;</li><li>= \(\frac{1}{\mathbb{P}(B)}\) \( \mathbb{P}\left(\left(\bigcup_i A_i\right) \cap B\right)\)</li><li>=&nbsp;\(\frac{1}{\mathbb{P}(B)} \) \( \mathbb{P}\left(\bigcup_i\left(A_i \cap B\right)\right)\)</li><li>=&nbsp;\(\frac{1}{\mathbb{P}(B)}\) \(\sum_i\) \(\mathbb{P}\left(A_i \cap B\right)\)</li><li>=&nbsp;\(\sum_i\) \(\mathbb{Q}\left(A_i\right)\).</li></ul>
-
-============================================================
-
-Note ID: 1711896236957
-  Field: Text
-    Before:
-      Exercise 1.35 Show that<br><br><ul><li>\(\mathbb{P}(B \mid A)\) = \(\mathbb{P}(A \mid B)\) \(\frac{\mathbb{P}(B)}{\mathbb{P}(A)}\)</li></ul><br>if \(\mathbb{P}(A)\) \(&gt;0\) and \(\mathbb{P}(B)\) \(&gt;0\).<br>
-
-    After:
-      Exercise 1.35 Show that<br><br><ul><li>\(\mathbb{P}(B \mid A)\) = \(\mathbb{P}(A \mid B)\) \(\frac{\mathbb{P}(B)}{\mathbb{P}(A)}\)</li></ul><br>if \(\mathbb{P}(A)\) \(&gt;0\) and \(\mathbb{P}(B)\) \(&gt;0\).<br>
-
-============================================================
-
-Note ID: 1711896515431
-  Field: Text
-    Before:
-      Definition 1.38 Events \(A\) and \(B\) of a probability space \((\Omega, \mathcal{F}, \mathbb{P})\) are called independent if<br><br><ul><li>\(\mathbb{P}(A \cap B)\) = \(\mathbb{P}(A)\)\( \mathbb{P}(B) \)</li></ul><br>and dependent otherwise.<br>
-
-    After:
-      Definition 1.38 Events \(A\) and \(B\) of a probability space \((\Omega, \mathcal{F}, \mathbb{P})\) are called independent if<br><br><ul><li>\(\mathbb{P}(A \cap B)\) = \(\mathbb{P}(A)\)\( \mathbb{P}(B) \)</li></ul><br>and dependent otherwise.<br>
-
-============================================================
-
-Note ID: 1711896995419
-  Field: Text
-    Before:
-      Exercise 1.44 Show that events \(A\) and \(B\) are independent if and only if \(A\) and \(\Omega \backslash B\) are independent.
-
-    After:
-      Exercise 1.44 Show that events \(A\) and \(B\) are independent if and only if \(A\) and \(\Omega \backslash B\) are independent.
-
-============================================================
-
-Note ID: 1711897130418
-  Field: Text
-    Before:
-      Exercise 1.45 Show that events \(A_{1}\), \(A_{2}\), \(\ldots\), \(A_{m}\) are independent if and only if \(\Omega \backslash A_{1}\), \(\Omega \backslash A_{2}\), \(\ldots\), \(\Omega \backslash A_{m}\) are independent.
-
-    After:
-      Exercise 1.45 Show that events \(A_{1}\), \(A_{2}\), \(\ldots\), \(A_{m}\) are independent if and only if \(\Omega \backslash A_{1}\), \(\Omega \backslash A_{2}\), \(\ldots\), \(\Omega \backslash A_{m}\) are independent.
-
-============================================================
-
-Note ID: 1711897657373
-  Field: Text
-    Before:
-      <img src="paste-d2321eec753f0c32ed65d5146a04a6f68102c54a.jpg"><br>Proof We have that:<br><ul><li>\(\mathbb{P}(A)\)&nbsp;</li><li>= \(\mathbb{P}\left(A \cap\left(\bigcup_i B_i\right)\right)\)<br></li><li>=&nbsp;\(\mathbb{P}\left(\bigcup_i\left(A \cap B_i\right)\right)\)</li><li>=&nbsp;\(\sum_i \mathbb{P}\) \(\left(A \cap B_i\right) \quad\)</li><li>=&nbsp;\(\sum_i \mathbb{P}\) \(\left(A \mid B_i\right) \mathbb{P}\left(B_i\right) \quad\)</li></ul>
-
-    After:
-      <img src="paste-d2321eec753f0c32ed65d5146a04a6f68102c54a.jpg"><br>Proof We have that:<br><ul><li>\(\mathbb{P}(A)\)&nbsp;</li><li>= \(\mathbb{P}\left(A \cap\left(\bigcup_i B_i\right)\right)\)<br></li><li>=&nbsp;\(\mathbb{P}\left(\bigcup_i\left(A \cap B_i\right)\right)\)</li><li>=&nbsp;\(\sum_i \mathbb{P}\) \(\left(A \cap B_i\right) \quad\)</li><li>=&nbsp;\(\sum_i \mathbb{P}\) \(\left(A \mid B_i\right) \mathbb{P}\left(B_i\right) \quad\)</li></ul>
-
-============================================================
-
-Note ID: 1711898228057
-  Field: Text
-    Before:
-      There are many practical situations in which you wish to deduce something from a piece of evidence. We write \(A\) for the evidence, and \(B_{1}, B_{2}, \ldots\) for the possible 'states of nature'.
-
-    After:
-      There are many practical situations in which you wish to deduce something from a piece of evidence. We write \(A\) for the evidence, and \(B_{1}, B_{2}, \ldots\) for the possible 'states of nature'.
-
-============================================================
-
-Note ID: 1711899368082
-  Field: Text
-    Before:
-      Example 1.55 It is intuitively clear that the chance of obtaining no heads in an infinite set of tosses of a fair coin is 0 . A rigorous proof goes as follows. Let \(A_{n}\) be the event that the first \(n\) tosses of the coin yield at least one head. Then<br><br><ul><li>\(A_{n} \subseteq A_{n+1} \quad \text { for } n=1,2, \ldots\)</li></ul><br>so that the \(A_{n}\) form an increasing sequence. The limit set \(A\) is the event that heads occurs sooner or later. By the continuity of \(\mathbb{P}\), Theorem 1.54,<br><br><ul><li>\(\mathbb{P}(A)\) = \(\lim _{n \rightarrow \infty} \mathbb{P}\left(A_{n}\right)\)</li></ul><br>However, \(\mathbb{P}\left(A_{n}\right)\) = \(1-\left(\frac{1}{2}\right)^{n}\), and so \(\mathbb{P}\left(A_{n}\right)\) \(\rightarrow\) \(1\) as \(n \rightarrow\) \(\infty\). Thus \(\mathbb{P}(A)\) = \(1\), giving that the probability \(\mathbb{P}(\Omega \backslash A)\), that no head ever appears, equals 0 .<br>
-
-    After:
-      Example 1.55 It is intuitively clear that the chance of obtaining no heads in an infinite set of tosses of a fair coin is 0 . A rigorous proof goes as follows. Let \(A_{n}\) be the event that the first \(n\) tosses of the coin yield at least one head. Then<br><br><ul><li>\(A_{n} \subseteq A_{n+1} \quad \text { for } n=1,2, \ldots\)</li></ul><br>so that the \(A_{n}\) form an increasing sequence. The limit set \(A\) is the event that heads occurs sooner or later. By the continuity of \(\mathbb{P}\), Theorem 1.54,<br><br><ul><li>\(\mathbb{P}(A)\) = \(\lim _{n \rightarrow \infty} \mathbb{P}\left(A_{n}\right)\)</li></ul><br>However, \(\mathbb{P}\left(A_{n}\right)\) = \(1-\left(\frac{1}{2}\right)^{n}\), and so \(\mathbb{P}\left(A_{n}\right)\) \(\rightarrow\) \(1\) as \(n \rightarrow\) \(\infty\). Thus \(\mathbb{P}(A)\) = \(1\), giving that the probability \(\mathbb{P}(\Omega \backslash A)\), that no head ever appears, equals 0 .<br>
-
-============================================================
-
-Note ID: 1711899990267
-  Field: Text
-    Before:
-      <img src="paste-51059e897772684fbb3a3b3e398a6e9e138a7635.jpg"><br>Proof of Theorem 1.54 Let \(B_{i}\) = \(A_{i} \backslash A_{i-1}\). Then<br><br>\[<br>A=A_{1} \cup B_{2} \cup B_{3} \cup \cdots<br>\]<br><br>is the union of disjoint events in \(\mathcal{F}\) (draw a Venn diagram to make this clear). By (1.14),<br><br><ul><li>\(\mathbb{P}(A)\) = \(\mathbb{P}\left(A_1\right)\) \(+\mathbb{P}\left(B_2\right)+\mathbb{P}\left(B_3\right)+\cdots\)<br></li><li>=&nbsp;\(\mathbb{P}\left(A_1\right)\) + \(\lim _{n \rightarrow \infty}\) \(\sum_{k=2}^n \mathbb{P}\left(B_k\right)\)</li></ul><br>However,<br><br><ul><li>\(\mathbb{P}\left(B_i\right)\) = \(\mathbb{P}\left(A_i\right)-\mathbb{P}\left(A_{i-1}\right)\)<br></li></ul><br>and so<br><br><ul><li>\(\mathbb{P}(A)\) = \(\mathbb{P}\left(A_1\right)\) + \(\lim _{n \rightarrow \infty} \sum_{k=2}^n\left[\mathbb{P}\left(A_k\right)-\mathbb{P}\left(A_{k-1}\right)\right]\)<br></li><li>=&nbsp;\(\lim _{n \rightarrow \infty} \mathbb{P}\left(A_n\right)\)</li></ul><br>as required, since the last sum collapses.<br>
-
-    After:
-      <img src="paste-51059e897772684fbb3a3b3e398a6e9e138a7635.jpg"><br>Proof of Theorem 1.54 Let \(B_{i}\) = \(A_{i} \backslash A_{i-1}\). Then<br><br>\[<br>A=A_{1} \cup B_{2} \cup B_{3} \cup \cdots<br>\]<br><br>is the union of disjoint events in \(\mathcal{F}\) (draw a Venn diagram to make this clear). By (1.14),<br><br><ul><li>\(\mathbb{P}(A)\) = \(\mathbb{P}\left(A_1\right)\) \(+\mathbb{P}\left(B_2\right)+\mathbb{P}\left(B_3\right)+\cdots\)<br></li><li>=&nbsp;\(\mathbb{P}\left(A_1\right)\) + \(\lim _{n \rightarrow \infty}\) \(\sum_{k=2}^n \mathbb{P}\left(B_k\right)\)</li></ul><br>However,<br><br><ul><li>\(\mathbb{P}\left(B_i\right)\) = \(\mathbb{P}\left(A_i\right)-\mathbb{P}\left(A_{i-1}\right)\)<br></li></ul><br>and so<br><br><ul><li>\(\mathbb{P}(A)\) = \(\mathbb{P}\left(A_1\right)\) + \(\lim _{n \rightarrow \infty} \sum_{k=2}^n\left[\mathbb{P}\left(A_k\right)-\mathbb{P}\left(A_{k-1}\right)\right]\)<br></li><li>=&nbsp;\(\lim _{n \rightarrow \infty} \mathbb{P}\left(A_n\right)\)</li></ul><br>as required, since the last sum collapses.<br>
-
-============================================================
-
-Note ID: 1711901607069
-  Field: Text
-    Before:
-      5. Two fair dice are thrown. Let \(A\) be the event that the first shows an odd number, \(B\) be the event that the second shows an even number, and \(C\) be the event that either both are odd or both are even. Show that \(A, B, C\) are pairwise independent but not independent.
-
-    After:
-      5. Two fair dice are thrown. Let \(A\) be the event that the first shows an odd number, \(B\) be the event that the second shows an even number, and \(C\) be the event that either both are odd or both are even. Show that \(A, B, C\) are pairwise independent but not independent.
-
-============================================================
-
-Note ID: 1711901854857
-  Field: Text
-    Before:
-      <br>* 12. Any number \(\omega \in[0,1]\) has a decimal expansion<br><br>\[<br>\omega=0 . x_{1} x_{2} \ldots,<br>\]<br><br>and we write \(f_{k}(\omega, n)\) for the proportion of times that the integer \(k\) appears in the first \(n\) digits in this expansion. We call \(\omega\) a normal number if<br><br>\[<br>f_{k}(\omega, n) \rightarrow \frac{1}{10} \quad \text { as } n \rightarrow \infty<br>\]<br><br>for \(k=0,1,2, \ldots, 9\). On intuitive grounds we may expect that most numbers \(\omega \in[0,1]\) are normal numbers, and Borel proved that this is indeed true. It is quite another matter to exhibit specific normal numbers. Prove the number \section*{\(0.1234567891011121314 \ldots\)}<br><br>is normal. It is an unsolved problem of mathematics to show that \(e-2\) and \(\pi-3\) are normal numbers also.
-
-    After:
-      <br>* 12. Any number \(\omega \in[0,1]\) has a decimal expansion<br><br>\[<br>\omega=0 . x_{1} x_{2} \ldots,<br>\]<br><br>and we write \(f_{k}(\omega, n)\) for the proportion of times that the integer \(k\) appears in the first \(n\) digits in this expansion. We call \(\omega\) a normal number if<br><br>\[<br>f_{k}(\omega, n) \rightarrow \frac{1}{10} \quad \text { as } n \rightarrow \infty<br>\]<br><br>for \(k=0,1,2, \ldots, 9\). On intuitive grounds we may expect that most numbers \(\omega \in[0,1]\) are normal numbers, and Borel proved that this is indeed true. It is quite another matter to exhibit specific normal numbers. Prove the number \section*{\(0.1234567891011121314 \ldots\)}<br><br>is normal. It is an unsolved problem of mathematics to show that \(e-2\) and \(\pi-3\) are normal numbers also.
-
-============================================================
-
-Note ID: 1711902045752
-  Field: Text
-    Before:
-      14. (a) Let \(\mathbb{P}(A)\) denote the probability of the occurrence of an event \(A\). Prove carefully, for events \(A_{1}, A_{2}, \ldots, A_{n}\), that<br><br><ul><li>\(\mathbb{P}\left(\bigcup_{i=1}^n A_i\right)\)<br></li><li>=&nbsp;\(\sum_i \mathbb{P}\left(A_i\right)\) -&nbsp;\(\sum_{i&lt;j} \mathbb{P}\left(A_i \cap A_j\right)\) +&nbsp;\(\sum_{i&lt;j&lt;k}\) \(\mathbb{P}\left(A_i \cap A_j \cap A_k\right)\) -&nbsp;\(\cdots\) +&nbsp;\((-1)^{n+1} \) \(\mathbb{P}\left(\bigcap_i A_i\right)\)</li></ul>
-
-    After:
-      14. (a) Let \(\mathbb{P}(A)\) denote the probability of the occurrence of an event \(A\). Prove carefully, for events \(A_{1}, A_{2}, \ldots, A_{n}\), that<br><br><ul><li>\(\mathbb{P}\left(\bigcup_{i=1}^n A_i\right)\)<br></li><li>=&nbsp;\(\sum_i \mathbb{P}\left(A_i\right)\) -&nbsp;\(\sum_{i&lt;j} \mathbb{P}\left(A_i \cap A_j\right)\) +&nbsp;\(\sum_{i&lt;j&lt;k}\) \(\mathbb{P}\left(A_i \cap A_j \cap A_k\right)\) -&nbsp;\(\cdots\) +&nbsp;\((-1)^{n+1} \) \(\mathbb{P}\left(\bigcap_i A_i\right)\)</li></ul>
-
-============================================================
-
-Note ID: 1711902291750
-  Field: Text
-    Before:
-      15. Two identical decks of cards, each containing \(N\) cards, are shuffled randomly. We say that a \(k\)-matching occurs if the two decks agree in exactly \(k\) places. Show that the probability that there is a \(k\)-matching is<br><br><ul><li>\(\pi_{k}\) = \(\frac{1}{k !}\) \(\left(1-\frac{1}{1 !}+\frac{1}{2 !}-\frac{1}{3 !}+\cdots+\frac{(-1)^{N-k} }{(N-k) !}\right)\)</li></ul><br>
-
-    After:
-      15. Two identical decks of cards, each containing \(N\) cards, are shuffled randomly. We say that a \(k\)-matching occurs if the two decks agree in exactly \(k\) places. Show that the probability that there is a \(k\)-matching is<br><br><ul><li>\(\pi_{k}\) = \(\frac{1}{k !}\) \(\left(1-\frac{1}{1 !}+\frac{1}{2 !}-\frac{1}{3 !}+\cdots+\frac{(-1)^{N-k} }{(N-k) !}\right)\)</li></ul><br>
-
-============================================================
-
-Note ID: 1711907926298
-  Field: Text
-    Before:
-      1. (Abbott 1.4.2) Let \(A \subset \mathbb{R}\) be non-empty and bounded above, and let \(s \in \mathbb{R}\) have the property that for all \(n \in \mathbb{N}, s+\frac{1}{n}\) is an upper bound for \(A\) and \(s-\frac{1}{n}\) is not an upper bound for \(A\). Show \(s=\sup A\).<br><br>(Now we will show that \(s\) is the l.u.b. by using Lemma 1.3.8.)<br><br>Let \(\epsilon&gt;0\) be given. By the Archimedean Property, there is an \(n \in \mathbb{N}\) such that \(\frac{1}{n}\) &lt; \(\epsilon\). By assumption \(s-\frac{1}{n}\) is not an upper bound. So there is an \(a \in A\) such that \(s-\frac{1}{n}\) &lt; \(a\). Then, \(s-\epsilon\) &lt; \(a\).<br><br>Therefore, by Lemma 1.3.8 \(s=\sup A\).
-
-    After:
-      1. (Abbott 1.4.2) Let \(A \subset \mathbb{R}\) be non-empty and bounded above, and let \(s \in \mathbb{R}\) have the property that for all \(n \in \mathbb{N}, s+\frac{1}{n}\) is an upper bound for \(A\) and \(s-\frac{1}{n}\) is not an upper bound for \(A\). Show \(s=\sup A\).<br><br>(Now we will show that \(s\) is the l.u.b. by using Lemma 1.3.8.)<br><br>Let \(\epsilon&gt;0\) be given. By the Archimedean Property, there is an \(n \in \mathbb{N}\) such that \(\frac{1}{n}\) &lt; \(\epsilon\). By assumption \(s-\frac{1}{n}\) is not an upper bound. So there is an \(a \in A\) such that \(s-\frac{1}{n}\) &lt; \(a\). Then, \(s-\epsilon\) &lt; \(a\).<br><br>Therefore, by Lemma 1.3.8 \(s=\sup A\).
-
-============================================================
-
-Note ID: 1711908061179
-  Field: Text
-    Before:
-      2. (Abbott 1.4.3) Prove that \(\bigcap_{n=1}^{\infty}(0,1 / n)=\emptyset\).<br><br>Solution. Suppose, for contradiction, that \(\bigcap_{n=1}^{\infty}(0,1 / n) \neq \emptyset\). Let \(x \in\) \(\bigcap_{n=1}^{\infty}(0,1 / n)\). Then, \(x\) \(\in\) \((0,1 / n)\) for all \(n \in \mathbb{N}\). It follows that \(0&lt;x&lt;\frac{1}{n}\) for all \(n \in \mathbb{N}\). This contradicts the Archimedean Property.
-
-    After:
-      2. (Abbott 1.4.3) Prove that \(\bigcap_{n=1}^{\infty}(0,1 / n)=\emptyset\).<br><br>Solution. Suppose, for contradiction, that \(\bigcap_{n=1}^{\infty}(0,1 / n) \neq \emptyset\). Let \(x \in\) \(\bigcap_{n=1}^{\infty}(0,1 / n)\). Then, \(x\) \(\in\) \((0,1 / n)\) for all \(n \in \mathbb{N}\). It follows that \(0&lt;x&lt;\frac{1}{n}\) for all \(n \in \mathbb{N}\). This contradicts the Archimedean Property.
-
-============================================================
-
-Note ID: 1711908787387
-  Field: Text
-    Before:
-      3. (Abbott 1.4.4) Let \(a&lt;b\) be real numbers and consider the set \(T=\mathbb{Q} \cap[a, b]\). Show that \(\sup T=b\).<br><br>By definition, if \(t \in T\) then \(t \leq b\). So \(b\) is an upper bound for \(T\).<br><br>Let \(\epsilon&gt;0\) be given. Then, by the density of \(\mathbb{Q}\) in \(\mathbb{R}\) there is a rational \(r \in \mathbb{Q}\) such that<br><br><ul><li>\(b-\epsilon\) &lt; \(r\) &lt; \(b \text {. }\)</li></ul><br>Moreover, this implies that \(r \in T\). Therefore, by Lemma 1.3.8 \(b=\sup T\).<br>
-
-    After:
-      3. (Abbott 1.4.4) Let \(a&lt;b\) be real numbers and consider the set \(T=\mathbb{Q} \cap[a, b]\). Show that \(\sup T=b\).<br><br>By definition, if \(t \in T\) then \(t \leq b\). So \(b\) is an upper bound for \(T\).<br><br>Let \(\epsilon&gt;0\) be given. Then, by the density of \(\mathbb{Q}\) in \(\mathbb{R}\) there is a rational \(r \in \mathbb{Q}\) such that<br><br><ul><li>\(b-\epsilon\) &lt; \(r\) &lt; \(b \text {. }\)</li></ul><br>Moreover, this implies that \(r \in T\). Therefore, by Lemma 1.3.8 \(b=\sup T\).<br>
-
-============================================================
-
-Note ID: 1711909867811
-  Field: Text
-    Before:
-      4. (Abbott 1.4.8)<br><br>(a) Give an example of two sets \(A, B \subset \mathbb{R}\) with \(A \cap B=\emptyset\), \(\sup A=\sup B\), \(\sup A \notin A\) and \(\sup B \notin B\).<br><br>(a) Let \(A=\left\{1-\frac{1}{2 j}: j \in \mathbb{N}\right\}\) and \(B=\left\{1-\frac{1}{2 k+1}: k \in \mathbb{N}\right\}\). By construction, \(1 \notin A\) and \(1 \notin B\).<br><br>First, we will show that \(A \cap B=\emptyset\). Suppose \(x \in A \cap B\). Then, \(x \in A\) and \(x \in B\). Thus, there are \(j, k \in \mathbb{N}\) such that \(x=\frac{1}{2 j}\) and \(x=\frac{1}{2 k+1}\). It follows that \(2 j=2 k+1 \Longrightarrow\) \(j-k\)= \(\frac{1}{2}\), but this is impossible since \(j-k\) is an integer. Thus, \(A \cap B=\emptyset\).<br><br>Next, we show that \(\sup A=1=\sup B\).<br><br>Let \(\epsilon&gt;0\) be given. Then, by the Archimedean Property there are natural numbers \(j, k \in \mathbb{N}\) such that \(\frac{1}{\epsilon}\) &lt; \(2 j\) and \(\frac{1}{\epsilon}\) &lt; \(2 k+1\). It follows that<br><br>\[<br>1-\epsilon&lt;1-\frac{1}{2 j}&lt;1 \quad \text { and } \quad 1-\epsilon&lt;1-\frac{1}{2 k+1}&lt;1<br>\]<br><br>By Lemma 1.3.8, \(\sup A=1=\sup B\).
-
-    After:
-      4. (Abbott 1.4.8)<br><br>(a) Give an example of two sets \(A, B \subset \mathbb{R}\) with \(A \cap B=\emptyset\), \(\sup A=\sup B\), \(\sup A \notin A\) and \(\sup B \notin B\).<br><br>(a) Let \(A=\left\{1-\frac{1}{2 j}: j \in \mathbb{N}\right\}\) and \(B=\left\{1-\frac{1}{2 k+1}: k \in \mathbb{N}\right\}\). By construction, \(1 \notin A\) and \(1 \notin B\).<br><br>First, we will show that \(A \cap B=\emptyset\). Suppose \(x \in A \cap B\). Then, \(x \in A\) and \(x \in B\). Thus, there are \(j, k \in \mathbb{N}\) such that \(x=\frac{1}{2 j}\) and \(x=\frac{1}{2 k+1}\). It follows that \(2 j=2 k+1 \Longrightarrow\) \(j-k\)= \(\frac{1}{2}\), but this is impossible since \(j-k\) is an integer. Thus, \(A \cap B=\emptyset\).<br><br>Next, we show that \(\sup A=1=\sup B\).<br><br>Let \(\epsilon&gt;0\) be given. Then, by the Archimedean Property there are natural numbers \(j, k \in \mathbb{N}\) such that \(\frac{1}{\epsilon}\) &lt; \(2 j\) and \(\frac{1}{\epsilon}\) &lt; \(2 k+1\). It follows that<br><br>\[<br>1-\epsilon&lt;1-\frac{1}{2 j}&lt;1 \quad \text { and } \quad 1-\epsilon&lt;1-\frac{1}{2 k+1}&lt;1<br>\]<br><br>By Lemma 1.3.8, \(\sup A=1=\sup B\).
-
-============================================================
-
-Note ID: 1711910239834
-  Field: Text
-    Before:
-      4. (Abbott 1.4.8)<br>(b) Prove or disprove the following: There exists a sequence of closed bounded (not necessarily nested) intervals \(I_{1}, I_{2}, I_{3}, \ldots\) with the property that \(\bigcap_{n=1}^{N} I_{n} \neq \emptyset\) for all \(N \in \mathbb{N}\), but \(\bigcap_{n=1}^{\infty} I_{n}=\emptyset\).<br><br><br>(b) Suppose there is a sequence of closed bounded intervals as in the problem, \(I_{n}=\left[a_{n}, b_{n}\right]\). Define the sequence of sets \(J_{N}\) = \(\bigcap_{n=1}^{N} I_{n}\) for \(N \in \mathbb{N}\). We will show that \(\left\{J_{N}\right\}\) is a nested sequence of intervals. By assumption \(J_{2}\) is non-empty, thus the intervals \(I_{1}\) and \(I_{2}\) must intersect one another. The only possibility is that \(J_{2}\) is a closed bounded interval of the following form, \(I_{1}, I_{2},\left[a_{1}, b_{2}\right]\) or \(\left[a_{2}, b_{1}\right]\)::4 possible options::4 possible options::4 possible options::4 possible options::4 possible options and moreover, \(J_{2}\) = \(I_{1} \cap I_{2}\) = \(J_{1} \cap I_{2} \subset J_{1}\), that is, the intervals are nested. For induction, suppose \(J_{N}\) is a closed bounded set and nested such that \(J_{N+1} \subset J_{N}\). Then,<br><br><ul><li>\(J_{N+1}\) = \(\bigcap_{n=1}^{N+1} I_{n}\) = \(\left(\bigcap_{n=1}^{N} I_{n}\right) \cap I_{N+1}\) = \(J_{N} \cap I_{N+1}\)</li></ul><br>Since \(J_{N}\) and \(I_{N+1}\) are closed bounded and \(J_{N+1}\) is non-empty, it follows from a similar argument to above that \(J_{N+1}\) is a closed bounded interval. Moreover, \(J_{N+1} \subset J_{N}\) are nested. But, we also have that<br><br>\[<br>\bigcap_{n=1}^{\infty} J_{n}=\bigcap_{n=1}^{\infty} I_{n}=\emptyset<br>\]<br><br>This contradicts the nested interval property which states that \(\bigcap_{n=1}^{\infty} J_{n}\) should be non-empty.<br>
-
-    After:
-      4. (Abbott 1.4.8)<br>(b) Prove or disprove the following: There exists a sequence of closed bounded (not necessarily nested) intervals \(I_{1}, I_{2}, I_{3}, \ldots\) with the property that \(\bigcap_{n=1}^{N} I_{n} \neq \emptyset\) for all \(N \in \mathbb{N}\), but \(\bigcap_{n=1}^{\infty} I_{n}=\emptyset\).<br><br><br>(b) Suppose there is a sequence of closed bounded intervals as in the problem, \(I_{n}=\left[a_{n}, b_{n}\right]\). Define the sequence of sets \(J_{N}\) = \(\bigcap_{n=1}^{N} I_{n}\) for \(N \in \mathbb{N}\). We will show that \(\left\{J_{N}\right\}\) is a nested sequence of intervals. By assumption \(J_{2}\) is non-empty, thus the intervals \(I_{1}\) and \(I_{2}\) must intersect one another. The only possibility is that \(J_{2}\) is a closed bounded interval of the following form, \(I_{1}, I_{2},\left[a_{1}, b_{2}\right]\) or \(\left[a_{2}, b_{1}\right]\)::4 possible options::4 possible options::4 possible options::4 possible options::4 possible options and moreover, \(J_{2}\) = \(I_{1} \cap I_{2}\) = \(J_{1} \cap I_{2} \subset J_{1}\), that is, the intervals are nested. For induction, suppose \(J_{N}\) is a closed bounded set and nested such that \(J_{N+1} \subset J_{N}\). Then,<br><br><ul><li>\(J_{N+1}\) = \(\bigcap_{n=1}^{N+1} I_{n}\) = \(\left(\bigcap_{n=1}^{N} I_{n}\right) \cap I_{N+1}\) = \(J_{N} \cap I_{N+1}\)</li></ul><br>Since \(J_{N}\) and \(I_{N+1}\) are closed bounded and \(J_{N+1}\) is non-empty, it follows from a similar argument to above that \(J_{N+1}\) is a closed bounded interval. Moreover, \(J_{N+1} \subset J_{N}\) are nested. But, we also have that<br><br>\[<br>\bigcap_{n=1}^{\infty} J_{n}=\bigcap_{n=1}^{\infty} I_{n}=\emptyset<br>\]<br><br>This contradicts the nested interval property which states that \(\bigcap_{n=1}^{\infty} J_{n}\) should be non-empty.<br>
-
-============================================================
-
-Note ID: 1711910397857
-  Field: Text
-    Before:
-      (a) Prove if \(A_{1}, \ldots, A_{m}\) are countable sets then \(A_{1} \cup \cdots \cup A_{m}\) is countable.<br><br>(a) Let \(B, C\) be disjoint countable sets. We use the same trick as with the integers and list them as<br><br><ul><li>\(B \cup C\) = \(\left\{b_{1}, c_{1}, b_{2}, c_{2}, \ldots\right\}\)</li></ul><div>Meaning \(B \cup C\) is countable, and \(A_{1} \cup A_{2}\) is also countable since we can let \(B=A_{1}\) and \(C\) = \(A_{2} \backslash A_{1}\).<br><br>Now induction: suppose \(A_{1} \cup \cdots \cup A_{n}\) is countable, \(\left(A_{1} \cup \cdots \cup A_{n}\right) \cup A_{n+1}\) is the union of two countable sets which by above is countable.<br></div>
-
-    After:
-      (a) Prove if \(A_{1}, \ldots, A_{m}\) are countable sets then \(A_{1} \cup \cdots \cup A_{m}\) is countable.<br><br>(a) Let \(B, C\) be disjoint countable sets. We use the same trick as with the integers and list them as<br><br><ul><li>\(B \cup C\) = \(\left\{b_{1}, c_{1}, b_{2}, c_{2}, \ldots\right\}\)</li></ul><div>Meaning \(B \cup C\) is countable, and \(A_{1} \cup A_{2}\) is also countable since we can let \(B=A_{1}\) and \(C\) = \(A_{2} \backslash A_{1}\).<br><br>Now induction: suppose \(A_{1} \cup \cdots \cup A_{n}\) is countable, \(\left(A_{1} \cup \cdots \cup A_{n}\right) \cup A_{n+1}\) is the union of two countable sets which by above is countable.<br></div>
-
-============================================================
-
-Note ID: 1711911135280
-  Field: Text
-    Before:
-      5. (Abbott 1.5.3(c)) Prove that if \(A_{n}\) is a countable set for each \(n \in \mathbb{N}\) then \(\bigcup_{n=1}^{\infty} A_{n}\) is countable. (See the book for a hint.)<br><br>Solution. From the hint in the book, we can visualize \(\bigcup_{n=1}^{\infty} A_{n}\) as a grid of numbers that is similar to the Cartesian product \(\mathbb{N} \times \mathbb{N}\). Moreover, in class we showed that \(\mathbb{N} \times \mathbb{N} \sim \mathbb{N}\) is countable.<br><br>Let \(f_{n}: A_{n} \rightarrow \mathbb{N}\) be a bijection. Define a function \(f: \bigcup_{n=1}^{\infty} A_{n} \rightarrow \mathbb{N} \times \mathbb{N}\) as follows. Since \(a\) could possibly appear in more than one of the sets \(A_{n}\), we must be careful so our function is well-defined. For each \(a \in \bigcup_{n=1}^{\infty} A_{n}\), let \(n_{a}\) be the smallest number such that \(a \in A_{n_{a}}\) ( if \(n&lt;n_{a}\) then \(a \notin A_{n_{a}}\) ). Then,<br><br>\[<br>f(a)=\left(n_{a}, f_{n_{a}}(a)\right)<br>\]<br><br>Every element of \(a \in \bigcup_{n=1}^{\infty} A_{n}\) has exactly one image in the range and thus, \(f\) is well-defined.<br><br>We claim that \(f\) is an injection. Suppose that \(f(a)=f(b)\). It follows that<br><br>\[<br>\begin{gathered}<br>\left(n_{a}, f_{n_{a} }(a)\right)=\left(n_{b}, f_{n_{b} }(b)\right) \\<br>\Longleftrightarrow \quad n_{a}=n_{b} \quad \text { and } \quad f_{n_{a} }(a)=f_{n_{a} }(b) .<br>\end{gathered}<br>\]<br><br>Since \(f_{n_{a}}\) is injective, it must follow that \(a=b\). We conclude that \(f\) is an injective function.<br><br>The set \(\bigcup_{n=1}^{\infty} A_{n}\) is infinite, since \(A_{1}\) is countably infinite and \(A_{1} \subset \bigcup_{n=1}^{\infty} A_{n}\). Therefore, by a lemma* we proved in class and by using that \(\sim\) is an equivalence relation we have that<br><br><ul><li>\(\bigcup_{n=1}^{\infty} A_{n}\) \(\sim\) \(f\left(\bigcup_{n=1}^{\infty} A_{n}\right)\) \( \sim\) \(\mathbb{N} \times \mathbb{N} \sim \mathbb{N}\)</li></ul><br>
-
-    After:
-      5. (Abbott 1.5.3(c)) Prove that if \(A_{n}\) is a countable set for each \(n \in \mathbb{N}\) then \(\bigcup_{n=1}^{\infty} A_{n}\) is countable. (See the book for a hint.)<br><br>Solution. From the hint in the book, we can visualize \(\bigcup_{n=1}^{\infty} A_{n}\) as a grid of numbers that is similar to the Cartesian product \(\mathbb{N} \times \mathbb{N}\). Moreover, in class we showed that \(\mathbb{N} \times \mathbb{N} \sim \mathbb{N}\) is countable.<br><br>Let \(f_{n}: A_{n} \rightarrow \mathbb{N}\) be a bijection. Define a function \(f: \bigcup_{n=1}^{\infty} A_{n} \rightarrow \mathbb{N} \times \mathbb{N}\) as follows. Since \(a\) could possibly appear in more than one of the sets \(A_{n}\), we must be careful so our function is well-defined. For each \(a \in \bigcup_{n=1}^{\infty} A_{n}\), let \(n_{a}\) be the smallest number such that \(a \in A_{n_{a}}\) ( if \(n&lt;n_{a}\) then \(a \notin A_{n_{a}}\) ). Then,<br><br>\[<br>f(a)=\left(n_{a}, f_{n_{a}}(a)\right)<br>\]<br><br>Every element of \(a \in \bigcup_{n=1}^{\infty} A_{n}\) has exactly one image in the range and thus, \(f\) is well-defined.<br><br>We claim that \(f\) is an injection. Suppose that \(f(a)=f(b)\). It follows that<br><br>\[<br>\begin{gathered}<br>\left(n_{a}, f_{n_{a} }(a)\right)=\left(n_{b}, f_{n_{b} }(b)\right) \\<br>\Longleftrightarrow \quad n_{a}=n_{b} \quad \text { and } \quad f_{n_{a} }(a)=f_{n_{a} }(b) .<br>\end{gathered}<br>\]<br><br>Since \(f_{n_{a}}\) is injective, it must follow that \(a=b\). We conclude that \(f\) is an injective function.<br><br>The set \(\bigcup_{n=1}^{\infty} A_{n}\) is infinite, since \(A_{1}\) is countably infinite and \(A_{1} \subset \bigcup_{n=1}^{\infty} A_{n}\). Therefore, by a lemma* we proved in class and by using that \(\sim\) is an equivalence relation we have that<br><br><ul><li>\(\bigcup_{n=1}^{\infty} A_{n}\) \(\sim\) \(f\left(\bigcup_{n=1}^{\infty} A_{n}\right)\) \( \sim\) \(\mathbb{N} \times \mathbb{N} \sim \mathbb{N}\)</li></ul><br>
-
-============================================================
-
-Note ID: 1711911185465
-  Field: Text
-    Before:
-      Lemma. Let \(A\) be an infinite set and \(B\) be countable. If there is a function \(f: A \rightarrow B\) that is injective, then \(A \sim B\) is countable.
-
-    After:
-      Lemma. Let \(A\) be an infinite set and \(B\) be countable. If there is a function \(f: A \rightarrow B\) that is injective, then \(A \sim B\) is countable.
-
-============================================================
-
-Note ID: 1711911734132
-  Field: Text
-    Before:
-      (b) Show that if \(A \sim B\) show that \(B \sim A\).<br><br>(b) Let \(f: A \rightarrow B\) be bijective. Define the inverse function \(f^{-1}: B \rightarrow A\) as follows: Let \(b \in B\). Since \(f\) is onto, there is an \(a \in A\) such that \(f(a)=b\). This leads to the definition \(f^{-1}(f(a))\) = \(a\).<br><br>Suppose \(f^{-1}(b)=f^{-1}\left(b^{\prime}\right)\). Let \(a, a^{\prime} \in A\) be such that \(f(a)=b\) and \(f\left(a^{\prime}\right)=b^{\prime}\). Then, the left hand side of the equation gives \(f^{-1}(b)=f^{-1}(f(a))=a\) and the right hand side gives \(f^{-1}\left(b^{\prime}\right)=f^{-1}\left(f\left(a^{\prime}\right)\right)=a^{\prime}\). Thus, \(a=a^{\prime}\) and \(b=f(a)=f\left(a^{\prime}\right)=b^{\prime}\). Therefore \(f^{-1}\) is \(1-1\).<br><br>Let \(a \in A\). Then, \(f^{-1}(f(a))\) = \(a\). Therefore, \(f^{-1}\) is onto.<br><br>We conclude that \(f^{-1}\) is bijective and \(B \sim A\).
-
-    After:
-      (b) Show that if \(A \sim B\) show that \(B \sim A\).<br><br>(b) Let \(f: A \rightarrow B\) be bijective. Define the inverse function \(f^{-1}: B \rightarrow A\) as follows: Let \(b \in B\). Since \(f\) is onto, there is an \(a \in A\) such that \(f(a)=b\). This leads to the definition \(f^{-1}(f(a))\) = \(a\).<br><br>Suppose \(f^{-1}(b)=f^{-1}\left(b^{\prime}\right)\). Let \(a, a^{\prime} \in A\) be such that \(f(a)=b\) and \(f\left(a^{\prime}\right)=b^{\prime}\). Then, the left hand side of the equation gives \(f^{-1}(b)=f^{-1}(f(a))=a\) and the right hand side gives \(f^{-1}\left(b^{\prime}\right)=f^{-1}\left(f\left(a^{\prime}\right)\right)=a^{\prime}\). Thus, \(a=a^{\prime}\) and \(b=f(a)=f\left(a^{\prime}\right)=b^{\prime}\). Therefore \(f^{-1}\) is \(1-1\).<br><br>Let \(a \in A\). Then, \(f^{-1}(f(a))\) = \(a\). Therefore, \(f^{-1}\) is onto.<br><br>We conclude that \(f^{-1}\) is bijective and \(B \sim A\).
-
-============================================================
-
-Note ID: 1711911853304
-  Field: Text
-    Before:
-      (c) Show that if \(A \sim B\) and \(B \sim C\) show that \(A \sim C\).<br><br>(c) Let \(f: A \rightarrow B\) and \(g: B \rightarrow C\) be bijective functions. Consider the composition \(g \circ f: A \rightarrow C\) defined by \(g \circ f(a)\)=\(g(f(a))\) .<br><br>Suppose \(g \circ f(a)=g \circ f\left(a^{\prime}\right)\). Since \(g\) is injective we have that \(f(a)=f\left(a^{\prime}\right)\). Since \(f\) is injective we have that \(a=a^{\prime}\). Thus, \(g \circ f\) is \(1-1\).<br><br>Let \(c \in C\). Since \(g\) is onto there is a \(b \in B\) such that \(g(b)=c\). Since \(f\) is onto there is a \(a \in A\) such that \(f(a)=b\). Thus, \(g \circ f(a)=g(f(a))=g(b)=c\). Therefore, \(g \circ f\) is onto.<br><br>We have shown that \(A \sim C\).
-
-    After:
-      (c) Show that if \(A \sim B\) and \(B \sim C\) show that \(A \sim C\).<br><br>(c) Let \(f: A \rightarrow B\) and \(g: B \rightarrow C\) be bijective functions. Consider the composition \(g \circ f: A \rightarrow C\) defined by \(g \circ f(a)\)=\(g(f(a))\) .<br><br>Suppose \(g \circ f(a)=g \circ f\left(a^{\prime}\right)\). Since \(g\) is injective we have that \(f(a)=f\left(a^{\prime}\right)\). Since \(f\) is injective we have that \(a=a^{\prime}\). Thus, \(g \circ f\) is \(1-1\).<br><br>Let \(c \in C\). Since \(g\) is onto there is a \(b \in B\) such that \(g(b)=c\). Since \(f\) is onto there is a \(a \in A\) such that \(f(a)=b\). Thus, \(g \circ f(a)=g(f(a))=g(b)=c\). Therefore, \(g \circ f\) is onto.<br><br>We have shown that \(A \sim C\).
-
-============================================================
-
-Note ID: 1711958155870
-  Field: Text
-    Before:
-      7. (Abbott 1.5.8) Let \(B\) be a set of positive real numbers with the property that adding together any finite subset of elements from \(B\) always gives a sum of 2 or less. Show that \(B\) must be finite or countable.<br><br><br>Consider the set \(B_{n} \equiv B \backslash\left\{b_{1}, b_{2}, \ldots, b_{n-1}\right\}\) and define inductively \(b_{n}\) = \(\max B_{n}\). We have constructed a subset<br><br>\[<br>\left\{b_{1}, b_{2}, b_{3}, \ldots\right\} \subset B<br>\]<br><br>where \(b_{1}&gt;b_{2}&gt;b_{3}&gt;\ldots\) Finally, we claim that \(B\) = \(\left\{b_{1}, b_{2}, b_{3}, \ldots\right\}\).<br><br>Suppose, for contradiction, that \(b \in B\) and \(b \neq b_{n}\) for all \(n \in \mathbb{N}\). By assumption on the set \(B\) we have that \(0&lt;b\). We claim that \(b&lt;b_{n}\) for all \(n \in \mathbb{N}\). (Base case) since \(b_{1}=\max B\) and \(b \neq b_{1}\) it follows that \(b&lt;b_{1}\).<br><br>Suppose that \(b&lt;b_{n}\). Then, \(b_{n+1}\) = \(\max B \backslash\left\{b_{1}, b_{2}, \ldots, b_{n}\right\}\). By assumption, \(b \neq b_{n+1}\). Then, either \(b&gt;b_{n+1}\) or \(b&lt;b_{n+1}\). If \(b&gt;b_{n+1}\) then \(b\) must be one of the previous maximums, but that cannot be true. Thus, \(b&lt;b_{n}\).<br><br>Choose \(N\) large enough so that<br><br>\[<br>\frac{1}{N}&lt;\frac{b}{2} \quad \text { (by Archimedean Property). }<br>\]<br><br>It follows that<br><br>\[<br>\begin{aligned}<br>2 &amp; &lt;N b=b+b+\cdots+b \quad(N \text { times }) \\<br>&amp; \leq b_{1}+b_{2}+\cdots+b_{N} .<br>\end{aligned}<br>\]<br><br>The right hand side is a finite sum of elements of \(B\) and we reach a contradiction.<br><br>Therefore, \(B=\left\{b_{1}, b_{2}, b_{3}, \ldots\right\}\) and \(B\) is a countable set.
-
-    After:
-      7. (Abbott 1.5.8) Let \(B\) be a set of positive real numbers with the property that adding together any finite subset of elements from \(B\) always gives a sum of 2 or less. Show that \(B\) must be finite or countable.<br><br><br>Consider the set \(B_{n} \equiv B \backslash\left\{b_{1}, b_{2}, \ldots, b_{n-1}\right\}\) and define inductively \(b_{n}\) = \(\max B_{n}\). We have constructed a subset<br><br>\[<br>\left\{b_{1}, b_{2}, b_{3}, \ldots\right\} \subset B<br>\]<br><br>where \(b_{1}&gt;b_{2}&gt;b_{3}&gt;\ldots\) Finally, we claim that \(B\) = \(\left\{b_{1}, b_{2}, b_{3}, \ldots\right\}\).<br><br>Suppose, for contradiction, that \(b \in B\) and \(b \neq b_{n}\) for all \(n \in \mathbb{N}\). By assumption on the set \(B\) we have that \(0&lt;b\). We claim that \(b&lt;b_{n}\) for all \(n \in \mathbb{N}\). (Base case) since \(b_{1}=\max B\) and \(b \neq b_{1}\) it follows that \(b&lt;b_{1}\).<br><br>Suppose that \(b&lt;b_{n}\). Then, \(b_{n+1}\) = \(\max B \backslash\left\{b_{1}, b_{2}, \ldots, b_{n}\right\}\). By assumption, \(b \neq b_{n+1}\). Then, either \(b&gt;b_{n+1}\) or \(b&lt;b_{n+1}\). If \(b&gt;b_{n+1}\) then \(b\) must be one of the previous maximums, but that cannot be true. Thus, \(b&lt;b_{n}\).<br><br>Choose \(N\) large enough so that<br><br>\[<br>\frac{1}{N}&lt;\frac{b}{2} \quad \text { (by Archimedean Property). }<br>\]<br><br>It follows that<br><br>\[<br>\begin{aligned}<br>2 &amp; &lt;N b=b+b+\cdots+b \quad(N \text { times }) \\<br>&amp; \leq b_{1}+b_{2}+\cdots+b_{N} .<br>\end{aligned}<br>\]<br><br>The right hand side is a finite sum of elements of \(B\) and we reach a contradiction.<br><br>Therefore, \(B=\left\{b_{1}, b_{2}, b_{3}, \ldots\right\}\) and \(B\) is a countable set.
-
-============================================================
-
-Note ID: 1711958550348
-  Field: Text
-    Before:
-      \section*{8. (Abbott 1.6.10(a)(b))}<br><br>(a) Is the set of all functions from \(\{0,1\}\) to \(\mathbb{N}\) countable or uncountable?<br><br>(a) Let \(F(\{0,1\}, \mathbb{N}) \equiv\{f:\{0,1\} \rightarrow \mathbb{N}\}\) denote the set of all functions. Each function \(f \in F(\{0,1\}, \mathbb{N})\) is uniquely determined by the pair \((f(0), f(1))\). Define a function, \(\phi: F(\{0,1\}, \mathbb{N}) \rightarrow \mathbb{N} \times \mathbb{N}\) by \(\phi(f)\) = \((f(0), f(1))\). We claim that \(\phi\) is bijective.<br><br>Suppose \(\phi(f)=\phi(g)\). Then, \((f(0), f(1))\) = \((g(0), g(1))\) \( \Longleftrightarrow\)&nbsp; \(f(0)\) = \(g(0)\) and \(f(1)\) = \(g(1)\) \(\Longleftrightarrow\) \(f=g\). Thus, \(\phi\) is \(1-1\).<br><br>Let \((m, n) \in \mathbb{N} \times \mathbb{N}\). Define the function \(f_{m, n} \in F(\{0,1\}, \mathbb{N})\) by \(f_{m, n}\) \((0)\) = \(m\) and \(f_{m, n}\) \((1)\) = \(n\). It follows that<br><br>\[<br>\phi\left(f_{m, n}=\left(f_{m, n}(0), f_{m, n}(1)\right)=(m, n)\right.<br>\]<br><br>Therefore, \(\phi\) is onto.<br><br>We conclude that \(\phi\) is a bijection and that \(F(\{0,1\}, \mathbb{N}) \sim \mathbb{N} \times \mathbb{N} \sim \mathbb{N}\) is countable.
-
-    After:
-      \section*{8. (Abbott 1.6.10(a)(b))}<br><br>(a) Is the set of all functions from \(\{0,1\}\) to \(\mathbb{N}\) countable or uncountable?<br><br>(a) Let \(F(\{0,1\}, \mathbb{N}) \equiv\{f:\{0,1\} \rightarrow \mathbb{N}\}\) denote the set of all functions. Each function \(f \in F(\{0,1\}, \mathbb{N})\) is uniquely determined by the pair \((f(0), f(1))\). Define a function, \(\phi: F(\{0,1\}, \mathbb{N}) \rightarrow \mathbb{N} \times \mathbb{N}\) by \(\phi(f)\) = \((f(0), f(1))\). We claim that \(\phi\) is bijective.<br><br>Suppose \(\phi(f)=\phi(g)\). Then, \((f(0), f(1))\) = \((g(0), g(1))\) \( \Longleftrightarrow\)&nbsp; \(f(0)\) = \(g(0)\) and \(f(1)\) = \(g(1)\) \(\Longleftrightarrow\) \(f=g\). Thus, \(\phi\) is \(1-1\).<br><br>Let \((m, n) \in \mathbb{N} \times \mathbb{N}\). Define the function \(f_{m, n} \in F(\{0,1\}, \mathbb{N})\) by \(f_{m, n}\) \((0)\) = \(m\) and \(f_{m, n}\) \((1)\) = \(n\). It follows that<br><br>\[<br>\phi\left(f_{m, n}=\left(f_{m, n}(0), f_{m, n}(1)\right)=(m, n)\right.<br>\]<br><br>Therefore, \(\phi\) is onto.<br><br>We conclude that \(\phi\) is a bijection and that \(F(\{0,1\}, \mathbb{N}) \sim \mathbb{N} \times \mathbb{N} \sim \mathbb{N}\) is countable.
-
-============================================================
-
-Note ID: 1711958845327
-  Field: Text
-    Before:
-      (b) Is the set of all functions from \(\mathbb{N}\) to \(\{0,1\}\) countable or uncountable?<br><br>High-level argument:<br><ul><li>A function to binary values can be expressed as a mask</li><li>If you claim you have a list of all such masks I can create a new mask which differs from every mask in the list at 1 position via diagonalization</li></ul>
-
-    After:
-      (b) Is the set of all functions from \(\mathbb{N}\) to \(\{0,1\}\) countable or uncountable?<br><br>High-level argument:<br><ul><li>A function to binary values can be expressed as a mask</li><li>If you claim you have a list of all such masks I can create a new mask which differs from every mask in the list at 1 position via diagonalization</li></ul>
-
-============================================================
-
-Note ID: 1711971076144
-  Field: Text
-    Before:
-      Verify, using the definition of convergence of a sequence, that the following sequences converge to the proposed limit.<br><br>(b) \(\lim \frac{2 n^{2}}{n^{3}+3}=0\).<br><br><ul><li>\(\left|\frac{2 n^2}{n^3+3}-0\right|\)</li><li>= \(\frac{2 n^2}{n^3+3}\)</li><li>&lt; \(\frac{2 n^2}{n^3}\)</li><li>=&nbsp;&nbsp;\(\frac{2}{n}\)</li><li>&lt;&nbsp;\(\epsilon\)</li></ul><div>Whih implies&nbsp;</div><div><ul><li>\(n\)&nbsp;&nbsp;\(&gt;\)\(\frac{2}{\epsilon}\)</li></ul></div>
-
-    After:
-      Verify, using the definition of convergence of a sequence, that the following sequences converge to the proposed limit.<br><br>(b) \(\lim \frac{2 n^{2}}{n^{3}+3}=0\).<br><br><ul><li>\(\left|\frac{2 n^2}{n^3+3}-0\right|\)</li><li>= \(\frac{2 n^2}{n^3+3}\)</li><li>&lt; \(\frac{2 n^2}{n^3}\)</li><li>=&nbsp;&nbsp;\(\frac{2}{n}\)</li><li>&lt;&nbsp;\(\epsilon\)</li></ul><div>Whih implies&nbsp;</div><div><ul><li>\(n\)&nbsp;&nbsp;\(&gt;\)\(\frac{2}{\epsilon}\)</li></ul></div>
-
-============================================================
-
-Note ID: 1711974410716
-  Field: Text
-    Before:
-      \section*{Exercise 2.2.5}<br><br>Let \([[x]]\) be the greatest integer less than or equal to \(x\). For example, \([[\pi]]=3\) and \([[3]]=3\). For each sequence, find \(\lim a_{n}\) and verify it with the definition of convergence.<br><br>(b) \(a_{n}=[[(12+4 n) / 3 n]]\).<br><br><ul><li>Each individual term is bounded between 1 and 2 becaus the inner equation reduces to</li><ul><li>\(\frac{4}{n} + \frac{4}{3} \)<br></li><li>which means for&nbsp;\(n&gt;6\) this is less than 2 and more than 1</li></ul><li>Thus the whole part of the value that this converges to is 1</li><li>Then for n &gt; 6, epsilon is greater than 0 for all epsilon since the sequence reaches its limit</li></ul>
-
-    After:
-      \section*{Exercise 2.2.5}<br><br>Let \([[x]]\) be the greatest integer less than or equal to \(x\). For example, \([[\pi]]=3\) and \([[3]]=3\). For each sequence, find \(\lim a_{n}\) and verify it with the definition of convergence.<br><br>(b) \(a_{n}=[[(12+4 n) / 3 n]]\).<br><br><ul><li>Each individual term is bounded between 1 and 2 becaus the inner equation reduces to</li><ul><li>\(\frac{4}{n} + \frac{4}{3} \)<br></li><li>which means for&nbsp;\(n&gt;6\) this is less than 2 and more than 1</li></ul><li>Thus the whole part of the value that this converges to is 1</li><li>Then for n &gt; 6, epsilon is greater than 0 for all epsilon since the sequence reaches its limit</li></ul>
-
-============================================================
-
-Note ID: 1712043143598
-  Field: Text
-    Before:
-      Summary. Discrete random variables are studied via their probability mass functions. This leads to the definition of the 'mean value' or 'expectation' of a random variable. There are discussions of variance, and of functions of random variables. Methods are presented for calculating expectations, including the use of conditional expectation.
-
-    After:
-      Summary. Discrete random variables are studied via their probability mass functions. This leads to the definition of the 'mean value' or 'expectation' of a random variable. There are discussions of variance, and of functions of random variables. Methods are presented for calculating expectations, including the use of conditional expectation.
-
-============================================================
-
-Note ID: 1712043243299
-  Field: Text
-    Before:
-      Given a probability space \((\Omega, \mathcal{F}, \mathbb{P})\), we are often interested in situations involving some realvalued function \(X\) acting on \(\Omega\). For example, let \(\mathcal{E}\) be the experiment of throwing a fair die once, so that \(\Omega=\{1,2,3,4,5,6\}\), and suppose that we gamble on the outcome of \(\mathcal{E}\) in such a way that the profit is<br><br>-1 if the outcome is 1,2 , or 3 ,<br><br>0 if the outcome is 4 ,<br><br>2 if the outcome is 5 or 6 ,<br><br>where negative profits are positive losses. If the outcome is \(\omega\), then our profit is \(X(\omega)\), where \(X\) : \(\Omega\) \(\rightarrow\) \(\mathbb{R}\) is defined by<br><br>\[<br>X(1)=X(2)=X(3)=-1, \quad X(4)=0, \quad X(5)=X(6)=2 .<br>\]<br><br>The mapping \(X\) is an example of a 'discrete random variable'.
-
-    After:
-      Given a probability space \((\Omega, \mathcal{F}, \mathbb{P})\), we are often interested in situations involving some realvalued function \(X\) acting on \(\Omega\). For example, let \(\mathcal{E}\) be the experiment of throwing a fair die once, so that \(\Omega=\{1,2,3,4,5,6\}\), and suppose that we gamble on the outcome of \(\mathcal{E}\) in such a way that the profit is<br><br>-1 if the outcome is 1,2 , or 3 ,<br><br>0 if the outcome is 4 ,<br><br>2 if the outcome is 5 or 6 ,<br><br>where negative profits are positive losses. If the outcome is \(\omega\), then our profit is \(X(\omega)\), where \(X\) : \(\Omega\) \(\rightarrow\) \(\mathbb{R}\) is defined by<br><br>\[<br>X(1)=X(2)=X(3)=-1, \quad X(4)=0, \quad X(5)=X(6)=2 .<br>\]<br><br>The mapping \(X\) is an example of a 'discrete random variable'.
-
-============================================================
-
-Note ID: 1712044121626
-  Field: Text
-    Before:
-      The most interesting things about a discrete random variable are the values which it may take and the probabilities associated with these values. If \(X\) is a discrete random variable on the probability space \((\Omega, \mathcal{F}, \mathbb{P})\), then its image \(\operatorname{Im} X\)::notation::notation::notation::notation is the image of \(\Omega\) under \(X\), that is, the set of values taken by \(X\).
-
-    After:
-      The most interesting things about a discrete random variable are the values which it may take and the probabilities associated with these values. If \(X\) is a discrete random variable on the probability space \((\Omega, \mathcal{F}, \mathbb{P})\), then its image \(\operatorname{Im} X\)::notation::notation::notation::notation is the image of \(\Omega\) under \(X\), that is, the set of values taken by \(X\).
-
-============================================================
-
-Note ID: 1712044791514
-  Field: Text
-    Before:
-      <img src="paste-68536203334620a674137d81785f808412d54d04.jpg"><br>Note that \(\operatorname{Im} X\) is countable for any discrete random variable \(X\), and:<br><ul><li>\(p_X(x)=0\) if&nbsp;\(x \notin \operatorname{Im} X\)<br></li><li>\(\sum_{x \in \operatorname{Im} X}\) \(p_X(x)\)&nbsp; =</li><ul><li>=&nbsp;\(\mathbb{P}\)(\(\bigcup_{x \in \operatorname{Im} X}) \(\omega \in \Omega: X(\omega)=x\}\))</li><li>=&nbsp;\( \mathbb{P}(\Omega \))<br></li><li>= 1</li></ul></ul>
-
-    After:
-      <img src="paste-68536203334620a674137d81785f808412d54d04.jpg"><br>Note that \(\operatorname{Im} X\) is countable for any discrete random variable \(X\), and:<br><ul><li>\(p_X(x)=0\) if&nbsp;\(x \notin \operatorname{Im} X\)<br></li><li>\(\sum_{x \in \operatorname{Im} X}\) \(p_X(x)\)&nbsp; =</li><ul><li>=&nbsp;\(\mathbb{P}\)(\(\bigcup_{x \in \operatorname{Im} X}) \(\omega \in \Omega: X(\omega)=x\}\))</li><li>=&nbsp;\( \mathbb{P}(\Omega \))<br></li><li>= 1</li></ul></ul>
-
-============================================================
-
-Note ID: 1712054807713
-  Field: Text
-    Before:
-      Theorem 2.7 Let \(S=\left\{s_{i}: i \in I\right\}\) be a countable set of distinct real numbers, and let \(\left\{\pi_{i}: i \in I\right\}\) be a collection of real numbers satisfying<br><br><ul><li>\(\pi_{i}\) \(\geq 0\)&nbsp; \(\text { for }\)&nbsp; \(i \in I\)&nbsp;</li><li>\(\sum_{i \in I}\) \(\pi_{i}\) = \(1\)</li></ul><br>There exists a probability space \((\Omega, \mathcal{F}, \mathbb{P})\) and a discrete random variable \(X\) on \((\Omega, \mathcal{F}, \mathbb{P})\) such that the probability mass function of \(X\) is given by<br><br><ul><li>\(p_X\left(s_i\right)\) = \(\pi_i\)&nbsp;for \(i \in I\),<br></li><li>\(p_X(s)\) = \(0\) if \(s \notin S\).<br></li></ul>
-
-    After:
-      Theorem 2.7 Let \(S=\left\{s_{i}: i \in I\right\}\) be a countable set of distinct real numbers, and let \(\left\{\pi_{i}: i \in I\right\}\) be a collection of real numbers satisfying<br><br><ul><li>\(\pi_{i}\) \(\geq 0\)&nbsp; \(\text { for }\)&nbsp; \(i \in I\)&nbsp;</li><li>\(\sum_{i \in I}\) \(\pi_{i}\) = \(1\)</li></ul><br>There exists a probability space \((\Omega, \mathcal{F}, \mathbb{P})\) and a discrete random variable \(X\) on \((\Omega, \mathcal{F}, \mathbb{P})\) such that the probability mass function of \(X\) is given by<br><br><ul><li>\(p_X\left(s_i\right)\) = \(\pi_i\)&nbsp;for \(i \in I\),<br></li><li>\(p_X(s)\) = \(0\) if \(s \notin S\).<br></li></ul>
-
-============================================================
-
-Note ID: 1712054858533
-  Field: Text
-    Before:
-      Theorem 2.7 Let \(S=\left\{s_{i}: i \in I\right\}\) be a countable set of distinct real numbers, and let \(\left\{\pi_{i}: i \in I\right\}\) be a collection of real numbers satisfying<br><br>\[<br>\pi_{i} \geq 0 \quad \text { for } i \in I, \text { and } \quad \sum_{i \in I} \pi_{i}=1<br>\]<br><br>There exists a probability space \((\Omega, \mathcal{F}, \mathbb{P})\) and a discrete random variable \(X\) on \((\Omega, \mathcal{F}, \mathbb{P})\) such that the probability mass function of \(X\) is given by<br><br>\[<br>\begin{aligned}<br>p_{X}\left(s_{i}\right) &amp; =\pi_{i} &amp; &amp; \text { for } i \in I, \\<br>p_{X}(s) &amp; =0 &amp; &amp; \text { if } s \notin S .<br>\end{aligned}<br>\]
-
-    After:
-      Theorem 2.7 Let \(S=\left\{s_{i}: i \in I\right\}\) be a countable set of distinct real numbers, and let \(\left\{\pi_{i}: i \in I\right\}\) be a collection of real numbers satisfying<br><br>\[<br>\pi_{i} \geq 0 \quad \text { for } i \in I, \text { and } \quad \sum_{i \in I} \pi_{i}=1<br>\]<br><br>There exists a probability space \((\Omega, \mathcal{F}, \mathbb{P})\) and a discrete random variable \(X\) on \((\Omega, \mathcal{F}, \mathbb{P})\) such that the probability mass function of \(X\) is given by<br><br>\[<br>\begin{aligned}<br>p_{X}\left(s_{i}\right) &amp; =\pi_{i} &amp; &amp; \text { for } i \in I, \\<br>p_{X}(s) &amp; =0 &amp; &amp; \text { if } s \notin S .<br>\end{aligned}<br>\]
-
-============================================================
-
-Note ID: 1712055239482
-  Field: Text
-    Before:
-      Exercise 2.8 If \(X\) and \(Y\) are discrete random variables on the probability space \((\Omega, \mathcal{F}, \mathbb{P})\), show that \(U\) and \(V\) are discrete random variables on this space also, where<br><br><ul><li>\(U(\omega)\) = \(X(\omega)\) + \(Y(\omega)\)<br></li><li>\(V(\omega)\) = \(X(\omega)\) \(Y(\omega)\)<br></li><li>for \(\omega \in \Omega\).<br></li></ul>
-
-    After:
-      Exercise 2.8 If \(X\) and \(Y\) are discrete random variables on the probability space \((\Omega, \mathcal{F}, \mathbb{P})\), show that \(U\) and \(V\) are discrete random variables on this space also, where<br><br><ul><li>\(U(\omega)\) = \(X(\omega)\) + \(Y(\omega)\)<br></li><li>\(V(\omega)\) = \(X(\omega)\) \(Y(\omega)\)<br></li><li>for \(\omega \in \Omega\).<br></li></ul>
-
-============================================================
-
-Note ID: 1712057551290
-  Field: Text
-    Before:
-      Sets, as they are usually conceived, have elements or members. An element of a set may be a wolf, a grape, or a pigeon. It is important to know that a set itself may be an element of some other set. Mathematics is full of examples of sets of sets. A line, for instance, is a set of points; the set of all lines in the plane is a natural example of a set of sets (of points). What may be surprising is not so much that sets may occur as elements, but that for mathematical purposes no other elements need ever be considered
-
-    After:
-      Sets, as they are usually conceived, have elements or members. An element of a set may be a wolf, a grape, or a pigeon. It is important to know that a set itself may be an element of some other set. Mathematics is full of examples of sets of sets. A line, for instance, is a set of points; the set of all lines in the plane is a natural example of a set of sets (of points). What may be surprising is not so much that sets may occur as elements, but that for mathematical purposes no other elements need ever be considered
-
-============================================================
-
-Note ID: 1712057813894
-  Field: Text
-    Before:
-      Whenever possible, however, we shall informally indicate the status of a set in a particular hierarchy under consideration by means of the convention that letters at the beginning of the alphabet denote elements, and letters at the end denote sets containing them; similarly letters of a relatively simple kind denote elements, and letters of the larger and gaudier fonts denote sets containing them. Examples: \(x\) \(\in\) \(A\) , \(A\) \(\in\) \(X\)&nbsp; \(X\) \(\in\) \(\mathbb{C}\)
-
-    After:
-      Whenever possible, however, we shall informally indicate the status of a set in a particular hierarchy under consideration by means of the convention that letters at the beginning of the alphabet denote elements, and letters at the end denote sets containing them; similarly letters of a relatively simple kind denote elements, and letters of the larger and gaudier fonts denote sets containing them. Examples: \(x\) \(\in\) \(A\) , \(A\) \(\in\) \(X\)&nbsp; \(X\) \(\in\) \(\mathbb{C}\)
-
-============================================================
-
-Note ID: 1712057853740
-  Field: Text
-    Before:
-      A possible relation between sets, more elementary than belonging, is equality. The equality of two sets \(A\) and \(B\) is universally denoted by the familiar symbol<br><br><ul><li>\(A\)=\(B\)</li></ul>
-
-    After:
-      A possible relation between sets, more elementary than belonging, is equality. The equality of two sets \(A\) and \(B\) is universally denoted by the familiar symbol<br><br><ul><li>\(A\)=\(B\)</li></ul>
-
-============================================================
-
-Note ID: 1712058050277
-  Field: Text
-    Before:
-      It is valuable to understand that the axiom of extension is not just a logically necessary property of equality but a non-trivial statement about belonging. One way to come to understand the point is to consider a partially analogous situation in which the analogue of the axiom of extension does not hold. Suppose, for instance, that we consider human beings instead of sets, and that, if \(x\) and \(A\) are human beings, we write \(x \in A\) whenever \(x\) is an ancestor of \(A\). (The ancestors of a human being are his parents, his parents' parents, their parents, etc., etc.) The analogue of the axiom of extension would say here that if two human beings are equal, then they have the same ancestors (this is the "only if" part, and it is true), and also that if two human beings have the same ancestors, then they are equal (this is the "if" part, and it is false).
-
-    After:
-      It is valuable to understand that the axiom of extension is not just a logically necessary property of equality but a non-trivial statement about belonging. One way to come to understand the point is to consider a partially analogous situation in which the analogue of the axiom of extension does not hold. Suppose, for instance, that we consider human beings instead of sets, and that, if \(x\) and \(A\) are human beings, we write \(x \in A\) whenever \(x\) is an ancestor of \(A\). (The ancestors of a human being are his parents, his parents' parents, their parents, etc., etc.) The analogue of the axiom of extension would say here that if two human beings are equal, then they have the same ancestors (this is the "only if" part, and it is true), and also that if two human beings have the same ancestors, then they are equal (this is the "if" part, and it is false).
-
-============================================================
-
-Note ID: 1712058179732
-  Field: Text
-    Before:
-      <br><br>The wording of the definition implies that each set must be considered to be included in itself \((A \subset A)\); this fact is described by saying that set inclusion is reflexive. (Note that, in the same sense of the word, equality also is reflexive.)&nbsp;
-
-    After:
-      <br><br>The wording of the definition implies that each set must be considered to be included in itself \((A \subset A)\); this fact is described by saying that set inclusion is reflexive. (Note that, in the same sense of the word, equality also is reflexive.)&nbsp;
-
-============================================================
-
-Note ID: 1712058231154
-  Field: Text
-    Before:
-      If \(A\) and \(B\) are sets such that \(A \subset B\) and \(A \neq B\), the word proper is used (proper subset, proper inclusion).&nbsp;
-
-    After:
-      If \(A\) and \(B\) are sets such that \(A \subset B\) and \(A \neq B\), the word proper is used (proper subset, proper inclusion).&nbsp;
-
-============================================================
-
-Note ID: 1712058409978
-  Field: Text
-    Before:
-      Equality is symmetric, in the sense that if \(A\) = \(B\), then necessarily \(B\) = \(A\).)&nbsp;
-
-    After:
-      Equality is symmetric, in the sense that if \(A\) = \(B\), then necessarily \(B\) = \(A\).)&nbsp;
-
-============================================================
-
-Note ID: 1712058595381
-  Field: Text
-    Before:
-      Observe that belonging \((\epsilon)\)::math::math::math::math and inclusion ( \(\subset\) )::math::math::math::math are conceptually very different things indeed. One important difference has already manifested itself above: inclusion is always reflexive, whereas it is not at all clear that belonging is ever reflexive.
-
-    After:
-      Observe that belonging \((\epsilon)\)::math::math::math::math and inclusion ( \(\subset\) )::math::math::math::math are conceptually very different things indeed. One important difference has already manifested itself above: inclusion is always reflexive, whereas it is not at all clear that belonging is ever reflexive.
-
-============================================================
-
-Note ID: 1712058655572
-  Field: Text
-    Before:
-      That is: \(A \subset A\) is always true; is \(A \epsilon A\) ever true? It is certainly not true of any reasonable set that anyone has ever seen. Observe, along the same lines, that inclusion is transitive, whereas belonging is not. Everyday examples, involving, for instance, super-organizations whose members are organizations, will readily occur to the interested reader.
-
-    After:
-      That is: \(A \subset A\) is always true; is \(A \epsilon A\) ever true? It is certainly not true of any reasonable set that anyone has ever seen. Observe, along the same lines, that inclusion is transitive, whereas belonging is not. Everyday examples, involving, for instance, super-organizations whose members are organizations, will readily occur to the interested reader.
-
-============================================================
-
-Note ID: 1712062422624
-  Field: Text
-    Before:
-      All that is lacking for the precise general formulation that underlies the examples above is a definition of sentence. Here is a quick and informal one. There are two basic types of sentences, namely, assertions of belonging,<br><br>\[<br>x \in A,<br>\]<br><br>and assertions of equality,<br><br>\[<br>A=B<br>\]
-
-    After:
-      All that is lacking for the precise general formulation that underlies the examples above is a definition of sentence. Here is a quick and informal one. There are two basic types of sentences, namely, assertions of belonging,<br><br>\[<br>x \in A,<br>\]<br><br>and assertions of equality,<br><br>\[<br>A=B<br>\]
-
-============================================================
-
-Note ID: 1712063102674
-  Field: Text
-    Before:
-      Axiom of extension. Two sets are equal if and only if they have the same elements.<br>Axiom of specification. To every set \(A\) and to every condition \(S(x)\) there corresponds a set \(B\) whose elements are exactly those elements \(x\) of \(A\) for which \(S(x)\) holds.<br><br>It is an immediate consequence of the axiom of extension that the axiom of specification determines the set \(B\) uniquely. To indicate the way \(B\) is obtained from \(A\) and from&nbsp;\(S(x)\) it is customary to write<br><br><ul><li>\(B\) = \(\{x \in A: S(x)\} .\)<br></li></ul>
-
-    After:
-      Axiom of extension. Two sets are equal if and only if they have the same elements.<br>Axiom of specification. To every set \(A\) and to every condition \(S(x)\) there corresponds a set \(B\) whose elements are exactly those elements \(x\) of \(A\) for which \(S(x)\) holds.<br><br>It is an immediate consequence of the axiom of extension that the axiom of specification determines the set \(B\) uniquely. To indicate the way \(B\) is obtained from \(A\) and from&nbsp;\(S(x)\) it is customary to write<br><br><ul><li>\(B\) = \(\{x \in A: S(x)\} .\)<br></li></ul>
-
-============================================================
-
-Note ID: 1712063626001
-  Field: Text
-    Before:
-      To obtain an amusing and instructive application of the axiom of specification, consider, in the role of \(S(x)\), the sentence<br><br>\[<br>\operatorname{not}(x \in x) \text {. }<br>\]<br><br>It will be convenient, here and throughout, to write " \(x \epsilon^{\prime} A\) " (alternatively " \(x \notin A\) ") instead of "not \((x \in A)\) "; in this notation, the role of \(S(x)\) is now played by<br><br>\[<br>x \epsilon^{\prime} x<br>\]<br><br>It follows that, whatever the set \(A\) may be, if \(B=\left\{x \in A: x \epsilon^{\prime} x\right\}\), then, for all \(y\),<br><br>\[<br>\begin{equation*}<br>y \epsilon B \text { if and only if }\left(y \epsilon A \text { and } y \epsilon^{\prime} y\right) \tag{*}<br>\end{equation*}<br>\]<br><br>Can it be that \(B \epsilon A\) ? We proceed to prove that the answer is no. Indeed, if \(B \epsilon A\), then either \(B \epsilon B\) also (unlikely, but not obviously impossible), or else \(B \epsilon^{\prime} B\). If \(B \epsilon B\), then, by (*), the assumption \(B \epsilon A\) yields \(B \epsilon^{\prime} B-a\) contradiction. If \(B \epsilon^{\prime} B\), then, by (*) again, the assumption \(B \in A\) yields \(B \in B\)-a contradiction again. This completes the proof that \(B \epsilon A\) is impossible, so that we must have \(B \epsilon^{\prime} A\). The most interesting part of this conclusion is that there exists something (namely \(B\) ) that does not belong to \(A\)
-
-    After:
-      To obtain an amusing and instructive application of the axiom of specification, consider, in the role of \(S(x)\), the sentence<br><br>\[<br>\operatorname{not}(x \in x) \text {. }<br>\]<br><br>It will be convenient, here and throughout, to write " \(x \epsilon^{\prime} A\) " (alternatively " \(x \notin A\) ") instead of "not \((x \in A)\) "; in this notation, the role of \(S(x)\) is now played by<br><br>\[<br>x \epsilon^{\prime} x<br>\]<br><br>It follows that, whatever the set \(A\) may be, if \(B=\left\{x \in A: x \epsilon^{\prime} x\right\}\), then, for all \(y\),<br><br>\[<br>\begin{equation*}<br>y \epsilon B \text { if and only if }\left(y \epsilon A \text { and } y \epsilon^{\prime} y\right) \tag{*}<br>\end{equation*}<br>\]<br><br>Can it be that \(B \epsilon A\) ? We proceed to prove that the answer is no. Indeed, if \(B \epsilon A\), then either \(B \epsilon B\) also (unlikely, but not obviously impossible), or else \(B \epsilon^{\prime} B\). If \(B \epsilon B\), then, by (*), the assumption \(B \epsilon A\) yields \(B \epsilon^{\prime} B-a\) contradiction. If \(B \epsilon^{\prime} B\), then, by (*) again, the assumption \(B \in A\) yields \(B \in B\)-a contradiction again. This completes the proof that \(B \epsilon A\) is impossible, so that we must have \(B \epsilon^{\prime} A\). The most interesting part of this conclusion is that there exists something (namely \(B\) ) that does not belong to \(A\)
-
-============================================================
-
-Note ID: 1712063671234
-  Field: Text
-    Before:
-      The set \(A\) in this argument was quite arbitrary. We have proved, in other words, that<br><br><ul><li>nothing contains everything,</li></ul><br>or, more spectacularly,<br><br><ul><li>there is no universe.</li></ul><br>"Universe" here is used in the sense of "universe of discourse," meaning, in any particular discussion, a set that contains all the objects that enter into that discussion<br>
-
-    After:
-      The set \(A\) in this argument was quite arbitrary. We have proved, in other words, that<br><br><ul><li>nothing contains everything,</li></ul><br>or, more spectacularly,<br><br><ul><li>there is no universe.</li></ul><br>"Universe" here is used in the sense of "universe of discourse," meaning, in any particular discussion, a set that contains all the objects that enter into that discussion<br>
-
-============================================================
-
-Note ID: 1712075802119
-  Field: Text
-    Before:
-      To prove that something is true about the empty set, prove that it cannot be false. How, for instance, could it be false that \(\emptyset\) \(\subset A\) ? It could be false only if \(\emptyset\) had an element that did not belong to \(A\). Since \(\emptyset\) has no elements at all, this is absurd. Conclusion: \(\emptyset\) \(\subset A\) is not false, and therefore \(\emptyset\) \(\subset A\)&nbsp;for every \(A\).
-
-    After:
-      To prove that something is true about the empty set, prove that it cannot be false. How, for instance, could it be false that \(\emptyset\) \(\subset A\) ? It could be false only if \(\emptyset\) had an element that did not belong to \(A\). Since \(\emptyset\) has no elements at all, this is absurd. Conclusion: \(\emptyset\) \(\subset A\) is not false, and therefore \(\emptyset\) \(\subset A\)&nbsp;for every \(A\).
-
-============================================================
-
-Note ID: 1712076052700
-  Field: Text
-    Before:
-      Indeed, if \(a\) and \(b\) are sets, and if \(A\) is a set such that \(a \epsilon A\) and \(b \epsilon A\), then we can apply the axiom of specification to \(A\) with the sentence " \(x=a\) or \(x=b\)." The result is the set<br><br>\[<br>\{x \in A: x=a \text { or } x=b\},<br>\]<br><br>and that set, clearly, contains just \(a\) and \(b\). The axiom of extension implies that there can be only one set with this property. The usual symbol for that set is<br><br>\[<br>\{a, b\}<br>\]<br><br>the set is called the pair (or, by way of emphatic comparison with a subsequent concept, the unordered pair) formed by \(a\) and \(b\).
-
-    After:
-      Indeed, if \(a\) and \(b\) are sets, and if \(A\) is a set such that \(a \epsilon A\) and \(b \epsilon A\), then we can apply the axiom of specification to \(A\) with the sentence " \(x=a\) or \(x=b\)." The result is the set<br><br>\[<br>\{x \in A: x=a \text { or } x=b\},<br>\]<br><br>and that set, clearly, contains just \(a\) and \(b\). The axiom of extension implies that there can be only one set with this property. The usual symbol for that set is<br><br>\[<br>\{a, b\}<br>\]<br><br>the set is called the pair (or, by way of emphatic comparison with a subsequent concept, the unordered pair) formed by \(a\) and \(b\).
-
-============================================================
-
-Note ID: 1712127601535
-  Field: Text
-    Before:
-      If, temporarily, we refer to the sentence " \(x=a\) or \(x=b\) " as \(S(x)\), we may express the axiom of pairing by saying that there exists a set \(B\) such that<br><br>\[<br>\begin{equation*}<br>x \in B \text { if and only if } S(x) . \tag{*}<br>\end{equation*}<br>\]<br><br>The axiom of specification, applied to a set \(A\), asserts the existence of a set \(B\) such that<br><br>(**) \(\quad x \in B\) if and only if \((x \in A\) and \(S(x))\).<br><br>The relation between \((*)\) and \((* *)\) typifies something that occurs quite frequently. All the remaining principles of set construction are pseudo-special cases of the axiom of specification in the sense in which \((*)\) is a pseudo-special case of \((* *)\). They all assert the existence of a set specified by a certain condition; if it were known in advance that there exists a set containing all the specified elements, then the existence of a set containing just them would indeed follow as a special case of the axiom of specification.
-
-    After:
-      If, temporarily, we refer to the sentence " \(x=a\) or \(x=b\) " as \(S(x)\), we may express the axiom of pairing by saying that there exists a set \(B\) such that<br><br>\[<br>\begin{equation*}<br>x \in B \text { if and only if } S(x) . \tag{*}<br>\end{equation*}<br>\]<br><br>The axiom of specification, applied to a set \(A\), asserts the existence of a set \(B\) such that<br><br>(**) \(\quad x \in B\) if and only if \((x \in A\) and \(S(x))\).<br><br>The relation between \((*)\) and \((* *)\) typifies something that occurs quite frequently. All the remaining principles of set construction are pseudo-special cases of the axiom of specification in the sense in which \((*)\) is a pseudo-special case of \((* *)\). They all assert the existence of a set specified by a certain condition; if it were known in advance that there exists a set containing all the specified elements, then the existence of a set containing just them would indeed follow as a special case of the axiom of specification.
-
-============================================================
-
-Note ID: 1712127904366
-  Field: Text
-    Before:
-      The axiom of pairing ensures that every set is an element of some set and that any two sets are simultaneously elements of some one and the same set. (The corresponding questions for three and four and more sets will be answered later.) Another pertinent comment is that from the assumptions we have made so far we can infer the existence of very many sets indeed. For examples consider the sets \(\emptyset\), \(\{\emptyset\},\{\{\emptyset\}\},\{\{\{\emptyset\}\}\}\), etc.; consider the pairs, such as \(\{\emptyset,\{\emptyset\}\}\), formed by any two of them; consider the pairs formed by any two such pairs and proceed so on ad infinitum.
-
-    After:
-      The axiom of pairing ensures that every set is an element of some set and that any two sets are simultaneously elements of some one and the same set. (The corresponding questions for three and four and more sets will be answered later.) Another pertinent comment is that from the assumptions we have made so far we can infer the existence of very many sets indeed. For examples consider the sets \(\emptyset\), \(\{\emptyset\},\{\{\emptyset\}\},\{\{\{\emptyset\}\}\}\), etc.; consider the pairs, such as \(\{\emptyset,\{\emptyset\}\}\), formed by any two of them; consider the pairs formed by any two such pairs and proceed so on ad infinitum.
-
-============================================================
-
-Note ID: 1712128238604
-  Field: Text
-    Before:
-      If \(A\) and \(B\) are sets, it is sometimes natural to wish to unite their elements into one comprehensive set. One way of describing such a comprehensive set is to require it to contain all the elements that belong to at least one of the two members of the pair \(\{A, B\}\).&nbsp;
-
-    After:
-      If \(A\) and \(B\) are sets, it is sometimes natural to wish to unite their elements into one comprehensive set. One way of describing such a comprehensive set is to require it to contain all the elements that belong to at least one of the two members of the pair \(\{A, B\}\).&nbsp;
-
-============================================================
-
-Note ID: 1712128932185
-  Field: Text
-    Before:
-      For the time being we restrict our study of the theory of unions to the simplest facts only. The simplest fact of all is that<br><ul><li>\(\bigcup\{X: X \in \emptyset\}\) = \(\emptyset\)</li></ul><br>and the next simplest fact is that<br><ul><li>\(\bigcup\{X: X \epsilon\{A\}\}\) = \(A\)</li></ul><br>In the brutally simple notation mentioned above these facts are expressed by<br><br><ul><li>\(\bigcup \emptyset\) = \(\emptyset\)</li></ul><br>and<br><br><ul><li>\(\bigcup\{A\}\) = \(A\)</li></ul><br>The proofs are immediate from the definitions.<br>
-
-    After:
-      For the time being we restrict our study of the theory of unions to the simplest facts only. The simplest fact of all is that<br><ul><li>\(\bigcup\{X: X \in \emptyset\}\) = \(\emptyset\)</li></ul><br>and the next simplest fact is that<br><ul><li>\(\bigcup\{X: X \epsilon\{A\}\}\) = \(A\)</li></ul><br>In the brutally simple notation mentioned above these facts are expressed by<br><br><ul><li>\(\bigcup \emptyset\) = \(\emptyset\)</li></ul><br>and<br><br><ul><li>\(\bigcup\{A\}\) = \(A\)</li></ul><br>The proofs are immediate from the definitions.<br>
-
-============================================================
-
-Note ID: 1712130691022
-  Field: Text
-    Before:
-      An equally simple but quite suggestive fact is that<br><br><ul><li>\(\{a\} \cup\{b\}\) = \(\{a, b\}\)</li></ul><br>What this suggests is the way to generalize pairs. Specifically, we write<br><br><ul><li>\(\{a, b, c\}\) = \(\{a\} \cup\{b\} \cup\{c\} .\)</li></ul><br>The equation defines its left side. The right side should by rights have at least one pair of parentheses in it, but, in view of the associative law, their omission can lead to no misunderstanding. Since it is easy to prove that<br><br><ul><li>\(\{a, b, c\}\) = \(\{x: x=a \text { or } x=b \text { or } x=c\}\)</li></ul><br>we know now that for every three sets there exists a set that contains them and nothing else; it is natural to call that uniquely determined set the (unordered) triple formed by them. The extension of the notation and terminology thus introduced to more terms (quadruples, etc.) is obvious.<br>
-
-    After:
-      An equally simple but quite suggestive fact is that<br><br><ul><li>\(\{a\} \cup\{b\}\) = \(\{a, b\}\)</li></ul><br>What this suggests is the way to generalize pairs. Specifically, we write<br><br><ul><li>\(\{a, b, c\}\) = \(\{a\} \cup\{b\} \cup\{c\} .\)</li></ul><br>The equation defines its left side. The right side should by rights have at least one pair of parentheses in it, but, in view of the associative law, their omission can lead to no misunderstanding. Since it is easy to prove that<br><br><ul><li>\(\{a, b, c\}\) = \(\{x: x=a \text { or } x=b \text { or } x=c\}\)</li></ul><br>we know now that for every three sets there exists a set that contains them and nothing else; it is natural to call that uniquely determined set the (unordered) triple formed by them. The extension of the notation and terminology thus introduced to more terms (quadruples, etc.) is obvious.<br>
-
-============================================================
-
-Note ID: 1712130844778
-  Field: Text
-    Before:
-      The formation of unions has many points of similarity with another set-theoretic operation. If \(A\) and \(B\) are sets, the intersection of \(A\) and \(B\) is the set<br><br>\[<br>A \cap B<br>\]<br><br>defined by<br><br><ul><li>\(A \cap B\) = \(\{x \in A: x \in B\} .\)</li></ul><br>The definition is symmetric in \(A\) and \(B\) even if it looks otherwise; we have<br><br><ul><li>\(A \cap B\) = \(\{x \in B: x \in A\}\)</li></ul><br>and, in fact, since \(x \in\) \(A \cap B\) if and only if \(x\) belongs to both \(A\) and \(B\), it follows that<br><br><ul><li>\(A \cap B\) = \(\{x: x \in A \text { and } x \in B\} .\)</li></ul>
-
-    After:
-      The formation of unions has many points of similarity with another set-theoretic operation. If \(A\) and \(B\) are sets, the intersection of \(A\) and \(B\) is the set<br><br>\[<br>A \cap B<br>\]<br><br>defined by<br><br><ul><li>\(A \cap B\) = \(\{x \in A: x \in B\} .\)</li></ul><br>The definition is symmetric in \(A\) and \(B\) even if it looks otherwise; we have<br><br><ul><li>\(A \cap B\) = \(\{x \in B: x \in A\}\)</li></ul><br>and, in fact, since \(x \in\) \(A \cap B\) if and only if \(x\) belongs to both \(A\) and \(B\), it follows that<br><br><ul><li>\(A \cap B\) = \(\{x: x \in A \text { and } x \in B\} .\)</li></ul>
-
-============================================================
-
-Note ID: 1712131066919
-  Field: Text
-    Before:
-      \(A\) \(\subset\) \(B\) if and only if \(A \cap B\)= \(A\).
-
-    After:
-      \(A\) \(\subset\) \(B\) if and only if \(A \cap B\)= \(A\).
-
-============================================================
-
-Note ID: 1712131127283
-  Field: Text
-    Before:
-      Pairs of sets with an empty&nbsp;intersection occur frequently enough to justify the use of a special word: if \(A \cap B\) = \(\emptyset\), the sets \(A\) and \(B\) are called disjoint. The same word is sometimes applied to a collection of sets to indicate that any two distinct sets of the collection are disjoint; alternatively we may speak in such a situation of a pairwise disjoint collection.
-
-    After:
-      Pairs of sets with an empty&nbsp;intersection occur frequently enough to justify the use of a special word: if \(A \cap B\) = \(\emptyset\), the sets \(A\) and \(B\) are called disjoint. The same word is sometimes applied to a collection of sets to indicate that any two distinct sets of the collection are disjoint; alternatively we may speak in such a situation of a pairwise disjoint collection.
-
-============================================================
-
-Note ID: 1712131342284
-  Field: Text
-    Before:
-      Two useful facts about unions and intersections involve both the operations at the same time:<br><br>\[<br>\begin{aligned}<br>&amp; A \cap(B \cup C)=(A \cap B) \cup(A \cap C), \\<br>&amp; A \cup(B \cap C)=(A \cup B) \cap(A \cup C) .<br>\end{aligned}<br>\]<br><br>These identities are called the distributive laws.By way of a sample of a settheoretic proof, we prove the second one:<br><ul><li>&nbsp;If \(x\) belongs to the left side, then \(x\) belongs either to \(A\) or to both \(B\) and \(C\);&nbsp;</li><li>if \(x\) is in \(A\), then \(x\) is in both \(A \cup B\) and \(A \cup C\),&nbsp;</li><li>and if \(x\) is in both \(B\) and \(C\), then, again, \(x\) is in both \(A \cup B\) and \(A \cup C\); it follows that, in any case, \(x\) belongs to the right side.&nbsp;</li><li>This proves that the right side includes the left. To prove the reverse inclusion, just observe that if \(x\) belongs to both \(A \cup B\) and \(A \cup C\), then \(x\) belongs either to \(A\) or to both \(B\) and \(C\).</li></ul>
-
-    After:
-      Two useful facts about unions and intersections involve both the operations at the same time:<br><br>\[<br>\begin{aligned}<br>&amp; A \cap(B \cup C)=(A \cap B) \cup(A \cap C), \\<br>&amp; A \cup(B \cap C)=(A \cup B) \cap(A \cup C) .<br>\end{aligned}<br>\]<br><br>These identities are called the distributive laws.By way of a sample of a settheoretic proof, we prove the second one:<br><ul><li>&nbsp;If \(x\) belongs to the left side, then \(x\) belongs either to \(A\) or to both \(B\) and \(C\);&nbsp;</li><li>if \(x\) is in \(A\), then \(x\) is in both \(A \cup B\) and \(A \cup C\),&nbsp;</li><li>and if \(x\) is in both \(B\) and \(C\), then, again, \(x\) is in both \(A \cup B\) and \(A \cup C\); it follows that, in any case, \(x\) belongs to the right side.&nbsp;</li><li>This proves that the right side includes the left. To prove the reverse inclusion, just observe that if \(x\) belongs to both \(A \cup B\) and \(A \cup C\), then \(x\) belongs either to \(A\) or to both \(B\) and \(C\).</li></ul>
-
-============================================================
-
-Note ID: 1712131735267
-  Field: Text
-    Before:
-      Let \(A\) be any particular set in a collection \(\mathfrak{C}\) (this step is justified by the fact that \(\mathfrak{C}\) \(\neq \emptyset\) ) and write<br><br><ul><li>\(V\) = \(\{x \in A: x \in X \text { for every } X \text { in } \mathfrak{C}\} .\)</li></ul><br>(The condition means "for all \(X\) (if \(X \in(\mathbb{\complement}\), then \(x \epsilon X\) ).") The dependence of \(V\) on the arbitrary choice of \(A\) is illusory; in fact<br><br><ul><li>\(V\) = \(\{x: x \in X \text { for every } X \text { in } \mathfrak{C}\} \)</li></ul><br>The set \(V\) is called the intersection of the collection \(\mathfrak{C}\) of sets; the axiom of extension guarantees its uniqueness.&nbsp;<br>
-
-    After:
-      Let \(A\) be any particular set in a collection \(\mathfrak{C}\) (this step is justified by the fact that \(\mathfrak{C}\) \(\neq \emptyset\) ) and write<br><br><ul><li>\(V\) = \(\{x \in A: x \in X \text { for every } X \text { in } \mathfrak{C}\} .\)</li></ul><br>(The condition means "for all \(X\) (if \(X \in(\mathbb{\complement}\), then \(x \epsilon X\) ).") The dependence of \(V\) on the arbitrary choice of \(A\) is illusory; in fact<br><br><ul><li>\(V\) = \(\{x: x \in X \text { for every } X \text { in } \mathfrak{C}\} \)</li></ul><br>The set \(V\) is called the intersection of the collection \(\mathfrak{C}\) of sets; the axiom of extension guarantees its uniqueness.&nbsp;<br>
-
-============================================================
-
-Note ID: 1712137350228
-  Field: Text
-    Before:
-      Exercise 1.17 Let \(p_{1}, p_{2}, \ldots, p_{N}\) be non-negative numbers such that \(p_{1}+p_{2}+\cdots+p_{N}=1\), and let \(\Omega=\left\{\omega_{1}, \omega_{2}, \ldots, \omega_{N}\right\}\), with \(\mathcal{F}\) the power set of \(\Omega\), as in Example 1.16. Show that the function \(\mathbb{Q}\) given by<br><br>\[<br>\mathbb{Q}(A)=\sum_{i: \omega_{i} \in A} p_{i} \quad \text { for } A \in \mathcal{F}<br>\]<br><br>is a probability measure on \((\Omega, \mathcal{F})\). Is \(\mathbb{Q}\) a probability measure on \((\Omega, \mathcal{F})\) if \(\mathcal{F}\) is not the power set of \(\Omega\) but merely some event space of subsets of \(\Omega\) ?<br><br>Condition (a): \(\mathbb{Q}(A) \geq 0\) for \(A \in \mathcal{F}\) because<br>\[<br>\mathbb{Q}(A)=\sum_{i: \omega_i \in A} p_i \geq 0 \quad \text { for } A \in \mathcal{F}<br>\]<br>since \(p_i \geq 0\) for all \(i\). <br><br>Condition \((\mathrm{b}): \mathbb{Q}(\Omega)=1\) because<br>\[<br>\mathbb{Q}(\Omega)=\sum_{i: \omega_i \in \Omega} p_i=\sum_{i=1}^N p_i=1 .<br>\]<br>\(\mathbb{Q}(\varnothing)=0\) by its defintion. <br><br>Condition (c):<br><ul><li>Induction gives the right answer, may not hold in infinite case</li><li>However, the union of countable sets is countable and everything here is countable so I am pretty sure about it</li><li>The answer to the question is yes&nbsp;</li></ul>
-
-    After:
-      Exercise 1.17 Let \(p_{1}, p_{2}, \ldots, p_{N}\) be non-negative numbers such that \(p_{1}+p_{2}+\cdots+p_{N}=1\), and let \(\Omega=\left\{\omega_{1}, \omega_{2}, \ldots, \omega_{N}\right\}\), with \(\mathcal{F}\) the power set of \(\Omega\), as in Example 1.16. Show that the function \(\mathbb{Q}\) given by<br><br>\[<br>\mathbb{Q}(A)=\sum_{i: \omega_{i} \in A} p_{i} \quad \text { for } A \in \mathcal{F}<br>\]<br><br>is a probability measure on \((\Omega, \mathcal{F})\). Is \(\mathbb{Q}\) a probability measure on \((\Omega, \mathcal{F})\) if \(\mathcal{F}\) is not the power set of \(\Omega\) but merely some event space of subsets of \(\Omega\) ?<br><br>Condition (a): \(\mathbb{Q}(A) \geq 0\) for \(A \in \mathcal{F}\) because<br>\[<br>\mathbb{Q}(A)=\sum_{i: \omega_i \in A} p_i \geq 0 \quad \text { for } A \in \mathcal{F}<br>\]<br>since \(p_i \geq 0\) for all \(i\). <br><br>Condition \((\mathrm{b}): \mathbb{Q}(\Omega)=1\) because<br>\[<br>\mathbb{Q}(\Omega)=\sum_{i: \omega_i \in \Omega} p_i=\sum_{i=1}^N p_i=1 .<br>\]<br>\(\mathbb{Q}(\varnothing)=0\) by its defintion. <br><br>Condition (c):<br><ul><li>Induction gives the right answer, may not hold in infinite case</li><li>However, the union of countable sets is countable and everything here is countable so I am pretty sure about it</li><li>The answer to the question is yes&nbsp;</li></ul>
-
-============================================================
-
-Note ID: 1712140293009
-  Field: Text
-    Before:
-      Exercise 1.21 Let \(A, B, C\) be three events such that<br><br>\[<br>\begin{array}{rlrlrl}<br>\mathbb{P}(A) &amp; =\frac{5}{10}, &amp; \mathbb{P}(B) &amp; =\frac{7}{10}, &amp; \mathbb{P}(C) &amp; =\frac{6}{10}, \\<br>\mathbb{P}(A \cap B) &amp; =\frac{3}{10}, &amp; \mathbb{P}(B \cap C) &amp; =\frac{4}{10}, &amp; \mathbb{P}(A \cap C) &amp; =\frac{2}{10}, \\<br>\mathbb{P}(A \cap B \cap C) &amp; =\frac{1}{10} . &amp; &amp;<br>\end{array}<br>\]<br><br>By drawing a Venn diagram or otherwise, find the probability that exactly two of the events \(A, B, C\) occur.<br><br>Solution:<br><ul><li>Venn diagram<img src="paste-f2ca2b5eb3daf5f0878be3a074e64b7d371b437e.jpg"><br></li><li>Thus we need to add the sum of the pairwise intersections minus the shared bit</li><li>The trick is that the interesection of A,B,C is repeated in each pairwise intersection</li><li>Final equaiton:</li><ul><li>\(\mathcal{P}(A\cap B)\) + \(\mathcal{P}(A\cap C)\)&nbsp; + \(\mathcal{P}(B\cap C)\) - 3*\(\mathcal{P}(A\cap B \cap C)\)&nbsp;<br></li></ul></ul>
-
-    After:
-      Exercise 1.21 Let \(A, B, C\) be three events such that<br><br>\[<br>\begin{array}{rlrlrl}<br>\mathbb{P}(A) &amp; =\frac{5}{10}, &amp; \mathbb{P}(B) &amp; =\frac{7}{10}, &amp; \mathbb{P}(C) &amp; =\frac{6}{10}, \\<br>\mathbb{P}(A \cap B) &amp; =\frac{3}{10}, &amp; \mathbb{P}(B \cap C) &amp; =\frac{4}{10}, &amp; \mathbb{P}(A \cap C) &amp; =\frac{2}{10}, \\<br>\mathbb{P}(A \cap B \cap C) &amp; =\frac{1}{10} . &amp; &amp;<br>\end{array}<br>\]<br><br>By drawing a Venn diagram or otherwise, find the probability that exactly two of the events \(A, B, C\) occur.<br><br>Solution:<br><ul><li>Venn diagram<img src="paste-f2ca2b5eb3daf5f0878be3a074e64b7d371b437e.jpg"><br></li><li>Thus we need to add the sum of the pairwise intersections minus the shared bit</li><li>The trick is that the interesection of A,B,C is repeated in each pairwise intersection</li><li>Final equaiton:</li><ul><li>\(\mathcal{P}(A\cap B)\) + \(\mathcal{P}(A\cap C)\)&nbsp; + \(\mathcal{P}(B\cap C)\) - 3*\(\mathcal{P}(A\cap B \cap C)\)&nbsp;<br></li></ul></ul>
-
-============================================================
-
-Note ID: 1712215325154
-  Field: Text
-    Before:
-      Here are some easy exercises on complementation:<br><ul><li>\(A-B\)&nbsp;= \(A \cap B^{\prime} . \)<br></li><li>\(A \subset B\) \(\text { if and only if }\) \(A-B\) = \(\emptyset . \)<br></li><li>\(A-(A-B)\) = \(A \cap B . \)<br></li><li>\(A \cap(B-C)\) = \((A \cap B)-(A \cap C) \)<br></li></ul>
-
-    After:
-      Here are some easy exercises on complementation:<br><ul><li>\(A-B\)&nbsp;= \(A \cap B^{\prime} . \)<br></li><li>\(A \subset B\) \(\text { if and only if }\) \(A-B\) = \(\emptyset . \)<br></li><li>\(A-(A-B)\) = \(A \cap B . \)<br></li><li>\(A \cap(B-C)\) = \((A \cap B)-(A \cap C) \)<br></li></ul>
-
-============================================================
-
-Note ID: 1712215615063
-  Field: Text
-    Before:
-      If \(A\) and \(B\) are sets, the symmetric difference (or Boolean sum) of \(A\) and \(B\) is the set \(A+B\) defined by<br><br><ul><li>\(A+B\) = \((A-B)\) \(\cup\) \((B-A) \text {. }\)</li></ul><br>This operation is commutative and associative, and is such that \(A+\emptyset\) = \(A\) and \(A+A\) = \(\emptyset\).<br>
-
-    After:
-      If \(A\) and \(B\) are sets, the symmetric difference (or Boolean sum) of \(A\) and \(B\) is the set \(A+B\) defined by<br><br><ul><li>\(A+B\) = \((A-B)\) \(\cup\) \((B-A) \text {. }\)</li></ul><br>This operation is commutative and associative, and is such that \(A+\emptyset\) = \(A\) and \(A+A\) = \(\emptyset\).<br>
-
-============================================================
-
-Note ID: 1712215818089
-  Field: Text
-    Before:
-      This may be the right time to straighten out a trivial but occasionally puzzling part of the theory of intersections. Recall, to begin with, that intersections were defined for non-empty collections only. The reason is that the same approach to the empty collection does not define a set. Which \(x\) 's are specified by the sentence<br><br>\[<br>x \in X \text { for every } X \text { in } \emptyset ?<br>\]<br><br>As usual for questions about \(\emptyset\) the answer is easier to see for the corresponding negative question. Which \(x\) 's do not satisfy the stated condition? If it is not true that \(x \in X\) for every \(X\) in \(\emptyset\), then there must exist an \(X\) in \(\emptyset\) such that \(x \epsilon^{\prime} X\); since, however, there do not exist any \(X\) 's in \(\emptyset\) at all, this is absurd. Conclusion: no \(x\) fails to satisfy the stated condition, or, equivalently, every \(x\) does satisfy it. In other words, the \(x\) 's that the condition specifies exhaust the (nonexistent) universe.&nbsp;
-
-    After:
-      This may be the right time to straighten out a trivial but occasionally puzzling part of the theory of intersections. Recall, to begin with, that intersections were defined for non-empty collections only. The reason is that the same approach to the empty collection does not define a set. Which \(x\) 's are specified by the sentence<br><br>\[<br>x \in X \text { for every } X \text { in } \emptyset ?<br>\]<br><br>As usual for questions about \(\emptyset\) the answer is easier to see for the corresponding negative question. Which \(x\) 's do not satisfy the stated condition? If it is not true that \(x \in X\) for every \(X\) in \(\emptyset\), then there must exist an \(X\) in \(\emptyset\) such that \(x \epsilon^{\prime} X\); since, however, there do not exist any \(X\) 's in \(\emptyset\) at all, this is absurd. Conclusion: no \(x\) fails to satisfy the stated condition, or, equivalently, every \(x\) does satisfy it. In other words, the \(x\) 's that the condition specifies exhaust the (nonexistent) universe.&nbsp;
-
-============================================================
-
-Note ID: 1712216109298
-  Field: Text
-    Before:
-      If we restrict our attention to subsets of a particular set \(E\), as we have temporarily agreed to do, then the unpleasantness described in the preceding paragraph appears to go away. The point is that in that case we can define the intersection of a collection ( \(\mathfrak{C}\) (of subsets of \(E\) ) to be the set<br><br>\[<br>\{x \in E: x \in X \text { for every } X \text { in } \mathfrak{C}\} .<br>\]<br><br>This is nothing revolutionary; for each non-empty collection, the new definition agrees with the old one. The difference is in the way the old and the new definitions treat the empty collection; according to the new definition \(\bigcap_{X \in \emptyset} X\) is equal to \(E\). The difference is just a matter of language. A little reflection reveals that the "new" definition offered for the intersection of a collection \(\mathfrak{C}\) of subsets of \(E\) is really the same as the old definition of the intersection of the collection \(\mathfrak{C} \cup\{E\}\), and the latter is never empty.
-
-    After:
-      If we restrict our attention to subsets of a particular set \(E\), as we have temporarily agreed to do, then the unpleasantness described in the preceding paragraph appears to go away. The point is that in that case we can define the intersection of a collection ( \(\mathfrak{C}\) (of subsets of \(E\) ) to be the set<br><br>\[<br>\{x \in E: x \in X \text { for every } X \text { in } \mathfrak{C}\} .<br>\]<br><br>This is nothing revolutionary; for each non-empty collection, the new definition agrees with the old one. The difference is in the way the old and the new definitions treat the empty collection; according to the new definition \(\bigcap_{X \in \emptyset} X\) is equal to \(E\). The difference is just a matter of language. A little reflection reveals that the "new" definition offered for the intersection of a collection \(\mathfrak{C}\) of subsets of \(E\) is really the same as the old definition of the intersection of the collection \(\mathfrak{C} \cup\{E\}\), and the latter is never empty.
-
-============================================================
-
-Note ID: 1712217023844
-  Field: Text
-    Before:
-      If \(\mathfrak{C}\) is a collection of subsets of a set \(E\) (that is, \(\mathfrak{C}\) is a subcollection of \(\mathfrak{P}(E)\) ), then write<br><br>\[<br>\mathfrak{D}=\left\{X \in \mathfrak{P}(E): X^{\prime} \epsilon \mathfrak{C}\right\} .<br>\]&nbsp;<br><br>It is customary to denote the union and the intersection of the collection \(\mathfrak{D}\) of subsets by the symbols:<br><ul><li>\(\bigcup_{X \in \mathbb{C} } X^{\prime}\)<br></li><li>\(\bigcap_{X \in \mathbb{E} } X^{\prime}\).<br></li></ul><div>In this notation the general forms of the De Morgan laws become<br></div><div><ul><li>\(\left(\bigcup_{X \in \mathbb{C} } X\right)^{\prime}\)= \(\bigcap_{X \in \mathbb{C} } X^{\prime}\)<br></li><li>\(\left(\bigcap_{X \in \mathbb{S} } X\right)^{\prime}\) = \(\bigcup_{X \in \mathbb{C} } X^{\prime}\)<br></li></ul></div>
-
-    After:
-      If \(\mathfrak{C}\) is a collection of subsets of a set \(E\) (that is, \(\mathfrak{C}\) is a subcollection of \(\mathfrak{P}(E)\) ), then write<br><br>\[<br>\mathfrak{D}=\left\{X \in \mathfrak{P}(E): X^{\prime} \epsilon \mathfrak{C}\right\} .<br>\]&nbsp;<br><br>It is customary to denote the union and the intersection of the collection \(\mathfrak{D}\) of subsets by the symbols:<br><ul><li>\(\bigcup_{X \in \mathbb{C} } X^{\prime}\)<br></li><li>\(\bigcap_{X \in \mathbb{E} } X^{\prime}\).<br></li></ul><div>In this notation the general forms of the De Morgan laws become<br></div><div><ul><li>\(\left(\bigcup_{X \in \mathbb{C} } X\right)^{\prime}\)= \(\bigcap_{X \in \mathbb{C} } X^{\prime}\)<br></li><li>\(\left(\bigcap_{X \in \mathbb{S} } X\right)^{\prime}\) = \(\bigcup_{X \in \mathbb{C} } X^{\prime}\)<br></li></ul></div>
-
-============================================================
-
-Note ID: 1712332118170
-  Field: Text
-    Before:
-      The ordered pair of \(a\) and \(b\), with first coordinate \(a\) and second coordinate \(b\), is the set \((a, b)\) defined by<br><br><ul><li>\((a, b)\) = \(\{\{a\},\{a, b\}\}\)</li></ul>
-
-    After:
-      The ordered pair of \(a\) and \(b\), with first coordinate \(a\) and second coordinate \(b\), is the set \((a, b)\) defined by<br><br><ul><li>\((a, b)\) = \(\{\{a\},\{a, b\}\}\)</li></ul>
-
-============================================================
-
-Note ID: 1712332592994
-  Field: Text
-    Before:
-      The ordered pair of \(a\) and \(b\), with first coordinate \(a\) and second coordinate \(b\), is the set \((a, b)\) defined by<br><br>\[<br>(a, b)=\{\{a\},\{a, b\}\}<br>\]<br><br>However convincing the motivation of this definition may be, we must still prove that the result has the main property that an ordered pair must have to deserve its name:<br><ul><li>We must show that if \((a, b)\) and \((x, y)\) are ordered pairs and if \((a, b)=(x, y)\), then \(a=x\) and \(b=y\).&nbsp;</li><li>Suppose now that \((a, b)=(x, y)\). If \(a=b\), then both \((a, b)\) and \((x, y)\) are singletons, so that \(x=y\); since \(\{x\} \epsilon(a, b)\) and \(\{a\} \epsilon(x, y)\), it follows that \(a, b, x\), and \(y\) are all equal.&nbsp;</li><li>If \(a \neq b\), then both \((a, b)\) and \((x, y)\) contain exactly one singleton, namely \(\{a\}\) and \(\{x\}\) respectively, so that \(a\) = \(x\). Since in this case it is also true that both \((a, b)\) and \((x, y)\) contain exactly one unordered pair that is not a singleton, namely \(\{a, b\}\) and \(\{x, y\}\) respectively, it follows that \(\{a, b\}\) = \(\{x, y\}\), and therefore, in particular, \(b \in\) \(\{x, y\}\). Since \(b\) cannot be \(x\) (for then we should have \(a\) = \(x\) and \(b\) = \(x\), and, therefore, \(a\) = \(b\) ), we must have \(b\) = \(y\), and the proof is complete.</li></ul>
-
-    After:
-      The ordered pair of \(a\) and \(b\), with first coordinate \(a\) and second coordinate \(b\), is the set \((a, b)\) defined by<br><br>\[<br>(a, b)=\{\{a\},\{a, b\}\}<br>\]<br><br>However convincing the motivation of this definition may be, we must still prove that the result has the main property that an ordered pair must have to deserve its name:<br><ul><li>We must show that if \((a, b)\) and \((x, y)\) are ordered pairs and if \((a, b)=(x, y)\), then \(a=x\) and \(b=y\).&nbsp;</li><li>Suppose now that \((a, b)=(x, y)\). If \(a=b\), then both \((a, b)\) and \((x, y)\) are singletons, so that \(x=y\); since \(\{x\} \epsilon(a, b)\) and \(\{a\} \epsilon(x, y)\), it follows that \(a, b, x\), and \(y\) are all equal.&nbsp;</li><li>If \(a \neq b\), then both \((a, b)\) and \((x, y)\) contain exactly one singleton, namely \(\{a\}\) and \(\{x\}\) respectively, so that \(a\) = \(x\). Since in this case it is also true that both \((a, b)\) and \((x, y)\) contain exactly one unordered pair that is not a singleton, namely \(\{a, b\}\) and \(\{x, y\}\) respectively, it follows that \(\{a, b\}\) = \(\{x, y\}\), and therefore, in particular, \(b \in\) \(\{x, y\}\). Since \(b\) cannot be \(x\) (for then we should have \(a\) = \(x\) and \(b\) = \(x\), and, therefore, \(a\) = \(b\) ), we must have \(b\) = \(y\), and the proof is complete.</li></ul>
-
-============================================================
-
-Note ID: 1712564391736
-  Field: Text
-    Before:
-      <ul><li>If \(A\) and \(B\) are sets, does there exist a set that contains all the ordered pairs \((a, b)\) with \(a\) in \(A\) and \(b\) in \(B\) ?&nbsp;</li><li>It is quite easy to see that the answer is yes. Indeed, if \(a \in A\) and \(b \in B\), then \(\{a\}\) \(\subset A\) and \(\{b\} \)\(\subset B\), and therefore \(\{a, b\}\) \(\subset A \cup B\).&nbsp;</li><li>Since also \(\{a\}\) \(\subset A \cup B\), it follows that both \(\{a\}\) and \(\{a, b\}\) are elements of \(\mathfrak{P}(A \cup B)\).&nbsp;</li><li>This implies that \(\{\{a\},\{a, b\}\}\) is a subset of \(\mathfrak{P}(A \cup B)\), and hence that it is an element of \(\mathfrak{P}(\mathfrak{P}(A \cup B))\);&nbsp;</li><li>in other words \((a, b)\) \(\in\) \(\mathfrak{P}(\mathfrak{P}(A \cup B))\) whenever \(a\) \(\epsilon A\) and \(b\) \(\in B\).</li><li>&nbsp;Once this is known, it is a routine matter to apply the axiom of specification and the axiom of extension to produce the unique set \(A \times B\) that consists exactly of the ordered pairs \((a, b)\) with \(a\) in \(A\) and \(b\) in \(B\). This set is called the Cartesian product</li></ul>
-
-    After:
-      <ul><li>If \(A\) and \(B\) are sets, does there exist a set that contains all the ordered pairs \((a, b)\) with \(a\) in \(A\) and \(b\) in \(B\) ?&nbsp;</li><li>It is quite easy to see that the answer is yes. Indeed, if \(a \in A\) and \(b \in B\), then \(\{a\}\) \(\subset A\) and \(\{b\} \)\(\subset B\), and therefore \(\{a, b\}\) \(\subset A \cup B\).&nbsp;</li><li>Since also \(\{a\}\) \(\subset A \cup B\), it follows that both \(\{a\}\) and \(\{a, b\}\) are elements of \(\mathfrak{P}(A \cup B)\).&nbsp;</li><li>This implies that \(\{\{a\},\{a, b\}\}\) is a subset of \(\mathfrak{P}(A \cup B)\), and hence that it is an element of \(\mathfrak{P}(\mathfrak{P}(A \cup B))\);&nbsp;</li><li>in other words \((a, b)\) \(\in\) \(\mathfrak{P}(\mathfrak{P}(A \cup B))\) whenever \(a\) \(\epsilon A\) and \(b\) \(\in B\).</li><li>&nbsp;Once this is known, it is a routine matter to apply the axiom of specification and the axiom of extension to produce the unique set \(A \times B\) that consists exactly of the ordered pairs \((a, b)\) with \(a\) in \(A\) and \(b\) in \(B\). This set is called the Cartesian product</li></ul>
-
-============================================================
-
-Note ID: 1712566325095
-  Field: Text
-    Before:
-      sets. In other words: if \(R\) is a set such that every element of \(R\) is an ordered pair, then there exist two sets \(A\) and \(B\) such that \(R \subset A \times B\). The proof is elementary. Suppose indeed that \(x \in R\), so that \(x=\{\{a\},\{a, b\}\}\) for some \(a\) and for some \(b\). The problem is to dig out \(a\) and \(b\) from under the braces:<br><ul><li>&nbsp;Since the elements of \(R\) are sets, we can form the union of the sets in \(R\); since \(x\) is one of the sets in \(R\), the elements of \(x\) belong to that union.&nbsp;</li><li>Since \(\{a, b\}\) is one of the elements of \(x\), we may write \(\{a, b\} \) \(\epsilon\)&nbsp; \(\cup R\).&nbsp;</li><li>Form the union of the sets in \(\bigcup R\). Since \(\{a, b\}\) is one of those sets, it&nbsp; follows that the elements of \(\{a, b\}\) belong to that union, and hence both \(a\) and \(b\) belong to \(\bigcup \bigcup R\).&nbsp;</li><li>This fulfills the promise made above; to exhibit \(R\) as a subset of some \(A \times B\), we may take both \(A\) and \(B\) to be \(\bigcup \bigcup R\).&nbsp;</li><li>Then we can make them small by applying the axiom of specification</li></ul>
-
-    After:
-      sets. In other words: if \(R\) is a set such that every element of \(R\) is an ordered pair, then there exist two sets \(A\) and \(B\) such that \(R \subset A \times B\). The proof is elementary. Suppose indeed that \(x \in R\), so that \(x=\{\{a\},\{a, b\}\}\) for some \(a\) and for some \(b\). The problem is to dig out \(a\) and \(b\) from under the braces:<br><ul><li>&nbsp;Since the elements of \(R\) are sets, we can form the union of the sets in \(R\); since \(x\) is one of the sets in \(R\), the elements of \(x\) belong to that union.&nbsp;</li><li>Since \(\{a, b\}\) is one of the elements of \(x\), we may write \(\{a, b\} \) \(\epsilon\)&nbsp; \(\cup R\).&nbsp;</li><li>Form the union of the sets in \(\bigcup R\). Since \(\{a, b\}\) is one of those sets, it&nbsp; follows that the elements of \(\{a, b\}\) belong to that union, and hence both \(a\) and \(b\) belong to \(\bigcup \bigcup R\).&nbsp;</li><li>This fulfills the promise made above; to exhibit \(R\) as a subset of some \(A \times B\), we may take both \(A\) and \(B\) to be \(\bigcup \bigcup R\).&nbsp;</li><li>Then we can make them small by applying the axiom of specification</li></ul>
-
-============================================================
-
-Note ID: 1712566878606
-  Field: Text
-    Before:
-      If either \(A\) = \(\emptyset\) or \(B\) = \(\emptyset\), then \(A \times B\) = \(\emptyset\), and conversely. If \(A\) \(\subset\) \(X\) and \(B \) \(\subset\) \(Y\), then \(A \times B\) \(\subset\) \(X \times Y\), and (provided \(A \times B \neq\) \(\emptyset\) ) conversely.
-
-    After:
-      If either \(A\) = \(\emptyset\) or \(B\) = \(\emptyset\), then \(A \times B\) = \(\emptyset\), and conversely. If \(A\) \(\subset\) \(X\) and \(B \) \(\subset\) \(Y\), then \(A \times B\) \(\subset\) \(X \times Y\), and (provided \(A \times B \neq\) \(\emptyset\) ) conversely.
-
-============================================================
-
-Note ID: 1712596772452
-  Field: Text
-    Before:
-      Exercise 1.44 Show that events \(A\) and \(B\) are independent if and only if \(A\) and \(\Omega \backslash B\) are independent.<br><br>Exercise 1.45 Show that events \(A_{1}, A_{2}, \ldots, A_{m}\) are independent if and only if \(\Omega \backslash A_{1}, \Omega \backslash A_{2}, \ldots\), \(\Omega \backslash A_{m}\) are independent.<br><br>Proof:<br><ul><li>Assume that previous excerise has been shown to be true</li><li>For n sets, you can prove that the intersection of the first n-1 and the n'th being independent means that the intersection of the first n-1 is independent w.r.t the complement of the n'th</li><li>Repeat down or induction up</li></ul>
-
-    After:
-      Exercise 1.44 Show that events \(A\) and \(B\) are independent if and only if \(A\) and \(\Omega \backslash B\) are independent.<br><br>Exercise 1.45 Show that events \(A_{1}, A_{2}, \ldots, A_{m}\) are independent if and only if \(\Omega \backslash A_{1}, \Omega \backslash A_{2}, \ldots\), \(\Omega \backslash A_{m}\) are independent.<br><br>Proof:<br><ul><li>Assume that previous excerise has been shown to be true</li><li>For n sets, you can prove that the intersection of the first n-1 and the n'th being independent means that the intersection of the first n-1 is independent w.r.t the complement of the n'th</li><li>Repeat down or induction up</li></ul>
-
-============================================================
-
-Note ID: 1712697077671
-  Field: Text
-    Before:
-      Example 1.4.4 (Chessboard). How many squares are there in an \(8 \times 8\) chessboard, as in Figure 1.3? Even the name " \(8 \times 8\) chessboard" makes this easy: there are \(8 \cdot 8=64\) squares on the board. The grid structure makes this clear, but we can also think of this as an example of the multiplication rule: to specify a square, we can specify which row and which column it is in. There are 8 choices of row, for each of which there are 8 choices of column.
-
-    After:
-      Example 1.4.4 (Chessboard). How many squares are there in an \(8 \times 8\) chessboard, as in Figure 1.3? Even the name " \(8 \times 8\) chessboard" makes this easy: there are \(8 \cdot 8=64\) squares on the board. The grid structure makes this clear, but we can also think of this as an example of the multiplication rule: to specify a square, we can specify which row and which column it is in. There are 8 choices of row, for each of which there are 8 choices of column.
-
-============================================================
-
-Note ID: 1712727777931
-  Field: Text
-    Before:
-      Theorem 1.4.8 (Sampling without replacement). Consider \(n\) objects and making \(k\) choices from them, one at a time without replacement (i.e., choosing a certain object precludes it from being chosen again). Then there are \(n(n-1) \cdots(n-k+1)\) possible outcomes for \(1 \leq k \leq n\), and 0 possibilities for \(k&gt;n\) (where order matters). By convention, \(n(n-1) \cdots(n-k+1)\) = \(n\) for \(k=1\).
-
-    After:
-      Theorem 1.4.8 (Sampling without replacement). Consider \(n\) objects and making \(k\) choices from them, one at a time without replacement (i.e., choosing a certain object precludes it from being chosen again). Then there are \(n(n-1) \cdots(n-k+1)\) possible outcomes for \(1 \leq k \leq n\), and 0 possibilities for \(k&gt;n\) (where order matters). By convention, \(n(n-1) \cdots(n-k+1)\) = \(n\) for \(k=1\).
-
-============================================================
-
-Note ID: 1712729506288
-  Field: Text
-    Before:
-      Theorem 1.4.15 (Binomial coefficient formula). For \(k \leq n\), we have<br><ul><li>\(\left(\begin{array}{l}n \\ k\end{array}\right)\) =&nbsp;\(\frac{n(n-1) \cdots(n-k+1)}{k !}\) =&nbsp;\(\frac{n !}{(n-k) ! k !}\)<br></li></ul>For&nbsp;\(k &gt; n\), \(\left(\begin{array}{l}n \\ k\end{array}\right)\) = \(0\).<br>
-
-    After:
-      Theorem 1.4.15 (Binomial coefficient formula). For \(k \leq n\), we have<br><ul><li>\(\left(\begin{array}{l}n \\ k\end{array}\right)\) =&nbsp;\(\frac{n(n-1) \cdots(n-k+1)}{k !}\) =&nbsp;\(\frac{n !}{(n-k) ! k !}\)<br></li></ul>For&nbsp;\(k &gt; n\), \(\left(\begin{array}{l}n \\ k\end{array}\right)\) = \(0\).<br>
-
-============================================================
-
-Note ID: 1712731432556
-  Field: Text
-    Before:
-      Example 1.5.1 (Choosing the complement). For any nonnegative integers \(n\) and \(k\) with \(k \leq n\), we have<br><br>\[<br>\left(\begin{array}{l}<br>n \\<br>k<br>\end{array}\right)=\left(\begin{array}{c}<br>n \\<br>n-k<br>\end{array}\right)<br>\]<br><br>This is easy to check algebraically (by writing the binomial coefficients in terms of factorials), but a story proof makes the result easier to understand intuitively.<br><br>Story proof: Consider choosing a committee of size \(k\) in a group of \(n\) people. We know that there are \(\left(\begin{array}{l}n \\ k\end{array}\right)\) possibilities. But another way to choose the committee is to specify which \(n-k\) people are not on the committee; specifying who is on the committee determines who is not on the committee, and vice versa.
-
-    After:
-      Example 1.5.1 (Choosing the complement). For any nonnegative integers \(n\) and \(k\) with \(k \leq n\), we have<br><br>\[<br>\left(\begin{array}{l}<br>n \\<br>k<br>\end{array}\right)=\left(\begin{array}{c}<br>n \\<br>n-k<br>\end{array}\right)<br>\]<br><br>This is easy to check algebraically (by writing the binomial coefficients in terms of factorials), but a story proof makes the result easier to understand intuitively.<br><br>Story proof: Consider choosing a committee of size \(k\) in a group of \(n\) people. We know that there are \(\left(\begin{array}{l}n \\ k\end{array}\right)\) possibilities. But another way to choose the committee is to specify which \(n-k\) people are not on the committee; specifying who is on the committee determines who is not on the committee, and vice versa.
-
-============================================================
-
-Note ID: 1712731729754
-  Field: Text
-    Before:
-      Example 1.5.3 (Vandermonde's identity). A famous relationship between binomial coefficients, called Vandermonde's identity, \({ }^{2}\) says that:<br><ul><li>\(\left(\begin{array}{c}m+n \\ k\end{array}\right)\) =&nbsp;\(\sum_{j=0}^k\) \(\left(\begin{array}{c}m \\ j\end{array}\right)\) \(\left(\begin{array}{c}n \\ k-j\end{array}\right)\).<br></li></ul>
-
-    After:
-      Example 1.5.3 (Vandermonde's identity). A famous relationship between binomial coefficients, called Vandermonde's identity, \({ }^{2}\) says that:<br><ul><li>\(\left(\begin{array}{c}m+n \\ k\end{array}\right)\) =&nbsp;\(\sum_{j=0}^k\) \(\left(\begin{array}{c}m \\ j\end{array}\right)\) \(\left(\begin{array}{c}n \\ k-j\end{array}\right)\).<br></li></ul>
-
-============================================================
-
-Note ID: 1712733984212
-  Field: Text
-    Before:
-      Theorem 1.6.3 (Inclusion-exclusion). For any events \(A_{1}, \ldots, A_{n}\),<br><ul><li>\(P\left(\bigcup_{i=1}^n A_i\right)\) =&nbsp;\(\sum_i P\left(A_i\right)\) -&nbsp;\(\sum_{i&lt;j} P\left(A_i \cap A_j\right)\) +&nbsp;\(\sum_{i&lt;j&lt;k} P\left(A_i \cap A_j \cap A_k\right)\) -&nbsp;\(\ldots\) +&nbsp;\((-1)^{n+1}\) \(P\left(A_1 \cap \cdots \cap A_n\right)\).<br></li></ul>
-
-    After:
-      Theorem 1.6.3 (Inclusion-exclusion). For any events \(A_{1}, \ldots, A_{n}\),<br><ul><li>\(P\left(\bigcup_{i=1}^n A_i\right)\) =&nbsp;\(\sum_i P\left(A_i\right)\) -&nbsp;\(\sum_{i&lt;j} P\left(A_i \cap A_j\right)\) +&nbsp;\(\sum_{i&lt;j&lt;k} P\left(A_i \cap A_j \cap A_k\right)\) -&nbsp;\(\ldots\) +&nbsp;\((-1)^{n+1}\) \(P\left(A_1 \cap \cdots \cap A_n\right)\).<br></li></ul>
-
-============================================================
-
-Note ID: 1712739447054
-  Field: Text
-    Before:
-      Example 1.6.4 (de Montmort's matching problem). Consider a well-shuffled deck of \(n\) cards, labeled 1 through \(n\). You flip over the cards one by one, saying the numbers 1 through \(n\) as you do so. You win the game if, at some point, the number you say aloud is the same as the number on the card being flipped over (for example, if the 7th card in the deck has the label 7). What is the probability of winning?<br><br>In the inclusion-exclusion formula, there are \(n\) terms involving one event, \(\left(\begin{array}{l}n \\ 2\end{array}\right)\) terms involving two events, \(\left(\begin{array}{l}n \\ 3\end{array}\right)\) terms involving three events, and so forth. By the symmetry of the problem, all \(n\) terms of the form \(P\left(A_{i}\right)\) are equal, all \(\left(\begin{array}{l}n \\ 2\end{array}\right)\) terms of the form \(P\left(A_{i} \cap A_{j}\right)\) are equal, and the whole expression simplifies considerably:<br><br>\[<br>\begin{aligned}<br>P\left(\bigcup_{i=1}^{n} A_{i}\right) &amp; =\frac{n}{n}-\frac{\left(\begin{array}{l}<br>n \\<br>2<br>\end{array}\right)}{n(n-1)}+\frac{\left(\begin{array}{l}<br>n \\<br>3<br>\end{array}\right)}{n(n-1)(n-2)}-\cdots+(-1)^{n+1} \cdot \frac{1}{n !} \\<br>&amp; =1-\frac{1}{2 !}+\frac{1}{3 !}-\cdots+(-1)^{n+1} \cdot \frac{1}{n !} .<br>\end{aligned}<br>\]<br><br>Comparing this to the Taylor series for 1/e (see Section A. 8 of the math appendix),<br><br>\[<br>e^{-1}=1-\frac{1}{1 !}+\frac{1}{2 !}-\frac{1}{3 !}+\ldots<br>\]<br><br>we see that for large \(n\), the probability of winning the game is extremely close to \(1-1 / e\), or about 0.63 . Interestingly, as \(n\) grows, the probability of winning approaches \(1-1 / e\) instead of going to 0 or 1 . With a lot of cards in the deck, the number of possible locations for matching cards increases while the probability of any particular match decreases, and these two forces offset each other and balance to give a probability of about \(1-1 / e\).
-
-    After:
-      Example 1.6.4 (de Montmort's matching problem). Consider a well-shuffled deck of \(n\) cards, labeled 1 through \(n\). You flip over the cards one by one, saying the numbers 1 through \(n\) as you do so. You win the game if, at some point, the number you say aloud is the same as the number on the card being flipped over (for example, if the 7th card in the deck has the label 7). What is the probability of winning?<br><br>In the inclusion-exclusion formula, there are \(n\) terms involving one event, \(\left(\begin{array}{l}n \\ 2\end{array}\right)\) terms involving two events, \(\left(\begin{array}{l}n \\ 3\end{array}\right)\) terms involving three events, and so forth. By the symmetry of the problem, all \(n\) terms of the form \(P\left(A_{i}\right)\) are equal, all \(\left(\begin{array}{l}n \\ 2\end{array}\right)\) terms of the form \(P\left(A_{i} \cap A_{j}\right)\) are equal, and the whole expression simplifies considerably:<br><br>\[<br>\begin{aligned}<br>P\left(\bigcup_{i=1}^{n} A_{i}\right) &amp; =\frac{n}{n}-\frac{\left(\begin{array}{l}<br>n \\<br>2<br>\end{array}\right)}{n(n-1)}+\frac{\left(\begin{array}{l}<br>n \\<br>3<br>\end{array}\right)}{n(n-1)(n-2)}-\cdots+(-1)^{n+1} \cdot \frac{1}{n !} \\<br>&amp; =1-\frac{1}{2 !}+\frac{1}{3 !}-\cdots+(-1)^{n+1} \cdot \frac{1}{n !} .<br>\end{aligned}<br>\]<br><br>Comparing this to the Taylor series for 1/e (see Section A. 8 of the math appendix),<br><br>\[<br>e^{-1}=1-\frac{1}{1 !}+\frac{1}{2 !}-\frac{1}{3 !}+\ldots<br>\]<br><br>we see that for large \(n\), the probability of winning the game is extremely close to \(1-1 / e\), or about 0.63 . Interestingly, as \(n\) grows, the probability of winning approaches \(1-1 / e\) instead of going to 0 or 1 . With a lot of cards in the deck, the number of possible locations for matching cards increases while the probability of any particular match decreases, and these two forces offset each other and balance to give a probability of about \(1-1 / e\).
-
-============================================================
-
-Note ID: 1712741063107
-  Field: Text
-    Before:
-      How can one define a logfactorial function in scipy? (log(n!))<br><br><pre><span style="color: var(--jp-mirror-editor-keyword-color); font-weight: bold;">def</span> logfactorial<span style="color: var(--jp-mirror-editor-punctuation-color);">(</span>n<span style="color: var(--jp-mirror-editor-punctuation-color);">):</span>
-    <span style="color: var(--jp-mirror-editor-keyword-color); font-weight: bold;">return</span> gammaln<span style="color: var(--jp-mirror-editor-punctuation-color);">(</span>n<span style="color: var(--jp-mirror-editor-operator-color); font-weight: bold;">+</span><span style="color: var(--jp-mirror-editor-number-color);">1</span><span style="color: var(--jp-mirror-editor-punctuation-color);">)</span></pre>
-
-    After:
-      How can one define a logfactorial function in scipy? (log(n!))<br><br><pre><span style="color: var(--jp-mirror-editor-keyword-color); font-weight: bold;">def</span> logfactorial<span style="color: var(--jp-mirror-editor-punctuation-color);">(</span>n<span style="color: var(--jp-mirror-editor-punctuation-color);">):</span>
-    <span style="color: var(--jp-mirror-editor-keyword-color); font-weight: bold;">return</span> gammaln<span style="color: var(--jp-mirror-editor-punctuation-color);">(</span>n<span style="color: var(--jp-mirror-editor-operator-color); font-weight: bold;">+</span><span style="color: var(--jp-mirror-editor-number-color);">1</span><span style="color: var(--jp-mirror-editor-punctuation-color);">)</span></pre>
-
-============================================================
-
-Note ID: 1712741158688
-  Field: Text
-    Before:
-      How can one define a logchoose (\(\log(comb(n,k))\) ) in scipy?<br><pre><span style="color: var(--jp-mirror-editor-keyword-color); font-weight: bold;">def</span> logfactorial<span style="color: var(--jp-mirror-editor-punctuation-color);">(</span>n<span style="color: var(--jp-mirror-editor-punctuation-color);">):</span>
-    <span style="color: var(--jp-mirror-editor-keyword-color); font-weight: bold;">return</span> gammaln<span style="color: var(--jp-mirror-editor-punctuation-color);">(</span>n<span style="color: var(--jp-mirror-editor-operator-color); font-weight: bold;">+</span><span style="color: var(--jp-mirror-editor-number-color);">1</span><span style="color: var(--jp-mirror-editor-punctuation-color);">)</span>
-
-<span style="color: var(--jp-mirror-editor-keyword-color); font-weight: bold;">def</span> <span style="color: rgb(0, 0, 255);">logchoose</span><span style="color: var(--jp-mirror-editor-punctuation-color);">(</span>n<span style="color: var(--jp-mirror-editor-punctuation-color);">,</span> k<span style="color: var(--jp-mirror-editor-punctuation-color);">):</span>
-    num <span style="color: var(--jp-mirror-editor-operator-color); font-weight: bold;">=</span> logfactorial<span style="color: var(--jp-mirror-editor-punctuation-color);">(</span>n<span style="color: var(--jp-mirror-editor-punctuation-color);">)</span>
-    denom <span style="color: var(--jp-mirror-editor-operator-color); font-weight: bold;">=</span> logfactorial<span style="color: var(--jp-mirror-editor-punctuation-color);">(</span>n<span style="color: var(--jp-mirror-editor-operator-color); font-weight: bold;">-</span>k<span style="color: var(--jp-mirror-editor-punctuation-color);">)</span> <span style="color: var(--jp-mirror-editor-operator-color); font-weight: bold;">+</span> logfactorial<span style="color: var(--jp-mirror-editor-punctuation-color);">(</span>k<span style="color: var(--jp-mirror-editor-punctuation-color);">)</span>
-    <span style="color: var(--jp-mirror-editor-keyword-color); font-weight: bold;">return</span> num <span style="color: var(--jp-mirror-editor-operator-color); font-weight: bold;">-</span> denom</pre>
-
-    After:
-      How can one define a logchoose (\(\log(comb(n,k))\) ) in scipy?<br><pre><span style="color: var(--jp-mirror-editor-keyword-color); font-weight: bold;">def</span> logfactorial<span style="color: var(--jp-mirror-editor-punctuation-color);">(</span>n<span style="color: var(--jp-mirror-editor-punctuation-color);">):</span>
-    <span style="color: var(--jp-mirror-editor-keyword-color); font-weight: bold;">return</span> gammaln<span style="color: var(--jp-mirror-editor-punctuation-color);">(</span>n<span style="color: var(--jp-mirror-editor-operator-color); font-weight: bold;">+</span><span style="color: var(--jp-mirror-editor-number-color);">1</span><span style="color: var(--jp-mirror-editor-punctuation-color);">)</span>
-
-<span style="color: var(--jp-mirror-editor-keyword-color); font-weight: bold;">def</span> <span style="color: rgb(0, 0, 255);">logchoose</span><span style="color: var(--jp-mirror-editor-punctuation-color);">(</span>n<span style="color: var(--jp-mirror-editor-punctuation-color);">,</span> k<span style="color: var(--jp-mirror-editor-punctuation-color);">):</span>
-    num <span style="color: var(--jp-mirror-editor-operator-color); font-weight: bold;">=</span> logfactorial<span style="color: var(--jp-mirror-editor-punctuation-color);">(</span>n<span style="color: var(--jp-mirror-editor-punctuation-color);">)</span>
-    denom <span style="color: var(--jp-mirror-editor-operator-color); font-weight: bold;">=</span> logfactorial<span style="color: var(--jp-mirror-editor-punctuation-color);">(</span>n<span style="color: var(--jp-mirror-editor-operator-color); font-weight: bold;">-</span>k<span style="color: var(--jp-mirror-editor-punctuation-color);">)</span> <span style="color: var(--jp-mirror-editor-operator-color); font-weight: bold;">+</span> logfactorial<span style="color: var(--jp-mirror-editor-punctuation-color);">(</span>k<span style="color: var(--jp-mirror-editor-punctuation-color);">)</span>
-    <span style="color: var(--jp-mirror-editor-keyword-color); font-weight: bold;">return</span> num <span style="color: var(--jp-mirror-editor-operator-color); font-weight: bold;">-</span> denom</pre>
-
-============================================================
-
-Note ID: 1712741674888
-  Field: Text
-    Before:
-      <pre><table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">13</span><span class="p">)</span><br><br><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">365</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="mi">23</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><br><span class="n">u</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table>We can use the above code to see how many unique birthdays, abd their counts exist in a population of 23 people</pre>
-
-    After:
-      <pre><table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">13</span><span class="p">)</span><br><br><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">365</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="mi">23</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><br><span class="n">u</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table>We can use the above code to see how many unique birthdays, abd their counts exist in a population of 23 people</pre>
-
-============================================================
-
-Note ID: 1712779912466
-  Field: Text
-    Before:
-      Explicitly: a set \(R\) is a relation if each element of \(R\) is an ordered pair; this means, of course, that if \(z \in R\), then there exist \(x\) and \(y\) so that \(z\) = \((x, y)\). If \(R\) is a relation, it is sometimes convenient to express the fact that \((x, y) \in R\) by writing<br><br><ul><li>\(x R y\)</li></ul><br>and saying, as in everyday language, that \(x\) stands in the relation \(R\) to \(y\).<br>
-
-    After:
-      Explicitly: a set \(R\) is a relation if each element of \(R\) is an ordered pair; this means, of course, that if \(z \in R\), then there exist \(x\) and \(y\) so that \(z\) = \((x, y)\). If \(R\) is a relation, it is sometimes convenient to express the fact that \((x, y) \in R\) by writing<br><br><ul><li>\(x R y\)</li></ul><br>and saying, as in everyday language, that \(x\) stands in the relation \(R\) to \(y\).<br>
-
-============================================================
-
-Note ID: 1712780104523
-  Field: Text
-    Before:
-      Here is a slightly more interesting example of a relation: let \(X\) be any set, and let \(R\) be the set of all those pairs \((x, y)\) in \(X \times X\) for which \(x\) = \(y\). The relation \(R\) is just the relation of equality between elements of \(X\); if \(x\) and \(y\) are in \(X\), then \(x R y\) means the same as \(x\) = \(y\).&nbsp;
-
-    After:
-      Here is a slightly more interesting example of a relation: let \(X\) be any set, and let \(R\) be the set of all those pairs \((x, y)\) in \(X \times X\) for which \(x\) = \(y\). The relation \(R\) is just the relation of equality between elements of \(X\); if \(x\) and \(y\) are in \(X\), then \(x R y\) means the same as \(x\) = \(y\).&nbsp;
-
-============================================================
-
-Note ID: 1712781293918
-  Field: Text
-    Before:
-      <ul><li>If \(R\) is an equivalence relation in \(X\), and if \(x\) is in \(X\), the equivalence class of \(x\) with respect to \(R\) is the set of all those elements \(y\) in \(X\) for which \(x R y\).&nbsp;<br></li></ul>
-
-    After:
-      <ul><li>If \(R\) is an equivalence relation in \(X\), and if \(x\) is in \(X\), the equivalence class of \(x\) with respect to \(R\) is the set of all those elements \(y\) in \(X\) for which \(x R y\).&nbsp;<br></li></ul>
-
-============================================================
-
-Note ID: 1712781431964
-  Field: Text
-    Before:
-      There is no standard notation for the equivalence class of \(x\) with respect to \(R\); we shall usually denote it by \(x / R\), and we shall write \(X / R\) for the set of all equivalence classes. (Pronounce \(X / R\) as " \(X\) modulo \(R\),"
-
-    After:
-      There is no standard notation for the equivalence class of \(x\) with respect to \(R\); we shall usually denote it by \(x / R\), and we shall write \(X / R\) for the set of all equivalence classes. (Pronounce \(X / R\) as " \(X\) modulo \(R\),"
-
-============================================================
-
-Note ID: 1712781639785
-  Field: Text
-    Before:
-      Now forget \(R\) for a moment and begin anew with a partition \(\mathfrak{C}\) of \(X\). A relation, which we shall call \(X / \mathfrak{C}\), is defined in \(X\) by writing<br><br><ul><li>\(x\) \(\quad X / \mathfrak{C} \quad\) \(y\)</li></ul><br>just in case \(x\) and \(y\) belong to the same set of the collection \(\mathfrak{C}\). We shall call \(X / \mathfrak{C}\) the relation induced by the partition \(\mathfrak{C}\).<br>
-
-    After:
-      Now forget \(R\) for a moment and begin anew with a partition \(\mathfrak{C}\) of \(X\). A relation, which we shall call \(X / \mathfrak{C}\), is defined in \(X\) by writing<br><br><ul><li>\(x\) \(\quad X / \mathfrak{C} \quad\) \(y\)</li></ul><br>just in case \(x\) and \(y\) belong to the same set of the collection \(\mathfrak{C}\). We shall call \(X / \mathfrak{C}\) the relation induced by the partition \(\mathfrak{C}\).<br>
-
-============================================================
-
-Note ID: 1712782054936
-  Field: Text
-    Before:
-      More explicitly: if \(R\) is an equivalence relation in \(X\), then the set of equivalence classes is a partition of \(X\) that induces the relation \(R\), and if \(\mathfrak{C}\) is a partition of \(X\), then the induced relation is an equivalence relation whose set of equivalence classes is exactly \(\mathfrak{C}\).
-
-    After:
-      More explicitly: if \(R\) is an equivalence relation in \(X\), then the set of equivalence classes is a partition of \(X\) that induces the relation \(R\), and if \(\mathfrak{C}\) is a partition of \(X\), then the induced relation is an equivalence relation whose set of equivalence classes is exactly \(\mathfrak{C}\).
-
-============================================================
-
-Note ID: 1712782355500
-  Field: Text
-    Before:
-      The second half is easier. Start with a partition \(\mathfrak{C}\) and consider the induced relation. Since every element of \(X\) belongs to some set of \(\mathfrak{C}\), reflexivity just says that \(x\) and \(x\) are in the same set of \(\mathbb{C}\). Symmetry says that if \(x\) and \(y\) are in the same set of \(\mathfrak{C}\), then \(y\) and \(x\) are in the same set of \(\mathfrak{C}\), and this is obviously true. Transitivity says that if \(x\) and \(y\) are in the same set of \(\mathfrak{C}\) and if \(y\) and \(z\) are in the same set of \(\mathfrak{C}\), then \(x\) and \(z\) are in the same set of \(\mathfrak{C}\), and this too is obvious. The equivalence class of each \(x\) in \(X\) is just the set of \(\mathfrak{C}\) to which \(x\) belongs.&nbsp;
-
-    After:
-      The second half is easier. Start with a partition \(\mathfrak{C}\) and consider the induced relation. Since every element of \(X\) belongs to some set of \(\mathfrak{C}\), reflexivity just says that \(x\) and \(x\) are in the same set of \(\mathbb{C}\). Symmetry says that if \(x\) and \(y\) are in the same set of \(\mathfrak{C}\), then \(y\) and \(x\) are in the same set of \(\mathfrak{C}\), and this is obviously true. Transitivity says that if \(x\) and \(y\) are in the same set of \(\mathfrak{C}\) and if \(y\) and \(z\) are in the same set of \(\mathfrak{C}\), then \(x\) and \(z\) are in the same set of \(\mathfrak{C}\), and this too is obvious. The equivalence class of each \(x\) in \(X\) is just the set of \(\mathfrak{C}\) to which \(x\) belongs.&nbsp;
-
-============================================================
-
-Note ID: 1712864220546
-  Field: Text
-    Before:
-      The domain of a function \(f\) from \(X\) into \(Y\) is, by definition, equal to \(X\), but its range need not be equal to \(Y\); the range consists of those elements \(y\) of \(Y\) for which there exists an \(x\) in \(X\) such that \(f(x)\) = \(y\). If
-
-    After:
-      The domain of a function \(f\) from \(X\) into \(Y\) is, by definition, equal to \(X\), but its range need not be equal to \(Y\); the range consists of those elements \(y\) of \(Y\) for which there exists an \(x\) in \(X\) such that \(f(x)\) = \(y\). If
-
-============================================================
-
-Note ID: 1712865145590
-  Field: Text
-    Before:
-      The definition of restriction can be expressed by writing \((f \mid X)\) (x)=\(f(x)\) for each \(x\) in \(X\); observe also that \(\operatorname{range}\) \((f \mid X)\) = \(f(X)\).
-
-    After:
-      The definition of restriction can be expressed by writing \((f \mid X)\) (x)=\(f(x)\) for each \(x\) in \(X\); observe also that \(\operatorname{range}\) \((f \mid X)\) = \(f(X)\).
-
-============================================================
-
-Note ID: 1712865308316
-  Field: Text
-    Before:
-      Here is a simple but useful example of a function. Consider any two sets \(X\) and \(Y\), and define a function \(f\) from \(X \times Y\) onto \(X\) by writing \(f(x, y)\) = \(x\).&nbsp; The function \(f\) is called the projection from \(X \times Y\) onto \(X\); if, similarly, \(g(x, y)\) = \(y\), then \(g\) is the projection from \(X \times Y\) onto \(Y\)
-
-    After:
-      Here is a simple but useful example of a function. Consider any two sets \(X\) and \(Y\), and define a function \(f\) from \(X \times Y\) onto \(X\) by writing \(f(x, y)\) = \(x\).&nbsp; The function \(f\) is called the projection from \(X \times Y\) onto \(X\); if, similarly, \(g(x, y)\) = \(y\), then \(g\) is the projection from \(X \times Y\) onto \(Y\)
-
-============================================================
-
-Note ID: 1712865758395
-  Field: Text
-    Before:
-      <ul><li>For each element \(y\) of \(Y\), let \(g(y)\) be the set of all those elements \(x\) in \(X\) for which \(f(x)=y\).&nbsp;</li><li>The function \(g\) has the following special property: if \(u\) and \(v\) are distinct elements of \(Y\), then \(g(u)\) and \(g(v)\) are distinct elements of \(X / R\).&nbsp;</li><li>A function that always maps distinct elements onto distinct elements is called one-to-one (usually a one-to-one correspondence).<br></li></ul>
-
-    After:
-      <ul><li>For each element \(y\) of \(Y\), let \(g(y)\) be the set of all those elements \(x\) in \(X\) for which \(f(x)=y\).&nbsp;</li><li>The function \(g\) has the following special property: if \(u\) and \(v\) are distinct elements of \(Y\), then \(g(u)\) and \(g(v)\) are distinct elements of \(X / R\).&nbsp;</li><li>A function that always maps distinct elements onto distinct elements is called one-to-one (usually a one-to-one correspondence).<br></li></ul>
-
-============================================================
-
-Note ID: 1712869662643
-  Field: Text
-    Before:
-      &nbsp;(ii) if \(X\) is not empty, then \(\emptyset^{X}\)::set of functions to X is empty.
-
-    After:
-      &nbsp;(ii) if \(X\) is not empty, then \(\emptyset^{X}\)::set of functions to X is empty.
-
-============================================================
-
-Note ID: 1712902797388
-  Field: Text
-    Before:
-      Example 2.2.7 (A girl born in winter). A family has two children. Find the probability that both children are girls, given that at least one of the two is a girl who was born in winter. In addition to the assumptions from Example 2.2.5, assume that the four seasons are equally likely and that gender is independent of season. (This means that knowing the gender gives no information about the probabilities of the seasons, and vice versa; see Section 2.5 for much more about independence.)<br><br>Solution:<br><ul><li>\(P(\) both girls|at least one winter girl \()\) = \(P(\) both girls, at least one winter girl \()\)<br>\(P(\) at least one winter girl \()\)</li></ul><div>Since the probability that a specific child is a winter-born girl is \(1 / 8\), the denominator equals:<br></div><div><ul><li>\(P(\) at least one winter girl \()\) =&nbsp;&nbsp;\(1-(7 / 8)^2\)<br></li></ul><div>To compute the numerator, use the fact that "both girls, at least one winter girl" is the same event as "both girls, at least one winter child"; then use the assumption that gender and season are independent:</div></div><div><ul><li>\(P(\) both girls, at least one winter girl \()\)&nbsp;<br></li><li>&nbsp;=&nbsp;\(P\) (both girls, at least one winter child)</li><li>=&nbsp;\((1 / 4)(1-P(\) both are non-winter \())\)</li><li>=&nbsp; \((1 / 4)\left(1-(3 / 4)^2\right)\).<br></li></ul><div>Thus:<br><ul><li>\(P(\) both girls|at least one winter girl) =&nbsp;\(\frac{(1 / 4)\left(1-(3 / 4)^2\right)}{1-(7 / 8)^2}\) =&nbsp;\(7 / 15\).<br></li></ul><div>The point is that information about the birth season brings "at least one is a girl" closer to "a specific one is a girl". Conditioning on more and more specific information brings the probability closer and closer to \(1 / 2\).<br></div></div></div>
-
-    After:
-      Example 2.2.7 (A girl born in winter). A family has two children. Find the probability that both children are girls, given that at least one of the two is a girl who was born in winter. In addition to the assumptions from Example 2.2.5, assume that the four seasons are equally likely and that gender is independent of season. (This means that knowing the gender gives no information about the probabilities of the seasons, and vice versa; see Section 2.5 for much more about independence.)<br><br>Solution:<br><ul><li>\(P(\) both girls|at least one winter girl \()\) = \(P(\) both girls, at least one winter girl \()\)<br>\(P(\) at least one winter girl \()\)</li></ul><div>Since the probability that a specific child is a winter-born girl is \(1 / 8\), the denominator equals:<br></div><div><ul><li>\(P(\) at least one winter girl \()\) =&nbsp;&nbsp;\(1-(7 / 8)^2\)<br></li></ul><div>To compute the numerator, use the fact that "both girls, at least one winter girl" is the same event as "both girls, at least one winter child"; then use the assumption that gender and season are independent:</div></div><div><ul><li>\(P(\) both girls, at least one winter girl \()\)&nbsp;<br></li><li>&nbsp;=&nbsp;\(P\) (both girls, at least one winter child)</li><li>=&nbsp;\((1 / 4)(1-P(\) both are non-winter \())\)</li><li>=&nbsp; \((1 / 4)\left(1-(3 / 4)^2\right)\).<br></li></ul><div>Thus:<br><ul><li>\(P(\) both girls|at least one winter girl) =&nbsp;\(\frac{(1 / 4)\left(1-(3 / 4)^2\right)}{1-(7 / 8)^2}\) =&nbsp;\(7 / 15\).<br></li></ul><div>The point is that information about the birth season brings "at least one is a girl" closer to "a specific one is a girl". Conditioning on more and more specific information brings the probability closer and closer to \(1 / 2\).<br></div></div></div>
-
-============================================================
-
-Note ID: 1712903078924
-  Field: Text
-    Before:
-      Theorem 2.3.1 (Probability of the intersection of two events). For any events \(A\) and \(B\) with positive probabilities,<br><br><ul><li>\(P(A \cap B)\) =<br></li><li>=&nbsp;\(P(A \cap B)\)</li><li>=&nbsp;\(P(A) P(B \mid A)\).</li></ul>
-
-    After:
-      Theorem 2.3.1 (Probability of the intersection of two events). For any events \(A\) and \(B\) with positive probabilities,<br><br><ul><li>\(P(A \cap B)\) =<br></li><li>=&nbsp;\(P(A \cap B)\)</li><li>=&nbsp;\(P(A) P(B \mid A)\).</li></ul>
-
-============================================================
-
-Note ID: 1712904373749
-  Field: Text
-    Before:
-      2.3.8 (Prior vs. posterior). It would not be correct in the calculation in the above example to say after the first step, " \(P(A)=1\) because we know \(A\) happened." It is true that \(P(A \mid A)=1\), but \(P(A)\) is the prior probability of \(A\) and \(P(F)\) is the prior probability of \(F\)-both are the probabilities before we observe any data in the&nbsp; experiment. These must not be confused with posterior probabilities conditional on the evidence \(A\).
-
-    After:
-      2.3.8 (Prior vs. posterior). It would not be correct in the calculation in the above example to say after the first step, " \(P(A)=1\) because we know \(A\) happened." It is true that \(P(A \mid A)=1\), but \(P(A)\) is the prior probability of \(A\) and \(P(F)\) is the prior probability of \(F\)-both are the probabilities before we observe any data in the&nbsp; experiment. These must not be confused with posterior probabilities conditional on the evidence \(A\).
-
-============================================================
-
-Note ID: 1712905272571
-  Field: Text
-    Before:
-      2.4.1. When we write \(P(A \mid E)\), it does not mean that \(A \mid E\) is an event and we're taking its probability; \(A \mid E\) is not an event. Rather, \(P(\cdot \mid E)\) is a probability function which assigns probabilities in accordance with the knowledge that \(E\) has occurred, and \(P(\cdot)\) is a different probability function which assigns probabilities without regard for whether \(E\) has occurred or not.
-
-    After:
-      2.4.1. When we write \(P(A \mid E)\), it does not mean that \(A \mid E\) is an event and we're taking its probability; \(A \mid E\) is not an event. Rather, \(P(\cdot \mid E)\) is a probability function which assigns probabilities in accordance with the knowledge that \(E\) has occurred, and \(P(\cdot)\) is a different probability function which assigns probabilities without regard for whether \(E\) has occurred or not.
-
-============================================================
-
-Note ID: 1712990484369
-  Field: Text
-    Before:
-      Conditional probabilities are probabilities, and all probabilities are conditional.
-
-    After:
-      Conditional probabilities are probabilities, and all probabilities are conditional.
-
-============================================================
-
-Note ID: 1712991271668
-  Field: Text
-    Before:
-      <img src="paste-4dee3e4cec81c49d1de42bd1923a7e4a668c825d.jpg"><br>Example 2.4.4 (Random coin, continued). Continuing with the scenario from Example 2.3.7, suppose that we have now seen our chosen coin land Heads three times. If we toss the coin a fourth time, what is the probability that it will land Heads once more?<br><br><br>As before, let \(A\) be the event that the chosen coin lands Heads three times, and define a new event \(H\) for the chosen coin landing Heads on the fourth toss. We are interested in \(P(H \mid A)\). It would be very helpful to know whether we have the fair coin. LOTP with extra conditioning gives us \(P(H \mid A)\) as a weighted average of \(P(H \mid F, A)\) and \(P\left(H \mid F^{c}, A\right)\), and within these two conditional probabilities we \(d o\) know whether we have the fair coin:<br><ul><li>\(P(H \mid A)\) =&nbsp; \(P(H \mid F, A) P(F \mid A)\) + \(P\left(H \mid F^c, A\right) P\left(F^c \mid A\right)\)<br></li></ul><div>The posterior probabilities \(P(F \mid A)\) and \(P\left(F^{c} \mid A\right)\) are from our answer to Example 2.3.7.<br></div>
-
-    After:
-      <img src="paste-4dee3e4cec81c49d1de42bd1923a7e4a668c825d.jpg"><br>Example 2.4.4 (Random coin, continued). Continuing with the scenario from Example 2.3.7, suppose that we have now seen our chosen coin land Heads three times. If we toss the coin a fourth time, what is the probability that it will land Heads once more?<br><br><br>As before, let \(A\) be the event that the chosen coin lands Heads three times, and define a new event \(H\) for the chosen coin landing Heads on the fourth toss. We are interested in \(P(H \mid A)\). It would be very helpful to know whether we have the fair coin. LOTP with extra conditioning gives us \(P(H \mid A)\) as a weighted average of \(P(H \mid F, A)\) and \(P\left(H \mid F^{c}, A\right)\), and within these two conditional probabilities we \(d o\) know whether we have the fair coin:<br><ul><li>\(P(H \mid A)\) =&nbsp; \(P(H \mid F, A) P(F \mid A)\) + \(P\left(H \mid F^c, A\right) P\left(F^c \mid A\right)\)<br></li></ul><div>The posterior probabilities \(P(F \mid A)\) and \(P\left(F^{c} \mid A\right)\) are from our answer to Example 2.3.7.<br></div>
-
-============================================================
-
-Note ID: 1712992952570
-  Field: Text
-    Before:
-      We often want to condition on more than one piece of information, and we now have several ways of doing that. For example, here are some approaches for finding \(P(A \mid B, C)\) :<br><br><ul><li>1. We can think of \(B, C\) as the single event \(B \cap C\) and use the definition of conditional probability to get<br></li><ul><li>\(P(A \mid B, C)\) =&nbsp;\(\frac{P(A, B, C)}{P(B, C)}\).</li><li>This is a natural approach if it's easiest to think about \(B\) and \(C\) in tandem.&nbsp;</li></ul><li>2. We can use Bayes' rule with extra conditioning on \(C\) to get</li><ul><li>\(P(A \mid B, C)\) = \(\frac{P(B \mid A, C) P(A \mid C)}{P(B \mid C)}\).<br></li><li>This is a natural approach if we want to think of everything in our problem as being conditioned on \(C\).<br></li></ul><li>3. We can use Bayes' rule with extra conditioning on \(B\) to get<br></li><ul><li>\(P(A \mid B, C)\) = \(\frac{P(C \mid A, B) P(A \mid B)}{P(C \mid B)}\).<br></li></ul></ul><br>
-
-    After:
-      We often want to condition on more than one piece of information, and we now have several ways of doing that. For example, here are some approaches for finding \(P(A \mid B, C)\) :<br><br><ul><li>1. We can think of \(B, C\) as the single event \(B \cap C\) and use the definition of conditional probability to get<br></li><ul><li>\(P(A \mid B, C)\) =&nbsp;\(\frac{P(A, B, C)}{P(B, C)}\).</li><li>This is a natural approach if it's easiest to think about \(B\) and \(C\) in tandem.&nbsp;</li></ul><li>2. We can use Bayes' rule with extra conditioning on \(C\) to get</li><ul><li>\(P(A \mid B, C)\) = \(\frac{P(B \mid A, C) P(A \mid C)}{P(B \mid C)}\).<br></li><li>This is a natural approach if we want to think of everything in our problem as being conditioned on \(C\).<br></li></ul><li>3. We can use Bayes' rule with extra conditioning on \(B\) to get<br></li><ul><li>\(P(A \mid B, C)\) = \(\frac{P(C \mid A, B) P(A \mid B)}{P(C \mid B)}\).<br></li></ul></ul><br>
-
-============================================================
-
-Note ID: 1713006250073
-  Field: Text
-    Before:
-      2.5.2. Independence is completely different from disjointness. If \(A\) and \(B\) are disjoint, then \(P(A \cap B)\) = \(0\), so disjoint events can be independent only if \(P(A)\) = \(0\) or \(P(B)\) = \(0\). Knowing that \(A\) occurs tells us that \(B\) definitely did not occur, so \(A\) clearly conveys information about \(B\), meaning the two events are not independent (except if \(A\) or \(B\) already has zero probability).
-
-    After:
-      2.5.2. Independence is completely different from disjointness. If \(A\) and \(B\) are disjoint, then \(P(A \cap B)\) = \(0\), so disjoint events can be independent only if \(P(A)\) = \(0\) or \(P(B)\) = \(0\). Knowing that \(A\) occurs tells us that \(B\) definitely did not occur, so \(A\) clearly conveys information about \(B\), meaning the two events are not independent (except if \(A\) or \(B\) already has zero probability).
-
-============================================================
-
-Note ID: 1713006349294
-  Field: Text
-    Before:
-      Intuitively, it makes sense that if \(A\) provides no information about whether or not \(B\) occurred, then it also provides no information about whether or not \(B^{c}\) occurred.
-
-    After:
-      Intuitively, it makes sense that if \(A\) provides no information about whether or not \(B\) occurred, then it also provides no information about whether or not \(B^{c}\) occurred.
-
-============================================================
-
-Note ID: 1713006417449
-  Field: Text
-    Before:
-      Proposition 2.5.3. If \(A\) and \(B\) are independent, then \(A\) and \(B^{c}\) are independent, \(A^{c}\) and \(B\) are independent, and \(A^{c}\) and \(B^{c}\) are independent.
-
-    After:
-      Proposition 2.5.3. If \(A\) and \(B\) are independent, then \(A\) and \(B^{c}\) are independent, \(A^{c}\) and \(B\) are independent, and \(A^{c}\) and \(B^{c}\) are independent.
-
-============================================================
-
-Note ID: 1713006693902
-  Field: Text
-    Before:
-      Example 2.5.5 (Pairwise independence doesn't imply independence). Consider two fair, independent coin tosses, and let \(A\) be the event that the first is Heads, \(B\) the event that the second is Heads, and \(C\) the event that both tosses have the same result. Then \(A, B\), and \(C\) are pairwise independent but not independent, since \(P(A \cap B \cap C)\) = \(1 / 4\) while \(P(A) P(B) P(C)\) = \(1 / 8\). The point is that just knowing about \(A\) or just knowing about \(B\) tells us nothing about \(C\), but knowing what happened with both \(A\) and \(B\) gives us perfect information about \(C\)
-
-    After:
-      Example 2.5.5 (Pairwise independence doesn't imply independence). Consider two fair, independent coin tosses, and let \(A\) be the event that the first is Heads, \(B\) the event that the second is Heads, and \(C\) the event that both tosses have the same result. Then \(A, B\), and \(C\) are pairwise independent but not independent, since \(P(A \cap B \cap C)\) = \(1 / 4\) while \(P(A) P(B) P(C)\) = \(1 / 8\). The point is that just knowing about \(A\) or just knowing about \(B\) tells us nothing about \(C\), but knowing what happened with both \(A\) and \(B\) gives us perfect information about \(C\)
-
-============================================================
-
-Note ID: 1713006760747
-  Field: Text
-    Before:
-      On the other hand, \(P(A \cap B \cap C)\) = \(P(A) P(B) P(C)\) does not imply pairwise independence; this can be seen quickly by looking at the extreme case \(P(A)\) = \(0\), when the equation becomes \(0\) = \(0\), which tells us nothing about \(B\) and \(C\).
-
-    After:
-      On the other hand, \(P(A \cap B \cap C)\) = \(P(A) P(B) P(C)\) does not imply pairwise independence; this can be seen quickly by looking at the extreme case \(P(A)\) = \(0\), when the equation becomes \(0\) = \(0\), which tells us nothing about \(B\) and \(C\).
-
-============================================================
-
-Note ID: 1713006870945
-  Field: Text
-    Before:
-      For infinitely many events, we say that they are independent if every finite subset of the events is independent.
-
-    After:
-      For infinitely many events, we say that they are independent if every finite subset of the events is independent.
-
-============================================================
-
-Note ID: 1713007002074
-  Field: Text
-    Before:
-      2.5.8. It is easy to make terrible blunders stemming from confusing independence and conditional independence. Two events can be conditionally independent given \(E\), but not independent given \(E^{c}\). Two events can be conditionally independent given \(E\), but not independent. Two events can be independent, but not conditionally independent given \(E\).
-
-    After:
-      2.5.8. It is easy to make terrible blunders stemming from confusing independence and conditional independence. Two events can be conditionally independent given \(E\), but not independent given \(E^{c}\). Two events can be conditionally independent given \(E\), but not independent. Two events can be independent, but not conditionally independent given \(E\).
-
-============================================================
-
-Note ID: 1713007151993
-  Field: Text
-    Before:
-      In particular, \(P(A, B)\) = \(P(A) P(B)\) does not imply \(P(A, B \mid E)\) = \(P(A \mid E) P(B \mid E)\); we can't just insert "given \(E\) " everywhere, as we did in going from LOTP to LOTP with extra conditioning. This is because LOTP always holds (it is a consequence of the axioms of probability), whereas \(P(A, B)\) may or may not equal \(P(A) P(B)\), depending on what \(A\) and \(B\) are.
-
-    After:
-      In particular, \(P(A, B)\) = \(P(A) P(B)\) does not imply \(P(A, B \mid E)\) = \(P(A \mid E) P(B \mid E)\); we can't just insert "given \(E\) " everywhere, as we did in going from LOTP to LOTP with extra conditioning. This is because LOTP always holds (it is a consequence of the axioms of probability), whereas \(P(A, B)\) may or may not equal \(P(A) P(B)\), depending on what \(A\) and \(B\) are.
-
-============================================================
-
-Note ID: 1713007223972
-  Field: Text
-    Before:
-      Example 2.5.9 (Conditional independence given \(E\) vs. given \(E^{c}\) ). Suppose there are two types of classes: good classes and bad classes. In good classes, if you work hard, you are very likely to get an A. In bad classes, the professor randomly assigns grades to students regardless of their effort. Let \(G\) be the event that a class is good, \(W\) be the event that you work hard, and \(A\) be the event that you receive an A. Then \(W\) and \(A\) are conditionally independent given \(G^{c}\), but they are not conditionally independent given \(G\).
-
-    After:
-      Example 2.5.9 (Conditional independence given \(E\) vs. given \(E^{c}\) ). Suppose there are two types of classes: good classes and bad classes. In good classes, if you work hard, you are very likely to get an A. In bad classes, the professor randomly assigns grades to students regardless of their effort. Let \(G\) be the event that a class is good, \(W\) be the event that you work hard, and \(A\) be the event that you receive an A. Then \(W\) and \(A\) are conditionally independent given \(G^{c}\), but they are not conditionally independent given \(G\).
-
-============================================================
-
-Note ID: 1713008205426
-  Field: Text
-    Before:
-      Example 2.5.10 (Conditional independence doesn't imply independence). Returning once more to the scenario from Example 2.3.7, suppose we have chosen either a fair coin or a biased coin with probability \(3 / 4\) of Heads, but we do not know which one we have chosen. We flip the coin a number of times. Conditional on choosing the fair coin, the coin tosses are independent,. Similarly, conditional on choosing the biased coin, the tosses are independent
-
-    After:
-      Example 2.5.10 (Conditional independence doesn't imply independence). Returning once more to the scenario from Example 2.3.7, suppose we have chosen either a fair coin or a biased coin with probability \(3 / 4\) of Heads, but we do not know which one we have chosen. We flip the coin a number of times. Conditional on choosing the fair coin, the coin tosses are independent,. Similarly, conditional on choosing the biased coin, the tosses are independent
-
-============================================================
-
-Note ID: 1713011597403
-  Field: Text
-    Before:
-      Exercise 2.8 If \(X\) and \(Y\) are discrete random variables on the probability space \((\Omega, \mathcal{F}, \mathbb{P})\), show that \(U\) and \(V\) are discrete random variables on this space also, where<br><br><ul><li>\(U(\omega)\) = \(X(\omega)\)+\(Y(\omega)\)<br></li><li>\(V(\omega)\) = \(X(\omega)\)\(\times\)\(Y(\omega)\)<br></li><li>for&nbsp;\(\omega \in \Omega\)</li></ul>
-
-    After:
-      Exercise 2.8 If \(X\) and \(Y\) are discrete random variables on the probability space \((\Omega, \mathcal{F}, \mathbb{P})\), show that \(U\) and \(V\) are discrete random variables on this space also, where<br><br><ul><li>\(U(\omega)\) = \(X(\omega)\)+\(Y(\omega)\)<br></li><li>\(V(\omega)\) = \(X(\omega)\)\(\times\)\(Y(\omega)\)<br></li><li>for&nbsp;\(\omega \in \Omega\)</li></ul>
-
-============================================================
-
-Note ID: 1713011761989
-  Field: Text
-    Before:
-      Bernoulli distribution. We say that the discrete random variable \(X\) has the Bernoulli distribution with parameter \(p\) if the image of \(X\) is \(\{0,1\}\), so that \(X\) takes the values 0 and 1 only.
-
-    After:
-      Bernoulli distribution. We say that the discrete random variable \(X\) has the Bernoulli distribution with parameter \(p\) if the image of \(X\) is \(\{0,1\}\), so that \(X\) takes the values 0 and 1 only.
-
-============================================================
-
-Note ID: 1713012321233
-  Field: Text
-    Before:
-      Poisson distribution. We say that \(X\) has the Poisson distribution with parameter \(\lambda(&gt;0)\) if \(X\) takes values in \(\{0,1,2, \ldots\}\) and<br><ul><li>\(\mathbb{P}(X=k)\) = \(\frac{1}{k !}\) \(\lambda^k\) \(e^{-\lambda}\)<br></li><li>for k = 0,1.2..</li></ul>
-
-    After:
-      Poisson distribution. We say that \(X\) has the Poisson distribution with parameter \(\lambda(&gt;0)\) if \(X\) takes values in \(\{0,1,2, \ldots\}\) and<br><ul><li>\(\mathbb{P}(X=k)\) = \(\frac{1}{k !}\) \(\lambda^k\) \(e^{-\lambda}\)<br></li><li>for k = 0,1.2..</li></ul>
-
-============================================================
-
-Note ID: 1713012519880
-  Field: Text
-    Before:
-      Poisson distribution. We say that \(X\) has the Poisson distribution with parameter \(\lambda(&gt;0)\) if \(X\) takes values in \(\{0,1,2, \ldots\}\) and<br><br>\[<br>\begin{equation*}<br>\mathbb{P}(X=k)=\frac{1}{k !} \lambda^{k} e^{-\lambda} \quad \text { for } k=0,1,2, \ldots \tag{2.15}<br>\end{equation*}<br>\]<br><br><ul><li>\(\sum_{k=0}^{\infty}\) \(\frac{1}{k !} \lambda^{k} e^{-\lambda}\)&nbsp;</li><li>= \(e^{-\lambda}\) \(\sum_{k=0}^{\infty}\) \(\frac{1}{k !} \lambda^{k}\)&nbsp;</li><li>= \(e^{-\lambda} e^{\lambda}\)&nbsp;</li><li>= \(1\)</li></ul><br>
-
-    After:
-      Poisson distribution. We say that \(X\) has the Poisson distribution with parameter \(\lambda(&gt;0)\) if \(X\) takes values in \(\{0,1,2, \ldots\}\) and<br><br>\[<br>\begin{equation*}<br>\mathbb{P}(X=k)=\frac{1}{k !} \lambda^{k} e^{-\lambda} \quad \text { for } k=0,1,2, \ldots \tag{2.15}<br>\end{equation*}<br>\]<br><br><ul><li>\(\sum_{k=0}^{\infty}\) \(\frac{1}{k !} \lambda^{k} e^{-\lambda}\)&nbsp;</li><li>= \(e^{-\lambda}\) \(\sum_{k=0}^{\infty}\) \(\frac{1}{k !} \lambda^{k}\)&nbsp;</li><li>= \(e^{-\lambda} e^{\lambda}\)&nbsp;</li><li>= \(1\)</li></ul><br>
-
-============================================================
-
-Note ID: 1713012952825
-  Field: Text
-    Before:
-      Negative binomial distribution. We say that \(X\) has the negative binomial distribution with parameters \(n\) and \(p \in(0,1)\) if \(X\) takes values in \(\{n, n+1, n+2, \ldots\}\) and<br><br>\[<br>\mathbb{P}(X=k)=\left(\begin{array}{l}<br>k-1&nbsp; \tag{2.17}\\<br>n-1<br>\end{array}\right) p^{n} q^{k-n} \quad \text { for } k=n, n+1, n+2, \ldots<br>\]<br><br><ul><li>\(\sum_{k=n}^{\infty}\) \(\left(\begin{array}{l}k-1 \\ n-1\end{array}\right) p^n q^{k-n}\)&nbsp;<br></li><li>=&nbsp;\(p^n\) \(\sum_{l=0}^{\infty}\) \(\left(\begin{array}{c}n+l-1 \\ l\end{array}\right)\) \(q^l\)</li><li>=&nbsp;\(p^n\) \(\sum_{l=0}^{\infty}\) \(\left(\begin{array}{c}-n \\ l\end{array}\right)(-q)^l\)</li><li>= \(p^n\) \((1-q)^{-n}\)&nbsp;</li><li>= 1<br></li></ul>
-
-    After:
-      Negative binomial distribution. We say that \(X\) has the negative binomial distribution with parameters \(n\) and \(p \in(0,1)\) if \(X\) takes values in \(\{n, n+1, n+2, \ldots\}\) and<br><br>\[<br>\mathbb{P}(X=k)=\left(\begin{array}{l}<br>k-1&nbsp; \tag{2.17}\\<br>n-1<br>\end{array}\right) p^{n} q^{k-n} \quad \text { for } k=n, n+1, n+2, \ldots<br>\]<br><br><ul><li>\(\sum_{k=n}^{\infty}\) \(\left(\begin{array}{l}k-1 \\ n-1\end{array}\right) p^n q^{k-n}\)&nbsp;<br></li><li>=&nbsp;\(p^n\) \(\sum_{l=0}^{\infty}\) \(\left(\begin{array}{c}n+l-1 \\ l\end{array}\right)\) \(q^l\)</li><li>=&nbsp;\(p^n\) \(\sum_{l=0}^{\infty}\) \(\left(\begin{array}{c}-n \\ l\end{array}\right)(-q)^l\)</li><li>= \(p^n\) \((1-q)^{-n}\)&nbsp;</li><li>= 1<br></li></ul>
-
-============================================================
-
-Note ID: 1713014020209
-  Field: Text
-    Before:
-      In the binomial distribution&nbsp;If \(n\) is very large and \(p\) is very small but \(n p\) is a 'reasonable size' \((n p\)= \(\lambda\), say) then the distribution of \(S_{n}\) may be approximated by the Poisson distribution with parameter \(\lambda\), as follows. For fixed \(k \geq 0\), write \(p\)= \(\lambda / n\) and suppose that \(n\) is large to find that<br><ul><li>\(\mathbb{P}\left(S_n=k\right)\)&nbsp;</li><li>= \(\left(\begin{array}{l}n \\ k\end{array}\right) p^k(1-p)^{n-k}\)<br></li><li>\(\approx\)&nbsp;\(\frac{n^k}{k !}\left(\frac{\lambda}{n}\right)^k\)&nbsp;\(\left(1-\frac{\lambda}{n}\right)^n\)&nbsp;\(\left(1-\frac{\lambda}{n}\right)^{-k}\)<br></li><li>\(\approx\)&nbsp;\(\frac{1}{k !} \lambda^k e^{-\lambda}\)<br></li></ul>
-
-    After:
-      In the binomial distribution&nbsp;If \(n\) is very large and \(p\) is very small but \(n p\) is a 'reasonable size' \((n p\)= \(\lambda\), say) then the distribution of \(S_{n}\) may be approximated by the Poisson distribution with parameter \(\lambda\), as follows. For fixed \(k \geq 0\), write \(p\)= \(\lambda / n\) and suppose that \(n\) is large to find that<br><ul><li>\(\mathbb{P}\left(S_n=k\right)\)&nbsp;</li><li>= \(\left(\begin{array}{l}n \\ k\end{array}\right) p^k(1-p)^{n-k}\)<br></li><li>\(\approx\)&nbsp;\(\frac{n^k}{k !}\left(\frac{\lambda}{n}\right)^k\)&nbsp;\(\left(1-\frac{\lambda}{n}\right)^n\)&nbsp;\(\left(1-\frac{\lambda}{n}\right)^{-k}\)<br></li><li>\(\approx\)&nbsp;\(\frac{1}{k !} \lambda^k e^{-\lambda}\)<br></li></ul>
-
-============================================================
-
-Note ID: 1713015151229
-  Field: Text
-    Before:
-      Definition 2.27 If \(X\) is a discrete random variable, the expectation of \(X\) is denoted by \(\mathbb{E}(X)\) and defined by<br><ul><li>\(\mathbb{E}(X)\) = \(\sum_{x \in \operatorname{Im} X}\) \(x\) \(\mathbb{P}(X=x)\)<br></li></ul><div>whenever this sum converges absolutely, in that \(\sum_{x}|x \mathbb{P}(X=x)|\) &lt; \(\infty\).<br></div>
-
-    After:
-      Definition 2.27 If \(X\) is a discrete random variable, the expectation of \(X\) is denoted by \(\mathbb{E}(X)\) and defined by<br><ul><li>\(\mathbb{E}(X)\) = \(\sum_{x \in \operatorname{Im} X}\) \(x\) \(\mathbb{P}(X=x)\)<br></li></ul><div>whenever this sum converges absolutely, in that \(\sum_{x}|x \mathbb{P}(X=x)|\) &lt; \(\infty\).<br></div>
-
-============================================================
-
-Note ID: 1713015236752
-  Field: Text
-    Before:
-      \[<br>\begin{equation*}<br>\mathbb{E}(X)=\sum_{x \in \operatorname{Im} X} x \mathbb{P}(X=x) \tag{2.28}<br>\end{equation*}<br>\]<br>This equation is often written<br><ul><li>\(\mathbb{E}(X)\) = \(\sum_{x}\) \(x \mathbb{P}(X=x)\) = \(\sum_{x}\) \(x p_{X}(x)\)<br></li></ul>
-
-    After:
-      \[<br>\begin{equation*}<br>\mathbb{E}(X)=\sum_{x \in \operatorname{Im} X} x \mathbb{P}(X=x) \tag{2.28}<br>\end{equation*}<br>\]<br>This equation is often written<br><ul><li>\(\mathbb{E}(X)\) = \(\sum_{x}\) \(x \mathbb{P}(X=x)\) = \(\sum_{x}\) \(x p_{X}(x)\)<br></li></ul>
-
-============================================================
-
-Note ID: 1713016355856
-  Field: Text
-    Before:
-      Definition 2.32 The variance \(\operatorname{var}(X)\) of a discrete random variable \(X\) is defined by<br><br><ul><li>\(\operatorname{var}(X)\)=\(\mathbb{E}\)\(\left([X-\mathbb{E}(X)]^2\right)\)<br></li></ul>
-
-    After:
-      Definition 2.32 The variance \(\operatorname{var}(X)\) of a discrete random variable \(X\) is defined by<br><br><ul><li>\(\operatorname{var}(X)\)=\(\mathbb{E}\)\(\left([X-\mathbb{E}(X)]^2\right)\)<br></li></ul>
-
-============================================================
-
-Note ID: 1713016523881
-  Field: Text
-    Before:
-      A rough motivation for this definition is as follows. If the dispersion of \(X\) about its expectation is very small, then \(|X-\mu|\) tends to be small, giving that \(\operatorname{var}(X)=\) \(\mathbb{E}\left(|X-\mu|^{2}\right)\) is small also; on the other hand, if there is often a considerable difference between \(X\) and its mean, then \(|X-\mu|\) may be large, giving that \(\operatorname{var}(X)\) is large also.
-
-    After:
-      A rough motivation for this definition is as follows. If the dispersion of \(X\) about its expectation is very small, then \(|X-\mu|\) tends to be small, giving that \(\operatorname{var}(X)=\) \(\mathbb{E}\left(|X-\mu|^{2}\right)\) is small also; on the other hand, if there is often a considerable difference between \(X\) and its mean, then \(|X-\mu|\) may be large, giving that \(\operatorname{var}(X)\) is large also.
-
-============================================================
-
-Note ID: 1713017254742
-  Field: Text
-    Before:
-      Definition 2.40 If \(X\) is a discrete random variable and \(\mathbb{P}(B)\) \(&gt;0\), the conditional expectation of \(X\) given \(B\) is denoted by \(\mathbb{E}(X \mid B)\) and defined by<br><br><ul><li>\(\mathbb{E}(X \mid B)\)=\(\sum_{x \in \operatorname{Im} X}\) \(x \)\(\mathbb{P}(X=x \mid B)\),<br></li></ul><br>whenever this sum converges absolutely.<br>
-
-    After:
-      Definition 2.40 If \(X\) is a discrete random variable and \(\mathbb{P}(B)\) \(&gt;0\), the conditional expectation of \(X\) given \(B\) is denoted by \(\mathbb{E}(X \mid B)\) and defined by<br><br><ul><li>\(\mathbb{E}(X \mid B)\)=\(\sum_{x \in \operatorname{Im} X}\) \(x \)\(\mathbb{P}(X=x \mid B)\),<br></li></ul><br>whenever this sum converges absolutely.<br>
-
-============================================================
-
-Note ID: 1713017969782
-  Field: Text
-    Before:
-      Example 2.7.3 (Gambler's ruin). Two gamblers, A and B, make a sequence of \(\$ 1\) bets. In each bet, gambler \(\mathrm{A}\) has probability \(p\) of winning, and gambler \(\mathrm{B}\) has probability \(q=1-p\) of winning. Gambler A starts with \(i\) dollars and gambler B starts with \(N-i\) dollars; the total wealth between the two remains constant since every time A loses a dollar, the dollar goes to B, and vice versa.<br><br>We can visualize this game as a random walk on the integers between 0 and \(N\), where \(p\) is the probability of going to the right in a given step. The game ends when the random walk reaches 0 or \(N\). What is the probability that A wins the game (walking away with all the money)?<br>Visualisation:<br><img src="paste-c5586c58fe232178adbb6396ee4c9d883d10055e.jpg">
-
-    After:
-      Example 2.7.3 (Gambler's ruin). Two gamblers, A and B, make a sequence of \(\$ 1\) bets. In each bet, gambler \(\mathrm{A}\) has probability \(p\) of winning, and gambler \(\mathrm{B}\) has probability \(q=1-p\) of winning. Gambler A starts with \(i\) dollars and gambler B starts with \(N-i\) dollars; the total wealth between the two remains constant since every time A loses a dollar, the dollar goes to B, and vice versa.<br><br>We can visualize this game as a random walk on the integers between 0 and \(N\), where \(p\) is the probability of going to the right in a given step. The game ends when the random walk reaches 0 or \(N\). What is the probability that A wins the game (walking away with all the money)?<br>Visualisation:<br><img src="paste-c5586c58fe232178adbb6396ee4c9d883d10055e.jpg">
-
-============================================================
-
-Note ID: 1713018728987
-  Field: Text
-    Before:
-      2.8.1 (Prosecutor's fallacy). In 1998, Sally Clark was tried for murder after two of her sons died shortly after birth. During the trial, an expert witness for the prosecution testified that the probability of a newborn dying of sudden infant death syndrome (SIDS) was \(1 / 8500\), so the probability of two deaths due to SIDS in one family was \((1 / 8500)^{2}\), or about one in 73 million. Therefore, he continued, the probability of Clark's innocence was one in 73 million.<br><br>Second, the so-called expert has confused two different conditional probabilities: \(P\) (innocence|evidence) is different from \(P\) (evidence|innocence). The witness claims that the probability of observing two newborn deaths if the defendant were innocent is extremely low; that is, \(P\) (evidence|innocence) is small. What we are interested in<br>however, is \(P\) (innocence|evidence), the probability that the defendant is innocent given all the evidence.&nbsp; So to calculate the conditional probability of innocence given the evidence, we must take into account \(P\) (innocence), the prior probability of innocence.&nbsp;<br><br>The posterior probability of innocence given the evidence depends strongly on both \(P\) (evidence|innocence), which is very low, and \(P\) (innocence), which is very high. The expert's probability of \((1 / 8500)^{2}\), questionable in and of itself, is only part of the equation.
-
-    After:
-      2.8.1 (Prosecutor's fallacy). In 1998, Sally Clark was tried for murder after two of her sons died shortly after birth. During the trial, an expert witness for the prosecution testified that the probability of a newborn dying of sudden infant death syndrome (SIDS) was \(1 / 8500\), so the probability of two deaths due to SIDS in one family was \((1 / 8500)^{2}\), or about one in 73 million. Therefore, he continued, the probability of Clark's innocence was one in 73 million.<br><br>Second, the so-called expert has confused two different conditional probabilities: \(P\) (innocence|evidence) is different from \(P\) (evidence|innocence). The witness claims that the probability of observing two newborn deaths if the defendant were innocent is extremely low; that is, \(P\) (evidence|innocence) is small. What we are interested in<br>however, is \(P\) (innocence|evidence), the probability that the defendant is innocent given all the evidence.&nbsp; So to calculate the conditional probability of innocence given the evidence, we must take into account \(P\) (innocence), the prior probability of innocence.&nbsp;<br><br>The posterior probability of innocence given the evidence depends strongly on both \(P\) (evidence|innocence), which is very low, and \(P\) (innocence), which is very high. The expert's probability of \((1 / 8500)^{2}\), questionable in and of itself, is only part of the equation.
-
-============================================================
-
-Note ID: 1713019438793
-  Field: Text
-    Before:
-      2.8.2 (Defense attorney's fallacy). A woman has been murdered, and her husband is put on trial for this crime. Evidence comes to light that the defendant had a history of abusing his wife. The defense attorney argues that the evidence of abuse should be excluded on grounds of irrelevance, since only 1 in 10,000 men with wives they abuse subsequently murder their wives. Should the judge grant the defense attorney's motion to bar this evidence from trial?<br><br>Let \(A\) be the event that the husband commits abuse against his wife, and let \(G\) be the event that the husband is guilty. The defense's argument is that \(P(G \mid A)\) = \(1 / 10,000\), so guilt is still extremely unlikely conditional on a previous history of abuse.<br><br>However, the defense attorney fails to condition on a crucial fact: in this case, we know that the wife was murdered. Therefore, the relevant probability is not \(P(G \mid A)\), but \(P(G \mid A, M)\), where \(M\) is the event that the wife was murdered.<br><ul><li>\(P(G \mid A, M)\) = \(\frac{P(A \mid G, M) P(G \mid M)}{P(A \mid G, M) P(G \mid M)+P\left(A \mid G^c, M\right) P\left(G^c \mid M\right)}\)<br></li></ul><div>In the above calculation of \(P(G \mid A, M)\), we did not use the defense attorney's \(P(G \mid A)\) number anywhere; it is irrelevant to our calculation because it does not account for the fact that the wife was murdered. We must condition on all the evidence.<br></div>
-
-    After:
-      2.8.2 (Defense attorney's fallacy). A woman has been murdered, and her husband is put on trial for this crime. Evidence comes to light that the defendant had a history of abusing his wife. The defense attorney argues that the evidence of abuse should be excluded on grounds of irrelevance, since only 1 in 10,000 men with wives they abuse subsequently murder their wives. Should the judge grant the defense attorney's motion to bar this evidence from trial?<br><br>Let \(A\) be the event that the husband commits abuse against his wife, and let \(G\) be the event that the husband is guilty. The defense's argument is that \(P(G \mid A)\) = \(1 / 10,000\), so guilt is still extremely unlikely conditional on a previous history of abuse.<br><br>However, the defense attorney fails to condition on a crucial fact: in this case, we know that the wife was murdered. Therefore, the relevant probability is not \(P(G \mid A)\), but \(P(G \mid A, M)\), where \(M\) is the event that the wife was murdered.<br><ul><li>\(P(G \mid A, M)\) = \(\frac{P(A \mid G, M) P(G \mid M)}{P(A \mid G, M) P(G \mid M)+P\left(A \mid G^c, M\right) P\left(G^c \mid M\right)}\)<br></li></ul><div>In the above calculation of \(P(G \mid A, M)\), we did not use the defense attorney's \(P(G \mid A)\) number anywhere; it is irrelevant to our calculation because it does not account for the fact that the wife was murdered. We must condition on all the evidence.<br></div>
-
-============================================================
-
-Note ID: 1713019661604
-  Field: Text
-    Before:
-      Let's use event notation to make this precise. For events \(A, B\), and \(C\), we say that we have a Simpson's paradox if<br><ul><li>\(P(A \mid B, C)\) &lt;&nbsp;\(P\left(A \mid B^c, C\right)\)</li><li>\(P\left(A \mid B, C^c\right)\) &lt;&nbsp;\(P\left(A \mid B^c, C^c\right)\),<br></li></ul><div>but</div><div><ul><li>\(P(A \mid B)\) &gt; \(P\left(A \mid B^c\right)\).<br></li></ul></div>
-
-    After:
-      Let's use event notation to make this precise. For events \(A, B\), and \(C\), we say that we have a Simpson's paradox if<br><ul><li>\(P(A \mid B, C)\) &lt;&nbsp;\(P\left(A \mid B^c, C\right)\)</li><li>\(P\left(A \mid B, C^c\right)\) &lt;&nbsp;\(P\left(A \mid B^c, C^c\right)\),<br></li></ul><div>but</div><div><ul><li>\(P(A \mid B)\) &gt; \(P\left(A \mid B^c\right)\).<br></li></ul></div>
-
-============================================================
-
-Note ID: 1713019850414
-  Field: Text
-    Before:
-      Let's use event notation to make this precise. For events \(A, B\), and \(C\), we say that we have a Simpson's paradox if<br><br>\[<br>\begin{gathered}<br>P(A \mid B, C)&lt;P\left(A \mid B^{c}, C\right) \\<br>P\left(A \mid B, C^{c}\right)&lt;P\left(A \mid B^{c}, C^{c}\right),<br>\end{gathered}<br>\]<br><br>but<br><br>\[<br>P(A \mid B)&gt;P\left(A \mid B^{c}\right)<br>\]<br><br>In this case, let \(A\) be the event of a successful surgery, \(B\) be the event that Dr. Nick is the surgeon, and \(C\) be the event that the surgery is a heart surgery. The conditions for Simpson's paradox are fulfilled because the probability of a successful surgery is lower under Dr. Nick than under Dr. Hibbert whether we condition on heart surgery or on Band-Aid removal, but the overall probability of success is higher for Dr. Nick.<br><br>The law of total probability tells us mathematically why this can happen:<br><ul><li>\(P(A \mid B)\) = \(P(A \mid C, B) P(C \mid B)\)+\(P\left(A \mid C^c, B\right) P\left(C^c \mid B\right)\)<br></li><li>\(P\left(A \mid B^c\right)\) =&nbsp;\(P\left(A \mid C, B^c\right) P\left(C \mid B^c\right)\)+\(P\left(A \mid C^c, B^c\right) P\left(C^c \mid B^c\right)\).<br></li></ul><div>The above equations express \(P(A \mid B)\) as a weighted average of \(P(A \mid C, B)\) and \(P\left(A \mid C^{c}, B\right)\), and \(P\left(A \mid B^{c}\right)\) as a weighted average of \(P\left(A \mid C, B^{c}\right)\) and \(P\left(A \mid C^{c}, B^{c}\right)\). If the corresponding weights were the same in both of these weighted averages, then Simpson's paradox could not occur. But the weights here are different:<br><br>\[<br>P(C \mid B)&lt;P\left(C \mid B^{c}\right) \text { and } P\left(C^{c} \mid B\right)&gt;P\left(C^{c} \mid B^{c}\right) \text {, }<br>\]<br><br>since Dr. Nick is much less likely than Dr. Hibbert to be performing a heart surgery.<br></div>
-
-    After:
-      Let's use event notation to make this precise. For events \(A, B\), and \(C\), we say that we have a Simpson's paradox if<br><br>\[<br>\begin{gathered}<br>P(A \mid B, C)&lt;P\left(A \mid B^{c}, C\right) \\<br>P\left(A \mid B, C^{c}\right)&lt;P\left(A \mid B^{c}, C^{c}\right),<br>\end{gathered}<br>\]<br><br>but<br><br>\[<br>P(A \mid B)&gt;P\left(A \mid B^{c}\right)<br>\]<br><br>In this case, let \(A\) be the event of a successful surgery, \(B\) be the event that Dr. Nick is the surgeon, and \(C\) be the event that the surgery is a heart surgery. The conditions for Simpson's paradox are fulfilled because the probability of a successful surgery is lower under Dr. Nick than under Dr. Hibbert whether we condition on heart surgery or on Band-Aid removal, but the overall probability of success is higher for Dr. Nick.<br><br>The law of total probability tells us mathematically why this can happen:<br><ul><li>\(P(A \mid B)\) = \(P(A \mid C, B) P(C \mid B)\)+\(P\left(A \mid C^c, B\right) P\left(C^c \mid B\right)\)<br></li><li>\(P\left(A \mid B^c\right)\) =&nbsp;\(P\left(A \mid C, B^c\right) P\left(C \mid B^c\right)\)+\(P\left(A \mid C^c, B^c\right) P\left(C^c \mid B^c\right)\).<br></li></ul><div>The above equations express \(P(A \mid B)\) as a weighted average of \(P(A \mid C, B)\) and \(P\left(A \mid C^{c}, B\right)\), and \(P\left(A \mid B^{c}\right)\) as a weighted average of \(P\left(A \mid C, B^{c}\right)\) and \(P\left(A \mid C^{c}, B^{c}\right)\). If the corresponding weights were the same in both of these weighted averages, then Simpson's paradox could not occur. But the weights here are different:<br><br>\[<br>P(C \mid B)&lt;P\left(C \mid B^{c}\right) \text { and } P\left(C^{c} \mid B\right)&gt;P\left(C^{c} \mid B^{c}\right) \text {, }<br>\]<br><br>since Dr. Nick is much less likely than Dr. Hibbert to be performing a heart surgery.<br></div>
-
-============================================================
-
-Note ID: 1713019961058
-  Field: Text
-    Before:
-      Simpson's paradox arises in many real-world contexts. In the following examples, you should try to identify the events \(A, B\), and \(C\) that create the paradox.<br><br>- Gender discrimination in college admissions: In the 1970s, men were significantly more likely than women to be admitted for graduate study at the University of California, Berkeley, leading to charges of gender discrimination. Yet within most individual departments, women were admitted at a higher rate than men. It was found that women tended to apply to the departments with more competitive admissions, while men tended to apply to less competitive departments.
-
-    After:
-      Simpson's paradox arises in many real-world contexts. In the following examples, you should try to identify the events \(A, B\), and \(C\) that create the paradox.<br><br>- Gender discrimination in college admissions: In the 1970s, men were significantly more likely than women to be admitted for graduate study at the University of California, Berkeley, leading to charges of gender discrimination. Yet within most individual departments, women were admitted at a higher rate than men. It was found that women tended to apply to the departments with more competitive admissions, while men tended to apply to less competitive departments.
-
-============================================================
-
-Note ID: 1713020238517
-  Field: Text
-    Before:
-      Figure 2.7 illustrates how probabilities can be updated as new evidence comes in sequentially. Imagine that there is some event \(A\) that we are interested in. On Monday morning, for example, our prior probability for \(A\) is \(P(A)\). If we observe on Monday afternoon that \(B\) occurred, then we can use Bayes' rule (or the definition of conditional probability) to compute the posterior probability \(P(A \mid B)\).<br><br>We use this posterior probability for \(A\) as the new prior on Tuesday morning, and then we continue to collect evidence. Repeat as often as necessary
-
-    After:
-      Figure 2.7 illustrates how probabilities can be updated as new evidence comes in sequentially. Imagine that there is some event \(A\) that we are interested in. On Monday morning, for example, our prior probability for \(A\) is \(P(A)\). If we observe on Monday afternoon that \(B\) occurred, then we can use Bayes' rule (or the definition of conditional probability) to compute the posterior probability \(P(A \mid B)\).<br><br>We use this posterior probability for \(A\) as the new prior on Tuesday morning, and then we continue to collect evidence. Repeat as often as necessary
-
-============================================================
-
-Note ID: 1713021659071
-  Field: Text
-    Before:
-      <ul><li>Suppose, for instance, that \(x\) is a function from a set \(I\) to a set \(X\).</li><li>&nbsp;An element of the domain \(I\) is called an index, \(I\) is called the index set, the range of the function is called an indexed set, the function itself is called a family, and the value of the function \(x\) at an index \(i\), called a term of the family, is denoted by \(x_{i}\).&nbsp;</li></ul>
-
-    After:
-      <ul><li>Suppose, for instance, that \(x\) is a function from a set \(I\) to a set \(X\).</li><li>&nbsp;An element of the domain \(I\) is called an index, \(I\) is called the index set, the range of the function is called an indexed set, the function itself is called a family, and the value of the function \(x\) at an index \(i\), called a term of the family, is denoted by \(x_{i}\).&nbsp;</li></ul>
-
-============================================================
-
-Note ID: 1713021778778
-  Field: Text
-    Before:
-      An unacceptable but generally accepted way of communicating the notation and indicating the emphasis is to speak of a family \(\left\{x_{i}\right\}\) in \(X\), or of a family \(\left\{x_{i}\right\}\) of whatever the elements of \(X\) may be; when necessary, the index set \(I\) is indicated by some such parenthetical expression as \((i \epsilon I)\). Thus, for instance, the phrase "a family \(\left\{A_{i}\right\}\) of subsets of \(X\) " is usually understood to refer to a function \(A\), from some set \(I\) of indices, into \(\mathfrak{P}(X)\).
-
-    After:
-      An unacceptable but generally accepted way of communicating the notation and indicating the emphasis is to speak of a family \(\left\{x_{i}\right\}\) in \(X\), or of a family \(\left\{x_{i}\right\}\) of whatever the elements of \(X\) may be; when necessary, the index set \(I\) is indicated by some such parenthetical expression as \((i \epsilon I)\). Thus, for instance, the phrase "a family \(\left\{A_{i}\right\}\) of subsets of \(X\) " is usually understood to refer to a function \(A\), from some set \(I\) of indices, into \(\mathfrak{P}(X)\).
-
-============================================================
-
-Note ID: 1713021897694
-  Field: Text
-    Before:
-      If \(\left\{A_{i}\right\}\) is a family of subsets of \(X\), the union of the range of the family is called the union of the family \(\left\{A_{i}\right\}\), or the union of the sets \(A_{i}\); the standard notation for it is<br><ul><li>\(\bigcup_{i \in I}\) \(A_{i}\)</li><li>&nbsp;or \(\bigcup_{i}\) \(A_{i}\)</li></ul><br>
-
-    After:
-      If \(\left\{A_{i}\right\}\) is a family of subsets of \(X\), the union of the range of the family is called the union of the family \(\left\{A_{i}\right\}\), or the union of the sets \(A_{i}\); the standard notation for it is<br><ul><li>\(\bigcup_{i \in I}\) \(A_{i}\)</li><li>&nbsp;or \(\bigcup_{i}\) \(A_{i}\)</li></ul><br>
-
-============================================================
-
-Note ID: 1713091074604
-  Field: Text
-    Before:
-      An empty union makes sense (and is empty), but an empty intersection does not make sense.&nbsp;
-
-    After:
-      An empty union makes sense (and is empty), but an empty intersection does not make sense.&nbsp;
-
-============================================================
-
-Note ID: 1713091157079
-  Field: Text
-    Before:
-      Thus, for instance, if \(\left\{A_{i}\right\}\) is a non-empty family of sets, the intersection of the range of the family is called the intersection of the family \(\left\{A_{i}\right\}\), or the intersection of the sets \(A_{i}\); the standard notation for it is<br><br><ul><li>\(\bigcap_{i \in I} \) \(A_{i}\)&nbsp;</li><li>\(\bigcap_{i}\) \(A_{i}\)</li></ul>
-
-    After:
-      Thus, for instance, if \(\left\{A_{i}\right\}\) is a non-empty family of sets, the intersection of the range of the family is called the intersection of the family \(\left\{A_{i}\right\}\), or the intersection of the sets \(A_{i}\); the standard notation for it is<br><br><ul><li>\(\bigcap_{i \in I} \) \(A_{i}\)&nbsp;</li><li>\(\bigcap_{i}\) \(A_{i}\)</li></ul>
-
-============================================================
-
-Note ID: 1713091193451
-  Field: Text
-    Before:
-      (By a "non-empty family" we mean a family whose domain \(I\) is not empty.
-
-    After:
-      (By a "non-empty family" we mean a family whose domain \(I\) is not empty.
-
-============================================================
-
-Note ID: 1713091252517
-  Field: Text
-    Before:
-      It follows immediately from the definition of intersections that if \(I\) \(\neq \emptyset\), then a necessary and sufficient condition that \(x\) belong to \(\bigcap_{i} A_{i}\) is that \(x\) belong to \(A_{i}\) for all \(i\).
-
-    After:
-      It follows immediately from the definition of intersections that if \(I\) \(\neq \emptyset\), then a necessary and sufficient condition that \(x\) belong to \(\bigcap_{i} A_{i}\) is that \(x\) belong to \(A_{i}\) for all \(i\).
-
-============================================================
-
-Note ID: 1713091654935
-  Field: Text
-    Before:
-      The notation of families is the one normally used in generalizing the concept of Cartesian product. The Cartesian product of two sets \(X\) and \(Y\) was defined as the set of all ordered pairs \((x, y)\) with \(x\) in \(X\) and \(y\) in \(Y\). There is a natural one-to-one correspondence between this set and a certain set of families.
-
-    After:
-      The notation of families is the one normally used in generalizing the concept of Cartesian product. The Cartesian product of two sets \(X\) and \(Y\) was defined as the set of all ordered pairs \((x, y)\) with \(x\) in \(X\) and \(y\) in \(Y\). There is a natural one-to-one correspondence between this set and a certain set of families.
-
-============================================================
-
-Note ID: 1713091832564
-  Field: Text
-    Before:
-      The notation of families is the one normally used in generalizing the concept of Cartesian product.&nbsp;<br><br>Consider, indeed, any particular unordered pair \(\{a, b\}\), with \(a \neq b\), and consider the set \(Z\) of all families \(z\), indexed by \(\{a, b\}\), such that \(z_{a}\) \(\in X\) and \(z_{b}\) \(\in Y\). If the function \(f\) from \(Z\) to \(X \times Y\) is defined by \(f(z)\)=\(\left(z_{a}, z_{b}\right)\), then \(f\) is the promised one-to-one correspondence. The difference between \(Z\) and \(X \times Y\) is merely a matter of notation. The generalization of Cartesian products generalizes \(Z\) rather than \(X \times Y\) itself.
-
-    After:
-      The notation of families is the one normally used in generalizing the concept of Cartesian product.&nbsp;<br><br>Consider, indeed, any particular unordered pair \(\{a, b\}\), with \(a \neq b\), and consider the set \(Z\) of all families \(z\), indexed by \(\{a, b\}\), such that \(z_{a}\) \(\in X\) and \(z_{b}\) \(\in Y\). If the function \(f\) from \(Z\) to \(X \times Y\) is defined by \(f(z)\)=\(\left(z_{a}, z_{b}\right)\), then \(f\) is the promised one-to-one correspondence. The difference between \(Z\) and \(X \times Y\) is merely a matter of notation. The generalization of Cartesian products generalizes \(Z\) rather than \(X \times Y\) itself.
-
-============================================================
-
-Note ID: 1713092844067
-  Field: Text
-    Before:
-      It is clear that if every \(X_{i}\) is equal to one and the same set \(X\), then \( \bigtimes_i X_{i}\) =&nbsp; \(X^{I}\). If \(I\) is a pair \(\{a, b\}\), with \(a \neq b\), then it is customary to identify \(\bigtimes_{i \epsilon I} X_{i}\) with the Cartesian product \(X_{a} \times X_{b}\) as defined earlier, and if \(I\) is a singleton \(\{a\}\), then, similarly, we identify \(\bigtimes_{i \epsilon I} X_{i}\) with \(X_{a}\) itself.
-
-    After:
-      It is clear that if every \(X_{i}\) is equal to one and the same set \(X\), then \( \bigtimes_i X_{i}\) =&nbsp; \(X^{I}\). If \(I\) is a pair \(\{a, b\}\), with \(a \neq b\), then it is customary to identify \(\bigtimes_{i \epsilon I} X_{i}\) with the Cartesian product \(X_{a} \times X_{b}\) as defined earlier, and if \(I\) is a singleton \(\{a\}\), then, similarly, we identify \(\bigtimes_{i \epsilon I} X_{i}\) with \(X_{a}\) itself.
-
-============================================================
-
-Note ID: 1713093521556
-  Field: Text
-    Before:
-      Suppose that \(\left\{X_{i}\right\}\) is a family of sets \((i \in I)\) and let \(X\) be its Cartesian product. If \(J\) is a subset of \(I\), then to each element of \(X\) there corresponds in a natural way an element of the partial Cartesian product \(\bigtimes_{i \epsilon J} X_{i}\)::math::math::math::math.&nbsp;
-
-    After:
-      Suppose that \(\left\{X_{i}\right\}\) is a family of sets \((i \in I)\) and let \(X\) be its Cartesian product. If \(J\) is a subset of \(I\), then to each element of \(X\) there corresponds in a natural way an element of the partial Cartesian product \(\bigtimes_{i \epsilon J} X_{i}\)::math::math::math::math.&nbsp;
-
-============================================================
-
-Note ID: 1713094238945
-  Field: Text
-    Before:
-      Prove that:<br><ul><li>&nbsp;\(\left(\bigcup_{i} A_{i}\right)\)\(\times\)\(\left(\bigcup_{j} B_{j}\right)\) = \(\bigcup_{i, j}\left(A_{i} \times B_{j}\right)\)</li><li>&nbsp;\(\left(\bigcap{i} A_{i}\right)\)\(\times\)\(\left(\bigcap{j} B_{j}\right)\) = \(\bigcap{i, j}\left(A_{i} \times B_{j}\right)\) if the domains of the families involved are {{c4<b>va</b>::non-empty}}</li></ul>
-
-    After:
-      Prove that:<br><ul><li>&nbsp;\(\left(\bigcup_{i} A_{i}\right)\)\(\times\)\(\left(\bigcup_{j} B_{j}\right)\) = \(\bigcup_{i, j}\left(A_{i} \times B_{j}\right)\)</li><li>&nbsp;\(\left(\bigcap{i} A_{i}\right)\)\(\times\)\(\left(\bigcap{j} B_{j}\right)\) = \(\bigcap{i, j}\left(A_{i} \times B_{j}\right)\) if the domains of the families involved are {{c4<b>va</b>::non-empty}}</li></ul>
-
-============================================================
-
-Note ID: 1713094355456
-  Field: Text
-    Before:
-      Prove also (with appropriate provisos about empty families) that \(\bigcap_{i} X_{i}\) \(\subset\) \(X_{j}\) \(\subset\) \(\bigcup_{i} X_{i}\) for each index \(j\) and that intersection and union can in fact be characterized as the extreme solutions of these inclusions.&nbsp;
-
-    After:
-      Prove also (with appropriate provisos about empty families) that \(\bigcap_{i} X_{i}\) \(\subset\) \(X_{j}\) \(\subset\) \(\bigcup_{i} X_{i}\) for each index \(j\) and that intersection and union can in fact be characterized as the extreme solutions of these inclusions.&nbsp;
-
-============================================================
-
-Note ID: 1713094420365
-  Field: Text
-    Before:
-      Prove also (with appropriate provisos about empty families) that \(\bigcap_{i} X_{i} \subset X_{j} \subset \bigcup_{i} X_{i}\) for each index \(j\) and that intersection and union can in fact be characterized as the extreme solutions of these inclusions. This means that if \(X_{j}\) \(\subset Y\) for each index \(j\), then \(\bigcup_{i} X_{i}\) \(\subset Y\), and that \(\bigcup_{i} X_{i}\) is the only set satisfying this minimality condition; the formulation for intersections is similar.
-
-    After:
-      Prove also (with appropriate provisos about empty families) that \(\bigcap_{i} X_{i} \subset X_{j} \subset \bigcup_{i} X_{i}\) for each index \(j\) and that intersection and union can in fact be characterized as the extreme solutions of these inclusions. This means that if \(X_{j}\) \(\subset Y\) for each index \(j\), then \(\bigcup_{i} X_{i}\) \(\subset Y\), and that \(\bigcup_{i} X_{i}\) is the only set satisfying this minimality condition; the formulation for intersections is similar.
-
-============================================================
-
-Note ID: 1713096389948
-  Field: Text
-    Before:
-      Given a function \(f\) from \(X\) to \(Y\), let \(f^{-1}\), the inverse of \(f\), be the function from \(\mathfrak{P}(Y)\) to \(\mathfrak{P}(X)\) such that if \(B\) \(\subset Y\)&nbsp;then<br><ul><li>\(f^{-1}\)\((B)\) = \(\{x \in X: f(x) \in B\} .\)<br></li></ul>
-
-    After:
-      Given a function \(f\) from \(X\) to \(Y\), let \(f^{-1}\), the inverse of \(f\), be the function from \(\mathfrak{P}(Y)\) to \(\mathfrak{P}(X)\) such that if \(B\) \(\subset Y\)&nbsp;then<br><ul><li>\(f^{-1}\)\((B)\) = \(\{x \in X: f(x) \in B\} .\)<br></li></ul>
-
-============================================================
-
-Note ID: 1713096421042
-  Field: Text
-    Before:
-      In words: \(f^{-1}(B)\) consists of exactly those elements of \(X\) that \(f\) maps into \(B\); the set \(f^{-1}(B)\) is called the inverse image of \(B\) under \(f\).&nbsp;
-
-    After:
-      In words: \(f^{-1}(B)\) consists of exactly those elements of \(X\) that \(f\) maps into \(B\); the set \(f^{-1}(B)\) is called the inverse image of \(B\) under \(f\).&nbsp;
-
-============================================================
-
-Note ID: 1713096634055
-  Field: Text
-    Before:
-      <ul><li>&nbsp;A necessary and sufficient condition that \(f\) be one-to-one is that the inverse image under \(f\) of each singleton in the range of \(f\) be a singleton in \(X\).</li><li>If the last condition is satisfied, then the symbol \(f^{-1}\) is frequently assigned a second interpretation, namely as the function whose domain is the range of \(f\), and whose value for each \(y\) in the range of \(f\) is the unique \(x\) in \(X\) for which \(f(x)\) = \(y\).<br></li></ul>
-
-    After:
-      <ul><li>&nbsp;A necessary and sufficient condition that \(f\) be one-to-one is that the inverse image under \(f\) of each singleton in the range of \(f\) be a singleton in \(X\).</li><li>If the last condition is satisfied, then the symbol \(f^{-1}\) is frequently assigned a second interpretation, namely as the function whose domain is the range of \(f\), and whose value for each \(y\) in the range of \(f\) is the unique \(x\) in \(X\) for which \(f(x)\) = \(y\).<br></li></ul>
-
-============================================================
-
-Note ID: 1713096809733
-  Field: Text
-    Before:
-      If \(B\) \(\subset\) \(Y\), then<br><br><ul><li>\(f\left(f^{-1}(B)\right)\) \(\subset\) \(B .\)</li></ul>
-
-    After:
-      If \(B\) \(\subset\) \(Y\), then<br><br><ul><li>\(f\left(f^{-1}(B)\right)\) \(\subset\) \(B .\)</li></ul>
-
-============================================================
-
-Note ID: 1713096920433
-  Field: Text
-    Before:
-      If \(B \subset Y\), then<br><br>\[<br>f\left(f^{-1}(B)\right) \subset B .<br>\]<br><br>Proof. If \(y \epsilon\) \(f\left(f^{-1}(B)\right)\), then \(y\)=\(f(x)\) for some \(x\) in \(f^{-1}(B)\); this means that \(y\)=\(f(x)\) and \(f(x)\) \(\epsilon B\), and therefore \(y\) \(\in\) \(B\).
-
-    After:
-      If \(B \subset Y\), then<br><br>\[<br>f\left(f^{-1}(B)\right) \subset B .<br>\]<br><br>Proof. If \(y \epsilon\) \(f\left(f^{-1}(B)\right)\), then \(y\)=\(f(x)\) for some \(x\) in \(f^{-1}(B)\); this means that \(y\)=\(f(x)\) and \(f(x)\) \(\epsilon B\), and therefore \(y\) \(\in\) \(B\).
-
-============================================================
-
-Note ID: 1713098228999
-  Field: Text
-    Before:
-      If \(f\) maps \(X\) onto \(Y\), then<br><br>\[<br>f\left(f^{-1}(B)\right)=B .<br>\]<br><br>Proof. If \(y\) \(\in B\), then \(y\)=\(f(x)\) for some \(x\) in \(X\), and therefore for some \(x\) in \(f^{-1}(B)\); this means that \(y \in\) \(f\left(f^{-1}(B)\right)\).
-
-    After:
-      If \(f\) maps \(X\) onto \(Y\), then<br><br>\[<br>f\left(f^{-1}(B)\right)=B .<br>\]<br><br>Proof. If \(y\) \(\in B\), then \(y\)=\(f(x)\) for some \(x\) in \(X\), and therefore for some \(x\) in \(f^{-1}(B)\); this means that \(y \in\) \(f\left(f^{-1}(B)\right)\).
-
-============================================================
-
-Note ID: 1713098281983
-  Field: Text
-    Before:
-      If \(A\) \(\subset\) \(X\), then<br><br><ul><li>\(A\) \(\subset\) \(f^{-1}(f(A))\)</li></ul>
-
-    After:
-      If \(A\) \(\subset\) \(X\), then<br><br><ul><li>\(A\) \(\subset\) \(f^{-1}(f(A))\)</li></ul>
-
-============================================================
-
-Note ID: 1713098358444
-  Field: Text
-    Before:
-      If \(A \subset X\), then<br><br>\[<br>A \subset f^{-1}(f(A))<br>\]<br>Proof. If \(x \in A\), then \(f(x)\) \(\in\) \(f(A)\); this means that \(x\) \(\in\) \(f^{-1}(f(A))\).
-
-    After:
-      If \(A \subset X\), then<br><br>\[<br>A \subset f^{-1}(f(A))<br>\]<br>Proof. If \(x \in A\), then \(f(x)\) \(\in\) \(f(A)\); this means that \(x\) \(\in\) \(f^{-1}(f(A))\).
-
-============================================================
-
-Note ID: 1713098635472
-  Field: Text
-    Before:
-      If \(f\) is one-to-one, then<br><br>\[<br>A=f^{-1}(f(A)) \text {. }<br>\]<br><br>Proof. If \(x\) \(\in\) \(f^{-1}(f(A))\), then \(f(x)\) \(\epsilon\) \(f(A)\), and therefore \(f(x)\)=\(f(u)\) for some \(u\) in \(A\); this implies that \(x\)=\(u\) and hence that \(x\) \(\in\) \(A\).
-
-    After:
-      If \(f\) is one-to-one, then<br><br>\[<br>A=f^{-1}(f(A)) \text {. }<br>\]<br><br>Proof. If \(x\) \(\in\) \(f^{-1}(f(A))\), then \(f(x)\) \(\epsilon\) \(f(A)\), and therefore \(f(x)\)=\(f(u)\) for some \(u\) in \(A\); this implies that \(x\)=\(u\) and hence that \(x\) \(\in\) \(A\).
-
-============================================================
-
-Note ID: 1713098724012
-  Field: Text
-    Before:
-      The algebraic behavior of \(f^{-1}\) is unexceptionable. If \(\left\{B_{i}\right\}\) is a family of subsets of \(Y\), then<br><ul><li>\(f^{-1}\) \(\left(\bigcup_i B_i\right)\)=\(\bigcup_i f^{-1}\left(B_i\right)\)<br></li><li>\(f^{-1}\) \(\left(\bigcap_i B_i\right)\)=\(\bigcap_i f^{-1}\left(B_i\right)\)<br></li></ul>
-
-    After:
-      The algebraic behavior of \(f^{-1}\) is unexceptionable. If \(\left\{B_{i}\right\}\) is a family of subsets of \(Y\), then<br><ul><li>\(f^{-1}\) \(\left(\bigcup_i B_i\right)\)=\(\bigcup_i f^{-1}\left(B_i\right)\)<br></li><li>\(f^{-1}\) \(\left(\bigcap_i B_i\right)\)=\(\bigcap_i f^{-1}\left(B_i\right)\)<br></li></ul>
-
-============================================================
-
-Note ID: 1713098845792
-  Field: Text
-    Before:
-      &nbsp;If, for instance, \(x\) \(\in\) \(f^{-1}\left(\bigcap_{i} B_{i}\right)\), then \(f(x)\) \(\in\) \(B_{i}\) for all \(i\), so that \(x\) \(\in\) \(f^{-1}\left(B_{i}\right)\) for all \(i\), and therefore \(x\)&nbsp;\(\in\) \(\bigcap_{i} f^{-1}\left(B_{i}\right)\); all the steps in this argument are reversible.&nbsp;
-
-    After:
-      &nbsp;If, for instance, \(x\) \(\in\) \(f^{-1}\left(\bigcap_{i} B_{i}\right)\), then \(f(x)\) \(\in\) \(B_{i}\) for all \(i\), so that \(x\) \(\in\) \(f^{-1}\left(B_{i}\right)\) for all \(i\), and therefore \(x\)&nbsp;\(\in\) \(\bigcap_{i} f^{-1}\left(B_{i}\right)\); all the steps in this argument are reversible.&nbsp;
-
-============================================================
-
-Note ID: 1713099110202
-  Field: Text
-    Before:
-      The formation of inverse images commutes with complementation also; i.e.,<br><br>\[<br>f^{-1}(Y-B)=X-f^{-1}(B)<br>\]<br><br>(Observe that the last equation is indeed a kind of commutative law: it says that complementation followed by inversion is the same as inversion followed by complementation.)
-
-    After:
-      The formation of inverse images commutes with complementation also; i.e.,<br><br>\[<br>f^{-1}(Y-B)=X-f^{-1}(B)<br>\]<br><br>(Observe that the last equation is indeed a kind of commutative law: it says that complementation followed by inversion is the same as inversion followed by complementation.)
-
-============================================================
-
-Note ID: 1713099194694
-  Field: Text
-    Before:
-      If, to be explicit, \(f\) is a function from \(X\) to \(Y\) and \(g\) is a function from \(Y\) to \(Z\), then every element in the range of \(f\) belongs to the domain of \(g\), and, consequently, \(g(f(x))\) makes sense for each \(x\) in \(X\).
-
-    After:
-      If, to be explicit, \(f\) is a function from \(X\) to \(Y\) and \(g\) is a function from \(Y\) to \(Z\), then every element in the range of \(f\) belongs to the domain of \(g\), and, consequently, \(g(f(x))\) makes sense for each \(x\) in \(X\).
-
-============================================================
-
-Note ID: 1713099935978
-  Field: Text
-    Before:
-      Observe that the order of events is important in the theory of functional composition. In order that \(g f\) be defined, the range of \(f\) must be included in the domain of \(g\), and this can happen without it necessarily happening in the other direction at the same time. Even if both \(f g\) and \(g f\) are defined, which happens if, for instance, \(f\) maps \(X\) into \(Y\) and \(g\) maps \(Y\) into \(X\), the functions \(f g\) and \(g f\) need not be the same; in other words, functional composition is not necessarily commutative.
-
-    After:
-      Observe that the order of events is important in the theory of functional composition. In order that \(g f\) be defined, the range of \(f\) must be included in the domain of \(g\), and this can happen without it necessarily happening in the other direction at the same time. Even if both \(f g\) and \(g f\) are defined, which happens if, for instance, \(f\) maps \(X\) into \(Y\) and \(g\) maps \(Y\) into \(X\), the functions \(f g\) and \(g f\) need not be the same; in other words, functional composition is not necessarily commutative.
-
-============================================================
-
-Note ID: 1713100037337
-  Field: Text
-    Before:
-      Functional composition may not be commutative, but it is always associative. If \(f\) maps \(X\) into \(Y\), if \(g\) maps \(Y\) into \(Z\), and if \(h\) maps \(Z\) into \(U\), then we can form the composite of \(h\) with \(g f\) and the composite of \(h g\) with \(f\);&nbsp;
-
-    After:
-      Functional composition may not be commutative, but it is always associative. If \(f\) maps \(X\) into \(Y\), if \(g\) maps \(Y\) into \(Z\), and if \(h\) maps \(Z\) into \(U\), then we can form the composite of \(h\) with \(g f\) and the composite of \(h g\) with \(f\);&nbsp;
-
-============================================================
-
-Note ID: 1713100353899
-  Field: Text
-    Before:
-      If \(f\) maps \(X\) into \(Y\) and \(g\) maps \(Y\) into \(Z\), then \(f^{-1}\) maps \(\mathfrak{P}(Y)\) into \(\mathfrak{P}(X)\) and \(g^{-1}\) maps \(\mathfrak{P}(Z)\) into \(\mathfrak{P}(Y)\). In this situation, the composites that are formable are \(g f\) and \(f^{-1} g^{-1}\); the assertion is that the latter is the inverse of the former. <br><br>Proof: if \(x \epsilon\) \((g f)^{-1}(C)\), where \(x \epsilon\) \(X\) and \(C\) \(\subset\) \(Z\), then \(g(f(x))\) \(\epsilon\) \(C\), so that \(f(x)\) \(\epsilon\) \(g^{-1}(C)\), and therefore \(x\) \(\epsilon\) \(f^{-1}\left(g^{-1}(C)\right)\); the steps of the argument are reversible.
-
-    After:
-      If \(f\) maps \(X\) into \(Y\) and \(g\) maps \(Y\) into \(Z\), then \(f^{-1}\) maps \(\mathfrak{P}(Y)\) into \(\mathfrak{P}(X)\) and \(g^{-1}\) maps \(\mathfrak{P}(Z)\) into \(\mathfrak{P}(Y)\). In this situation, the composites that are formable are \(g f\) and \(f^{-1} g^{-1}\); the assertion is that the latter is the inverse of the former. <br><br>Proof: if \(x \epsilon\) \((g f)^{-1}(C)\), where \(x \epsilon\) \(X\) and \(C\) \(\subset\) \(Z\), then \(g(f(x))\) \(\epsilon\) \(C\), so that \(f(x)\) \(\epsilon\) \(g^{-1}(C)\), and therefore \(x\) \(\epsilon\) \(f^{-1}\left(g^{-1}(C)\right)\); the steps of the argument are reversible.
-
-============================================================
-
-Note ID: 1713100533003
-  Field: Text
-    Before:
-      By definition \(y\) \(R^{-1}\) \(x\) means that \(x\) \(R\) \(y\).
-
-    After:
-      By definition \(y\) \(R^{-1}\) \(x\) means that \(x\) \(R\) \(y\).
-
-============================================================
-
-Note ID: 1713100563284
-  Field: Text
-    Before:
-      &nbsp;Example: if \(R\) is the relation of belonging, from \(X\) to \(\mathfrak{P}(X)\), then \(R^{-1}\) is the relation of containing, from \(\mathfrak{P}(X)\) to \(X\).&nbsp;
-
-    After:
-      &nbsp;Example: if \(R\) is the relation of belonging, from \(X\) to \(\mathfrak{P}(X)\), then \(R^{-1}\) is the relation of containing, from \(\mathfrak{P}(X)\) to \(X\).&nbsp;
-
-============================================================
-
-Note ID: 1713101001272
-  Field: Text
-    Before:
-      Thus, in particular, composition is commutative&nbsp;&nbsp;by accident only, but it is always associative, and it is always connected with inversion via the equation \((S R)^{-1}\)=\(R^{-1} S^{-1}\)
-
-    After:
-      Thus, in particular, composition is commutative&nbsp;&nbsp;by accident only, but it is always associative, and it is always connected with inversion via the equation \((S R)^{-1}\)=\(R^{-1} S^{-1}\)
-
-============================================================
-
-Note ID: 1713101267156
-  Field: Text
-    Before:
-      The three defining properties of an equivalence relation can be formulated in algebraic terms as follows:<br><ul><li>&nbsp;reflexivity means \(I\) \(\subset\) \(R\),&nbsp;</li><li>symmetry means \(R\) \(\subset\) \(R^{-1}\)</li><li>and transitivity means \(R R\) \(\subset\) \(R\).</li></ul>
-
-    After:
-      The three defining properties of an equivalence relation can be formulated in algebraic terms as follows:<br><ul><li>&nbsp;reflexivity means \(I\) \(\subset\) \(R\),&nbsp;</li><li>symmetry means \(R\) \(\subset\) \(R^{-1}\)</li><li>and transitivity means \(R R\) \(\subset\) \(R\).</li></ul>
-
-============================================================
-
-Note ID: 1713102088992
-  Field: Text
-    Before:
-      <ul><li>&nbsp;If \(X\) is a discrete random variable and \(\mathbb{E}\)\(\left(X^{2}\right)\) =\(0\)</li><li>Show that \(\mathbb{P}\)\((X=0)\) = \(1\).&nbsp;</li><li>Deduce that, if \(\operatorname{var}\)\((X)\)=\(0\), then \(\mathbb{P}\)\((X=\mu)\)=\(1\), whenever \(\mu=\mathbb{E}(X)\) is finite.</li></ul>
-
-    After:
-      <ul><li>&nbsp;If \(X\) is a discrete random variable and \(\mathbb{E}\)\(\left(X^{2}\right)\) =\(0\)</li><li>Show that \(\mathbb{P}\)\((X=0)\) = \(1\).&nbsp;</li><li>Deduce that, if \(\operatorname{var}\)\((X)\)=\(0\), then \(\mathbb{P}\)\((X=\mu)\)=\(1\), whenever \(\mu=\mathbb{E}(X)\) is finite.</li></ul>
-
-============================================================
-
-Note ID: 1713110817514
-  Field: Text
-    Before:
-      (b) \(\left(\right.\) probability that a random 2 letter word is a palindrome \(\left.{ }^1\right)=(\) probability that a random 3 letter word is a palindrome)<br><br>Solution:<br><ul><li>For two letters you have 26 options and then the second is determined, out of&nbsp; 26^2 total</li><li>For three letters you have 26 options for the first, then the last is determined and then another 26 for the middle. Out of a total of 26^3</li></ul>
-
-    After:
-      (b) \(\left(\right.\) probability that a random 2 letter word is a palindrome \(\left.{ }^1\right)=(\) probability that a random 3 letter word is a palindrome)<br><br>Solution:<br><ul><li>For two letters you have 26 options and then the second is determined, out of&nbsp; 26^2 total</li><li>For three letters you have 26 options for the first, then the last is determined and then another 26 for the middle. Out of a total of 26^3</li></ul>
-
-============================================================
-
-Note ID: 1713111629562
-  Field: Text
-    Before:
-      4. A norepeatword is a sequence of at least one (and possibly all) of the usual 26 letters a,b,c,..,z, with repetitions not allowed. For example, "course" is a norepeatword, but "statistics" is not. Order matters, e.g., "course" is not the same as "source".<br><ul><li>Since nonrepeat words out of a n letter alphabet can contain all arangements of the alphabet into k letters the probability of a word of length k is:</li><ul><li>\(\frac{n!}{(n-k)!}\)<br></li></ul><li>The probability of a word of length n is n!</li><li>thus we have:</li><ul><li>\(n! / (\sum \frac{n!}{(n-k)!}) \)<br></li><li>Where&nbsp;&nbsp;\((\sum \frac{n!}{(n-k)!})\) = \((n ! \sum \frac{1}{(n-k)!})\) =&nbsp;&nbsp;\((n ! \sum \frac{1}{(k)!})\) by symmetry</li></ul><li>Thus:&nbsp;</li><ul><li>\(n! / (\sum \frac{n!}{(n-k)!}) \)</li><li>=&nbsp;&nbsp;</li><li>= \(1 / (\sum \frac{1}{(k)!}) \)</li><li>=&nbsp;\(1/e\)</li></ul></ul><br><br>
-
-    After:
-      4. A norepeatword is a sequence of at least one (and possibly all) of the usual 26 letters a,b,c,..,z, with repetitions not allowed. For example, "course" is a norepeatword, but "statistics" is not. Order matters, e.g., "course" is not the same as "source".<br><ul><li>Since nonrepeat words out of a n letter alphabet can contain all arangements of the alphabet into k letters the probability of a word of length k is:</li><ul><li>\(\frac{n!}{(n-k)!}\)<br></li></ul><li>The probability of a word of length n is n!</li><li>thus we have:</li><ul><li>\(n! / (\sum \frac{n!}{(n-k)!}) \)<br></li><li>Where&nbsp;&nbsp;\((\sum \frac{n!}{(n-k)!})\) = \((n ! \sum \frac{1}{(n-k)!})\) =&nbsp;&nbsp;\((n ! \sum \frac{1}{(k)!})\) by symmetry</li></ul><li>Thus:&nbsp;</li><ul><li>\(n! / (\sum \frac{n!}{(n-k)!}) \)</li><li>=&nbsp;&nbsp;</li><li>= \(1 / (\sum \frac{1}{(k)!}) \)</li><li>=&nbsp;\(1/e\)</li></ul></ul><br><br>
-
-============================================================
-
-Note ID: 1713118465970
-  Field: Text
-    Before:
-      1. A certain family has 6 children, consisting of 3 boys and 3 girls. Assuming that all birth orders are equally likely, what is the probability that the 3 eldest children are the 3 girls?<br><br><ul><li>Ways to permute 1,2,3 = 3!,&nbsp;</li><li>Ways to permute 4,5,6 = 3!</li><li>P =&nbsp;\(\frac{(3 !)^2}{6 !}\)</li><li>Alternatively, we can use the fact that there are \(\left(\begin{array}{l}6 \\ 3\end{array}\right)\) options for where the girls appear in the birth order ignoring their odering, out of which only 1 has them as consecutive<br></li></ul>
-
-    After:
-      1. A certain family has 6 children, consisting of 3 boys and 3 girls. Assuming that all birth orders are equally likely, what is the probability that the 3 eldest children are the 3 girls?<br><br><ul><li>Ways to permute 1,2,3 = 3!,&nbsp;</li><li>Ways to permute 4,5,6 = 3!</li><li>P =&nbsp;\(\frac{(3 !)^2}{6 !}\)</li><li>Alternatively, we can use the fact that there are \(\left(\begin{array}{l}6 \\ 3\end{array}\right)\) options for where the girls appear in the birth order ignoring their odering, out of which only 1 has them as consecutive<br></li></ul>
-
-============================================================
-
-Note ID: 1713119109666
-  Field: Text
-    Before:
-      3. A college has 10 (non-overlapping) time slots for its courses, and blithely assigns<br>courses to time slots randomly and independently. A student randomly chooses 3 of<br>the courses to enroll in (for the PTP, to avoid getting fined). What is the probability<br>that there is a conflict in the student's schedule?<br><br>The probability of no conflict is&nbsp;\(\frac{10 \cdot 9 \cdot 8}{10^3}\) since we need to pick different positions. Thus the probability of conflict is 1-that
-
-    After:
-      3. A college has 10 (non-overlapping) time slots for its courses, and blithely assigns<br>courses to time slots randomly and independently. A student randomly chooses 3 of<br>the courses to enroll in (for the PTP, to avoid getting fined). What is the probability<br>that there is a conflict in the student's schedule?<br><br>The probability of no conflict is&nbsp;\(\frac{10 \cdot 9 \cdot 8}{10^3}\) since we need to pick different positions. Thus the probability of conflict is 1-that
-
-============================================================
-
-Note ID: 1713851716409
-  Field: Text
-    Before:
-      A typical task in analysis is to decipher whether a property possessed by every term in a convergent sequence is necessarily inherited by the limit. Assume \(\left(a_{n}\right) \rightarrow a\), and determine the validity of each claim. Try to produce a counterexample for any that are false.<br><br>(a) If every \(a_{n}\) is an upper bound for a set \(B\), then \(a\) is also an upper bound for \(B\).<br><br><ul><li>Assume that&nbsp;\(a_n &gt; M &gt; a\)&nbsp;</li><li>This implies:</li><li>\(a_n - a\)&nbsp;\(&gt;\)&nbsp;\(M - a\) &gt;&nbsp;\(0\)<br></li><li>However, by the definition of convergence:</li><li>\(a_n - a\)&nbsp;\(&lt;\)&nbsp;\(\epsilon\) for all&nbsp;\(\epsilon\) after a given n<br></li><li>So pick \(\epsilon\) =&nbsp;\(M-a -1\) leading to a contradiction</li></ul>
-
-    After:
-      A typical task in analysis is to decipher whether a property possessed by every term in a convergent sequence is necessarily inherited by the limit. Assume \(\left(a_{n}\right) \rightarrow a\), and determine the validity of each claim. Try to produce a counterexample for any that are false.<br><br>(a) If every \(a_{n}\) is an upper bound for a set \(B\), then \(a\) is also an upper bound for \(B\).<br><br><ul><li>Assume that&nbsp;\(a_n &gt; M &gt; a\)&nbsp;</li><li>This implies:</li><li>\(a_n - a\)&nbsp;\(&gt;\)&nbsp;\(M - a\) &gt;&nbsp;\(0\)<br></li><li>However, by the definition of convergence:</li><li>\(a_n - a\)&nbsp;\(&lt;\)&nbsp;\(\epsilon\) for all&nbsp;\(\epsilon\) after a given n<br></li><li>So pick \(\epsilon\) =&nbsp;\(M-a -1\) leading to a contradiction</li></ul>
-
-============================================================
-
-Note ID: 1713852144462
-  Field: Text
-    Before:
-      A typical task in analysis is to decipher whether a property possessed by every term in a convergent sequence is necessarily inherited by the limit. Assume \(\left(a_{n}\right) \rightarrow a\), and determine the validity of each claim. Try to produce a counterexample for any that are false.<br><br>(b) If every \(a_{n}\) is in the complement of the interval \((0,1)\), then \(a\) is also in the complement of \((0,1)\).<br><br><ul><li>(b) True, since if \(a \in(0,1)\) then there would exist an \(\epsilon\)-neighborhood inside \((0,1)\) that \(a_{n}\) would have to fall in, contradicting the fact that \(a_{n}\) \(\notin(0,1)\).</li></ul>
-
-    After:
-      A typical task in analysis is to decipher whether a property possessed by every term in a convergent sequence is necessarily inherited by the limit. Assume \(\left(a_{n}\right) \rightarrow a\), and determine the validity of each claim. Try to produce a counterexample for any that are false.<br><br>(b) If every \(a_{n}\) is in the complement of the interval \((0,1)\), then \(a\) is also in the complement of \((0,1)\).<br><br><ul><li>(b) True, since if \(a \in(0,1)\) then there would exist an \(\epsilon\)-neighborhood inside \((0,1)\) that \(a_{n}\) would have to fall in, contradicting the fact that \(a_{n}\) \(\notin(0,1)\).</li></ul>
-
-============================================================
-
-Note ID: 1713852585931
-  Field: Text
-    Before:
-      (a) Prove that the sequence defined by \(x_{1}=3\) and<br><br>\[<br>x_{n+1}=\frac{1}{4-x_{n}}<br>\]<br><br>converges.<br><br>(b) Now that we know \(\lim x_{n}\) exists, explain why \(\lim x_{n+1}\) must also exist and equal the same value.<br><br>(c) Take the limit of each side of the recursive equation in part (a) to explicitly compute \(\lim x_{n}\).<br><br><ul><li>\(x\)=\(\frac{1}{4-x}\)<br></li><li>\(\Longleftrightarrow\) \(x^2-4 x+1\)=\(0\)<br></li><li>\((x-2)^2\) = \(3\)<br></li><li>\(x\) = \(2 \pm \sqrt{3}\)</li></ul>
-
-    After:
-      (a) Prove that the sequence defined by \(x_{1}=3\) and<br><br>\[<br>x_{n+1}=\frac{1}{4-x_{n}}<br>\]<br><br>converges.<br><br>(b) Now that we know \(\lim x_{n}\) exists, explain why \(\lim x_{n+1}\) must also exist and equal the same value.<br><br>(c) Take the limit of each side of the recursive equation in part (a) to explicitly compute \(\lim x_{n}\).<br><br><ul><li>\(x\)=\(\frac{1}{4-x}\)<br></li><li>\(\Longleftrightarrow\) \(x^2-4 x+1\)=\(0\)<br></li><li>\((x-2)^2\) = \(3\)<br></li><li>\(x\) = \(2 \pm \sqrt{3}\)</li></ul>
-
-============================================================
-
-Note ID: 1713853220596
-  Field: Text
-    Before:
-      <div>Let \(\left(a_{n}\right)\) be a bounded sequence.<br><br>(a) Prove that the sequence defined by \(y_{n}=\sup \left\{a_{k}: k \geq n\right\}\) converges.<br></div><div><br></div><div><ul><li>\(y_n\) is monotone since the supremum of&nbsp;\(y_m\) must also be an upper bound of&nbsp;\(y_{m+1}\)<br></li><li>\(y_n\) is bounded since the original sequence is bounded and the supremum is the least upper bound<br></li></ul></div>
-
-    After:
-      <div>Let \(\left(a_{n}\right)\) be a bounded sequence.<br><br>(a) Prove that the sequence defined by \(y_{n}=\sup \left\{a_{k}: k \geq n\right\}\) converges.<br></div><div><br></div><div><ul><li>\(y_n\) is monotone since the supremum of&nbsp;\(y_m\) must also be an upper bound of&nbsp;\(y_{m+1}\)<br></li><li>\(y_n\) is bounded since the original sequence is bounded and the supremum is the least upper bound<br></li></ul></div>
-
-============================================================
-
-Note ID: 1713853388632
-  Field: Text
-    Before:
-      (c) Prove that \(\lim \inf a_{n} \leq \lim \sup a_{n}\) for every bounded sequence, and give an example of a sequence for which the inequality is strict.<br><br>(d) If \(\liminf a_{n}=\lim \sup a_{n}\) then the squeeze theorem (Exercise 2.3.3) implies \(a_{n}\) converges to the same value, since \(\inf \left\{a_{k \geq n}\right\}\) \(\leq\) \(a_{n}\) \(\leq\) \(\sup \left\{a_{k \geq n}\right\}\)
-
-    After:
-      (c) Prove that \(\lim \inf a_{n} \leq \lim \sup a_{n}\) for every bounded sequence, and give an example of a sequence for which the inequality is strict.<br><br>(d) If \(\liminf a_{n}=\lim \sup a_{n}\) then the squeeze theorem (Exercise 2.3.3) implies \(a_{n}\) converges to the same value, since \(\inf \left\{a_{k \geq n}\right\}\) \(\leq\) \(a_{n}\) \(\leq\) \(\sup \left\{a_{k \geq n}\right\}\)
-
-============================================================
-
-Note ID: 1714200370008
-  Field: Text
-    Before:
-      1. A certain family has 6 children, consisting of 3 boys and 3 girls. Assuming that all birth orders are equally likely, what is the probability that the 3 eldest children are the 3 girls?<br><br>Solution:<br><ul><li>How many ways can you permute 4,5,6 and 1,2,3 out of the ways to permute 1,2,3,4,5,6</li><li>P =&nbsp;\(\frac{(3 !)^2}{6 !}\)</li></ul>
-
-    After:
-      1. A certain family has 6 children, consisting of 3 boys and 3 girls. Assuming that all birth orders are equally likely, what is the probability that the 3 eldest children are the 3 girls?<br><br>Solution:<br><ul><li>How many ways can you permute 4,5,6 and 1,2,3 out of the ways to permute 1,2,3,4,5,6</li><li>P =&nbsp;\(\frac{(3 !)^2}{6 !}\)</li></ul>
-
-============================================================
-
-Note ID: 1714201137874
-  Field: Text
-    Before:
-      4. A city with 6 districts has 6 robberies in a particular week. Assume the robberies<br>are located randomly, with all possibilities for which robbery occurred where equally<br>likely. What is the probability that some district had more than 1 robbery?<br><br>There are \(6^6\) possible configurations for which robbery occurred where. There are 6 ! configurations where each district had exactly 1 of the 6 , so the probability of the complement of the desired event is \(6 ! / 6^6\). So the probability of some district having more than 1 robbery is<br>\[<br>1-6 ! / 6^6 \approx 0.9846 \text {. }<br>\]<br><br>Note that this also says that if a fair die is rolled 6 times, there's over a \(98 \%\) chance that some value is repeated!
-
-    After:
-      4. A city with 6 districts has 6 robberies in a particular week. Assume the robberies<br>are located randomly, with all possibilities for which robbery occurred where equally<br>likely. What is the probability that some district had more than 1 robbery?<br><br>There are \(6^6\) possible configurations for which robbery occurred where. There are 6 ! configurations where each district had exactly 1 of the 6 , so the probability of the complement of the desired event is \(6 ! / 6^6\). So the probability of some district having more than 1 robbery is<br>\[<br>1-6 ! / 6^6 \approx 0.9846 \text {. }<br>\]<br><br>Note that this also says that if a fair die is rolled 6 times, there's over a \(98 \%\) chance that some value is repeated!
-
-============================================================
-
-Note ID: 1714204892893
-  Field: Text
-    Before:
-      6. A jar contains r red balls and g green balls, where r and g are fixed positive<br>integers. A ball is drawn from the jar randomly (with all possibilities equally likely),<br>and then a second ball is drawn randomly.<br><br><br>(a) Explain intuitively why the probability of the second ball being green is the same<br>as the probability of the first ball being green.<br><br>This is true by symmetry. The first ball is equally likely to be any of the \(g+r\) balls, so the probability of it being green is \(g /(g+r)\). But the second ball is also equally likely to be any of the \(g+r\) balls (there aren't certain balls that enjoy being chosen second and others that have an aversion to being chosen second); once we know whether the first ball is green we have information that affects our uncertainty about the second ball, but before we have this information, the second ball is equally likely to be any of the balls.<br><br>Alternatively, intuitively it shouldn't matter if we pick one ball at a time, or take one ball with the left hand and one with the right hand at the same time. By symmetry, the probabilities for the ball drawn with the left hand should be the same as those for the ball drawn with the right hand.
-
-    After:
-      6. A jar contains r red balls and g green balls, where r and g are fixed positive<br>integers. A ball is drawn from the jar randomly (with all possibilities equally likely),<br>and then a second ball is drawn randomly.<br><br><br>(a) Explain intuitively why the probability of the second ball being green is the same<br>as the probability of the first ball being green.<br><br>This is true by symmetry. The first ball is equally likely to be any of the \(g+r\) balls, so the probability of it being green is \(g /(g+r)\). But the second ball is also equally likely to be any of the \(g+r\) balls (there aren't certain balls that enjoy being chosen second and others that have an aversion to being chosen second); once we know whether the first ball is green we have information that affects our uncertainty about the second ball, but before we have this information, the second ball is equally likely to be any of the balls.<br><br>Alternatively, intuitively it shouldn't matter if we pick one ball at a time, or take one ball with the left hand and one with the right hand at the same time. By symmetry, the probabilities for the ball drawn with the left hand should be the same as those for the ball drawn with the right hand.
-
-============================================================
-
-Note ID: 1714212831822
-  Field: Text
-    Before:
-      7. (a) Show using a story proof that<br>\[<br>\left(\begin{array}{l}<br>k \\<br>k<br>\end{array}\right)+\left(\begin{array}{c}<br>k+1 \\<br>k<br>\end{array}\right)+\left(\begin{array}{c}<br>k+2 \\<br>k<br>\end{array}\right)+\cdots+\left(\begin{array}{l}<br>n \\<br>k<br>\end{array}\right)=\left(\begin{array}{l}<br>n+1 \\<br>k+1<br>\end{array}\right),<br>\]<br>where \(n\) and \(k\) are positive integers with \(n \geq k\).<br>Hint: imagine arranging a group of people by age, and then think about the oldest person in a chosen subgroup.<br><br><br>Consider choosing \(k+1\) people out of a group of \(n+1\) people. Call the oldest person in the subgroup "Aemon." If Aemon is also the oldest person in the full group, then there are \(\left(\begin{array}{l}n \\ k\end{array}\right)\) choices for the rest of the subgroup. If Aemon is the second oldest in the full group, then there are \(\left(\begin{array}{c}n-1 \\ k\end{array}\right)\) choices since the oldest person in the full group can't be chosen. In general, if there are \(j\) people in the full group who are younger than Aemon, then there are \(\left(\begin{array}{l}j \\ k\end{array}\right)\) possible choices for the rest of the subgroup. Thus,<br>\[<br>\sum_{j=k}^n\left(\begin{array}{l}<br>j \\<br>k<br>\end{array}\right)=\left(\begin{array}{l}<br>n+1 \\<br>k+1<br>\end{array}\right)<br>\]
-
-    After:
-      7. (a) Show using a story proof that<br>\[<br>\left(\begin{array}{l}<br>k \\<br>k<br>\end{array}\right)+\left(\begin{array}{c}<br>k+1 \\<br>k<br>\end{array}\right)+\left(\begin{array}{c}<br>k+2 \\<br>k<br>\end{array}\right)+\cdots+\left(\begin{array}{l}<br>n \\<br>k<br>\end{array}\right)=\left(\begin{array}{l}<br>n+1 \\<br>k+1<br>\end{array}\right),<br>\]<br>where \(n\) and \(k\) are positive integers with \(n \geq k\).<br>Hint: imagine arranging a group of people by age, and then think about the oldest person in a chosen subgroup.<br><br><br>Consider choosing \(k+1\) people out of a group of \(n+1\) people. Call the oldest person in the subgroup "Aemon." If Aemon is also the oldest person in the full group, then there are \(\left(\begin{array}{l}n \\ k\end{array}\right)\) choices for the rest of the subgroup. If Aemon is the second oldest in the full group, then there are \(\left(\begin{array}{c}n-1 \\ k\end{array}\right)\) choices since the oldest person in the full group can't be chosen. In general, if there are \(j\) people in the full group who are younger than Aemon, then there are \(\left(\begin{array}{l}j \\ k\end{array}\right)\) possible choices for the rest of the subgroup. Thus,<br>\[<br>\sum_{j=k}^n\left(\begin{array}{l}<br>j \\<br>k<br>\end{array}\right)=\left(\begin{array}{l}<br>n+1 \\<br>k+1<br>\end{array}\right)<br>\]
-
-============================================================
-
-Note ID: 1716570936910
-  Field: Text
-    Before:
-      The preceding paragraph motivates a set-theoretic construction that makes sense for every set, but that is of interest in the construction of numbers only. For every set \(x\) we define the successor \(x^{+}\) of \(x\) to be the set obtained by adjoining \(x\) to the elements of \(x\); in other words,<br><br><ul><li>\(x^{+}\) = \(x \cup\{x\}\)</li></ul>
-
-    After:
-      The preceding paragraph motivates a set-theoretic construction that makes sense for every set, but that is of interest in the construction of numbers only. For every set \(x\) we define the successor \(x^{+}\) of \(x\) to be the set obtained by adjoining \(x\) to the elements of \(x\); in other words,<br><br><ul><li>\(x^{+}\) = \(x \cup\{x\}\)</li></ul>
-
-============================================================
-
-Note ID: 1716663406132
-  Field: Text
-    Before:
-      (Abbott 2.5.5) Assume that \(\left(a_n\right)\) is a bounded sequence with the property that every convergent subsequence of \(\left(a_n\right)\) converges to the same limit \(a \in \mathbb{R}\). Show that \(\left(a_n\right)\) must converge to \(a\).<br><br>Solution if it diverges<ul><li>A convergent subsqeuence must exist by Bolzano<br></li><li>\(\left(a_n\right)\) does not converge to \(a\) if there exists \(\epsilon&gt;0\) such that for every \(k \in \mathbb{N}\) there exists a number \(m\) such that \(m \geq k\) with \(\left|a-a_m\right| \geq \epsilon\).<br></li><li>Construct a new subsequence \(\left(a_{m_k}\right)\) as follows. Let \(m_1 \geq 1\) be such that \(\left|a-a_{m_1}\right|\)&nbsp; \(\geq \epsilon\).&nbsp;</li><li>And define \(m_k\) inductively by, \(m_k\) \(\geq m_{k-1}+1\) be such that \(\left|a-a_{m_k}\right|\) \(\geq \epsilon\).&nbsp;</li><li>We apply Bolzano-Weierstrass to the subsequence \(\left(a_{m_k}\right)\) to get that there exists a subsequence of \(\left(a_{m_k}\right)\) that converges.&nbsp;</li><li>Every subsequence of \(\left(a_{m_k}\right)\) is also a subsequence of \(\left(a_n\right)\).&nbsp;</li><li>Abusing notation, we call this convergent subsequence \(\left(a_{m_k}\right)\) and let \(a^{\prime}\) be the limit.<br></li><li>Now we show that \(a \neq a^{\prime}\). Since \(\left|a-a_{m_k}\right|\) \(\geq \epsilon\) for all \(k \in \mathbb{N}\) we have that either \(a-a_{m_k}\) \(&gt;\epsilon\) or \(a_{m_k}-a\) \(&gt;\epsilon\) for all \(k \in \mathbb{N}\). By monotonicity of order, we have that \(a-a^{\prime}\) \(\geq \epsilon\) or \(a^{\prime}-a\) \(\geq \epsilon\). In either case, \(a \neq a^{\prime}\). Therefore we have constructed two convergent subsequences \(a_{n_k}\) and \(a_{m_k}\) that converge to distinct limits.<br></li></ul>
-
-    After:
-      (Abbott 2.5.5) Assume that \(\left(a_n\right)\) is a bounded sequence with the property that every convergent subsequence of \(\left(a_n\right)\) converges to the same limit \(a \in \mathbb{R}\). Show that \(\left(a_n\right)\) must converge to \(a\).<br><br>Solution if it diverges<ul><li>A convergent subsqeuence must exist by Bolzano<br></li><li>\(\left(a_n\right)\) does not converge to \(a\) if there exists \(\epsilon&gt;0\) such that for every \(k \in \mathbb{N}\) there exists a number \(m\) such that \(m \geq k\) with \(\left|a-a_m\right| \geq \epsilon\).<br></li><li>Construct a new subsequence \(\left(a_{m_k}\right)\) as follows. Let \(m_1 \geq 1\) be such that \(\left|a-a_{m_1}\right|\)&nbsp; \(\geq \epsilon\).&nbsp;</li><li>And define \(m_k\) inductively by, \(m_k\) \(\geq m_{k-1}+1\) be such that \(\left|a-a_{m_k}\right|\) \(\geq \epsilon\).&nbsp;</li><li>We apply Bolzano-Weierstrass to the subsequence \(\left(a_{m_k}\right)\) to get that there exists a subsequence of \(\left(a_{m_k}\right)\) that converges.&nbsp;</li><li>Every subsequence of \(\left(a_{m_k}\right)\) is also a subsequence of \(\left(a_n\right)\).&nbsp;</li><li>Abusing notation, we call this convergent subsequence \(\left(a_{m_k}\right)\) and let \(a^{\prime}\) be the limit.<br></li><li>Now we show that \(a \neq a^{\prime}\). Since \(\left|a-a_{m_k}\right|\) \(\geq \epsilon\) for all \(k \in \mathbb{N}\) we have that either \(a-a_{m_k}\) \(&gt;\epsilon\) or \(a_{m_k}-a\) \(&gt;\epsilon\) for all \(k \in \mathbb{N}\). By monotonicity of order, we have that \(a-a^{\prime}\) \(\geq \epsilon\) or \(a^{\prime}-a\) \(\geq \epsilon\). In either case, \(a \neq a^{\prime}\). Therefore we have constructed two convergent subsequences \(a_{n_k}\) and \(a_{m_k}\) that converge to distinct limits.<br></li></ul>
-
-============================================================
-
-Note ID: 1716730844719
-  Field: Text
-    Before:
-      Let \(\left(a_n\right)\) be a bounded sequence. Recall from HW4, we defined a sequence \(y_n=\sup \left\{a_k: k \geq n\right\}\) and proved that \(\left(y_n\right)\) is bounded and monotone. Thus, by the Monotone Convergence Theorem \(\left(y_n\right)\) converges. Let \(y=\lim _{n \rightarrow \infty} y_n\).<br>Prove that there exists a subsequence \(\left(a_{n_k}\right)\) of \(\left(a_n\right)\) that converges to \(y\).<br><br>Proof:<br><ul><li>Since&nbsp;\(y_{n_i}\) is always a supremum of a set</li><li>There always exists</li><li>\(y_{n_{k-1}+1}-\frac{1}{k}\)&nbsp;\(\leq\)&nbsp;\(a_{n_{k}}\)&nbsp;\(\leq\)&nbsp;\(y_{n_{k-1}+1}\)<br></li><li>Since \(y_{n_{k-1}+1}\)=\(\sup A_{n_{k-1}+1}\) \(a_{n_k}\) there exists such an&nbsp;\(a_{n_k}\) from&nbsp; \(A_n=\left\{a_k: k \geq n\right\}\),&nbsp;</li><li>Since:</li><ul><li>\(\left(y_{n_{k-1}+1}\right)\) must converge to y&nbsp;</li><li>And &nbsp;\(\lim \frac{1}{k}\) = \(0\)&nbsp;</li></ul><li>By the sequeze theorem&nbsp;\(a_{n_k}\) must also converge to y</li></ul>
-
-    After:
-      Let \(\left(a_n\right)\) be a bounded sequence. Recall from HW4, we defined a sequence \(y_n=\sup \left\{a_k: k \geq n\right\}\) and proved that \(\left(y_n\right)\) is bounded and monotone. Thus, by the Monotone Convergence Theorem \(\left(y_n\right)\) converges. Let \(y=\lim _{n \rightarrow \infty} y_n\).<br>Prove that there exists a subsequence \(\left(a_{n_k}\right)\) of \(\left(a_n\right)\) that converges to \(y\).<br><br>Proof:<br><ul><li>Since&nbsp;\(y_{n_i}\) is always a supremum of a set</li><li>There always exists</li><li>\(y_{n_{k-1}+1}-\frac{1}{k}\)&nbsp;\(\leq\)&nbsp;\(a_{n_{k}}\)&nbsp;\(\leq\)&nbsp;\(y_{n_{k-1}+1}\)<br></li><li>Since \(y_{n_{k-1}+1}\)=\(\sup A_{n_{k-1}+1}\) \(a_{n_k}\) there exists such an&nbsp;\(a_{n_k}\) from&nbsp; \(A_n=\left\{a_k: k \geq n\right\}\),&nbsp;</li><li>Since:</li><ul><li>\(\left(y_{n_{k-1}+1}\right)\) must converge to y&nbsp;</li><li>And &nbsp;\(\lim \frac{1}{k}\) = \(0\)&nbsp;</li></ul><li>By the sequeze theorem&nbsp;\(a_{n_k}\) must also converge to y</li></ul>
-
-============================================================
-
-Note ID: 1716849947919
-  Field: Text
-    Before:
-      If \(\left(x_n\right)\) and \(\left(y_n\right)\) are Cauchy sequences prove directly that \(\left(x_n y_n\right)\) is a Cauchy sequence.<br><br>Solution:<br><ul><li>Both are bounded by some bound M = \(\max(M_x,M_y)\)</li><li>Let:</li><ul><li>\(\left|x_n-x_m\right|\)&nbsp;\(\leq\)&nbsp;\(\frac{\epsilon}{2 M}\)<br></li><li>\(\left|y_n-y_m\right|\)&nbsp;\(\leq\)&nbsp;\(\frac{\epsilon}{2 M}\)</li></ul><li>\(\left|x_n y_n-x_m y_m\right| \) \(\leq\) \(\left|x_n y_n-x_m y_n\right|+\left|x_m y_n-x_m y_m\right|\)<br>(by triangle inequality)<br></li><li>=&nbsp;\(\left|x_n-x_m\right|\left|y_n\right|+\left|x_m\right|\left|y_n-y_m\right|\)</li><li>=&nbsp;\(\frac{\epsilon}{2 M} M+M \frac{\epsilon}{2 M}\)&nbsp;</li><li>=&nbsp;\(\epsilon\)</li></ul>
-
-    After:
-      If \(\left(x_n\right)\) and \(\left(y_n\right)\) are Cauchy sequences prove directly that \(\left(x_n y_n\right)\) is a Cauchy sequence.<br><br>Solution:<br><ul><li>Both are bounded by some bound M = \(\max(M_x,M_y)\)</li><li>Let:</li><ul><li>\(\left|x_n-x_m\right|\)&nbsp;\(\leq\)&nbsp;\(\frac{\epsilon}{2 M}\)<br></li><li>\(\left|y_n-y_m\right|\)&nbsp;\(\leq\)&nbsp;\(\frac{\epsilon}{2 M}\)</li></ul><li>\(\left|x_n y_n-x_m y_m\right| \) \(\leq\) \(\left|x_n y_n-x_m y_n\right|+\left|x_m y_n-x_m y_m\right|\)<br>(by triangle inequality)<br></li><li>=&nbsp;\(\left|x_n-x_m\right|\left|y_n\right|+\left|x_m\right|\left|y_n-y_m\right|\)</li><li>=&nbsp;\(\frac{\epsilon}{2 M} M+M \frac{\epsilon}{2 M}\)&nbsp;</li><li>=&nbsp;\(\epsilon\)</li></ul>
-
-============================================================
-
-Note ID: 1716920968601
-  Field: Text
-    Before:
-      Alice attends a small college in which each class meets only once a week. She is deciding between 30 non-overlapping classes. There are 6 classes to choose from for each day of the week, Monday through Friday. Trusting in the benevolence of randomness, Alice decides to register for 7 randomly selected classes out of the 30 , with all choices equally likely. What is the probability that she will have classes every day, Monday through Friday? (This problem can be done either directly using the naive definition of probability, or using inclusion-exclusion.)<br><br>Solution:<br><ol><li>\(P(A_i)\) is the probability of having at least one class on a given day while \(P(A_i^c)\) is the probability of no classes on a given day&nbsp;</li><li>\(P(\cap A_i)\) = 1 -&nbsp;\(P(\cup A_i^c)\)<br></li><li>\(P(\cup A_i^c)\) = inclusion exclusion, all probabilities are symmetric so we just get binomial coefficients</li><li>\(P(A_i^c)\) =&nbsp;\(\binom{24}{7}\) \(/\) \({\binom{30}{7} }\) because:<br></li><ol><li>There are \({\binom{30}{7} }\)&nbsp;possible selections of classes</li><li>Out of which \(\binom{24}{7}\) do not include the 6 classes allocated to the exluded day</li></ol><li>\(P(A_i^c \cap A_j^c)\)&nbsp; = \(\binom{18}{7}\) \(/\) \({\binom{30}{7} }\)&nbsp;because we subtract the other 6 classes of day j&nbsp; from the allocation</li></ol>
-
-    After:
-      Alice attends a small college in which each class meets only once a week. She is deciding between 30 non-overlapping classes. There are 6 classes to choose from for each day of the week, Monday through Friday. Trusting in the benevolence of randomness, Alice decides to register for 7 randomly selected classes out of the 30 , with all choices equally likely. What is the probability that she will have classes every day, Monday through Friday? (This problem can be done either directly using the naive definition of probability, or using inclusion-exclusion.)<br><br>Solution:<br><ol><li>\(P(A_i)\) is the probability of having at least one class on a given day while \(P(A_i^c)\) is the probability of no classes on a given day&nbsp;</li><li>\(P(\cap A_i)\) = 1 -&nbsp;\(P(\cup A_i^c)\)<br></li><li>\(P(\cup A_i^c)\) = inclusion exclusion, all probabilities are symmetric so we just get binomial coefficients</li><li>\(P(A_i^c)\) =&nbsp;\(\binom{24}{7}\) \(/\) \({\binom{30}{7} }\) because:<br></li><ol><li>There are \({\binom{30}{7} }\)&nbsp;possible selections of classes</li><li>Out of which \(\binom{24}{7}\) do not include the 6 classes allocated to the exluded day</li></ol><li>\(P(A_i^c \cap A_j^c)\)&nbsp; = \(\binom{18}{7}\) \(/\) \({\binom{30}{7} }\)&nbsp;because we subtract the other 6 classes of day j&nbsp; from the allocation</li></ol>
-
-============================================================
-
-Note ID: 1718714664152
-  Field: Text
-    Before:
-      1. Arby has a belief system assigning a number \(P_{\text {Arby }}(A)\) between 0 and 1 to every event \(A\) (for some sample space). This represents Arby's subjective degree of belief about how likely \(A\) is to occur. For any event \(A\), Arby is willing to pay a price of \(1000 \cdot P_{\text {Arby }}(A)\) dollars to buy a certificate.<br><br>Likewise, Arby is willing to sell such a certificate at the same price. Indeed, Arby is willing to buy or sell any number of certificates at this price, as Arby considers it the "fair" price.<br><br>Arby, not having taken Stat 110, stubbornly refuses to accept the axioms of probability. In particular, suppose that there are two disjoint events \(A\) and \(B\) with<br>\[<br>P_{\text {Arby }}(A \cup B) \neq P_{\text {Arby }}(A)+P_{\text {Arby }}(B) .<br>\]<br><br>Show how to make Arby go bankrupt, by giving a list of transactions Arby is willing to make that will guarantee that Arby will lose money (you can assume it will be known whether \(A\) occurred and whether \(B\) occurred the day after any certificates are bought/sold).<br><br>Solution:<br><ul><li>Suppose that</li><ul><li>\(P_{\text {Arby }}(A \cup B)\) \(&lt;\) \(P_{\text {Arby }}(A)+P_{\text {Arby } }(B)\).</li><ul><li>Since the probability of the&nbsp; union is lower, Arby pays\(P_{\text {Arby } }(A)+P_{\text {Arby } }(B)\) and buys certificates for &nbsp;\(P_{\text {Arby } }(A \cup B)\)</li><li>Thus every time she loses the difference&nbsp;\(P_{\text {Arby } }(A)+P_{\text {Arby } }(B)-P_{\text {Arby } }(A \cup B)\)</li></ul><li>\(P_{\text {Arby } }(A \cup B)\) \(&lt;\) \(P_{\text {Arby } }(A)+P_{\text {Arby }}(B)\)</li><ul><li>Then arby loses \(P_{\text {Arby }&nbsp; }(A \cup B)-\left(P_{\text {Arby } }(A)+P_{\text {Arby } }(B)\right) \)  per transatction&nbsp;</li><li>Even if A and B are not disjoint in this case</li></ul></ul></ul>
-
-    After:
-      1. Arby has a belief system assigning a number \(P_{\text {Arby }}(A)\) between 0 and 1 to every event \(A\) (for some sample space). This represents Arby's subjective degree of belief about how likely \(A\) is to occur. For any event \(A\), Arby is willing to pay a price of \(1000 \cdot P_{\text {Arby }}(A)\) dollars to buy a certificate.<br><br>Likewise, Arby is willing to sell such a certificate at the same price. Indeed, Arby is willing to buy or sell any number of certificates at this price, as Arby considers it the "fair" price.<br><br>Arby, not having taken Stat 110, stubbornly refuses to accept the axioms of probability. In particular, suppose that there are two disjoint events \(A\) and \(B\) with<br>\[<br>P_{\text {Arby }}(A \cup B) \neq P_{\text {Arby }}(A)+P_{\text {Arby }}(B) .<br>\]<br><br>Show how to make Arby go bankrupt, by giving a list of transactions Arby is willing to make that will guarantee that Arby will lose money (you can assume it will be known whether \(A\) occurred and whether \(B\) occurred the day after any certificates are bought/sold).<br><br>Solution:<br><ul><li>Suppose that</li><ul><li>\(P_{\text {Arby }}(A \cup B)\) \(&lt;\) \(P_{\text {Arby }}(A)+P_{\text {Arby } }(B)\).</li><ul><li>Since the probability of the&nbsp; union is lower, Arby pays\(P_{\text {Arby } }(A)+P_{\text {Arby } }(B)\) and buys certificates for &nbsp;\(P_{\text {Arby } }(A \cup B)\)</li><li>Thus every time she loses the difference&nbsp;\(P_{\text {Arby } }(A)+P_{\text {Arby } }(B)-P_{\text {Arby } }(A \cup B)\)</li></ul><li>\(P_{\text {Arby } }(A \cup B)\) \(&lt;\) \(P_{\text {Arby } }(A)+P_{\text {Arby }}(B)\)</li><ul><li>Then arby loses \(P_{\text {Arby }&nbsp; }(A \cup B)-\left(P_{\text {Arby } }(A)+P_{\text {Arby } }(B)\right) \)  per transatction&nbsp;</li><li>Even if A and B are not disjoint in this case</li></ul></ul></ul>
-
-============================================================
-
-Note ID: 1718898318777
-  Field: Text
-    Before:
-      6. A family has two children. Assume that birth month is independent of gender, with boys and girls equally likely and all months equally likely, and assume that the elder child's characteristics are independent of the younger child's characteristics).<br>(a) Find the probability that both are girls, given that the elder child is a girl who was born in March.<br><br>Solution:<br><ul><li>Let \(G_j\) be the event that the \(j\) th born child is a girl and \(M_j\) be the event that the \(j\) th born child was born in March, for \(j \in\{1,2\}\). Then \(P\left(G_1 \cap G_2 \mid G_1 \cap M_1\right)\) =&nbsp; \(P\left(G_2 \mid G_1 \cap M_1\right)\), since if we know that \(G_1\) occurs, then \(G_1 \cap G_2\) occurring is the same thing as \(G_2\) occurring. By independence of the characteristics of the children, \(P\left(G_2 \mid G_1 \cap M_1\right)\) = \(P\left(G_2\right)\) = \(1 / 2\).<br></li></ul>
-
-    After:
-      6. A family has two children. Assume that birth month is independent of gender, with boys and girls equally likely and all months equally likely, and assume that the elder child's characteristics are independent of the younger child's characteristics).<br>(a) Find the probability that both are girls, given that the elder child is a girl who was born in March.<br><br>Solution:<br><ul><li>Let \(G_j\) be the event that the \(j\) th born child is a girl and \(M_j\) be the event that the \(j\) th born child was born in March, for \(j \in\{1,2\}\). Then \(P\left(G_1 \cap G_2 \mid G_1 \cap M_1\right)\) =&nbsp; \(P\left(G_2 \mid G_1 \cap M_1\right)\), since if we know that \(G_1\) occurs, then \(G_1 \cap G_2\) occurring is the same thing as \(G_2\) occurring. By independence of the characteristics of the children, \(P\left(G_2 \mid G_1 \cap M_1\right)\) = \(P\left(G_2\right)\) = \(1 / 2\).<br></li></ul>
-
-============================================================
-
-Note ID: 1718898638536
-  Field: Text
-    Before:
-      Where does the result come from?<br><br><ul><li>\(\frac{P(\text { both girls, at least one born in March })}{P(\text { at least one March-born girl })}\)</li><li>=&nbsp;\(\frac{(1 / 4)\left(1-(11 / 12)^2\right)}{1-(23 / 24)^2}\)</li></ul><div>Solution:</div><div><ul><li>Top: probability that they are both girls times probability at least one is born in march</li><li>Botttom: Probability that at least one is born in march when we do not know that they are both girls&nbsp;</li></ul></div>
-
-    After:
-      Where does the result come from?<br><br><ul><li>\(\frac{P(\text { both girls, at least one born in March })}{P(\text { at least one March-born girl })}\)</li><li>=&nbsp;\(\frac{(1 / 4)\left(1-(11 / 12)^2\right)}{1-(23 / 24)^2}\)</li></ul><div>Solution:</div><div><ul><li>Top: probability that they are both girls times probability at least one is born in march</li><li>Botttom: Probability that at least one is born in march when we do not know that they are both girls&nbsp;</li></ul></div>
-
-============================================================
-
-Note ID: 1718913074130
-  Field: Text
-    Before:
-      Problem 1. (25 pts) For each statement, circle \(T\) if it is true and \(F\) if it is false. If true, give a brief explanation (a complete proof is not required), and if false, give a counterexample.<br><br>c. \((T / F)\) If the sequence \(\left(a_n\right)\) converges and the sequence \(\left(a_n+b_n\right)\) converges then the sequence \(\left(b_n\right)\) converges.<br><br>Solution:<br><ul><li>\(|a_n + b_n - c|\)&nbsp;\(&lt;\)&nbsp;\(\epsilon/2\)</li><li>\(|a_n - a|\)&nbsp;\(&lt;\)&nbsp;\(\epsilon/2\)</li><li>\(b_n\) =&nbsp;\(a_n + b_n - a_n\)</li><li>= \(|a_n + b_n - a_n - c + a|\) by taking b=c-a</li><li>= \(|a_n + b_n - c + - (a_n - a)|\)&nbsp;&nbsp;</li><li>\(\leq\) \(|a_n + b_n - c|\)&nbsp; +&nbsp; \(|a_n - a|\)&nbsp; by triangle inequality<br></li><li>\(\leq\)&nbsp;\(\epsilon\)<br></li></ul>
-
-    After:
-      Problem 1. (25 pts) For each statement, circle \(T\) if it is true and \(F\) if it is false. If true, give a brief explanation (a complete proof is not required), and if false, give a counterexample.<br><br>c. \((T / F)\) If the sequence \(\left(a_n\right)\) converges and the sequence \(\left(a_n+b_n\right)\) converges then the sequence \(\left(b_n\right)\) converges.<br><br>Solution:<br><ul><li>\(|a_n + b_n - c|\)&nbsp;\(&lt;\)&nbsp;\(\epsilon/2\)</li><li>\(|a_n - a|\)&nbsp;\(&lt;\)&nbsp;\(\epsilon/2\)</li><li>\(b_n\) =&nbsp;\(a_n + b_n - a_n\)</li><li>= \(|a_n + b_n - a_n - c + a|\) by taking b=c-a</li><li>= \(|a_n + b_n - c + - (a_n - a)|\)&nbsp;&nbsp;</li><li>\(\leq\) \(|a_n + b_n - c|\)&nbsp; +&nbsp; \(|a_n - a|\)&nbsp; by triangle inequality<br></li><li>\(\leq\)&nbsp;\(\epsilon\)<br></li></ul>
-
-============================================================
-
-Note ID: 1718928568231
-  Field: Text
-    Before:
-      Problem 1. (25 pts) For each statement, circle \(T\) if it is true and \(F\) if it is false. If true, give a brief explanation (a complete proof is not required), and if false, give a counterexample.<br><br>e. \((T / F)\) If \(\left(a_n\right)\) converges to \(a\) and \(\left(b_n\right)\) converges to \(b\) then the sequence \(\left(a_{2 n} b_{2 n+1}\right)\) converges to \(a b\).<br><br>Solution:<br><ul><li>\(|a_{2n} - a|\)&nbsp;\( &lt;\)&nbsp;\(\frac{\epsilon} {M+b}\)<br></li><li>\(|b_{2n+1} - b|\) \( &lt;\)&nbsp; \(\frac{\epsilon} {M+b}\)<br></li><li>\(|a_{2n}b_{2n+1} - ab| \)</li><li>= \(|a_{2n}b_{2n+1} - ab| \)</li><li>= | \( a_{2n}b_{2n+1} - a_{2n} b\) + \(a_{2n}b -&nbsp; ab \) |</li><li>\(\leq\) \(|a_{2n}b_{2n+1} - a_{2n} b|\) + \(|a_{2n}b -&nbsp; ab| \)<br></li><li>\(\leq\) \(|a_{2n}(b_{2n+1} - b) |\)&nbsp; + \(|b (a_{2n} -&nbsp; a)| \)</li><li>\(\leq\) |\(a_{2n}\) \( \frac{\epsilon} {M+b} \)&nbsp;| + | \(b\) \(\frac{\epsilon} {M+b}\)&nbsp;|&nbsp;</li><li>\(\leq\)&nbsp;\(\frac{\epsilon} {M+b}\)&nbsp;\(|a_{2n} +b|\)<br></li><li>\(\leq\)&nbsp;\(\frac{\epsilon} {M+b}\)&nbsp;\(|M +b|\)</li><li>\(\leq\)&nbsp;\(\epsilon\)<br></li></ul><div><br></div>
-
-    After:
-      Problem 1. (25 pts) For each statement, circle \(T\) if it is true and \(F\) if it is false. If true, give a brief explanation (a complete proof is not required), and if false, give a counterexample.<br><br>e. \((T / F)\) If \(\left(a_n\right)\) converges to \(a\) and \(\left(b_n\right)\) converges to \(b\) then the sequence \(\left(a_{2 n} b_{2 n+1}\right)\) converges to \(a b\).<br><br>Solution:<br><ul><li>\(|a_{2n} - a|\)&nbsp;\( &lt;\)&nbsp;\(\frac{\epsilon} {M+b}\)<br></li><li>\(|b_{2n+1} - b|\) \( &lt;\)&nbsp; \(\frac{\epsilon} {M+b}\)<br></li><li>\(|a_{2n}b_{2n+1} - ab| \)</li><li>= \(|a_{2n}b_{2n+1} - ab| \)</li><li>= | \( a_{2n}b_{2n+1} - a_{2n} b\) + \(a_{2n}b -&nbsp; ab \) |</li><li>\(\leq\) \(|a_{2n}b_{2n+1} - a_{2n} b|\) + \(|a_{2n}b -&nbsp; ab| \)<br></li><li>\(\leq\) \(|a_{2n}(b_{2n+1} - b) |\)&nbsp; + \(|b (a_{2n} -&nbsp; a)| \)</li><li>\(\leq\) |\(a_{2n}\) \( \frac{\epsilon} {M+b} \)&nbsp;| + | \(b\) \(\frac{\epsilon} {M+b}\)&nbsp;|&nbsp;</li><li>\(\leq\)&nbsp;\(\frac{\epsilon} {M+b}\)&nbsp;\(|a_{2n} +b|\)<br></li><li>\(\leq\)&nbsp;\(\frac{\epsilon} {M+b}\)&nbsp;\(|M +b|\)</li><li>\(\leq\)&nbsp;\(\epsilon\)<br></li></ul><div><br></div>
-
-============================================================
-
-Note ID: 1718929968727
-  Field: Text
-    Before:
-      Problem 4. Define a sequence recursively by \(x_1=1\) and<br>\[<br>x_{n+1}=\sqrt{6+x_n} \quad \text { for } \quad n \in \mathbb{N} \text {. }<br>\]<br>a. (10 pts) Show that the sequence is increasing.<br><br>Solution:<br><ul><li>Assume induction</li><li>\(x_n\) &gt; \(x_{n-1}\)</li><li>\(6+ x_n\) &gt; \(6+x_{n-1} \)<br></li><li>\(\sqrt{6+x_n}\) &gt; \(\sqrt{6+x_{n-1} }\)<br></li><li>\(\sqrt{6+x_n}\) &gt; \(x_n\)</li><li>\(x_{n+1}\) &gt;&nbsp;\(x_n\)<br></li></ul>
-
-    After:
-      Problem 4. Define a sequence recursively by \(x_1=1\) and<br>\[<br>x_{n+1}=\sqrt{6+x_n} \quad \text { for } \quad n \in \mathbb{N} \text {. }<br>\]<br>a. (10 pts) Show that the sequence is increasing.<br><br>Solution:<br><ul><li>Assume induction</li><li>\(x_n\) &gt; \(x_{n-1}\)</li><li>\(6+ x_n\) &gt; \(6+x_{n-1} \)<br></li><li>\(\sqrt{6+x_n}\) &gt; \(\sqrt{6+x_{n-1} }\)<br></li><li>\(\sqrt{6+x_n}\) &gt; \(x_n\)</li><li>\(x_{n+1}\) &gt;&nbsp;\(x_n\)<br></li></ul>
-
-============================================================
-
-Note ID: 1719257618509
-  Field: Text
-    Before:
-      e. \((T / F)\) If \(0 \leq a_n \leq 7\) for all \(n \geq 10\), then \(\left(a_n\right)\) has a convergent subsequence.<br><br>Solution:<br><ul><li>Let \(M\) = \(\max\) ( \(\left|a_1\right|,\left|a_2\right|, \ldots,\left|a_9\right|, 8\) )<br></li><li>It follows that&nbsp;\(a_n\) is bounded and that, by Bolzano-Weierstrass, there exists a convergent subsequence</li></ul>
-
-    After:
-      e. \((T / F)\) If \(0 \leq a_n \leq 7\) for all \(n \geq 10\), then \(\left(a_n\right)\) has a convergent subsequence.<br><br>Solution:<br><ul><li>Let \(M\) = \(\max\) ( \(\left|a_1\right|,\left|a_2\right|, \ldots,\left|a_9\right|, 8\) )<br></li><li>It follows that&nbsp;\(a_n\) is bounded and that, by Bolzano-Weierstrass, there exists a convergent subsequence</li></ul>
-
-============================================================
-
-Note ID: 1719271143290
-  Field: Text
-    Before:
-      Problem 3. Let \(a \in \mathbb{R}\) and define the set \(S=\{x \in \mathbb{Q}: x&lt;a\} \subset \mathbb{R}\).<br>a. (5 pts) Prove that \(S\) has no lower bound.<br><br>Solution:<br><ul><li>Suppose S is bounded bellow by q &lt; a for all a</li><li>By archimedean property: there exists M such that -q &lt; M thus -M &lt; q &lt; a&nbsp;</li></ul>
-
-    After:
-      Problem 3. Let \(a \in \mathbb{R}\) and define the set \(S=\{x \in \mathbb{Q}: x&lt;a\} \subset \mathbb{R}\).<br>a. (5 pts) Prove that \(S\) has no lower bound.<br><br>Solution:<br><ul><li>Suppose S is bounded bellow by q &lt; a for all a</li><li>By archimedean property: there exists M such that -q &lt; M thus -M &lt; q &lt; a&nbsp;</li></ul>
-
-============================================================
-
-Note ID: 1719271288398
-  Field: Text
-    Before:
-      Problem 3. Let \(a \in \mathbb{R}\) and define the set \(S=\{x \in \mathbb{Q}: x&lt;a\} \subset \mathbb{R}\).<br><ul><li>b. (10 pts) Prove that \(\sup S=a\).<br></li></ul><div>Solution:</div><div><ul><li>By definition, a is an upper bound</li><li>If a lower upper bound existed, s &lt; a, then by density of rationals in R there would exist s &lt; q &lt;a with q in S, meaning s was not an upper bound</li></ul></div>
-
-    After:
-      Problem 3. Let \(a \in \mathbb{R}\) and define the set \(S=\{x \in \mathbb{Q}: x&lt;a\} \subset \mathbb{R}\).<br><ul><li>b. (10 pts) Prove that \(\sup S=a\).<br></li></ul><div>Solution:</div><div><ul><li>By definition, a is an upper bound</li><li>If a lower upper bound existed, s &lt; a, then by density of rationals in R there would exist s &lt; q &lt;a with q in S, meaning s was not an upper bound</li></ul></div>
-
-============================================================
-
-Note ID: 1719271522415
-  Field: Text
-    Before:
-      c. (10 pts) Find a sequence \(\left(a_n\right)\) such that \(a_n \in S\) where S contains all rational for all \(n \in \mathbb{N}\) and \(\left(a_n\right)\) converges to \(a\).<br><br>Solution:<br><ul><li>Notice that \(a-\frac{1}{n}\) &lt; \(a\) for all \(n \in \mathbb{N}\).<br></li><li>By densitory of rationals:</li><ul><li>\(a-\frac{1}{n}\)&nbsp; &lt; \(q_n\) &lt; \(a\).<br></li></ul><li>Since&nbsp;\(\lim _{n \rightarrow \infty}\) \(a-\frac{1}{n}\) = \(a\) = \(\lim _{n \rightarrow \infty}\) \(a\). by the squeeze theorem&nbsp;\(q_n\) must also converge to &nbsp;\(a\)</li></ul>
-
-    After:
-      c. (10 pts) Find a sequence \(\left(a_n\right)\) such that \(a_n \in S\) where S contains all rational for all \(n \in \mathbb{N}\) and \(\left(a_n\right)\) converges to \(a\).<br><br>Solution:<br><ul><li>Notice that \(a-\frac{1}{n}\) &lt; \(a\) for all \(n \in \mathbb{N}\).<br></li><li>By densitory of rationals:</li><ul><li>\(a-\frac{1}{n}\)&nbsp; &lt; \(q_n\) &lt; \(a\).<br></li></ul><li>Since&nbsp;\(\lim _{n \rightarrow \infty}\) \(a-\frac{1}{n}\) = \(a\) = \(\lim _{n \rightarrow \infty}\) \(a\). by the squeeze theorem&nbsp;\(q_n\) must also converge to &nbsp;\(a\)</li></ul>
-
-============================================================
-
-Note ID: 1719332205081
-  Field: Text
-    Before:
-      Previous guidelines recommended picking the largest vocabulary such that over 95% of tokens appear more than 100 times. However, the optimal baseline does not have this property as only ~80% of tokens appear more than 100 times
-
-    After:
-      Previous guidelines recommended picking the largest vocabulary such that over 95% of tokens appear more than 100 times. However, the optimal baseline does not have this property as only ~80% of tokens appear more than 100 times
-
-============================================================
-
-Note ID: 1719352105793
-  Field: Text
-    Before:
-      Definition 3.1.1 (Random variable). Given an experiment with sample space \(S\), a random variable (r.v.) is a function from the sample space \(S\) to the real numbers \(\mathbb{R}\). It is common, but not required, to denote random variables by capital letters.
-
-    After:
-      Definition 3.1.1 (Random variable). Given an experiment with sample space \(S\), a random variable (r.v.) is a function from the sample space \(S\) to the real numbers \(\mathbb{R}\). It is common, but not required, to denote random variables by capital letters.
-
-============================================================
-
-Note ID: 1719352509617
-  Field: Text
-    Before:
-      As we've mentioned earlier, the source of the randomness in a random variable is the experiment itself, in which a sample outcome \(s \in S\) is chosen according to a probability function \(P\). Before we perform the experiment, the outcome \(s\) has not yet been realized, so we don't know the value of \(X\), though we could calculate the probability that \(X\) will take on a given value or range of values. After we perform the experiment and the outcome \(s\) has been realized, the random variable crystallizes into the numerical value \(X(s)\).
-
-    After:
-      As we've mentioned earlier, the source of the randomness in a random variable is the experiment itself, in which a sample outcome \(s \in S\) is chosen according to a probability function \(P\). Before we perform the experiment, the outcome \(s\) has not yet been realized, so we don't know the value of \(X\), though we could calculate the probability that \(X\) will take on a given value or range of values. After we perform the experiment and the outcome \(s\) has been realized, the random variable crystallizes into the numerical value \(X(s)\).
-
-============================================================
-
-Note ID: 1719475687388
-  Field: Text
-    Before:
-      Definition 3.2.1 (Discrete random variable). A random variable \(X\) is said to be discrete if there is a finite list of values \(a_{1}, a_{2}, \ldots, a_{n}\) or an infinite list of values \(a_{1}, a_{2}, \ldots\) such that \(P\left(X=a_{j}\right.\) for some \(\left.j\right)\) = \(1\). <br><br>If \(X\) is a discrete r.v., then the&nbsp; finite or countably infinite set of values \(x\) such that \(P(X=x)\) &gt;\(0\) is called the support of \(X\).
-
-    After:
-      Definition 3.2.1 (Discrete random variable). A random variable \(X\) is said to be discrete if there is a finite list of values \(a_{1}, a_{2}, \ldots, a_{n}\) or an infinite list of values \(a_{1}, a_{2}, \ldots\) such that \(P\left(X=a_{j}\right.\) for some \(\left.j\right)\) = \(1\). <br><br>If \(X\) is a discrete r.v., then the&nbsp; finite or countably infinite set of values \(x\) such that \(P(X=x)\) &gt;\(0\) is called the support of \(X\).
-
-============================================================
-
-Note ID: 1719476271531
-  Field: Text
-    Before:
-      3.2.3. In writing \(P(X=x)\), we are using \(X=x\) to denote an event, consisting of all outcomes \(s\) to which \(X\) assigns the number \(x\). This event is also written as \(\{X=x\}\); formally, \(\{X=x\}\) is defined as \(\{s \in S: X(s)=x\}\), but writing \(\{X=x\}\) is shorter and more intuitive.
-
-    After:
-      3.2.3. In writing \(P(X=x)\), we are using \(X=x\) to denote an event, consisting of all outcomes \(s\) to which \(X\) assigns the number \(x\). This event is also written as \(\{X=x\}\); formally, \(\{X=x\}\) is defined as \(\{s \in S: X(s)=x\}\), but writing \(\{X=x\}\) is shorter and more intuitive.
-
-============================================================
-
-Note ID: 1719476708184
-  Field: Text
-    Before:
-      <img src="FH90eCriWUT2kprWenSQSUhyHsojoq8VZMDsPK2jY1E.original.fullsize.png"><img src="FH90eCriWUT2kprWenSQSUhyHsojoq8VZMDsPK2jY1E.original.fullsize.png"><img src="FH90eCriWUT2kprWenSQSUhyHsojoq8VZMDsPK2jY1E.original.fullsize.png"><br><br>FIGURE 3.3<br><br>Left to right: PMFs of \(X, Y\), and \(I\), with \(X\) the number of Heads in two fair coin tosses, \(Y\) the number of Tails, and \(I\) the indicator of Heads on the first toss.
-
-    After:
-      <img src="FH90eCriWUT2kprWenSQSUhyHsojoq8VZMDsPK2jY1E.original.fullsize.png"><img src="FH90eCriWUT2kprWenSQSUhyHsojoq8VZMDsPK2jY1E.original.fullsize.png"><img src="FH90eCriWUT2kprWenSQSUhyHsojoq8VZMDsPK2jY1E.original.fullsize.png"><br><br>FIGURE 3.3<br><br>Left to right: PMFs of \(X, Y\), and \(I\), with \(X\) the number of Heads in two fair coin tosses, \(Y\) the number of Tails, and \(I\) the indicator of Heads on the first toss.
-
-============================================================
-
-Note ID: 1719481475568
-  Field: Text
-    Before:
-      Theorem 3.2.7 (Valid PMFs). Let \(X\) be a discrete r.v. with support \(x_{1}, x_{2}, \ldots\) (assume these values are distinct and, for notational simplicity, that the support is countably infinite; the analogous results hold if the support is finite). The PMF \(p_{X}\) of \(X\) must satisfy the following two criteria:<br><br><ul><li>- Nonnegative: \(p_{X}(x)&gt;0\) if \(x=x_{j}\) for some \(j\), and \(p_{X}(x)=0\) otherwise;</li><li>- Sums to 1: \(\sum_{j=1}^{\infty} p_{X}\left(x_{j}\right)=1\).</li></ul>
-
-    After:
-      Theorem 3.2.7 (Valid PMFs). Let \(X\) be a discrete r.v. with support \(x_{1}, x_{2}, \ldots\) (assume these values are distinct and, for notational simplicity, that the support is countably infinite; the analogous results hold if the support is finite). The PMF \(p_{X}\) of \(X\) must satisfy the following two criteria:<br><br><ul><li>- Nonnegative: \(p_{X}(x)&gt;0\) if \(x=x_{j}\) for some \(j\), and \(p_{X}(x)=0\) otherwise;</li><li>- Sums to 1: \(\sum_{j=1}^{\infty} p_{X}\left(x_{j}\right)=1\).</li></ul>
-
-============================================================
-
-Note ID: 1719482669530
-  Field: Text
-    Before:
-      Definition 3.3.1 (Bernoulli distribution). An r.v. \(X\) is said to have the Bernoulli distribution with parameter \(p\) if P\((X=1)\) = \(p\) and P\((X=0)\) = \(1-p\), where \(0\) &lt; \(p\) &lt; \(1\). We write this as \(X \sim\)&nbsp; \(\operatorname{Bern}(p)\). The symbol \(\sim\) is read "is distributed as".
-
-    After:
-      Definition 3.3.1 (Bernoulli distribution). An r.v. \(X\) is said to have the Bernoulli distribution with parameter \(p\) if P\((X=1)\) = \(p\) and P\((X=0)\) = \(1-p\), where \(0\) &lt; \(p\) &lt; \(1\). We write this as \(X \sim\)&nbsp; \(\operatorname{Bern}(p)\). The symbol \(\sim\) is read "is distributed as".
-
-============================================================
-
-Note ID: 1719482722444
-  Field: Text
-    Before:
-      Any r.v. whose possible values are 0 and 1 has a \(\operatorname{Bern}(p)\) distribution, with \(p\) the probability of the r.v. equaling 1 . This number \(p\) in \(\operatorname{Bern}(p)\) is called the parameter of the distribution; it determines which specific Bernoulli distribution we have.
-
-    After:
-      Any r.v. whose possible values are 0 and 1 has a \(\operatorname{Bern}(p)\) distribution, with \(p\) the probability of the r.v. equaling 1 . This number \(p\) in \(\operatorname{Bern}(p)\) is called the parameter of the distribution; it determines which specific Bernoulli distribution we have.
-
-============================================================
-
-Note ID: 1719487769397
-  Field: Text
-    Before:
-      Definition 3.3.2 (Indicator random variable). The indicator random variable of an event \(A\) is the r.v. which equals 1 if \(A\) occurs and 0 otherwise. We will denote the indicator r.v. of \(A\) by \(I_{A}\) or \(I(A)\). Note that \(I_{A}\) \(\sim \) \(\operatorname{Bern}(p)\) with \(p\)=\(P(A)\).
-
-    After:
-      Definition 3.3.2 (Indicator random variable). The indicator random variable of an event \(A\) is the r.v. which equals 1 if \(A\) occurs and 0 otherwise. We will denote the indicator r.v. of \(A\) by \(I_{A}\) or \(I(A)\). Note that \(I_{A}\) \(\sim \) \(\operatorname{Bern}(p)\) with \(p\)=\(P(A)\).
-
-============================================================
-
-Note ID: 1719487871529
-  Field: Text
-    Before:
-      Story 3.3.3 (Bernoulli trial). An experiment that can result in either a "success" or a "failure" (but not both) is called a Bernoulli trial. A Bernoulli random variable can be thought of as the indicator of success in a Bernoulli trial: it equals 1 if success occurs and 0 if failure occurs in the trial.
-
-    After:
-      Story 3.3.3 (Bernoulli trial). An experiment that can result in either a "success" or a "failure" (but not both) is called a Bernoulli trial. A Bernoulli random variable can be thought of as the indicator of success in a Bernoulli trial: it equals 1 if success occurs and 0 if failure occurs in the trial.
-
-============================================================
-
-Note ID: 1719488129019
-  Field: Text
-    Before:
-      Story 3.3.4 (Binomial distribution). Suppose that \(n\) independent Bernoulli trials are performed, each with the same success probability \(p\). Let \(X\) be the number of successes. The distribution of \(X\) is called the Binomial distribution with parameters \(n\) and \(p\). We write \(X\) \(\sim\) \(\operatorname{Bin}(n, p)\) to mean that \(X\) has the Binomial distribution with parameters \(n\) and \(p\), where \(n\) is a positive integer and \(0\)&lt;\(p\)&lt;\(1\).
-
-    After:
-      Story 3.3.4 (Binomial distribution). Suppose that \(n\) independent Bernoulli trials are performed, each with the same success probability \(p\). Let \(X\) be the number of successes. The distribution of \(X\) is called the Binomial distribution with parameters \(n\) and \(p\). We write \(X\) \(\sim\) \(\operatorname{Bin}(n, p)\) to mean that \(X\) has the Binomial distribution with parameters \(n\) and \(p\), where \(n\) is a positive integer and \(0\)&lt;\(p\)&lt;\(1\).
-
-============================================================
-
-Note ID: 1719488358863
-  Field: Text
-    Before:
-      3.3.6. To save writing, it is often left implicit that a PMF is zero wherever it is not specified to be nonzero, but in any case it is important to understand what the support of a random variable is, and good practice to check that PMFs are valid. If two discrete r.v.s have the same PMF, then they also must have the same support. So we sometimes refer to the support of a discrete distribution; this is the support of any r.v. with that distribution.
-
-    After:
-      3.3.6. To save writing, it is often left implicit that a PMF is zero wherever it is not specified to be nonzero, but in any case it is important to understand what the support of a random variable is, and good practice to check that PMFs are valid. If two discrete r.v.s have the same PMF, then they also must have the same support. So we sometimes refer to the support of a discrete distribution; this is the support of any r.v. with that distribution.
-
-============================================================
-
-Note ID: 1719489492663
-  Field: Text
-    Before:
-      We've used Story 3.3.4 to find the \(\operatorname{Bin}(n, p)\) PMF. The story also gives us a straightforward proof of the fact that if \(X\) is Binomial, then \(n-X\) is also Binomial.
-
-    After:
-      We've used Story 3.3.4 to find the \(\operatorname{Bin}(n, p)\) PMF. The story also gives us a straightforward proof of the fact that if \(X\) is Binomial, then \(n-X\) is also Binomial.
-
-============================================================
-
-Note ID: 1719489861449
-  Field: Text
-    Before:
-      Theorem 3.3.7. Let \(X \sim \operatorname{Bin}(n, p)\), and \(q=1-p\) (we often use \(q\) to denote the failure probability of a Bernoulli trial). Then \(n-X \sim \operatorname{Bin}(n, q)\).<br><br>Proof. Using the story of the Binomial, interpret \(X\) as the number of successes in \(n\) independent Bernoulli trials. Then \(n-X\) is the number of failures in those trials. Interchanging the roles of success and failure, we have \(n-X \sim\) \(\operatorname{Bin}(n, q)\). Alternatively, we can check that \(n-X\) has the \(\operatorname{Bin}(n, q)\) PMF. Let \(Y=n-X\). The PMF of \(Y\) is<br><br><ul><li>\(P(Y=k)\)&nbsp;<br></li><li>= \(P(X=n-k)\)</li><li>=&nbsp;\(\binom{n}{n-k}\)&nbsp;\(p^{n-k}\)&nbsp;\(q^k\)</li><li>=&nbsp;\(\binom{n}{k}\)&nbsp;\(q^k\)&nbsp;\(p^{n-k}\)</li></ul>
-
-    After:
-      Theorem 3.3.7. Let \(X \sim \operatorname{Bin}(n, p)\), and \(q=1-p\) (we often use \(q\) to denote the failure probability of a Bernoulli trial). Then \(n-X \sim \operatorname{Bin}(n, q)\).<br><br>Proof. Using the story of the Binomial, interpret \(X\) as the number of successes in \(n\) independent Bernoulli trials. Then \(n-X\) is the number of failures in those trials. Interchanging the roles of success and failure, we have \(n-X \sim\) \(\operatorname{Bin}(n, q)\). Alternatively, we can check that \(n-X\) has the \(\operatorname{Bin}(n, q)\) PMF. Let \(Y=n-X\). The PMF of \(Y\) is<br><br><ul><li>\(P(Y=k)\)&nbsp;<br></li><li>= \(P(X=n-k)\)</li><li>=&nbsp;\(\binom{n}{n-k}\)&nbsp;\(p^{n-k}\)&nbsp;\(q^k\)</li><li>=&nbsp;\(\binom{n}{k}\)&nbsp;\(q^k\)&nbsp;\(p^{n-k}\)</li></ul>
-
-============================================================
-
-Note ID: 1719669734121
-  Field: Text
-    Before:
-      Which ordered fileds have the least upper bound property?<br><ul><li>R and fields isomporphic to R</li></ul>
-
-    After:
-      Which ordered fileds have the least upper bound property?<br><ul><li>R and fields isomporphic to R</li></ul>
-
-============================================================
-
-Note ID: 1719679351974
-  Field: Text
-    Before:
-      \(A\) \(\subset\) \(B\)&nbsp;\(\implies\)&nbsp;\(\sup\) \(A \) \(&lt;\) \(\sup\) \(B\)
-
-    After:
-      \(A\) \(\subset\) \(B\)&nbsp;\(\implies\)&nbsp;\(\sup\) \(A \) \(&lt;\) \(\sup\) \(B\)
-
-============================================================
-
-Note ID: 1719679578911
-  Field: Text
-    Before:
-      To show that&nbsp;\(\sup\) \(A \) &nbsp;\(\leq\)&nbsp;\(\sup\) \(B\), you need to show that&nbsp;\(\forall\)::quant::quant::quant \(a \in A\)&nbsp; \(\exists\)::quant::quant::quant \(b \in B, s.t\)&nbsp;\(a\)&nbsp;\(\leq\)&nbsp;\(b\)
-
-    After:
-      To show that&nbsp;\(\sup\) \(A \) &nbsp;\(\leq\)&nbsp;\(\sup\) \(B\), you need to show that&nbsp;\(\forall\)::quant::quant::quant \(a \in A\)&nbsp; \(\exists\)::quant::quant::quant \(b \in B, s.t\)&nbsp;\(a\)&nbsp;\(\leq\)&nbsp;\(b\)
-
-============================================================
-
-Note ID: 1719687156322
-  Field: Text
-    Before:
-      <div>How can we extend addition to the extended reals?</div><div><ul><li>\(x\)&nbsp;\(+\)&nbsp;\(\infty\) =&nbsp;\(\infty\)<br></li><li>\(x\)&nbsp;\(+\)&nbsp;\(-\infty\) =&nbsp;\(-\infty\)</li></ul></div>
-
-    After:
-      <div>How can we extend addition to the extended reals?</div><div><ul><li>\(x\)&nbsp;\(+\)&nbsp;\(\infty\) =&nbsp;\(\infty\)<br></li><li>\(x\)&nbsp;\(+\)&nbsp;\(-\infty\) =&nbsp;\(-\infty\)</li></ul></div>
-
-============================================================
-
-Note ID: 1719687225682
-  Field: Text
-    Before:
-      <div>How can we extend multiplication to the extended reals?</div><div><ul><li>\(\forall x &gt; 0\)::quant+condition::quant+condition::quant+condition::quant+condition\(x\)&nbsp;\(\times\)&nbsp;\(\infty\) =&nbsp;\(\infty\)<br></li><li>\(\forall x &lt; 0\)::quant+condition::quant+condition::quant+condition::quant+condition \(x\)&nbsp;\(\times\)&nbsp;\(\infty\) =&nbsp;\(\infty\)</li></ul></div>
-
-    After:
-      <div>How can we extend multiplication to the extended reals?</div><div><ul><li>\(\forall x &gt; 0\)::quant+condition::quant+condition::quant+condition::quant+condition\(x\)&nbsp;\(\times\)&nbsp;\(\infty\) =&nbsp;\(\infty\)<br></li><li>\(\forall x &lt; 0\)::quant+condition::quant+condition::quant+condition::quant+condition \(x\)&nbsp;\(\times\)&nbsp;\(\infty\) =&nbsp;\(\infty\)</li></ul></div>
-
-============================================================
-
-Note ID: 1719687316283
-  Field: Text
-    Before:
-      What are the undefined operations of the extended reals?<br><ul><li>0 times infinity</li><li>infinity minus infinity&nbsp;</li><li>e.t.c</li></ul>
-
-    After:
-      What are the undefined operations of the extended reals?<br><ul><li>0 times infinity</li><li>infinity minus infinity&nbsp;</li><li>e.t.c</li></ul>
-
-============================================================
-
-Note ID: 1719871829023
-  Field: Text
-    Before:
-      If we have an urn filled with \(w\) white and \(b\) black balls, then drawing \(n\) balls out of the urn with replacement yields a \(\operatorname{Bin}(n, w /(w+b))\) distribution. If we instead sample without replacement, as illustrated in Figure 3.7, then the number of white balls follows a Hypergeometric distribution.
-
-    After:
-      If we have an urn filled with \(w\) white and \(b\) black balls, then drawing \(n\) balls out of the urn with replacement yields a \(\operatorname{Bin}(n, w /(w+b))\) distribution. If we instead sample without replacement, as illustrated in Figure 3.7, then the number of white balls follows a Hypergeometric distribution.
-
-============================================================
-
-Note ID: 1719872135660
-  Field: Text
-    Before:
-      <img src="paste-51727ddd92009de6b344b6e8ad4a4c89c511a354.jpg"><br>\section*{FIGURE 3.7}<br>Hypergeometric story. An urn contains \(w=6\) white balls and \(b=4\) black balls. We sample \(n=5\) without replacement. The number \(X\) of white balls in the sample is Hypergeometric; here we observe \(X\) = \(3\).
-
-    After:
-      <img src="paste-51727ddd92009de6b344b6e8ad4a4c89c511a354.jpg"><br>\section*{FIGURE 3.7}<br>Hypergeometric story. An urn contains \(w=6\) white balls and \(b=4\) black balls. We sample \(n=5\) without replacement. The number \(X\) of white balls in the sample is Hypergeometric; here we observe \(X\) = \(3\).
-
-============================================================
-
-Note ID: 1719872787940
-  Field: Text
-    Before:
-      Theorem 3.4.2 (Hypergeometric PMF). If \(X \sim\) \( \operatorname{HGeom}(w, b, n)\), then the PMF of \(X\) is<br><br><ul><li>\(P(X=k)\) =&nbsp;\(\binom{w}{k}\)&nbsp;\(\binom{b}{n-k}\)&nbsp;\(/\)&nbsp;\(\binom{w+b}{n}\)<br></li></ul><br>for integers \(k\) satisfying \(0\) \(\leq\) k \(\leq\) \(w\) and \(0\) \(\leq\) n-k \(\leq\) \(b\), and \(P(X=k)\) = \(0\) otherwise.<br>
-
-    After:
-      Theorem 3.4.2 (Hypergeometric PMF). If \(X \sim\) \( \operatorname{HGeom}(w, b, n)\), then the PMF of \(X\) is<br><br><ul><li>\(P(X=k)\) =&nbsp;\(\binom{w}{k}\)&nbsp;\(\binom{b}{n-k}\)&nbsp;\(/\)&nbsp;\(\binom{w+b}{n}\)<br></li></ul><br>for integers \(k\) satisfying \(0\) \(\leq\) k \(\leq\) \(w\) and \(0\) \(\leq\) n-k \(\leq\) \(b\), and \(P(X=k)\) = \(0\) otherwise.<br>
-
-============================================================
-
-Note ID: 1719873099165
-  Field: Text
-    Before:
-      The Hypergeometric distribution comes up in many scenarios which, on the surface, have little in common with white and black balls in an urn. The essential structure of the Hypergeometric story is that items in a population are classified using two sets of tags: in the urn story, each ball is either white or black (this is the first set of tags), and each ball is either sampled or not sampled (this is the second set of tags). Furthermore, at least one of these sets of tags is assigned completely at random (in the urn story, the balls are sampled randomly, with all sets of the correct size equally likely). Then \(X \sim \operatorname{HGeom}(w, b, n)\) represents the number of twice-tagged items: in the urn story, balls that are both white and sampled.
-
-    After:
-      The Hypergeometric distribution comes up in many scenarios which, on the surface, have little in common with white and black balls in an urn. The essential structure of the Hypergeometric story is that items in a population are classified using two sets of tags: in the urn story, each ball is either white or black (this is the first set of tags), and each ball is either sampled or not sampled (this is the second set of tags). Furthermore, at least one of these sets of tags is assigned completely at random (in the urn story, the balls are sampled randomly, with all sets of the correct size equally likely). Then \(X \sim \operatorname{HGeom}(w, b, n)\) represents the number of twice-tagged items: in the urn story, balls that are both white and sampled.
-
-============================================================
-
-Note ID: 1719904659048
-  Field: Text
-    Before:
-      Theorem 3.4.5. The \(\operatorname{HGeom}(w, b, n)\) and \(\operatorname{HGeom}(n, w+b-n, w)\) distributions are identical. That is, if \(X \sim\) \(\operatorname{HGeom}(w, b, n)\) and \(Y \sim\) \(\operatorname{HGeom}(n, w+b-n, w)\), then \(X\) and \(Y\) have the same distribution.
-
-    After:
-      Theorem 3.4.5. The \(\operatorname{HGeom}(w, b, n)\) and \(\operatorname{HGeom}(n, w+b-n, w)\) distributions are identical. That is, if \(X \sim\) \(\operatorname{HGeom}(w, b, n)\) and \(Y \sim\) \(\operatorname{HGeom}(n, w+b-n, w)\), then \(X\) and \(Y\) have the same distribution.
-
-============================================================
-
-Note ID: 1719904802313
-  Field: Text
-    Before:
-      Theorem 3.4.5. The \(\operatorname{HGeom}(w, b, n)\) and \(\operatorname{HGeom}(n, w+b-n, w)\) distributions are identical. That is, if \(X \sim \operatorname{HGeom}(w, b, n)\) and \(Y \sim \operatorname{HGeom}(n, w+b-n, w)\), then \(X\) and \(Y\) have the same distribution.<br><br>Proof. Using the story of the Hypergeometric, imagine an urn with \(w\) white balls, \(b\) black balls, and a sample of size \(n\) made without replacement. Let \(X \sim\) \(\operatorname{HGeom}(w, b, n)\) be the number of white balls in the sample, thinking of white/black as the first set of tags and sampled/not sampled as the second set of tags. Let \(Y \sim \operatorname{HGeom}(n, w+b-n, w)\) be the number of sampled balls among the white balls, thinking of sampled/not sampled as the first set of tags and white/black as&nbsp;<br>the second set of tags. Both \(X\) and \(Y\) count the number of white sampled balls, so they have the same distribution.
-
-    After:
-      Theorem 3.4.5. The \(\operatorname{HGeom}(w, b, n)\) and \(\operatorname{HGeom}(n, w+b-n, w)\) distributions are identical. That is, if \(X \sim \operatorname{HGeom}(w, b, n)\) and \(Y \sim \operatorname{HGeom}(n, w+b-n, w)\), then \(X\) and \(Y\) have the same distribution.<br><br>Proof. Using the story of the Hypergeometric, imagine an urn with \(w\) white balls, \(b\) black balls, and a sample of size \(n\) made without replacement. Let \(X \sim\) \(\operatorname{HGeom}(w, b, n)\) be the number of white balls in the sample, thinking of white/black as the first set of tags and sampled/not sampled as the second set of tags. Let \(Y \sim \operatorname{HGeom}(n, w+b-n, w)\) be the number of sampled balls among the white balls, thinking of sampled/not sampled as the first set of tags and white/black as&nbsp;<br>the second set of tags. Both \(X\) and \(Y\) count the number of white sampled balls, so they have the same distribution.
-
-============================================================
-
-Note ID: 1719904891984
-  Field: Text
-    Before:
-      3.4.6 (Binomial vs. Hypergeometric). The Binomial and Hypergeometric distributions are often confused. Both are discrete distributions taking on integer values between 0 and \(n\) for some \(n\), and both can be interpreted as the number of successes in \(n\) Bernoulli trials (for the Hypergeometric, each tagged elk in the recaptured sample can be considered a success and each untagged elk a failure). However, a crucial part of the Binomial story is that the Bernoulli trials involved are independent. The Bernoulli trials in the Hypergeometric story are dependent, since the sampling is done without replacement: knowing that one elk in our sample is tagged decreases the probability that the second elk will also be tagged.
-
-    After:
-      3.4.6 (Binomial vs. Hypergeometric). The Binomial and Hypergeometric distributions are often confused. Both are discrete distributions taking on integer values between 0 and \(n\) for some \(n\), and both can be interpreted as the number of successes in \(n\) Bernoulli trials (for the Hypergeometric, each tagged elk in the recaptured sample can be considered a success and each untagged elk a failure). However, a crucial part of the Binomial story is that the Bernoulli trials involved are independent. The Bernoulli trials in the Hypergeometric story are dependent, since the sampling is done without replacement: knowing that one elk in our sample is tagged decreases the probability that the second elk will also be tagged.
-
-============================================================
-
-Note ID: 1720892622031
-  Field: Text
-    Before:
-      Every infinite subset of a countable set is countable
-
-    After:
-      Every infinite subset of a countable set is countable
-
-============================================================
-
-Note ID: 1720892948837
-  Field: Text
-    Before:
-      Proof that every infinite subset E of a countable set&nbsp; A is countable:<br><ul><li>A can be written as a sequence&nbsp;\(\{x_1,x_2, \cdots,&nbsp; \}\)</li><li>Let&nbsp;\(n_1\) =&nbsp;\(\inf\) \(\{ i: x_i \in E\}\), which exists due to the well-ordering principle applied over the indicies</li><li>Let&nbsp;\(n_2\) = \(\inf\) \(\{ i: x_i \in E, i&gt;n_1\} \)</li><li>Let&nbsp;&nbsp;\(n_k\) = \(\inf\) \(\{ i: x_i \in E, i&gt;n_{k-1}\} \)</li></ul>
-
-    After:
-      Proof that every infinite subset E of a countable set&nbsp; A is countable:<br><ul><li>A can be written as a sequence&nbsp;\(\{x_1,x_2, \cdots,&nbsp; \}\)</li><li>Let&nbsp;\(n_1\) =&nbsp;\(\inf\) \(\{ i: x_i \in E\}\), which exists due to the well-ordering principle applied over the indicies</li><li>Let&nbsp;\(n_2\) = \(\inf\) \(\{ i: x_i \in E, i&gt;n_1\} \)</li><li>Let&nbsp;&nbsp;\(n_k\) = \(\inf\) \(\{ i: x_i \in E, i&gt;n_{k-1}\} \)</li></ul>
-
-============================================================
-
-Note ID: 1720893343279
-  Field: Text
-    Before:
-      If A is countable then&nbsp;\(A\)&nbsp;\(\times\)&nbsp;\(A\) is countable
-
-    After:
-      If A is countable then&nbsp;\(A\)&nbsp;\(\times\)&nbsp;\(A\) is countable
-
-============================================================
-
-Note ID: 1720894970515
-  Field: Text
-    Before:
-      A natural question to ask is whether the theorems we have proved about sequences, series, and functions in \(\mathbf{R}\) have analogues in the plane \(\mathbf{R}^{2}\) or in even higher dimensions. Looking back over the proofs, one crucial observation is that most of the arguments depend on just a few basic properties of the absolute value function. Interpreting the statement " \(|x-y|\) " to mean the "distance from \(x\) to \(y\) in \(\mathbf{R}\)," our aim is to experiment with other ways of measuring distance on other sets such as \(\mathbf{R}^{2}\) and \(C[0,1]\), the space of continuous functions on \([0,1]\).
-
-    After:
-      A natural question to ask is whether the theorems we have proved about sequences, series, and functions in \(\mathbf{R}\) have analogues in the plane \(\mathbf{R}^{2}\) or in even higher dimensions. Looking back over the proofs, one crucial observation is that most of the arguments depend on just a few basic properties of the absolute value function. Interpreting the statement " \(|x-y|\) " to mean the "distance from \(x\) to \(y\) in \(\mathbf{R}\)," our aim is to experiment with other ways of measuring distance on other sets such as \(\mathbf{R}^{2}\) and \(C[0,1]\), the space of continuous functions on \([0,1]\).
-
-============================================================
-
-Note ID: 1720895112796
-  Field: Text
-    Before:
-      Definition 8.2.1. Given a set \(X\), a function \(d: X \times X \rightarrow \mathbf{R}\) is a metric on \(X\) if for all \(x, y \in X\) :<br><br><ul><li>(i) \(d(x, y)\)&nbsp; \(\geq\) \(0\) with \(d(x, y)\) = \(0\) if and only if \(x\) = \(y\),</li></ul>
-
-    After:
-      Definition 8.2.1. Given a set \(X\), a function \(d: X \times X \rightarrow \mathbf{R}\) is a metric on \(X\) if for all \(x, y \in X\) :<br><br><ul><li>(i) \(d(x, y)\)&nbsp; \(\geq\) \(0\) with \(d(x, y)\) = \(0\) if and only if \(x\) = \(y\),</li></ul>
-
-============================================================
-
-Note ID: 1720895163043
-  Field: Text
-    Before:
-      Definition 8.2.1. Given a set \(X\), a function \(d: X \times X \rightarrow \mathbf{R}\) is a metric on \(X\) if for all \(x, y \in X\) :<br><br><ul><li>(i) \(d(x, y)\)&nbsp; \(\geq\) \(0\) with \(d(x, y)\) = \(0\) if and only if \(x\) = \(y\),</li><li>(ii) \(d(x, y)\) = \(d(y, x)\), and</li><li>(iii) for all \(z \in X\), \(d(x, y)\) \(\leq\) \(d(x, z)\)+\(d(z, y)\).</li></ul>
-
-    After:
-      Definition 8.2.1. Given a set \(X\), a function \(d: X \times X \rightarrow \mathbf{R}\) is a metric on \(X\) if for all \(x, y \in X\) :<br><br><ul><li>(i) \(d(x, y)\)&nbsp; \(\geq\) \(0\) with \(d(x, y)\) = \(0\) if and only if \(x\) = \(y\),</li><li>(ii) \(d(x, y)\) = \(d(y, x)\), and</li><li>(iii) for all \(z \in X\), \(d(x, y)\) \(\leq\) \(d(x, z)\)+\(d(z, y)\).</li></ul>
-
-============================================================
-
-Note ID: 1720895443934
-  Field: Text
-    Before:
-      Definition 8.2.2. Let \((X, d)\) be a metric space. A sequence \(\left(x_{n}\right) \subseteq X\) converges to an element \(x \in X\) if for all \(\epsilon\) \(&gt;0\) there exists an \(N \in \mathbf{N}\) such that \(d\left(x_{n}, x\right)\) \(&lt;\) \(\epsilon\) whenever \(n\) \(\geq N\).
-
-    After:
-      Definition 8.2.2. Let \((X, d)\) be a metric space. A sequence \(\left(x_{n}\right) \subseteq X\) converges to an element \(x \in X\) if for all \(\epsilon\) \(&gt;0\) there exists an \(N \in \mathbf{N}\) such that \(d\left(x_{n}, x\right)\) \(&lt;\) \(\epsilon\) whenever \(n\) \(\geq N\).
-
-============================================================
-
-Note ID: 1720895593420
-  Field: Text
-    Before:
-      Definition 8.2.3. A sequence \(\left(x_{n}\right)\) in a metric space \((X, d)\) is a Cauchy sequence if for all \(\epsilon\) \(&gt;0\) there exists an \(N \in \mathbf{N}\) such that \(d\) \(\left(x_{m}, x_{n}\right)\) \(&lt;\) \(\epsilon\) whenever \(m, n\) \(\geq N\).
-
-    After:
-      Definition 8.2.3. A sequence \(\left(x_{n}\right)\) in a metric space \((X, d)\) is a Cauchy sequence if for all \(\epsilon\) \(&gt;0\) there exists an \(N \in \mathbf{N}\) such that \(d\) \(\left(x_{m}, x_{n}\right)\) \(&lt;\) \(\epsilon\) whenever \(m, n\) \(\geq N\).
-
-============================================================
-
-Note ID: 1720896938838
-  Field: Text
-    Before:
-      The Cauchy Criterion, as it is called in \(\mathbf{R}\), was an "if and only if" statement. In the general metric space setting, however, the converse statement does not always hold. Recall that, in \(\mathbf{R}\), the assertion that "Cauchy sequences converge" was shown to be equivalent to the Axiom of Completeness. In order to transport the Axiom of Completeness into a metric space, we would need to have an ordering on our space so that we could discuss such things as upper bounds. It is an interesting observation that not every set can be ordered in a satisfying way (the points in \(\mathbf{R}^{2}\) for example). Even without an ordering, we are still going to want completeness. For metric spaces, the convergence of Cauchy sequences is taken to be the definition of completeness.
-
-    After:
-      The Cauchy Criterion, as it is called in \(\mathbf{R}\), was an "if and only if" statement. In the general metric space setting, however, the converse statement does not always hold. Recall that, in \(\mathbf{R}\), the assertion that "Cauchy sequences converge" was shown to be equivalent to the Axiom of Completeness. In order to transport the Axiom of Completeness into a metric space, we would need to have an ordering on our space so that we could discuss such things as upper bounds. It is an interesting observation that not every set can be ordered in a satisfying way (the points in \(\mathbf{R}^{2}\) for example). Even without an ordering, we are still going to want completeness. For metric spaces, the convergence of Cauchy sequences is taken to be the definition of completeness.
-
-============================================================
-
-Note ID: 1720897334381
-  Field: Text
-    Before:
-      Definition 8.2.5. Let \(\left(X, d_{1}\right)\) and \(\left(Y, d_{2}\right)\) be metric spaces. A function \(f\) : \(X \rightarrow Y\) is continuous at \(x \in X\) if for all \(\epsilon\) \(&gt;0\) there exists a \(\delta\) \(&gt;0\) such that \(d_{2}(f(x), f(y))\) \(&lt;\) \(\epsilon\) whenever \(d_{1}(x, y)\) \(&lt;\) \(\delta\).
-
-    After:
-      Definition 8.2.5. Let \(\left(X, d_{1}\right)\) and \(\left(Y, d_{2}\right)\) be metric spaces. A function \(f\) : \(X \rightarrow Y\) is continuous at \(x \in X\) if for all \(\epsilon\) \(&gt;0\) there exists a \(\delta\) \(&gt;0\) such that \(d_{2}(f(x), f(y))\) \(&lt;\) \(\epsilon\) whenever \(d_{1}(x, y)\) \(&lt;\) \(\delta\).
-
-============================================================
-
-Note ID: 1720897439006
-  Field: Text
-    Before:
-      Definition 8.2.6. Given \(\epsilon\) \(&gt;0\) and an element \(x\) in the metric space \((X, d)\), the \(\epsilon\)-neighborhood of \(x\) is the set \(V_{\epsilon}(x)\) = \(\{y \in X: d(x, y)&lt;\epsilon\}\).
-
-    After:
-      Definition 8.2.6. Given \(\epsilon\) \(&gt;0\) and an element \(x\) in the metric space \((X, d)\), the \(\epsilon\)-neighborhood of \(x\) is the set \(V_{\epsilon}(x)\) = \(\{y \in X: d(x, y)&lt;\epsilon\}\).
-
-============================================================
-
-Note ID: 1720897532706
-  Field: Text
-    Before:
-      With the definition of an \(\epsilon\)-neighborhood, we can now define open sets, limit points, and closed sets exactly as we did before. A set \(O \subseteq X\) is open if for every \(x \in O\) we can find a neighborhood \(V_{\epsilon}(x)\) \(\subseteq O\). A point \(x\) is a limit point of a set \(A\) if every \(V_{\epsilon}(x)\) intersects \(A\) in some point other than \(x\). A set \(C\) is closed if it contains its limit points.
-
-    After:
-      With the definition of an \(\epsilon\)-neighborhood, we can now define open sets, limit points, and closed sets exactly as we did before. A set \(O \subseteq X\) is open if for every \(x \in O\) we can find a neighborhood \(V_{\epsilon}(x)\) \(\subseteq O\). A point \(x\) is a limit point of a set \(A\) if every \(V_{\epsilon}(x)\) intersects \(A\) in some point other than \(x\). A set \(C\) is closed if it contains its limit points.
-
-============================================================
-
-Note ID: 1720897603031
-  Field: Text
-    Before:
-      Definition 8.2.7. A subset \(K\) of a metric space \((X, d)\) is compact if every sequence in \(K\) has a convergent subsequence that converges to a limit in \(K\).
-
-    After:
-      Definition 8.2.7. A subset \(K\) of a metric space \((X, d)\) is compact if every sequence in \(K\) has a convergent subsequence that converges to a limit in \(K\).
-
-============================================================
-
-Note ID: 1720897903222
-  Field: Text
-    Before:
-      Definition 8.2.9. A set \(A \subseteq X\) is dense in the metric space \((X, d)\) if \(\bar{A}\) = \(X\). A subset \(E\) of a metric space \((X, d)\) is nowhere-dense in \(X\) if \(\bar{E}^{\circ}\) is empty.
-
-    After:
-      Definition 8.2.9. A set \(A \subseteq X\) is dense in the metric space \((X, d)\) if \(\bar{A}\) = \(X\). A subset \(E\) of a metric space \((X, d)\) is nowhere-dense in \(X\) if \(\bar{E}^{\circ}\) is empty.
-
-============================================================
-
-Note ID: 1720899087484
-  Field: Text
-    Before:
-      Definition 8.2.8. Given a subset \(E\) of a metric space \((X, d)\):<br><ul><li>The closure \(\bar{E}\) is the union of \(E\) together with its limit points.&nbsp;</li><li>The interior of \(E\) is denoted by \(E^{\circ}\) and is defined as</li><ul><li>\(E^{\circ}\) = \(\left\{x \in E: \text { there exists } V_{\epsilon}(x) \subseteq E\right\}\)</li></ul></ul>
-
-    After:
-      Definition 8.2.8. Given a subset \(E\) of a metric space \((X, d)\):<br><ul><li>The closure \(\bar{E}\) is the union of \(E\) together with its limit points.&nbsp;</li><li>The interior of \(E\) is denoted by \(E^{\circ}\) and is defined as</li><ul><li>\(E^{\circ}\) = \(\left\{x \in E: \text { there exists } V_{\epsilon}(x) \subseteq E\right\}\)</li></ul></ul>
-
-============================================================
-
-Note ID: 1720901474524
-  Field: Text
-    Before:
-      Theorem 8.2.10. Let \((X, d)\) be a complete metric space, and let \(\left\{O_{n}\right\}\) be a countable collection of dense, open subsets of \(X\). Then, \(\bigcap_{n=1}^{\infty} O_{n}\) is not empty.<br><br>Proof. When we proved this theorem on \(\mathbf{R}\), completeness manifested itself in the form of the Nested Interval Property. We could derive something akin to NIP in the metric space setting, but instead let's take an approach that uses the convergence of Cauchy sequences (because this is how we have defined completeness).<br><br>Pick \(x_{1} \in O_{1}\). Because \(O_{1}\) is open, there exists an \(\epsilon_{1}&gt;0\) such that \(V_{\epsilon_{1}}\left(x_{1}\right) \subseteq O_{1}\).<br><br>Exercise 8.2.14. <br><br><ul><li>(a) Give the details for why we know there exists a point \(x_{2} \in\) \(V_{\epsilon_{1} }\left(x_{1}\right)\) \(\cap\) \(O_{2}\) and an \(\epsilon_{2}&gt;0\) satisfying \(\epsilon_{2}&lt;\epsilon_{1} / 2\) with \(V_{\epsilon_{2} }\left(x_{2}\right)\) contained in \(\mathrm{O}_{2}\) and</li><ul><li>\(\overline{V_{\epsilon_{2} }\left(x_{2}\right)}\) \(\subseteq\) \(V_{\epsilon_{1} }\left(x_{1}\right)\)</li></ul></ul>
-
-    After:
-      Theorem 8.2.10. Let \((X, d)\) be a complete metric space, and let \(\left\{O_{n}\right\}\) be a countable collection of dense, open subsets of \(X\). Then, \(\bigcap_{n=1}^{\infty} O_{n}\) is not empty.<br><br>Proof. When we proved this theorem on \(\mathbf{R}\), completeness manifested itself in the form of the Nested Interval Property. We could derive something akin to NIP in the metric space setting, but instead let's take an approach that uses the convergence of Cauchy sequences (because this is how we have defined completeness).<br><br>Pick \(x_{1} \in O_{1}\). Because \(O_{1}\) is open, there exists an \(\epsilon_{1}&gt;0\) such that \(V_{\epsilon_{1}}\left(x_{1}\right) \subseteq O_{1}\).<br><br>Exercise 8.2.14. <br><br><ul><li>(a) Give the details for why we know there exists a point \(x_{2} \in\) \(V_{\epsilon_{1} }\left(x_{1}\right)\) \(\cap\) \(O_{2}\) and an \(\epsilon_{2}&gt;0\) satisfying \(\epsilon_{2}&lt;\epsilon_{1} / 2\) with \(V_{\epsilon_{2} }\left(x_{2}\right)\) contained in \(\mathrm{O}_{2}\) and</li><ul><li>\(\overline{V_{\epsilon_{2} }\left(x_{2}\right)}\) \(\subseteq\) \(V_{\epsilon_{1} }\left(x_{1}\right)\)</li></ul></ul>
-
-============================================================
-
-Note ID: 1720901640560
-  Field: Text
-    Before:
-      This result is called the Baire Category Theorem because it creates two categories of size for subsets in a metric space. A set of "first category" is one that can be written as a countable union of nowhere-dense sets. These are the small, intuitively thin subsets of a metric space. We now see that if our metric space is complete, then it is necessarily of "second category," meaning it cannot be written as a countable union of nowhere-dense sets.
-
-    After:
-      This result is called the Baire Category Theorem because it creates two categories of size for subsets in a metric space. A set of "first category" is one that can be written as a countable union of nowhere-dense sets. These are the small, intuitively thin subsets of a metric space. We now see that if our metric space is complete, then it is necessarily of "second category," meaning it cannot be written as a countable union of nowhere-dense sets.
-
-============================================================
-
-Note ID: 1720901668536
-  Field: Text
-    Before:
-      Given a subset \(A\) of a complete metric space \(X\), showing that \(A\) is of first category is a mathematically precise way of demonstrating that \(A\) constitutes a very minor portion of the set \(X\). The term "meager" is often used to mean a set of first category.
-
-    After:
-      Given a subset \(A\) of a complete metric space \(X\), showing that \(A\) is of first category is a mathematically precise way of demonstrating that \(A\) constitutes a very minor portion of the set \(X\). The term "meager" is often used to mean a set of first category.
-
-============================================================
-
-Note ID: 1720904100955
-  Field: Text
-    Before:
-      Theorem 8.2.12. The set<br><br>\[<br>D=\left\{f \in C[0,1]: f^{\prime}(x) \text { exists for some } x \in[0,1]\right\}<br>\]<br><br>is a set of first category in \(C[0,1]\).<br><br>Proof. For each pair of natural numbers \(m, n\), define<br><br>\[<br>\begin{aligned}<br>&amp; A_{m, n}=\{f \in C[0,1]: \text { there exists } x \in[0,1] \text { where } \\<br>&amp; \left.\qquad\left|\frac{f(x)-f(t)}{x-t}\right| \leq n \text { whenever } 0&lt;|x-t|&lt;\frac{1}{m}\right\} .<br>\end{aligned}<br>\]<br><br>This definition takes some time to digest. Think of \(1 / m\) as defining a \(\delta\) neighborhood around the point \(x\), and view \(n\) as an upper bound on the magnitude of the slopes of lines through the two points \((x, f(x))\) and \((t, f(t))\). The set \(A_{m, n}\) contains any function in \(C[0,1]\) for which it is possible to find at least one point \(x\) where the slopes through \((x, f(x))\) and points on the function nearby — within \(1 / m\) to be precise - are bounded by \(n\).<br><br>Exercise 8.2.16. Show that if \(f \in C[0,1]\) is differentiable at a point \(x \in[0,1]\), then \(f \in A_{m, n}\) for some pair \(m, n \in \mathbf{N}\).<br><br>The collection of subsets \(\left\{A_{m, n}: m, n \in \mathbf{N}\right\}\) is countable, and we have just seen that the union of these sets contains our set \(D\). Because it is not difficult to see that a subset of a set of first category is first category, the final hurdle in the argument is to prove that each \(A_{m, n}\) is nowhere-dense in \(C[0,1]\).
-
-    After:
-      Theorem 8.2.12. The set<br><br>\[<br>D=\left\{f \in C[0,1]: f^{\prime}(x) \text { exists for some } x \in[0,1]\right\}<br>\]<br><br>is a set of first category in \(C[0,1]\).<br><br>Proof. For each pair of natural numbers \(m, n\), define<br><br>\[<br>\begin{aligned}<br>&amp; A_{m, n}=\{f \in C[0,1]: \text { there exists } x \in[0,1] \text { where } \\<br>&amp; \left.\qquad\left|\frac{f(x)-f(t)}{x-t}\right| \leq n \text { whenever } 0&lt;|x-t|&lt;\frac{1}{m}\right\} .<br>\end{aligned}<br>\]<br><br>This definition takes some time to digest. Think of \(1 / m\) as defining a \(\delta\) neighborhood around the point \(x\), and view \(n\) as an upper bound on the magnitude of the slopes of lines through the two points \((x, f(x))\) and \((t, f(t))\). The set \(A_{m, n}\) contains any function in \(C[0,1]\) for which it is possible to find at least one point \(x\) where the slopes through \((x, f(x))\) and points on the function nearby — within \(1 / m\) to be precise - are bounded by \(n\).<br><br>Exercise 8.2.16. Show that if \(f \in C[0,1]\) is differentiable at a point \(x \in[0,1]\), then \(f \in A_{m, n}\) for some pair \(m, n \in \mathbf{N}\).<br><br>The collection of subsets \(\left\{A_{m, n}: m, n \in \mathbf{N}\right\}\) is countable, and we have just seen that the union of these sets contains our set \(D\). Because it is not difficult to see that a subset of a set of first category is first category, the final hurdle in the argument is to prove that each \(A_{m, n}\) is nowhere-dense in \(C[0,1]\).
-
-============================================================
-
-Note ID: 1720910278124
-  Field: Text
-    Before:
-      The countable union of countable sets is countable
-
-    After:
-      The countable union of countable sets is countable
-
-============================================================
-
-Note ID: 1720944236465
-  Field: Text
-    Before:
-      The finite union of finite sets is finite
-
-    After:
-      The finite union of finite sets is finite
-
-============================================================
-
-Note ID: 1720953770873
-  Field: Text
-    Before:
-      The cardinality of the reals R is often called the cardinality of the continuum C
-
-    After:
-      The cardinality of the reals R is often called the cardinality of the continuum C
-
-============================================================
-
-Note ID: 1720956663643
-  Field: Text
-    Before:
-      A point&nbsp;\(p \in X\), with X a metric space, is a limit point of a subset&nbsp;\(E\), if every neigbourhoud of p contains a point \(q\) \(\neq p\) such that&nbsp;\(q\) \(\in E\)
-
-    After:
-      A point&nbsp;\(p \in X\), with X a metric space, is a limit point of a subset&nbsp;\(E\), if every neigbourhoud of p contains a point \(q\) \(\neq p\) such that&nbsp;\(q\) \(\in E\)
-
-============================================================
-
-Note ID: 1720973783470
-  Field: Text
-    Before:
-      Which points are limit points?<br><img src="paste-4f295a4aa9d76d0f2289e551c2b12ede6654b8b0.jpg"><br><ul><li>All points on the solid contour</li><li>All points on the dotted contour since a point does not have to be in the subset to be a limit point</li><li>All points on the inside</li><li>The point in the middle of the circle</li></ul>
-
-    After:
-      Which points are limit points?<br><img src="paste-4f295a4aa9d76d0f2289e551c2b12ede6654b8b0.jpg"><br><ul><li>All points on the solid contour</li><li>All points on the dotted contour since a point does not have to be in the subset to be a limit point</li><li>All points on the inside</li><li>The point in the middle of the circle</li></ul>
-
-============================================================
-
-Note ID: 1720979465104
-  Field: Text
-    Before:
-      <div>Taking out a finite number of points from an open set keeps it open</div>
-
-    After:
-      <div>Taking out a finite number of points from an open set keeps it open</div>
-
-============================================================
-
-Note ID: 1720980020762
-  Field: Text
-    Before:
-      <ul><li>A set that is both open and closed&nbsp; is called clopen</li></ul>
-
-    After:
-      <ul><li>A set that is both open and closed&nbsp; is called clopen</li></ul>
-
-============================================================
-
-Note ID: 1721167256126
-  Field: Text
-    Before:
-      If&nbsp;\(E\)&nbsp;\(\subset\) \(F\), with F closed, then&nbsp;\(\bar{E}\)&nbsp;\(\subset\)&nbsp;\(F\)
-
-    After:
-      If&nbsp;\(E\)&nbsp;\(\subset\) \(F\), with F closed, then&nbsp;\(\bar{E}\)&nbsp;\(\subset\)&nbsp;\(F\)
-
-============================================================
-
-Note ID: 1721167346975
-  Field: Text
-    Before:
-      If a set E is contained in a closed set, its closure must also be contained in the closed set. THus the closure is the smallest closed set containing E
-
-    After:
-      If a set E is contained in a closed set, its closure must also be contained in the closed set. THus the closure is the smallest closed set containing E
-
-============================================================
-
-Note ID: 1721199822714
-  Field: Text
-    Before:
-      The closure of a set&nbsp;\(\bar{E}\) is the smallest closed set containing a subset E of a closed set F:<br>Proof<br><ul><li>Since F is a closed set, if p is a limit point of E then it is a limit point of F</li><li>Since F contains all its limit points, it contains all its limit points including the limit points of E</li></ul>
-
-    After:
-      The closure of a set&nbsp;\(\bar{E}\) is the smallest closed set containing a subset E of a closed set F:<br>Proof<br><ul><li>Since F is a closed set, if p is a limit point of E then it is a limit point of F</li><li>Since F contains all its limit points, it contains all its limit points including the limit points of E</li></ul>
-
-============================================================
-
-Note ID: 1721200813022
-  Field: Text
-    Before:
-      Proof that a set is closed iff its complement is open:<br>Proof:<br><ul><li>Assume E is open</li><li>Which means any point is an interior point</li><li>\(\forall\)&nbsp;\(x\)&nbsp;\(\in\)&nbsp;\(E\),&nbsp;\(\exists\)&nbsp;\(N(x)\) s.t&nbsp;\(N(x)\)&nbsp;\(\subset\)&nbsp;\(E\)<br></li><li>\(N(x)\)&nbsp;\(\subset\)&nbsp;\(E\)&nbsp;\(\implies \) \(N(x)\) is disjoint from \(E^c\)&nbsp;<br></li><li>Pictographically:</li><ul><li><img src="paste-ed4668c47721b35d7232d909a780fffed0004256.jpg"><br></li></ul><li>Thus&nbsp;\(\forall\)&nbsp;\(x\)&nbsp;\(\in\)&nbsp;\(E\),&nbsp;\(x\) is not a limit point of&nbsp;\(E^c\)</li><li>Which means&nbsp;\(E^c\) contains all of its limit points</li></ul>
-
-    After:
-      Proof that a set is closed iff its complement is open:<br>Proof:<br><ul><li>Assume E is open</li><li>Which means any point is an interior point</li><li>\(\forall\)&nbsp;\(x\)&nbsp;\(\in\)&nbsp;\(E\),&nbsp;\(\exists\)&nbsp;\(N(x)\) s.t&nbsp;\(N(x)\)&nbsp;\(\subset\)&nbsp;\(E\)<br></li><li>\(N(x)\)&nbsp;\(\subset\)&nbsp;\(E\)&nbsp;\(\implies \) \(N(x)\) is disjoint from \(E^c\)&nbsp;<br></li><li>Pictographically:</li><ul><li><img src="paste-ed4668c47721b35d7232d909a780fffed0004256.jpg"><br></li></ul><li>Thus&nbsp;\(\forall\)&nbsp;\(x\)&nbsp;\(\in\)&nbsp;\(E\),&nbsp;\(x\) is not a limit point of&nbsp;\(E^c\)</li><li>Which means&nbsp;\(E^c\) contains all of its limit points</li></ul>
-
-============================================================
-
-Note ID: 1721200897938
-  Field: Text
-    Before:
-      If a set is clopen its complement is also clopen, e.g,&nbsp;\(\emptyset\) and&nbsp;\( \emptyset^c&nbsp; = R\)
-
-    After:
-      If a set is clopen its complement is also clopen, e.g,&nbsp;\(\emptyset\) and&nbsp;\( \emptyset^c&nbsp; = R\)
-
-============================================================
-
-Note ID: 1721201087099
-  Field: Text
-    Before:
-      If a set is neither closed nor open then its complement must be neither closed nor open
-
-    After:
-      If a set is neither closed nor open then its complement must be neither closed nor open
-
-============================================================
-
-Note ID: 1721201282274
-  Field: Text
-    Before:
-      Every point of the union of two open sets was interior to one of the sets, meaning that they will be interior to the union
-
-    After:
-      Every point of the union of two open sets was interior to one of the sets, meaning that they will be interior to the union
-
-============================================================
-
-Note ID: 1721225633165
-  Field: Text
-    Before:
-      The union of a finite number of closed sets is closed
-
-    After:
-      The union of a finite number of closed sets is closed
-
-============================================================
-
-Note ID: 1721227531413
-  Field: Text
-    Before:
-      <ul><li>The arbitrary union of open sets is open&nbsp;</li><ul><li>The arbitray intersection is not: \(\cap^\infty(-1/n,1/n)\) =&nbsp;\([0]\)</li></ul><li>The arbitrary intersection of closed sets is closed</li><ul><li>The arbitrary union is not:&nbsp; &nbsp;\(\cup^\infty(0,n)\) =&nbsp;\([0, \infty )\)</li></ul><li>The finite intersection of open sets is open:&nbsp;</li><li>The finite union of closed sets is closed</li></ul><br>
-
-    After:
-      <ul><li>The arbitrary union of open sets is open&nbsp;</li><ul><li>The arbitray intersection is not: \(\cap^\infty(-1/n,1/n)\) =&nbsp;\([0]\)</li></ul><li>The arbitrary intersection of closed sets is closed</li><ul><li>The arbitrary union is not:&nbsp; &nbsp;\(\cup^\infty(0,n)\) =&nbsp;\([0, \infty )\)</li></ul><li>The finite intersection of open sets is open:&nbsp;</li><li>The finite union of closed sets is closed</li></ul><br>
-
-============================================================
-
-Note ID: 1721227794972
-  Field: Text
-    Before:
-      The arbitrary union of open sets is open:<br><ul><li>\(x \in \cup_{\alpha} O_\alpha\)<br></li><li>This means&nbsp;\(x\)&nbsp;\(\in\) some&nbsp;\(O_{\alpha}\) which is open<br></li><li>Thus&nbsp;\(\exists\) a neighbourhood \(N(x)\) s.t&nbsp;\(N(x)\)&nbsp;\(\subset\)&nbsp;\(O_{\alpha}\)</li><li>However, \(O_{\alpha}\)&nbsp;\(\in\)&nbsp;\(\) \(\cup_{\alpha} O\)</li></ul>
-
-    After:
-      The arbitrary union of open sets is open:<br><ul><li>\(x \in \cup_{\alpha} O_\alpha\)<br></li><li>This means&nbsp;\(x\)&nbsp;\(\in\) some&nbsp;\(O_{\alpha}\) which is open<br></li><li>Thus&nbsp;\(\exists\) a neighbourhood \(N(x)\) s.t&nbsp;\(N(x)\)&nbsp;\(\subset\)&nbsp;\(O_{\alpha}\)</li><li>However, \(O_{\alpha}\)&nbsp;\(\in\)&nbsp;\(\) \(\cup_{\alpha} O\)</li></ul>
-
-============================================================
-
-Note ID: 1721380404724
-  Field: Text
-    Before:
-      The word "cover" always refers to an open cover
-
-    After:
-      The word "cover" always refers to an open cover
-
-============================================================
-
-Note ID: 1721382336587
-  Field: Text
-    Before:
-      What is the open cover of&nbsp;\([1/2, 1 )\)?<br><ul><li>\(\{\)&nbsp;\(W_{x}\) \(\}_{x \in [1/2, 1]}\) such that&nbsp;\(W_x\) =&nbsp;\((x - \frac{1}{10}\),&nbsp; \(x+ \frac{1}{10})\)<br></li><li>Pictographically: <img src="paste-469125cd5f89a49e0a3abc6f4773dfec42999f10.jpg"></li><li>There are in-fact uncountably many elements in the open cover as there are uncountably many balls of a finite radius on the interval</li></ul>
-
-    After:
-      What is the open cover of&nbsp;\([1/2, 1 )\)?<br><ul><li>\(\{\)&nbsp;\(W_{x}\) \(\}_{x \in [1/2, 1]}\) such that&nbsp;\(W_x\) =&nbsp;\((x - \frac{1}{10}\),&nbsp; \(x+ \frac{1}{10})\)<br></li><li>Pictographically: <img src="paste-469125cd5f89a49e0a3abc6f4773dfec42999f10.jpg"></li><li>There are in-fact uncountably many elements in the open cover as there are uncountably many balls of a finite radius on the interval</li></ul>
-
-============================================================
-
-Note ID: 1721382634282
-  Field: Text
-    Before:
-      Subcovers may be the original covers themselves
-
-    After:
-      Subcovers may be the original covers themselves
-
-============================================================
-
-Note ID: 1721395410736
-  Field: Text
-    Before:
-      Definition: A set&nbsp;\(K\) in a metric space&nbsp;\(X,d\) is bounded if&nbsp;\(\exists\) a&nbsp; point \(p\) and radius&nbsp;\(r\) such that&nbsp;\(K\)&nbsp;\(\subset\)&nbsp;\(N_r(p) \),&nbsp; where \(N_r(p) \) is the r-radius ball around p<br>Pictographically:&nbsp; <img src="paste-083edbdd71173d007966dbefa31a246e3060de01.jpg">
-
-    After:
-      Definition: A set&nbsp;\(K\) in a metric space&nbsp;\(X,d\) is bounded if&nbsp;\(\exists\) a&nbsp; point \(p\) and radius&nbsp;\(r\) such that&nbsp;\(K\)&nbsp;\(\subset\)&nbsp;\(N_r(p) \),&nbsp; where \(N_r(p) \) is the r-radius ball around p<br>Pictographically:&nbsp; <img src="paste-083edbdd71173d007966dbefa31a246e3060de01.jpg">
-
-============================================================
-
-Note ID: 1721400835774
-  Field: Text
-    Before:
-      An open set in R is no longer necessarily open once you go to higher dimensions<br><br>Pictographically: <img src="paste-4e88b9a510f63335ce3379ba8becf0460f1249de.jpg">
-
-    After:
-      An open set in R is no longer necessarily open once you go to higher dimensions<br><br>Pictographically: <img src="paste-4e88b9a510f63335ce3379ba8becf0460f1249de.jpg">
-
-============================================================
-
-Note ID: 1721400954308
-  Field: Text
-    Before:
-      Being an open set depends on the metric space in which you are embedded (think of an open set of R being translated to R^2), whilst being a compact set does not
-
-    After:
-      Being an open set depends on the metric space in which you are embedded (think of an open set of R being translated to R^2), whilst being a compact set does not
-
-============================================================
-
-Note ID: 1721401985716
-  Field: Text
-    Before:
-      Theorem:&nbsp; If \(E\)&nbsp;\(\subset\)&nbsp;\(Y\)\( \subset\)&nbsp;\(X\), then&nbsp;\(E\) is open in&nbsp;\(Y\) iff&nbsp;\(E\) =&nbsp;\(Y\)&nbsp;\(\cap\)&nbsp;\(G \) for some&nbsp;\(G\) open in X
-
-    After:
-      Theorem:&nbsp; If \(E\)&nbsp;\(\subset\)&nbsp;\(Y\)\( \subset\)&nbsp;\(X\), then&nbsp;\(E\) is open in&nbsp;\(Y\) iff&nbsp;\(E\) =&nbsp;\(Y\)&nbsp;\(\cap\)&nbsp;\(G \) for some&nbsp;\(G\) open in X
-
-============================================================
-
-Note ID: 1721404254798
-  Field: Text
-    Before:
-      Story 3.5.1 (Discrete Uniform distribution). Let \(C\) be a finite, nonempty set of numbers. Choose one of these numbers uniformly at random (i.e., all values in \(C\) are equally likely). Call the chosen number \(X\). Then \(X\) is said to have the Discrete Uniform distribution with parameter \(C\); we denote this by \(X \sim\) \(\operatorname{DUnif}(C)\).
-
-    After:
-      Story 3.5.1 (Discrete Uniform distribution). Let \(C\) be a finite, nonempty set of numbers. Choose one of these numbers uniformly at random (i.e., all values in \(C\) are equally likely). Call the chosen number \(X\). Then \(X\) is said to have the Discrete Uniform distribution with parameter \(C\); we denote this by \(X \sim\) \(\operatorname{DUnif}(C)\).
-
-============================================================
-
-Note ID: 1721406583574
-  Field: Text
-    Before:
-      Another function that describes the distribution of an r.v. is the cumulative distribution function (CDF). Unlike the PMF, which only discrete r.v.s possess, the CDF is defined for all r.v.s.
-
-    After:
-      Another function that describes the distribution of an r.v. is the cumulative distribution function (CDF). Unlike the PMF, which only discrete r.v.s possess, the CDF is defined for all r.v.s.
-
-============================================================
-
-Note ID: 1721406744674
-  Field: Text
-    Before:
-      Definition 3.6.1. The cumulative distribution function (CDF) of an r.v. \(X\) is the function \(F_{X}\) given by \(F_{X}\) \((x)\) = \(P\) \((X \leq x)\). When there is no risk of ambiguity, we sometimes drop the subscript and just write \(F\) (or some other letter) for a CDF.
-
-    After:
-      Definition 3.6.1. The cumulative distribution function (CDF) of an r.v. \(X\) is the function \(F_{X}\) given by \(F_{X}\) \((x)\) = \(P\) \((X \leq x)\). When there is no risk of ambiguity, we sometimes drop the subscript and just write \(F\) (or some other letter) for a CDF.
-
-============================================================
-
-Note ID: 1721406892743
-  Field: Text
-    Before:
-      <img src="paste-c4ab8dec94ca180185c41f13f9e7d808a7d467c4.jpg"><br>FIGURE 3.8<br><br>\(\operatorname{Bin}(4,1 / 2)\) PMF and CDF. The height of the vertical bar \(P(X=2)\) in the PMF is also the height of the jump in the CDF at 2.
-
-    After:
-      <img src="paste-c4ab8dec94ca180185c41f13f9e7d808a7d467c4.jpg"><br>FIGURE 3.8<br><br>\(\operatorname{Bin}(4,1 / 2)\) PMF and CDF. The height of the vertical bar \(P(X=2)\) in the PMF is also the height of the jump in the CDF at 2.
-
-============================================================
-
-Note ID: 1721407320646
-  Field: Text
-    Before:
-      <br>Theorem 3.6.3 (Valid CDFs). Any CDF \(F\) has the following properties.<br><br><ul><li>- Increasing: If \(x_{1}\) \(\leq\) \(x_{2}\), then \(F\left(x_{1}\right)\) \(\leq\) \(F\left(x_{2}\right)\).</li></ul><br>
-
-    After:
-      <br>Theorem 3.6.3 (Valid CDFs). Any CDF \(F\) has the following properties.<br><br><ul><li>- Increasing: If \(x_{1}\) \(\leq\) \(x_{2}\), then \(F\left(x_{1}\right)\) \(\leq\) \(F\left(x_{2}\right)\).</li></ul><br>
-
-============================================================
-
-Note ID: 1721407462089
-  Field: Text
-    Before:
-      <img src="paste-17a41104e00bfbadfaa5044fd41281a66ca4b1af.jpg"><br>Theorem 3.6.3 (Valid CDFs). Any CDF \(F\) has the following properties.<br><br><ul><li>Right-continuous: As in Figure 3.8, the CDF is continuous except possibly for having some jumps. Wherever there is a jump, the CDF is continuous from the right. That is, for any \(a\), we have</li><li>\(F(a)\) = \(\lim _{x \rightarrow a^{+} } \) \(F(x)\)<br></li></ul><br>
-
-    After:
-      <img src="paste-17a41104e00bfbadfaa5044fd41281a66ca4b1af.jpg"><br>Theorem 3.6.3 (Valid CDFs). Any CDF \(F\) has the following properties.<br><br><ul><li>Right-continuous: As in Figure 3.8, the CDF is continuous except possibly for having some jumps. Wherever there is a jump, the CDF is continuous from the right. That is, for any \(a\), we have</li><li>\(F(a)\) = \(\lim _{x \rightarrow a^{+} } \) \(F(x)\)<br></li></ul><br>
-
-============================================================
-
-Note ID: 1721407548626
-  Field: Text
-    Before:
-      <img src="paste-17a41104e00bfbadfaa5044fd41281a66ca4b1af.jpg"><br>Theorem 3.6.3 (Valid CDFs). Any CDF \(F\) has the following properties.<br><br><ul><li>Convergence to 0 and 1 in the limits:</li><li>\(<br>\lim _{x \rightarrow-\infty}\) \(F(x)\) = \(0\)</li><li>\(\lim _{x \rightarrow \infty}\) \(F(x)\)= \(1<br>\)<br></li></ul>
-
-    After:
-      <img src="paste-17a41104e00bfbadfaa5044fd41281a66ca4b1af.jpg"><br>Theorem 3.6.3 (Valid CDFs). Any CDF \(F\) has the following properties.<br><br><ul><li>Convergence to 0 and 1 in the limits:</li><li>\(<br>\lim _{x \rightarrow-\infty}\) \(F(x)\) = \(0\)</li><li>\(\lim _{x \rightarrow \infty}\) \(F(x)\)= \(1<br>\)<br></li></ul>
-
-============================================================
-
-Note ID: 1721408006301
-  Field: Text
-    Before:
-      <div>Theorem 3.6.3 (Valid CDFs). Any CDF \(F\) has the following properties.<br><br><ul><li>- Increasing:</li><li>- Right-continuous:&nbsp;</li><li>- Convergence to 0 and 1 in the limits:</li></ul></div><div>Proof for discrete RVs with values 0,1,...</div><div><ul><li>For the second criterion, note that:<br></li><ul><li>\(<br>P\) \((X\) \(\leq x\) ) = \(P\) \((X\) \(\leq\) \(\lfloor x\rfloor\) )<br></li><li>where \(\lfloor x\rfloor\) is the greatest integer less than or equal to \(x\). For example, \(P(X \leq 4.9)=\) \(P(X \leq 4)\) since \(X\) is integer-valued. So \(F(a+b)\) = \(F(a)\) for any \(b&gt;0\) that is small enough so that \(a+b\) &lt; \(\lfloor a\rfloor+1\), e.g., for \(a=4.9\), this holds for \(0&lt;b&lt;0.1\). This implies \(F(a)\) = \(\lim _{x \rightarrow a^{+}} F(x)\) (in fact, it's much stronger since it says \(F(x)\) equals \(F(a)\) when \(x\) is close enough to \(a\) and on the right).</li></ul></ul></div>
-
-    After:
-      <div>Theorem 3.6.3 (Valid CDFs). Any CDF \(F\) has the following properties.<br><br><ul><li>- Increasing:</li><li>- Right-continuous:&nbsp;</li><li>- Convergence to 0 and 1 in the limits:</li></ul></div><div>Proof for discrete RVs with values 0,1,...</div><div><ul><li>For the second criterion, note that:<br></li><ul><li>\(<br>P\) \((X\) \(\leq x\) ) = \(P\) \((X\) \(\leq\) \(\lfloor x\rfloor\) )<br></li><li>where \(\lfloor x\rfloor\) is the greatest integer less than or equal to \(x\). For example, \(P(X \leq 4.9)=\) \(P(X \leq 4)\) since \(X\) is integer-valued. So \(F(a+b)\) = \(F(a)\) for any \(b&gt;0\) that is small enough so that \(a+b\) &lt; \(\lfloor a\rfloor+1\), e.g., for \(a=4.9\), this holds for \(0&lt;b&lt;0.1\). This implies \(F(a)\) = \(\lim _{x \rightarrow a^{+}} F(x)\) (in fact, it's much stronger since it says \(F(x)\) equals \(F(a)\) when \(x\) is close enough to \(a\) and on the right).</li></ul></ul></div>
-
-============================================================
-
-Note ID: 1721408153579
-  Field: Text
-    Before:
-      <div>Theorem 3.6.3 (Valid CDFs). Any CDF \(F\) has the following properties.<br><br><ul><li>- Increasing:</li><li>- Right-continuous:&nbsp;</li><li>- Convergence to 0 and 1 in the limits:</li></ul></div><div>Proof for discrete RVs with values 0,1,...</div><div><ul><li>For the third criterion, we have \(F(x)=0\) for \(x&lt;0\), and<br></li><li>\(<br>\lim _{x \rightarrow \infty} F(x)\)&nbsp;</li><li>= \(\lim _{x \rightarrow \infty}\) \(P\) ( \(X \) \(\leq\) \(\lfloor x\rfloor\) )&nbsp;</li><li>= \(\lim _{x \rightarrow \infty}\) \( \sum_{n=0}^{\lfloor x\rfloor}\) \(P(X=n)\)</li><li>= \(\sum_{n=0}^{\infty}\) \(P(X=n)\)</li><li>=\(1\)<br></li></ul></div>
-
-    After:
-      <div>Theorem 3.6.3 (Valid CDFs). Any CDF \(F\) has the following properties.<br><br><ul><li>- Increasing:</li><li>- Right-continuous:&nbsp;</li><li>- Convergence to 0 and 1 in the limits:</li></ul></div><div>Proof for discrete RVs with values 0,1,...</div><div><ul><li>For the third criterion, we have \(F(x)=0\) for \(x&lt;0\), and<br></li><li>\(<br>\lim _{x \rightarrow \infty} F(x)\)&nbsp;</li><li>= \(\lim _{x \rightarrow \infty}\) \(P\) ( \(X \) \(\leq\) \(\lfloor x\rfloor\) )&nbsp;</li><li>= \(\lim _{x \rightarrow \infty}\) \( \sum_{n=0}^{\lfloor x\rfloor}\) \(P(X=n)\)</li><li>= \(\sum_{n=0}^{\infty}\) \(P(X=n)\)</li><li>=\(1\)<br></li></ul></div>
-
-============================================================
-
-Note ID: 1721421937225
-  Field: Text
-    Before:
-      Picking K times from a set of n objects when order does not matter, with replacement: \(\binom{n+k-1}{k}\)<br><ul><li>K indistinguishable objects in n distinguishable boxes</li><li>Pictoral Proof:</li><li><img src="paste-cf88e4a31f506953882613f2a6c7f5ea36f2ce5e.jpg"><br></li><li>E.g n=4, k = 6</li><li>We can represetns this as dots and bars</li><li>****||**|*</li><li>In this configuration there must be K dots and must be n-1 separators</li><li>The total number of positions is thus n-1+k, then the configuration is fully determined by the positions of the k dots as the remaining positions are those of the separators</li><li>Alternatively you can choose where the separators are and then the dots are determined</li><li>Pictographically:<img src="paste-50ff37b741728f3b8842c6eaa2cc22cb4a5156de.jpg"></li></ul>
-
-    After:
-      Picking K times from a set of n objects when order does not matter, with replacement: \(\binom{n+k-1}{k}\)<br><ul><li>K indistinguishable objects in n distinguishable boxes</li><li>Pictoral Proof:</li><li><img src="paste-cf88e4a31f506953882613f2a6c7f5ea36f2ce5e.jpg"><br></li><li>E.g n=4, k = 6</li><li>We can represetns this as dots and bars</li><li>****||**|*</li><li>In this configuration there must be K dots and must be n-1 separators</li><li>The total number of positions is thus n-1+k, then the configuration is fully determined by the positions of the k dots as the remaining positions are those of the separators</li><li>Alternatively you can choose where the separators are and then the dots are determined</li><li>Pictographically:<img src="paste-50ff37b741728f3b8842c6eaa2cc22cb4a5156de.jpg"></li></ul>
-
-============================================================
-
-Note ID: 1721483548651
-  Field: Text
-    Before:
-      <ul><li>A probability space is a sample space S together with&nbsp;</li><li>A probability measure&nbsp;\(P:\)&nbsp;\(2^S\)&nbsp;\(\to\)&nbsp;\((0,1)\)</li><li>P must obey the probability axioms :</li><ul><li>\(P\) (\(\emptyset\)) = 0 and \(P\) (\(S\)) = 0</li><li>\(P\) ( \(\cup_{i=1}^\infty\) \(A_i\)&nbsp; ) =&nbsp;\(\sum_{i=1}^\infty\)&nbsp;\(P\) (\(A_i\)) if&nbsp;\(A_i\) are disjoint</li></ul></ul>
-
-    After:
-      <ul><li>A probability space is a sample space S together with&nbsp;</li><li>A probability measure&nbsp;\(P:\)&nbsp;\(2^S\)&nbsp;\(\to\)&nbsp;\((0,1)\)</li><li>P must obey the probability axioms :</li><ul><li>\(P\) (\(\emptyset\)) = 0 and \(P\) (\(S\)) = 0</li><li>\(P\) ( \(\cup_{i=1}^\infty\) \(A_i\)&nbsp; ) =&nbsp;\(\sum_{i=1}^\infty\)&nbsp;\(P\) (\(A_i\)) if&nbsp;\(A_i\) are disjoint</li></ul></ul>
-
-============================================================
-
-Note ID: 1721483737109
-  Field: Text
-    Before:
-      <div>To recap, we have now seen three equivalent ways of expressing the distribution of a random variable. Two of these are the PMF and the CDF: we know these two functions contain the same information, since we can always figure out the CDF from the PMF and vice versa. Generally the PMF is easier to work with for discrete r.v.s, since evaluating the CDF requires a summation.<br></div>
-
-    After:
-      <div>To recap, we have now seen three equivalent ways of expressing the distribution of a random variable. Two of these are the PMF and the CDF: we know these two functions contain the same information, since we can always figure out the CDF from the PMF and vice versa. Generally the PMF is easier to work with for discrete r.v.s, since evaluating the CDF requires a summation.<br></div>
-
-============================================================
-
-Note ID: 1721483843382
-  Field: Text
-    Before:
-      <div>In this section we will discuss what it means to take a function of a random variable, and we will build understanding for why a function of a random variable is a random variable. That is, if \(X\) is a random variable, then \(X^{2}, e^{X}\), and \(\sin (X)\) are also random variables, as is \(g(X)\) for any function \(g: \mathbb{R} \rightarrow \mathbb{R}\).<br></div>
-
-    After:
-      <div>In this section we will discuss what it means to take a function of a random variable, and we will build understanding for why a function of a random variable is a random variable. That is, if \(X\) is a random variable, then \(X^{2}, e^{X}\), and \(\sin (X)\) are also random variables, as is \(g(X)\) for any function \(g: \mathbb{R} \rightarrow \mathbb{R}\).<br></div>
-
-============================================================
-
-Note ID: 1721484094149
-  Field: Text
-    Before:
-      Definition 3.7.1 (Function of an r.v.). For an experiment with sample space \(S\), an r.v. \(X\), and a function \(g:\) \(\mathbb{R}\) \(\rightarrow\) \(\mathbb{R}\)&nbsp; \(g(X)\) is the r.v. that maps \(s\) to \(g(X(s))\) for all \(s \in S\).
-
-    After:
-      Definition 3.7.1 (Function of an r.v.). For an experiment with sample space \(S\), an r.v. \(X\), and a function \(g:\) \(\mathbb{R}\) \(\rightarrow\) \(\mathbb{R}\)&nbsp; \(g(X)\) is the r.v. that maps \(s\) to \(g(X(s))\) for all \(s \in S\).
-
-============================================================
-
-Note ID: 1721484159250
-  Field: Text
-    Before:
-      Functions of an r.v are obtained via simple functional composition with the r.v
-
-    After:
-      Functions of an r.v are obtained via simple functional composition with the r.v
-
-============================================================
-
-Note ID: 1721484262138
-  Field: Text
-    Before:
-      <img src="paste-75a9a707ccc9a3df7603646b4abdc87e110152f3.jpg"><br>FIGURE 3.9<br><br>The r.v. \(X\) is defined on a sample space with 6 elements, and has possible values 0 , 1 , and 4 . The function \(g\) is the square root function. Composing \(X\) and \(g\) gives the random variable \(g(X)\) = \(\sqrt{X}\), which has possible values 0,1 , and 2 .
-
-    After:
-      <img src="paste-75a9a707ccc9a3df7603646b4abdc87e110152f3.jpg"><br>FIGURE 3.9<br><br>The r.v. \(X\) is defined on a sample space with 6 elements, and has possible values 0 , 1 , and 4 . The function \(g\) is the square root function. Composing \(X\) and \(g\) gives the random variable \(g(X)\) = \(\sqrt{X}\), which has possible values 0,1 , and 2 .
-
-============================================================
-
-Note ID: 1721484954947
-  Field: Text
-    Before:
-      Given a discrete r.v. \(X\) with a known PMF, how can we find the PMF of \(Y=g(X)\) ? In the case where \(g\) is a one-to-one function, the answer is straightforward: the support of \(Y\) is the set of all \(g(x)\) with \(x\) in the support of \(X\), and<br><br><ul><li>P ( \(Y\)&nbsp;= \(g(x) \) )&nbsp;</li><li>= P\( (g(X)=g(x)) \)&nbsp;</li><li>= P\((X=x)\)<br></li></ul>
-
-    After:
-      Given a discrete r.v. \(X\) with a known PMF, how can we find the PMF of \(Y=g(X)\) ? In the case where \(g\) is a one-to-one function, the answer is straightforward: the support of \(Y\) is the set of all \(g(x)\) with \(x\) in the support of \(X\), and<br><br><ul><li>P ( \(Y\)&nbsp;= \(g(x) \) )&nbsp;</li><li>= P\( (g(X)=g(x)) \)&nbsp;</li><li>= P\((X=x)\)<br></li></ul>
-
-============================================================
-
-Note ID: 1721485073150
-  Field: Text
-    Before:
-      <div>The case where \(Y=g(X)\) with \(g\) one-to-one is illustrated in the following tables; the idea is that if the distinct possible values of \(X\) are \(x_{1}, x_{2}, \ldots\) with probabilities \(p_{1}, p_{2}, \ldots\) (respectively), then the distinct possible values of \(Y\) are \(g\left(x_{1}\right), g\left(x_{2}\right), \ldots\), with the list \(p_{1}, p_{2}, \ldots\) of probabilities.<br></div><div><ul><li><img src="paste-6164b1862aa069256a4938f9007db245ddee2ec9.jpg"><br></li><ul><li>PMF of \(X\), in table form<br></li></ul><li><img src="paste-f3c99cabe13b7b13cde996bcb4082b7a530413a4.jpg"><br></li><ul><li>PMF of \(Y\), in table form<br></li></ul></ul></div>
-
-    After:
-      <div>The case where \(Y=g(X)\) with \(g\) one-to-one is illustrated in the following tables; the idea is that if the distinct possible values of \(X\) are \(x_{1}, x_{2}, \ldots\) with probabilities \(p_{1}, p_{2}, \ldots\) (respectively), then the distinct possible values of \(Y\) are \(g\left(x_{1}\right), g\left(x_{2}\right), \ldots\), with the list \(p_{1}, p_{2}, \ldots\) of probabilities.<br></div><div><ul><li><img src="paste-6164b1862aa069256a4938f9007db245ddee2ec9.jpg"><br></li><ul><li>PMF of \(X\), in table form<br></li></ul><li><img src="paste-f3c99cabe13b7b13cde996bcb4082b7a530413a4.jpg"><br></li><ul><li>PMF of \(Y\), in table form<br></li></ul></ul></div>
-
-============================================================
-
-Note ID: 1721486386073
-  Field: Text
-    Before:
-      Theorem 3.7.3 (PMF of \(g(X)\) ). Let \(X\) be a discrete r.v. and \(g:\) \(\mathbb{R}\) \(\rightarrow\) \(\mathbb{R}\). Then the support of \(g(X)\) is the set of all \(y\) such that \(g(x)=y\) for at least one \(x\) in the support of \(X\), and the PMF of \(g(X)\) is<br><br><ul><li>\(P\) \((g(X)=y)\) = \(\sum_{x: g(x)=y}\)&nbsp; \(P\) \((X=x)\)</li></ul><br><br>for all \(y\) in the support of \(g(X)\).<br>
-
-    After:
-      Theorem 3.7.3 (PMF of \(g(X)\) ). Let \(X\) be a discrete r.v. and \(g:\) \(\mathbb{R}\) \(\rightarrow\) \(\mathbb{R}\). Then the support of \(g(X)\) is the set of all \(y\) such that \(g(x)=y\) for at least one \(x\) in the support of \(X\), and the PMF of \(g(X)\) is<br><br><ul><li>\(P\) \((g(X)=y)\) = \(\sum_{x: g(x)=y}\)&nbsp; \(P\) \((X=x)\)</li></ul><br><br>for all \(y\) in the support of \(g(X)\).<br>
-
-============================================================
-
-Note ID: 1721490846042
-  Field: Text
-    Before:
-      3.7.7 (Category errors and sympathetic magic). Many common mistakes in probability can be traced to confusing two of the following fundamental objects with each other: distributions, random variables, events, and numbers. Such mistakes are examples of category errors. In general, a category error is a mistake that doesn't just happen to be wrong, but in fact is necessarily wrong since it is based on the wrong category of object. For example, answering the question "How many people live in Boston?" with " -42 " or " \(\pi\) " or "pink elephants" would be a category error-we may not know the population size of a city, but we do know that it is a nonnegative integer at any point in time. To help avoid being categorically wrong, always think about what category an answer should have.<br><br>An especially common category error is to confuse a random variable with its distribution. We call this error sympathetic magic; this term comes from anthropology, where it is used for the belief that one can influence an object by manipulating a representation of that object.&nbsp;
-
-    After:
-      3.7.7 (Category errors and sympathetic magic). Many common mistakes in probability can be traced to confusing two of the following fundamental objects with each other: distributions, random variables, events, and numbers. Such mistakes are examples of category errors. In general, a category error is a mistake that doesn't just happen to be wrong, but in fact is necessarily wrong since it is based on the wrong category of object. For example, answering the question "How many people live in Boston?" with " -42 " or " \(\pi\) " or "pink elephants" would be a category error-we may not know the population size of a city, but we do know that it is a nonnegative integer at any point in time. To help avoid being categorically wrong, always think about what category an answer should have.<br><br>An especially common category error is to confuse a random variable with its distribution. We call this error sympathetic magic; this term comes from anthropology, where it is used for the belief that one can influence an object by manipulating a representation of that object.&nbsp;
-
-============================================================
-
-Note ID: 1721490911650
-  Field: Text
-    Before:
-      We can think of the distribution of a random variable as a map or blueprint describing the r.v. Just as different houses can share the same blueprint, different r.v.s can have the same distribution, even if the experiments they summarize, and the sample spaces they map from, are not the same.
-
-    After:
-      We can think of the distribution of a random variable as a map or blueprint describing the r.v. Just as different houses can share the same blueprint, different r.v.s can have the same distribution, even if the experiments they summarize, and the sample spaces they map from, are not the same.
-
-============================================================
-
-Note ID: 1721494290406
-  Field: Text
-    Before:
-      It does not make sense to multiply a PMF by 2 to get the PMF of 2X , since the probabilities would no longer sum to 1 . As we saw above, if \(X\) takes on values \(x_{j}\) with probabilities \(p_{j}\), then \(2 X\) takes on values \(2 x_{j}\) with probabilities \(p_{j}\).&nbsp;
-
-    After:
-      It does not make sense to multiply a PMF by 2 to get the PMF of 2X , since the probabilities would no longer sum to 1 . As we saw above, if \(X\) takes on values \(x_{j}\) with probabilities \(p_{j}\), then \(2 X\) takes on values \(2 x_{j}\) with probabilities \(p_{j}\).&nbsp;
-
-============================================================
-
-Note ID: 1721494472438
-  Field: Text
-    Before:
-      Example of sympathetic magic:<br><ul><li>Claiming that because \(X\) and \(Y\) have the same distribution, \(X\) must always equal \(Y\), i.e., \(P(X=Y)\) = \(1\). Just because two r.v.s have the same distribution does not mean they are always equal, or ever equal. We saw this in Example 3.2.5. As another example, consider flipping a fair coin once. Let \(X\) be the indicator of Heads and \(Y=1-X\) be the indicator of Tails. Both \(X\) and \(Y\) have the \(\operatorname{Bern}(1 / 2)\) distribution, but the event \(X=Y\) is impossible. The PMFs of \(X\) and \(Y\) are the same function, but \(X\) and \(Y\) are different mappings from the sample space to the real numbers.<br></li></ul>
-
-    After:
-      Example of sympathetic magic:<br><ul><li>Claiming that because \(X\) and \(Y\) have the same distribution, \(X\) must always equal \(Y\), i.e., \(P(X=Y)\) = \(1\). Just because two r.v.s have the same distribution does not mean they are always equal, or ever equal. We saw this in Example 3.2.5. As another example, consider flipping a fair coin once. Let \(X\) be the indicator of Heads and \(Y=1-X\) be the indicator of Tails. Both \(X\) and \(Y\) have the \(\operatorname{Bern}(1 / 2)\) distribution, but the event \(X=Y\) is impossible. The PMFs of \(X\) and \(Y\) are the same function, but \(X\) and \(Y\) are different mappings from the sample space to the real numbers.<br></li></ul>
-
-============================================================
-
-Note ID: 1721574571405
-  Field: Text
-    Before:
-      Definition 3.8.2 (Independence of many r.v.s). Random variables \(X_{1}, \ldots, X_{n}\) are independent if<br><br><ul><li>\(P\)\(\left(X_{1} \leq x_{1}, \ldots, X_{n} \leq x_{n}\right)\) = \(P\left(X_{1} \leq x_{1}\right)\) \(\ldots\) \(P\left(X_{n} \leq x_{n}\right)\)</li></ul><br>for all \(x_{1}, \ldots, x_{n} \in \mathbb{R}\). For infinitely many r.v.s, we say that they are independent if every finite subset of the r.v.s is independent.<br>
-
-    After:
-      Definition 3.8.2 (Independence of many r.v.s). Random variables \(X_{1}, \ldots, X_{n}\) are independent if<br><br><ul><li>\(P\)\(\left(X_{1} \leq x_{1}, \ldots, X_{n} \leq x_{n}\right)\) = \(P\left(X_{1} \leq x_{1}\right)\) \(\ldots\) \(P\left(X_{n} \leq x_{n}\right)\)</li></ul><br>for all \(x_{1}, \ldots, x_{n} \in \mathbb{R}\). For infinitely many r.v.s, we say that they are independent if every finite subset of the r.v.s is independent.<br>
-
-============================================================
-
-Note ID: 1721575127735
-  Field: Text
-    Before:
-      <ul><li>3.8.3. If \(X_{1}, \ldots, X_{n}\) are independent, then they are pairwise independent,&nbsp;</li><li>The idea behind proving that \(X_{i}\) and \(X_{j}\) are independent is to let all the \(x_{k}\) other than \(x_{i}, x_{j}\) go to \(\infty\) in the definition of independence, since we already know \(X_{k}&lt;\infty\) is true (though it takes some work to give a complete justification for the limit). But pairwise independence does not imply independence in general, as we saw in Chapter 2 for events.</li></ul>
-
-    After:
-      <ul><li>3.8.3. If \(X_{1}, \ldots, X_{n}\) are independent, then they are pairwise independent,&nbsp;</li><li>The idea behind proving that \(X_{i}\) and \(X_{j}\) are independent is to let all the \(x_{k}\) other than \(x_{i}, x_{j}\) go to \(\infty\) in the definition of independence, since we already know \(X_{k}&lt;\infty\) is true (though it takes some work to give a complete justification for the limit). But pairwise independence does not imply independence in general, as we saw in Chapter 2 for events.</li></ul>
-
-============================================================
-
-Note ID: 1721575345836
-  Field: Text
-    Before:
-      <div>If \(X\) and \(Y\) are independent then it is also true, e.g., that \(X^{2}\) is independent of \(Y^{4}\), since if \(X^{2}\) provided information about \(Y^{4}\), then \(X\) would give information about \(Y\) (using \(X^{2}\) and \(Y^{4}\) as intermediaries: \(X\) determines \(X^{2}\), which would give information about \(Y^{4}\), which in turn would give information about \(Y\) ).&nbsp;<br></div>
-
-    After:
-      <div>If \(X\) and \(Y\) are independent then it is also true, e.g., that \(X^{2}\) is independent of \(Y^{4}\), since if \(X^{2}\) provided information about \(Y^{4}\), then \(X\) would give information about \(Y\) (using \(X^{2}\) and \(Y^{4}\) as intermediaries: \(X\) determines \(X^{2}\), which would give information about \(Y^{4}\), which in turn would give information about \(Y\) ).&nbsp;<br></div>
-
-============================================================
-
-Note ID: 1721575399297
-  Field: Text
-    Before:
-      Theorem 3.8.5 (Functions of independent r.v.s). If \(X\) and \(Y\) are independent r.v.s, then any function of \(X\) is independent of any function of \(Y\).
-
-    After:
-      Theorem 3.8.5 (Functions of independent r.v.s). If \(X\) and \(Y\) are independent r.v.s, then any function of \(X\) is independent of any function of \(Y\).
-
-============================================================
-
-Note ID: 1721575443663
-  Field: Text
-    Before:
-      Definition 3.8.6 (i.i.d.). We will often work with random variables that are independent and have the same distribution. We call such r.v.s independent and identically distributed, or i.i.d. for short.
-
-    After:
-      Definition 3.8.6 (i.i.d.). We will often work with random variables that are independent and have the same distribution. We call such r.v.s independent and identically distributed, or i.i.d. for short.
-
-============================================================
-
-Note ID: 1721575523156
-  Field: Text
-    Before:
-      3.8.7 (i. vs. i.d.). "Independent" and "identically distributed" are two oftenconfused but completely different concepts. Random variables are independent if they provide no information about each other; they are identically distributed if they have the same PMF (or equivalently, the same CDF).&nbsp;
-
-    After:
-      3.8.7 (i. vs. i.d.). "Independent" and "identically distributed" are two oftenconfused but completely different concepts. Random variables are independent if they provide no information about each other; they are identically distributed if they have the same PMF (or equivalently, the same CDF).&nbsp;
-
-============================================================
-
-Note ID: 1721576607143
-  Field: Text
-    Before:
-      Theorem 3.8.9. If \(X \sim \operatorname{Bin}(n, p), Y \sim \operatorname{Bin}(m, p)\), and \(X\) is independent of \(Y\), then \(X+Y \sim \operatorname{Bin}(n+m, p)\).<br><br>1. LOTP: We can directly find the PMF of \(X+Y\) by conditioning on \(X\) (or \(Y\), whichever we prefer) and using the law of total probability:<br><br><ul><li>\(P(X+Y=k) \)&nbsp;</li><li>= \(\sum_{j=0}^{k}\) \(P\) \((X+Y=k \mid X=j)\) \( P\) \((X=j) \)</li><li>= \(\sum_{j=0}^{k\)}\)&nbsp; \(P\) \((Y=k-j)\) \(P\) \((X=j) \)</li><li>= \(\sum_{j=0}^{k}\) \(\left(\begin{array}{c}m \\k-j\end{array}\right)\) \(p^{k-j} q^{mk+j}\) \(\left(\begin{array}{c}n \\j\end{array}\right) \) \(p^{j} q^{n-j} \)</li><li>= \(p^{k} q^{n+m-k}\) \( \sum_{j=0}^{k}\) \(\left(\begin{array}{c}m \\k-j\end{array}\right)\left(\begin{array}{c}n \\j\end{array}\right) \)</li><li>= \(\left(\begin{array}{c}n+m \\k\end{array}\right)\)&nbsp; \(p^{k} q^{n+m-k} \)</li></ul>
-
-    After:
-      Theorem 3.8.9. If \(X \sim \operatorname{Bin}(n, p), Y \sim \operatorname{Bin}(m, p)\), and \(X\) is independent of \(Y\), then \(X+Y \sim \operatorname{Bin}(n+m, p)\).<br><br>1. LOTP: We can directly find the PMF of \(X+Y\) by conditioning on \(X\) (or \(Y\), whichever we prefer) and using the law of total probability:<br><br><ul><li>\(P(X+Y=k) \)&nbsp;</li><li>= \(\sum_{j=0}^{k}\) \(P\) \((X+Y=k \mid X=j)\) \( P\) \((X=j) \)</li><li>= \(\sum_{j=0}^{k\)}\)&nbsp; \(P\) \((Y=k-j)\) \(P\) \((X=j) \)</li><li>= \(\sum_{j=0}^{k}\) \(\left(\begin{array}{c}m \\k-j\end{array}\right)\) \(p^{k-j} q^{mk+j}\) \(\left(\begin{array}{c}n \\j\end{array}\right) \) \(p^{j} q^{n-j} \)</li><li>= \(p^{k} q^{n+m-k}\) \( \sum_{j=0}^{k}\) \(\left(\begin{array}{c}m \\k-j\end{array}\right)\left(\begin{array}{c}n \\j\end{array}\right) \)</li><li>= \(\left(\begin{array}{c}n+m \\k\end{array}\right)\)&nbsp; \(p^{k} q^{n+m-k} \)</li></ul>
-
-============================================================
-
-Note ID: 1721576860934
-  Field: Text
-    Before:
-      Theorem 3.8.9. If \(X \sim \operatorname{Bin}(n, p), Y \sim \operatorname{Bin}(m, p)\), and \(X\) is independent of \(Y\), then \(X+Y \sim \operatorname{Bin}(n+m, p)\).<br><br>2. Representation: A much simpler proof is to represent both \(X\) and \(Y\) as the sum of i.i.d. \(\operatorname{Bern}(p)\) r.v.s: \(X=X_{1}+\cdots+X_{n}\) and \(Y=Y_{1}+\cdots+Y_{m}\), where the \(X_{i}\) and \(Y_{j}\) are all i.i.d. \(\operatorname{Bern}(p)\). Then \(X+Y\) is the sum of \(n+m\) i.i.d. \(\operatorname{Bern}(p)\) r.v.s, so its distribution, by the previous theorem, is \(\operatorname{Bin}(n+m, p)\).
-
-    After:
-      Theorem 3.8.9. If \(X \sim \operatorname{Bin}(n, p), Y \sim \operatorname{Bin}(m, p)\), and \(X\) is independent of \(Y\), then \(X+Y \sim \operatorname{Bin}(n+m, p)\).<br><br>2. Representation: A much simpler proof is to represent both \(X\) and \(Y\) as the sum of i.i.d. \(\operatorname{Bern}(p)\) r.v.s: \(X=X_{1}+\cdots+X_{n}\) and \(Y=Y_{1}+\cdots+Y_{m}\), where the \(X_{i}\) and \(Y_{j}\) are all i.i.d. \(\operatorname{Bern}(p)\). Then \(X+Y\) is the sum of \(n+m\) i.i.d. \(\operatorname{Bern}(p)\) r.v.s, so its distribution, by the previous theorem, is \(\operatorname{Bin}(n+m, p)\).
-
-============================================================
-
-Note ID: 1721576923869
-  Field: Text
-    Before:
-      Theorem 3.8.9. If \(X \sim \operatorname{Bin}(n, p), Y \sim \operatorname{Bin}(m, p)\), and \(X\) is independent of \(Y\), then \(X+Y \sim \operatorname{Bin}(n+m, p)\).<br><br>3. Story: By the Binomial story, \(X\) is the number of successes in \(n\) independent trials and \(Y\) is the number of successes in \(m\) additional independent trials, all with the same success probability, so \(X+Y\) is the total number of successes in the \(n+m\) trials, which is the story of the \(\operatorname{Bin}(n+m, p)\) distribution.
-
-    After:
-      Theorem 3.8.9. If \(X \sim \operatorname{Bin}(n, p), Y \sim \operatorname{Bin}(m, p)\), and \(X\) is independent of \(Y\), then \(X+Y \sim \operatorname{Bin}(n+m, p)\).<br><br>3. Story: By the Binomial story, \(X\) is the number of successes in \(n\) independent trials and \(Y\) is the number of successes in \(m\) additional independent trials, all with the same success probability, so \(X+Y\) is the total number of successes in the \(n+m\) trials, which is the story of the \(\operatorname{Bin}(n+m, p)\) distribution.
-
-============================================================
-
-Note ID: 1721607518975
-  Field: Text
-    Before:
-      Example 3.8.13 (Two friends). Consider again the "I have only two friends who ever call me" scenario from Example 2.5.11, except now with r.v. notation. Let \(X\) be the indicator of Alice calling me next Friday, \(Y\) be the indicator of Bob calling me next Friday, and \(Z\) be the indicator of exactly one of them calling me next Friday. Then \(X\) and \(Y\) are independent (by assumption). But given \(Z\) = \(1\), we have that \(X\) and \(Y\) are completely dependent: given that \(Z\) = \(1\), we have \(Y\) = \(1-X\).
-
-    After:
-      Example 3.8.13 (Two friends). Consider again the "I have only two friends who ever call me" scenario from Example 2.5.11, except now with r.v. notation. Let \(X\) be the indicator of Alice calling me next Friday, \(Y\) be the indicator of Bob calling me next Friday, and \(Z\) be the indicator of exactly one of them calling me next Friday. Then \(X\) and \(Y\) are independent (by assumption). But given \(Z\) = \(1\), we have that \(X\) and \(Y\) are completely dependent: given that \(Z\) = \(1\), we have \(Y\) = \(1-X\).
-
-============================================================
-
-Note ID: 1721607629570
-  Field: Text
-    Before:
-      Example 3.8.14 (Mystery opponent). Suppose that you are going to play two games of tennis against one of two identical twins. Against one of the twins, you are evenly matched, and against the other you have a 3/4 chance of winning. Suppose that you can't tell which twin you are playing against until after the two games. Let \(Z\) be the indicator of playing against the twin with whom you're evenly matched, and let \(X\) and \(Y\) be the indicators of victory in the first and second games, respectively.<br><br>Conditional on \(Z=1, X\) and \(Y\) are i.i.d. Bern(1/2), and conditional on \(Z=0\), \(X\) and \(Y\) are i.i.d. Bern(3/4). So \(X\) and \(Y\) are conditionally independent&nbsp; given \(Z\). Unconditionally, \(X\) and \(Y\) are dependent because observing \(X=1\) makes it more likely that we are playing the twin who is worse. That is,<br><br>\[<br>P(Y=1 \mid X=1)&gt;P(Y=1)<br>\]<br><br>Past games give us information which helps us infer who our opponent is, which in turn helps us predict future games! Note that this example is isomorphic to the "random coin" scenario from Example 2.3.7.
-
-    After:
-      Example 3.8.14 (Mystery opponent). Suppose that you are going to play two games of tennis against one of two identical twins. Against one of the twins, you are evenly matched, and against the other you have a 3/4 chance of winning. Suppose that you can't tell which twin you are playing against until after the two games. Let \(Z\) be the indicator of playing against the twin with whom you're evenly matched, and let \(X\) and \(Y\) be the indicators of victory in the first and second games, respectively.<br><br>Conditional on \(Z=1, X\) and \(Y\) are i.i.d. Bern(1/2), and conditional on \(Z=0\), \(X\) and \(Y\) are i.i.d. Bern(3/4). So \(X\) and \(Y\) are conditionally independent&nbsp; given \(Z\). Unconditionally, \(X\) and \(Y\) are dependent because observing \(X=1\) makes it more likely that we are playing the twin who is worse. That is,<br><br>\[<br>P(Y=1 \mid X=1)&gt;P(Y=1)<br>\]<br><br>Past games give us information which helps us infer who our opponent is, which in turn helps us predict future games! Note that this example is isomorphic to the "random coin" scenario from Example 2.3.7.
-
-============================================================
-
-Note ID: 1721607677391
-  Field: Text
-    Before:
-      The Binomial and Hypergeometric distributions are connected in two important ways. As we will see in this section, we can get from the Binomial to the Hypergeometric by conditioning, and we can get from the Hypergeometric to the Binomial by taking a limit.
-
-    After:
-      The Binomial and Hypergeometric distributions are connected in two important ways. As we will see in this section, we can get from the Binomial to the Hypergeometric by conditioning, and we can get from the Hypergeometric to the Binomial by taking a limit.
-
-============================================================
-
-Note ID: 1721607912323
-  Field: Text
-    Before:
-      <ul><li>Example 3.9.1 (Fisher exact test). A scientist wishes to study whether women or men are more likely to have a certain disease, or whether they are equally likely.&nbsp;</li><li>A random sample of \(n\) women and \(m\) men is gathered, and each person is tested for the disease (assume for this problem that the test is completely accurate).&nbsp;</li><li>The numbers of women and men in the sample who have the disease are \(X\) and \(Y\) respectively, with \(X \sim \operatorname{Bin}\left(n, p_{1}\right)\) and \(Y \sim \operatorname{Bin}\left(m, p_{2}\right)\), independently. Here \(p_{1}\) and \(p_{2}\) are unknown, and we are interested in testing whether \(p_{1}=p_{2}\) (this is known as a null hypothesis in statistics).</li><li>Consider a \(2 \times 2\) table with rows corresponding to disease status and columns corresponding to gender. Each entry is the count of how many people have that disease status and gender, so \(n+m\) is the sum of all 4 entries. Suppose that it is observed that \(X+Y=r\).<br></li><li>The Fisher exact test is based on conditioning on both the row and column sums, so \(n, m, r\) are all treated as fixed, and then seeing if the observed value of \(X\) is "extreme" compared to this conditional distribution.&nbsp;<br></li></ul>
-
-    After:
-      <ul><li>Example 3.9.1 (Fisher exact test). A scientist wishes to study whether women or men are more likely to have a certain disease, or whether they are equally likely.&nbsp;</li><li>A random sample of \(n\) women and \(m\) men is gathered, and each person is tested for the disease (assume for this problem that the test is completely accurate).&nbsp;</li><li>The numbers of women and men in the sample who have the disease are \(X\) and \(Y\) respectively, with \(X \sim \operatorname{Bin}\left(n, p_{1}\right)\) and \(Y \sim \operatorname{Bin}\left(m, p_{2}\right)\), independently. Here \(p_{1}\) and \(p_{2}\) are unknown, and we are interested in testing whether \(p_{1}=p_{2}\) (this is known as a null hypothesis in statistics).</li><li>Consider a \(2 \times 2\) table with rows corresponding to disease status and columns corresponding to gender. Each entry is the count of how many people have that disease status and gender, so \(n+m\) is the sum of all 4 entries. Suppose that it is observed that \(X+Y=r\).<br></li><li>The Fisher exact test is based on conditioning on both the row and column sums, so \(n, m, r\) are all treated as fixed, and then seeing if the observed value of \(X\) is "extreme" compared to this conditional distribution.&nbsp;<br></li></ul>
-
-============================================================
-
-Note ID: 1721613536513
-  Field: Text
-    Before:
-      Theorem 3.9.3. If \(X \sim \operatorname{HGeom}(w, b, n)\) and \(N=w+b \rightarrow \infty\) such that \(p=\) \(w /(w+b)\) remains fixed, then the PMF of \(X\) converges to the \(\operatorname{Bin}(n, p) \mathrm{PMF}\).<br><br>Proof. We take the stated limit of the \(\operatorname{HGeom}(w, b, n)\) PMF:<br><br><ul><li>\(P(X=k)\)&nbsp;</li><li>=\( \frac{\left(\begin{array}{l}w \\k\end{array}\right)\left(\begin{array}{c}b \\n-k\end{array}\right)}{\left(\begin{array}{c}w+b \\n\end{array}\right)} \)</li><li>= \(\left(\begin{array}{l}n \\k\end{array}\right)\frac{\left(\begin{array}{c}w+b-n \\w-k\end{array}\right)}{\left(\begin{array}{c}w+b \\w\end{array}\right)} \quad \text { by Theorem 3.4.5 } \)&nbsp;</li><li>= \(\left(\begin{array}{l}n \\k\end{array}\right) \frac{w !}{(w-k) !} \frac{b !}{(b-n+k) !} \frac{(w+b-n) !}{(w+b) !} \)</li><li>= \(\left(\begin{array}{l}n \\k\end{array}\right) \frac{w(w-1) \ldots(w-k+1) b(b-1) \ldots(b-n+k+1)}{(w+b)(w+b-1) \ldots(w+b-n+1)} \)</li><li>= \(\left(\begin{array}{l}n \\k\end{array}\right) \frac{p\left(p-\frac{1}{N}\right) \ldots\left(p-\frac{k-1}{N}\right) q\left(q-\frac{1}{N}\right) \ldots\left(q-\frac{n-k-1}{N}\right)}{\left(1-\frac{1}{N}\right)\left(1-\frac{2}{N}\right) \ldots\left(1-\frac{n-1}{N}\right)} \)</li></ul><div>As \(N \rightarrow \infty\), the denominator goes to 1 , and the numerator goes to \(p^{k}\) \(q^{n-k}\). Thus<br></div><div><ul><li>\(P\) \((X=k)\) \(\rightarrow\) \(\left(\begin{array}{l}n \\k\end{array}\right)\)&nbsp;\(p^{k}\) \(q^{n-k}\)<br></li></ul></div>
-
-    After:
-      Theorem 3.9.3. If \(X \sim \operatorname{HGeom}(w, b, n)\) and \(N=w+b \rightarrow \infty\) such that \(p=\) \(w /(w+b)\) remains fixed, then the PMF of \(X\) converges to the \(\operatorname{Bin}(n, p) \mathrm{PMF}\).<br><br>Proof. We take the stated limit of the \(\operatorname{HGeom}(w, b, n)\) PMF:<br><br><ul><li>\(P(X=k)\)&nbsp;</li><li>=\( \frac{\left(\begin{array}{l}w \\k\end{array}\right)\left(\begin{array}{c}b \\n-k\end{array}\right)}{\left(\begin{array}{c}w+b \\n\end{array}\right)} \)</li><li>= \(\left(\begin{array}{l}n \\k\end{array}\right)\frac{\left(\begin{array}{c}w+b-n \\w-k\end{array}\right)}{\left(\begin{array}{c}w+b \\w\end{array}\right)} \quad \text { by Theorem 3.4.5 } \)&nbsp;</li><li>= \(\left(\begin{array}{l}n \\k\end{array}\right) \frac{w !}{(w-k) !} \frac{b !}{(b-n+k) !} \frac{(w+b-n) !}{(w+b) !} \)</li><li>= \(\left(\begin{array}{l}n \\k\end{array}\right) \frac{w(w-1) \ldots(w-k+1) b(b-1) \ldots(b-n+k+1)}{(w+b)(w+b-1) \ldots(w+b-n+1)} \)</li><li>= \(\left(\begin{array}{l}n \\k\end{array}\right) \frac{p\left(p-\frac{1}{N}\right) \ldots\left(p-\frac{k-1}{N}\right) q\left(q-\frac{1}{N}\right) \ldots\left(q-\frac{n-k-1}{N}\right)}{\left(1-\frac{1}{N}\right)\left(1-\frac{2}{N}\right) \ldots\left(1-\frac{n-1}{N}\right)} \)</li></ul><div>As \(N \rightarrow \infty\), the denominator goes to 1 , and the numerator goes to \(p^{k}\) \(q^{n-k}\). Thus<br></div><div><ul><li>\(P\) \((X=k)\) \(\rightarrow\) \(\left(\begin{array}{l}n \\k\end{array}\right)\)&nbsp;\(p^{k}\) \(q^{n-k}\)<br></li></ul></div>
-
-============================================================
-
-Note ID: 1721613851724
-  Field: Text
-    Before:
-      The distribution of a discrete r.v. can be defined using a PMF, a CDF, or a story. The PMF of \(X\) is the function \(P(X=x)\) for \(x \in \mathbb{R}\). The CDF of \(X\) is the function \(P(X \leq x)\) for \(x \in \mathbb{R}\).
-
-    After:
-      The distribution of a discrete r.v. can be defined using a PMF, a CDF, or a story. The PMF of \(X\) is the function \(P(X=x)\) for \(x \in \mathbb{R}\). The CDF of \(X\) is the function \(P(X \leq x)\) for \(x \in \mathbb{R}\).
-
-============================================================
-
-Note ID: 1721613906394
-  Field: Text
-    Before:
-      For a PMF to be valid, it must be nonnegative and sum to 1 . For a CDF to be valid, it must be increasing, right-continuous, converge to 0 as \(x \rightarrow\) \(-\infty\), and converge to 1 as \(x \rightarrow\) \(\infty\).
-
-    After:
-      For a PMF to be valid, it must be nonnegative and sum to 1 . For a CDF to be valid, it must be increasing, right-continuous, converge to 0 as \(x \rightarrow\) \(-\infty\), and converge to 1 as \(x \rightarrow\) \(\infty\).
-
-============================================================
-
-Note ID: 1721675007517
-  Field: Text
-    Before:
-      For a PMF to be valid, it must be nonnegative and sum to 1 . For a CDF to be valid, it must be increasing, right-continuous, converge to 0 as \(x \rightarrow-\infty\), and converge to 1 as \(x \rightarrow \infty\).
-
-    After:
-      For a PMF to be valid, it must be nonnegative and sum to 1 . For a CDF to be valid, it must be increasing, right-continuous, converge to 0 as \(x \rightarrow-\infty\), and converge to 1 as \(x \rightarrow \infty\).
-
-============================================================
-
-Note ID: 1721675053148
-  Field: Text
-    Before:
-      It is important to distinguish between a random variable and its distribution: the distribution is a blueprint for building the r.v., but different r.v.s can have the same distribution, just as different houses can be built from the same blueprint.
-
-    After:
-      It is important to distinguish between a random variable and its distribution: the distribution is a blueprint for building the r.v., but different r.v.s can have the same distribution, just as different houses can be built from the same blueprint.
-
-============================================================
-
-Note ID: 1721675258420
-  Field: Text
-    Before:
-      A function of a random variable is still a random variable. If we know the PMF of \(X\), we can find \(P\) \((g(X)=k)\), the PMF of \(g(X)\), by translating the event \(\{g(X)\) = \(k\}\) into an equivalent event involving \(X\), then using the PMF of \(X\).
-
-    After:
-      A function of a random variable is still a random variable. If we know the PMF of \(X\), we can find \(P\) \((g(X)=k)\), the PMF of \(g(X)\), by translating the event \(\{g(X)\) = \(k\}\) into an equivalent event involving \(X\), then using the PMF of \(X\).
-
-============================================================
-
-Note ID: 1721688583895
-  Field: Text
-    Before:
-      <br>\(P(A \cup B) \)&nbsp;=&nbsp;\(P(A)\)&nbsp;+&nbsp;\(P(B) - P(A \cap B)\)<br>Proof:<br><ul><li>\(P(A \cup B)\) = \(P(A \cup (B \cap A^C))\) =&nbsp;\(P(A)\) +&nbsp;\(P(B \cap A^c)\)<br></li><li>Which means</li><li>\(P(A \cap B)\) +&nbsp;\(P(B \cap A^c)\) =&nbsp;\(P(B)\)<br></li><li>Which is true because the two are disjoint by complementation&nbsp;</li><li>Their union is B by LOTP</li></ul>
-
-    After:
-      <br>\(P(A \cup B) \)&nbsp;=&nbsp;\(P(A)\)&nbsp;+&nbsp;\(P(B) - P(A \cap B)\)<br>Proof:<br><ul><li>\(P(A \cup B)\) = \(P(A \cup (B \cap A^C))\) =&nbsp;\(P(A)\) +&nbsp;\(P(B \cap A^c)\)<br></li><li>Which means</li><li>\(P(A \cap B)\) +&nbsp;\(P(B \cap A^c)\) =&nbsp;\(P(B)\)<br></li><li>Which is true because the two are disjoint by complementation&nbsp;</li><li>Their union is B by LOTP</li></ul>
-
-============================================================
-
-Note ID: 1721689816524
-  Field: Text
-    Before:
-      The matchin problem?<br>Shuffle the cards, flip cards and you count what is the likelihood you match<br>Solution<br><ul><li>\(A_j\) = jth card matches<br></li><li>\(P( \cup A_j)\) =&nbsp;\(\sum P(A_j)\) - \(\sum P(A_j \cap A_i)\) + ....<br></li><li>\(P(A_j)\) =&nbsp;\(P(A_i)\) =&nbsp;\(\frac{1}{n}\) since all positions are equally likely for the card labelled j<br></li><li>&nbsp;\(\sum P(A_j \cap A_i)\)&nbsp;=&nbsp;&nbsp;\(\frac{(n-2)!}{n!}\) since all other cards can be in any position</li><li>\(P( \cup A_j)\) =&nbsp;\(\binom{n}{1}\) \(\frac{1}{n}\)&nbsp;-&nbsp; \(\binom{n}{2}\)&nbsp;&nbsp;&nbsp;\(\frac{(n-2)!}{n!}\)<br></li><li>e.t.c</li></ul>
-
-    After:
-      The matchin problem?<br>Shuffle the cards, flip cards and you count what is the likelihood you match<br>Solution<br><ul><li>\(A_j\) = jth card matches<br></li><li>\(P( \cup A_j)\) =&nbsp;\(\sum P(A_j)\) - \(\sum P(A_j \cap A_i)\) + ....<br></li><li>\(P(A_j)\) =&nbsp;\(P(A_i)\) =&nbsp;\(\frac{1}{n}\) since all positions are equally likely for the card labelled j<br></li><li>&nbsp;\(\sum P(A_j \cap A_i)\)&nbsp;=&nbsp;&nbsp;\(\frac{(n-2)!}{n!}\) since all other cards can be in any position</li><li>\(P( \cup A_j)\) =&nbsp;\(\binom{n}{1}\) \(\frac{1}{n}\)&nbsp;-&nbsp; \(\binom{n}{2}\)&nbsp;&nbsp;&nbsp;\(\frac{(n-2)!}{n!}\)<br></li><li>e.t.c</li></ul>
-
-============================================================
-
-Note ID: 1722090276759
-  Field: Text
-    Before:
-      Monty Hall problem, except that Monty enjoys opening Door 2 enjoys opening Door 3 , and if he has a choice between opening rs, he opens Door 2 with probability \(p\), where \(\frac{1}{2} \leq p \leq 1\).<br>re are three doors, behind one of which there is a car (which you ehind the other two of which there are goats (which you don't lly, all possibilities are equally likely for where the car is. You ;, which for concreteness we assume is Door 1. Monty Hall then to reveal a goat, and offers you the option of switching. Assume Iall knows which door has the car, will always open a goat door and offer the option of switching, and as above assume that if Monty Hall has a choice between opening Door 2 and Door 3, he chooses Door 2 with probability \(p\left(\right.\) with \(\left.\frac{1}{2} \leq p \leq 1\right)\)<br><br><ul><li>(a) Find the unconditional probability that the strategy of always switching succeeds (unconditional in the sense that we do not condition on which of Doors 2,3 Monty opens).<br></li><ul><li>Let \(C_j\) be the event that the car is hidden behind door \(j\) and let \(W\) be the event that we win using the switching strategy. Using the law of total probability, we can find the unconditional probability of winning in the same way as in class:</li><li>\(P(W)&nbsp;\) = \( P\left(W \mid C_1\right) P\left(C_1\right)\) + \(P\left(W \mid C_2\right) P\left(C_2\right)\) + \(P\left(W \mid C_3\right) P\left(C_3\right) \)<br></li><li>= \( 0 \cdot 1 / 3+1 \cdot 1 / 3+1 \cdot 1 / 3\)&nbsp; because we cannot win by switching if the car is behind door one</li><li>=&nbsp;\(2/3\)</li></ul></ul>
-
-    After:
-      Monty Hall problem, except that Monty enjoys opening Door 2 enjoys opening Door 3 , and if he has a choice between opening rs, he opens Door 2 with probability \(p\), where \(\frac{1}{2} \leq p \leq 1\).<br>re are three doors, behind one of which there is a car (which you ehind the other two of which there are goats (which you don't lly, all possibilities are equally likely for where the car is. You ;, which for concreteness we assume is Door 1. Monty Hall then to reveal a goat, and offers you the option of switching. Assume Iall knows which door has the car, will always open a goat door and offer the option of switching, and as above assume that if Monty Hall has a choice between opening Door 2 and Door 3, he chooses Door 2 with probability \(p\left(\right.\) with \(\left.\frac{1}{2} \leq p \leq 1\right)\)<br><br><ul><li>(a) Find the unconditional probability that the strategy of always switching succeeds (unconditional in the sense that we do not condition on which of Doors 2,3 Monty opens).<br></li><ul><li>Let \(C_j\) be the event that the car is hidden behind door \(j\) and let \(W\) be the event that we win using the switching strategy. Using the law of total probability, we can find the unconditional probability of winning in the same way as in class:</li><li>\(P(W)&nbsp;\) = \( P\left(W \mid C_1\right) P\left(C_1\right)\) + \(P\left(W \mid C_2\right) P\left(C_2\right)\) + \(P\left(W \mid C_3\right) P\left(C_3\right) \)<br></li><li>= \( 0 \cdot 1 / 3+1 \cdot 1 / 3+1 \cdot 1 / 3\)&nbsp; because we cannot win by switching if the car is behind door one</li><li>=&nbsp;\(2/3\)</li></ul></ul>
-
-============================================================
-
-Note ID: 1722092310960
-  Field: Text
-    Before:
-      (a) In the World Series of baseball, two teams (call them \(A\) and \(B\) ) play a sequence of games against each other, and the first team to win four games wins the series. Let \(p\) be the probability that \(A\) wins an individual game, and assume that the games are independent. What is the probability that team \(A\) wins the series?<br><br><ul><li>\(P(\text { A wins })\) =\(P \text { (A winning in } 4 \text { games })\) +&nbsp;\(P \text { (A winning in } 5 \text { games) }\) +&nbsp;\(P \text { (A wins in } 6 \text { games })\) +&nbsp;\(P \text { (A wins in } 6 \text { games })\)<br></li><ul><li>=&nbsp;\(p^4\)+\(\binom{4}{3}\) \(p^4\) \(q\)+\(\binom{5}{3}\) \(p^4\) \(q^2\)+\(\binom{6}{3}\) \(p^4\) \(q^3\)</li></ul><li>Because, for example:</li><li>\(P(\mathrm{~A} \text { wins in } 5)\) =&nbsp;\(P \text { (A wins } 3 \text { out of first } 4 \text { ) }\) * \(P(\text { A wins } 5 \text { th game } \mid \mathrm{A} \text { wins } 3 \text { out of first } 4)\)<br></li><li>=\(\binom{4}{3}\)\(p^3 q p\)</li><li>(This value can also be found from the PMF of a distribution known as the<br>Negative Binomial, which we will see later in the course.)<br></li></ul>
-
-    After:
-      (a) In the World Series of baseball, two teams (call them \(A\) and \(B\) ) play a sequence of games against each other, and the first team to win four games wins the series. Let \(p\) be the probability that \(A\) wins an individual game, and assume that the games are independent. What is the probability that team \(A\) wins the series?<br><br><ul><li>\(P(\text { A wins })\) =\(P \text { (A winning in } 4 \text { games })\) +&nbsp;\(P \text { (A winning in } 5 \text { games) }\) +&nbsp;\(P \text { (A wins in } 6 \text { games })\) +&nbsp;\(P \text { (A wins in } 6 \text { games })\)<br></li><ul><li>=&nbsp;\(p^4\)+\(\binom{4}{3}\) \(p^4\) \(q\)+\(\binom{5}{3}\) \(p^4\) \(q^2\)+\(\binom{6}{3}\) \(p^4\) \(q^3\)</li></ul><li>Because, for example:</li><li>\(P(\mathrm{~A} \text { wins in } 5)\) =&nbsp;\(P \text { (A wins } 3 \text { out of first } 4 \text { ) }\) * \(P(\text { A wins } 5 \text { th game } \mid \mathrm{A} \text { wins } 3 \text { out of first } 4)\)<br></li><li>=\(\binom{4}{3}\)\(p^3 q p\)</li><li>(This value can also be found from the PMF of a distribution known as the<br>Negative Binomial, which we will see later in the course.)<br></li></ul>
-
-============================================================
-
-Note ID: 1722093693433
-  Field: Text
-    Before:
-      2. A sequence of \(n\) independent experiments is performed. Each experiment is a success with probability \(p\) and a failure with probability \(q=1-p\). Show that conditional on the number of successes, all possibilities for the list of outcomes of the experiment are equally likely (of course, we only consider lists of outcomes where the number of successes is consistent with the information being conditioned on).<br><br><ul><li>Let \(X_j\) be 1 if the \(j\) th experiment is a success and 0 otherwise, and let \(X=\) \(X_1+\cdots+X_n\) be the total number of successes. Then for any \(k\) and any&nbsp;\(a_1, \ldots, a_n \in\{0,1\} \text { with } a_1+\cdots+a_n=k\)<br></li><li>\(P\left(X_1=a_1, \ldots, X_n=a_n \mid X=k\right)\) =&nbsp;<br></li><ul><li>= \(P\left(X_1=a_1, \ldots, X_n=a_n, X=k\right)\)&nbsp;\(/\)&nbsp;\(P(X=k)\)<br></li><li>=&nbsp;\(P\left(X_1=a_1, \ldots, X_n=a_n\right)\)&nbsp;\(/\) \(P(X=k)\)</li><li>=&nbsp;\(p^k q^{n-k}\)&nbsp;\(/\)&nbsp;\(\binom{n}{k} p^k q^{n-k}\)&nbsp;</li><li>= \(\frac{1}{\binom{n}{k} }\)</li></ul><li>This does not depend on \(a_1, \ldots, a_n\). Thus, for \(n\) independent Bernoulli trials, given that there are exactly \(k\) successes, the \(\binom{n}{k}\) possible sequences consisting of \(k\) successes and \(n-k\) failures are equally likely. Interestingly, the conditional probability above also does not depend on \(p\) (this leads to the notion of a sufficient statistic, which is studied in Stat 111).<br></li></ul>
-
-    After:
-      2. A sequence of \(n\) independent experiments is performed. Each experiment is a success with probability \(p\) and a failure with probability \(q=1-p\). Show that conditional on the number of successes, all possibilities for the list of outcomes of the experiment are equally likely (of course, we only consider lists of outcomes where the number of successes is consistent with the information being conditioned on).<br><br><ul><li>Let \(X_j\) be 1 if the \(j\) th experiment is a success and 0 otherwise, and let \(X=\) \(X_1+\cdots+X_n\) be the total number of successes. Then for any \(k\) and any&nbsp;\(a_1, \ldots, a_n \in\{0,1\} \text { with } a_1+\cdots+a_n=k\)<br></li><li>\(P\left(X_1=a_1, \ldots, X_n=a_n \mid X=k\right)\) =&nbsp;<br></li><ul><li>= \(P\left(X_1=a_1, \ldots, X_n=a_n, X=k\right)\)&nbsp;\(/\)&nbsp;\(P(X=k)\)<br></li><li>=&nbsp;\(P\left(X_1=a_1, \ldots, X_n=a_n\right)\)&nbsp;\(/\) \(P(X=k)\)</li><li>=&nbsp;\(p^k q^{n-k}\)&nbsp;\(/\)&nbsp;\(\binom{n}{k} p^k q^{n-k}\)&nbsp;</li><li>= \(\frac{1}{\binom{n}{k} }\)</li></ul><li>This does not depend on \(a_1, \ldots, a_n\). Thus, for \(n\) independent Bernoulli trials, given that there are exactly \(k\) successes, the \(\binom{n}{k}\) possible sequences consisting of \(k\) successes and \(n-k\) failures are equally likely. Interestingly, the conditional probability above also does not depend on \(p\) (this leads to the notion of a sufficient statistic, which is studied in Stat 111).<br></li></ul>
-
-============================================================
-
-Note ID: 1722093766179
-  Field: Text
-    Before:
-      &nbsp;Let \(X \sim \operatorname{Bin}(n, p)\) and \(Y \sim \operatorname{Bin}(m, p)\), independent of \(X\).<br><ul><li>(a) Show that \(X+Y \sim \operatorname{Bin}(n+m, p)\), using a story proof.<br></li><ul><li>Interpret \(X\) as the number of successes in \(n\) independent Bernoulli trials and \(Y\) as the number of successes in \(m\) more independent Bernoulli trials, where each trial has probability \(p\) of success. Then \(X+Y\) is the number of successes in the \(n+m\) trials, so \(X+Y \sim \) \(\operatorname{Bin}(n+m, p)\).<br></li></ul></ul>
-
-    After:
-      &nbsp;Let \(X \sim \operatorname{Bin}(n, p)\) and \(Y \sim \operatorname{Bin}(m, p)\), independent of \(X\).<br><ul><li>(a) Show that \(X+Y \sim \operatorname{Bin}(n+m, p)\), using a story proof.<br></li><ul><li>Interpret \(X\) as the number of successes in \(n\) independent Bernoulli trials and \(Y\) as the number of successes in \(m\) more independent Bernoulli trials, where each trial has probability \(p\) of success. Then \(X+Y\) is the number of successes in the \(n+m\) trials, so \(X+Y \sim \) \(\operatorname{Bin}(n+m, p)\).<br></li></ul></ul>
-
-============================================================
-
-Note ID: 1722093805278
-  Field: Text
-    Before:
-      &nbsp;Let \(X \sim \operatorname{Bin}(n, p)\) and \(Y \sim \operatorname{Bin}(m, p)\), independent of \(X\).<br><ul><li>\(\text { (b) Show that } X-Y \text { is not Binomial. }\)<br></li><ul><li>A Binomial can't be negative, but X - Y is negative with positive probability.<br></li></ul></ul>
-
-    After:
-      &nbsp;Let \(X \sim \operatorname{Bin}(n, p)\) and \(Y \sim \operatorname{Bin}(m, p)\), independent of \(X\).<br><ul><li>\(\text { (b) Show that } X-Y \text { is not Binomial. }\)<br></li><ul><li>A Binomial can't be negative, but X - Y is negative with positive probability.<br></li></ul></ul>
-
-============================================================
-
-Note ID: 1722094222975
-  Field: Text
-    Before:
-      &nbsp;Let \(X \sim \operatorname{Bin}(n, p)\) and \(Y \sim \operatorname{Bin}(m, p)\), independent of \(X\).<br><ul><li>(c) Find \(P(X=k \mid X+Y=j)\). How does this relate to the elk problem from HW 1?<br></li><ul><li>\(P(X=k \mid X+Y=j)&nbsp;\) =&nbsp;<br></li><li>=\( P(X=k, X+Y=j) \) \(/\)&nbsp;\(P(X+Y=j)\)</li><li>=&nbsp;\(P(X=k) P(Y=j-k)\) \(/\)&nbsp;\(P(X+Y=j)\)</li><li>since X=k is independent of Y= j-k this becomes</li><li>=\(&nbsp;\binom{n}{k}\) \(p^k(1-p)^{n-k}\) \(\binom{m}{j-k}\) \(p^{j-k}(1-p)^{m-(j-k)}\)&nbsp;\(/\) \(&nbsp;\binom{m+n}{j}\) \(p^j(1-p)^{m+n-j}\)</li><li>=\(\binom{n}{k}\)\(\binom{m}{j-k}\)&nbsp;\(/\)&nbsp;\(\binom{n+m}{j} .\)</li></ul><li>Note that the p disappeared! This is exactly the same distribution as in the<br>elk problem (it is called the Hypergeometric distribution). To see why, imagine that there are n male elk and m female elk, each of which is tagged with the word "success" with probability p (independently). Suppose we then want to know how many of the male elk are tagged, given that a total of j elk have been tagged. For this, p is no longer relevant, and we can "capture" the male elk and count how many are tagged, analogously to the original elk problem.<br></li></ul>
-
-    After:
-      &nbsp;Let \(X \sim \operatorname{Bin}(n, p)\) and \(Y \sim \operatorname{Bin}(m, p)\), independent of \(X\).<br><ul><li>(c) Find \(P(X=k \mid X+Y=j)\). How does this relate to the elk problem from HW 1?<br></li><ul><li>\(P(X=k \mid X+Y=j)&nbsp;\) =&nbsp;<br></li><li>=\( P(X=k, X+Y=j) \) \(/\)&nbsp;\(P(X+Y=j)\)</li><li>=&nbsp;\(P(X=k) P(Y=j-k)\) \(/\)&nbsp;\(P(X+Y=j)\)</li><li>since X=k is independent of Y= j-k this becomes</li><li>=\(&nbsp;\binom{n}{k}\) \(p^k(1-p)^{n-k}\) \(\binom{m}{j-k}\) \(p^{j-k}(1-p)^{m-(j-k)}\)&nbsp;\(/\) \(&nbsp;\binom{m+n}{j}\) \(p^j(1-p)^{m+n-j}\)</li><li>=\(\binom{n}{k}\)\(\binom{m}{j-k}\)&nbsp;\(/\)&nbsp;\(\binom{n+m}{j} .\)</li></ul><li>Note that the p disappeared! This is exactly the same distribution as in the<br>elk problem (it is called the Hypergeometric distribution). To see why, imagine that there are n male elk and m female elk, each of which is tagged with the word "success" with probability p (independently). Suppose we then want to know how many of the male elk are tagged, given that a total of j elk have been tagged. For this, p is no longer relevant, and we can "capture" the male elk and count how many are tagged, analogously to the original elk problem.<br></li></ul>
-
-============================================================
-
-Note ID: 1722094930357
-  Field: Text
-    Before:
-      Theorem: For a subset&nbsp;\(E\) \(\subset\)\(Y\)&nbsp;&nbsp;\(\subset\)&nbsp;\(X\), then&nbsp;\(E\) is open in \(Y\) iff&nbsp;\(E\) =&nbsp;\(Y\)&nbsp;\(\cap \)&nbsp;\(G\) for some&nbsp;\(G\) open&nbsp;in&nbsp;\(X\)
-
-    After:
-      Theorem: For a subset&nbsp;\(E\) \(\subset\)\(Y\)&nbsp;&nbsp;\(\subset\)&nbsp;\(X\), then&nbsp;\(E\) is open in \(Y\) iff&nbsp;\(E\) =&nbsp;\(Y\)&nbsp;\(\cap \)&nbsp;\(G\) for some&nbsp;\(G\) open&nbsp;in&nbsp;\(X\)
-
-============================================================
-
-Note ID: 1722095142971
-  Field: Text
-    Before:
-      Theorem: If \(Y\)&nbsp;\(\subset\)&nbsp;\(X\), then&nbsp;\(K\) compact in&nbsp;\(Y\) is equvalent to&nbsp;\(K\) being compact in X
-
-    After:
-      Theorem: If \(Y\)&nbsp;\(\subset\)&nbsp;\(X\), then&nbsp;\(K\) compact in&nbsp;\(Y\) is equvalent to&nbsp;\(K\) being compact in X
-
-============================================================
-
-Note ID: 1722096782970
-  Field: Text
-    Before:
-      Theorem: If \(Y\)&nbsp;\(\subset\)&nbsp;\(X\), then&nbsp;\(K\) compact in&nbsp;\(Y\) is equvalent to&nbsp;\(K\) being compact in X<br><br><b>Forward</b> Proof that \(K\) compact in&nbsp;\(Y\) is equvalent to&nbsp;\(K\) being compact in X<br><ul><li>Assume K is compact in Y</li><li>Consider an open cover of K in X, \(O_{\alpha}\), we need to find a finite subcover</li><li>Since K is also&nbsp; subset of Y, we can consider \(O_{\alpha} \cap Y\)</li><li>Since every set \(O_{\alpha} \cap Y\) is in&nbsp;\(Y\), then the open cover admits a finite sub-cover \(O_{i} \cap Y\)<br></li><li>Then the \(O_{i}\) original sets cover&nbsp;\(K\) in X</li></ul>
-
-    After:
-      Theorem: If \(Y\)&nbsp;\(\subset\)&nbsp;\(X\), then&nbsp;\(K\) compact in&nbsp;\(Y\) is equvalent to&nbsp;\(K\) being compact in X<br><br><b>Forward</b> Proof that \(K\) compact in&nbsp;\(Y\) is equvalent to&nbsp;\(K\) being compact in X<br><ul><li>Assume K is compact in Y</li><li>Consider an open cover of K in X, \(O_{\alpha}\), we need to find a finite subcover</li><li>Since K is also&nbsp; subset of Y, we can consider \(O_{\alpha} \cap Y\)</li><li>Since every set \(O_{\alpha} \cap Y\) is in&nbsp;\(Y\), then the open cover admits a finite sub-cover \(O_{i} \cap Y\)<br></li><li>Then the \(O_{i}\) original sets cover&nbsp;\(K\) in X</li></ul>
-
-============================================================
-
-Note ID: 1722097136665
-  Field: Text
-    Before:
-      Theorem: If \(Y\)&nbsp;\(\subset\)&nbsp;\(X\), then&nbsp;\(K\) compact in&nbsp;\(Y\) is equvalent to&nbsp;\(K\) being compact in X<br><br><b>Backward</b> Proof that \(K\) compact in&nbsp;\(Y\) is equvalent to&nbsp;\(K\) being compact in X<br><ul><li>Assume K is compact in X</li><li>Assume an open cover in Y exists&nbsp;\(O_{\alpha}\)</li><li>Every element&nbsp;&nbsp;\(O_{\alpha}\) =&nbsp;\(Y\)&nbsp;\(\cap\)&nbsp;\(G_{\alpha}\) for some open \(G_{\alpha}\) in X</li><li>Then \(G_{\alpha}\) is an open cover in X and thus admits a finite subcover \(G_{i}\)</li><li>Then \(O_{i}\) is a finite subcover for K in Y</li></ul>
-
-    After:
-      Theorem: If \(Y\)&nbsp;\(\subset\)&nbsp;\(X\), then&nbsp;\(K\) compact in&nbsp;\(Y\) is equvalent to&nbsp;\(K\) being compact in X<br><br><b>Backward</b> Proof that \(K\) compact in&nbsp;\(Y\) is equvalent to&nbsp;\(K\) being compact in X<br><ul><li>Assume K is compact in X</li><li>Assume an open cover in Y exists&nbsp;\(O_{\alpha}\)</li><li>Every element&nbsp;&nbsp;\(O_{\alpha}\) =&nbsp;\(Y\)&nbsp;\(\cap\)&nbsp;\(G_{\alpha}\) for some open \(G_{\alpha}\) in X</li><li>Then \(G_{\alpha}\) is an open cover in X and thus admits a finite subcover \(G_{i}\)</li><li>Then \(O_{i}\) is a finite subcover for K in Y</li></ul>
-
-============================================================
-
-Note ID: 1722097253903
-  Field: Text
-    Before:
-      Compactness is an intrinsic property of a set, since compactness in a subset always implies compactness in the enclosing set and vice-versa (as long as the same metric is used in both)
-
-    After:
-      Compactness is an intrinsic property of a set, since compactness in a subset always implies compactness in the enclosing set and vice-versa (as long as the same metric is used in both)
-
-============================================================
-
-Note ID: 1722121042185
-  Field: Text
-    Before:
-      (Observe that the last equation is indeed a kind of commutative law: it says that complementation followed by inversion is the same as inversion followed by complementation.)
-
-    After:
-      (Observe that the last equation is indeed a kind of commutative law: it says that complementation followed by inversion is the same as inversion followed by complementation.)
-
-============================================================
-
-Note ID: 1722172310370
-  Field: Text
-    Before:
-      Proof that compact sets are closed:<br><ul><li>Pictographically: <img src="paste-864c826b4377fd119e0a7488b59bc23b51c3a0fb.jpg"><br></li><li>Let K be compact and consider&nbsp;\(p\)&nbsp;\(\not \in\)&nbsp;\(K\)</li><li>We want to show that&nbsp;\(p\) has a neighbourhood that does not intersect K which is the same as showing it is not a limit point of K i.e it is interior to&nbsp;\(K^c\)</li><li>Or it is the same as showing that the complement of K is open</li><li>\(\forall\)&nbsp;\(q\)&nbsp;\(in\)&nbsp;\(K\), let&nbsp;\(V_q\) =&nbsp;\(N_{r/2} (q)\) be a ball around q and \(U_p\) =&nbsp;\(N_{r/2} (p)\)</li><ul><li>&nbsp;Where&nbsp;\(r\) =&nbsp;\(d\)&nbsp;\( (p,q)\)</li></ul><li>Notice that the collection \({V_q}\) is an open cover of K</li><li>By compactness of K there exists a finite subcover&nbsp;\(V_{q_{n} }\)</li><li>Which allows us to pick the equivalent finite set from the complement of K</li><li>Define&nbsp;\(W\) =&nbsp;\(\cap_n\) \(U_{q_{n} }\) which is open because it is a finite intersection of open sets</li><li>Specifically it is a ball whose radius is the minimum distance between&nbsp;\(p_i\) and&nbsp;\(q_i\)</li><li>Pictographically: <img src="paste-40a5b413ea121ae49567ef8eefa6c56396c2bf5e.jpg"></li><li>\(W\) \(\cap\) \(V_{q_i}\) =&nbsp;\(\emptyset\) because&nbsp;\(W\)&nbsp;\(\in\)&nbsp;\(U_{q_i}\) and&nbsp; \(U_{q_i}\) &nbsp;\(\cap\)\(V_{q_i}\) =&nbsp;\(\emptyset\)</li><li>Thus W is the desired neighbourhood showing that any arbitrary point p is interrior&nbsp;to&nbsp;\(K^c\) meaning that&nbsp; \(K^c\) is open, making&nbsp;\(K\) closed</li></ul>
-
-    After:
-      Proof that compact sets are closed:<br><ul><li>Pictographically: <img src="paste-864c826b4377fd119e0a7488b59bc23b51c3a0fb.jpg"><br></li><li>Let K be compact and consider&nbsp;\(p\)&nbsp;\(\not \in\)&nbsp;\(K\)</li><li>We want to show that&nbsp;\(p\) has a neighbourhood that does not intersect K which is the same as showing it is not a limit point of K i.e it is interior to&nbsp;\(K^c\)</li><li>Or it is the same as showing that the complement of K is open</li><li>\(\forall\)&nbsp;\(q\)&nbsp;\(in\)&nbsp;\(K\), let&nbsp;\(V_q\) =&nbsp;\(N_{r/2} (q)\) be a ball around q and \(U_p\) =&nbsp;\(N_{r/2} (p)\)</li><ul><li>&nbsp;Where&nbsp;\(r\) =&nbsp;\(d\)&nbsp;\( (p,q)\)</li></ul><li>Notice that the collection \({V_q}\) is an open cover of K</li><li>By compactness of K there exists a finite subcover&nbsp;\(V_{q_{n} }\)</li><li>Which allows us to pick the equivalent finite set from the complement of K</li><li>Define&nbsp;\(W\) =&nbsp;\(\cap_n\) \(U_{q_{n} }\) which is open because it is a finite intersection of open sets</li><li>Specifically it is a ball whose radius is the minimum distance between&nbsp;\(p_i\) and&nbsp;\(q_i\)</li><li>Pictographically: <img src="paste-40a5b413ea121ae49567ef8eefa6c56396c2bf5e.jpg"></li><li>\(W\) \(\cap\) \(V_{q_i}\) =&nbsp;\(\emptyset\) because&nbsp;\(W\)&nbsp;\(\in\)&nbsp;\(U_{q_i}\) and&nbsp; \(U_{q_i}\) &nbsp;\(\cap\)\(V_{q_i}\) =&nbsp;\(\emptyset\)</li><li>Thus W is the desired neighbourhood showing that any arbitrary point p is interrior&nbsp;to&nbsp;\(K^c\) meaning that&nbsp; \(K^c\) is open, making&nbsp;\(K\) closed</li></ul>
-
-============================================================
-
-Note ID: 1722174512947
-  Field: Text
-    Before:
-      A closed set subset B of a compact set K is also compact<br>Proof:<br><ul><li>Pictographically: <img src="paste-79cebeddeb9dc2c92d2259a1776cd7cc45f6318f.jpg"></li><li>Consider an open cover of B,&nbsp;\( {O_{\alpha} }\)</li><li>Pictographically:&nbsp; <img src="paste-93cca5e41f2d3dbccb9633e19f1422c9d6253674.jpg"></li><li>Since B is embedded in K,&nbsp;&nbsp;&nbsp;\( {O_{\alpha} }\) \(\in\) \(K\)</li><li>Since B is closed,&nbsp;\(B^c\) is open&nbsp;</li><li>Pictographically: <img src="paste-eea0fe6c837389cef626738870dd46c4af1264bf.jpg"></li><li>This means that&nbsp;\(O_{\alpha}\)&nbsp;\(\cup\)&nbsp;\(B^c\) is an open cover of K</li><li>By compactness, there must be a finite subcover {\(O_{\alpha_{i} }\) \(B^c\)}</li><li>Since&nbsp;\(B\) \(\cap\) \(B^c\) = \(\emptyset\), the finite subcover&nbsp; \(O_{\alpha_{i} }\) covers B&nbsp;&nbsp;</li></ul>
-
-    After:
-      A closed set subset B of a compact set K is also compact<br>Proof:<br><ul><li>Pictographically: <img src="paste-79cebeddeb9dc2c92d2259a1776cd7cc45f6318f.jpg"></li><li>Consider an open cover of B,&nbsp;\( {O_{\alpha} }\)</li><li>Pictographically:&nbsp; <img src="paste-93cca5e41f2d3dbccb9633e19f1422c9d6253674.jpg"></li><li>Since B is embedded in K,&nbsp;&nbsp;&nbsp;\( {O_{\alpha} }\) \(\in\) \(K\)</li><li>Since B is closed,&nbsp;\(B^c\) is open&nbsp;</li><li>Pictographically: <img src="paste-eea0fe6c837389cef626738870dd46c4af1264bf.jpg"></li><li>This means that&nbsp;\(O_{\alpha}\)&nbsp;\(\cup\)&nbsp;\(B^c\) is an open cover of K</li><li>By compactness, there must be a finite subcover {\(O_{\alpha_{i} }\) \(B^c\)}</li><li>Since&nbsp;\(B\) \(\cap\) \(B^c\) = \(\emptyset\), the finite subcover&nbsp; \(O_{\alpha_{i} }\) covers B&nbsp;&nbsp;</li></ul>
-
-============================================================
-
-Note ID: 1722174705007
-  Field: Text
-    Before:
-      If you have a closed set F and a compact set K in a metric space X, then their intersection is going to be compact<br>Pictographically:&nbsp;<img src="paste-49a5ebdc62c0cf09e18d7c2c5fecd8865482da91.jpg">
-
-    After:
-      If you have a closed set F and a compact set K in a metric space X, then their intersection is going to be compact<br>Pictographically:&nbsp;<img src="paste-49a5ebdc62c0cf09e18d7c2c5fecd8865482da91.jpg">
-
-============================================================
-
-Note ID: 1722175027156
-  Field: Text
-    Before:
-      Theorem:The intersection of nested closed intervals in R is not empty, which means the intersection closed nested \(K\)-cells in&nbsp; \(R^k\)  is not empty<br>Pictographically:&nbsp; <img src="paste-790e5ba446bf632b069fa2f8363543fc1d621e51.jpg">
-
-    After:
-      Theorem:The intersection of nested closed intervals in R is not empty, which means the intersection closed nested \(K\)-cells in&nbsp; \(R^k\)  is not empty<br>Pictographically:&nbsp; <img src="paste-790e5ba446bf632b069fa2f8363543fc1d621e51.jpg">
-
-============================================================
-
-Note ID: 1722175442148
-  Field: Text
-    Before:
-      Proof for the nested interval property:<br><ul><li>Pictographically:&nbsp; <img src="paste-7d1661b37ebc51d59c54e68c0e319a11b2c3fdc0.jpg"></li><li>\(I_n\) =&nbsp;\([a_n, b_n]\)<br></li><li>\(I_{n+1}\)&nbsp;\(\subset\)&nbsp;\(I_n\)<br></li><li>Let x =&nbsp;\(\sup\)&nbsp;\({a_i}\) which exists because all intervals are bounded by&nbsp;\(b_1\)</li><li>\(x\)&nbsp;\(\geq\)&nbsp;\(a_i\) because it is the supremum<br></li><li>\(x\)&nbsp;\(\ &lt; \)&nbsp;\(b_n\) for all&nbsp;\(n\) because&nbsp;\(b_n\)&nbsp;\(\geq\)&nbsp;\(a_i\) for all i<br></li><li>Thus&nbsp;\(x\)&nbsp;\(\in\)&nbsp;\(I_n\) for all n and thus&nbsp;\(x\)&nbsp;\(\in\)&nbsp;\(\cap I_n\)</li></ul>
-
-    After:
-      Proof for the nested interval property:<br><ul><li>Pictographically:&nbsp; <img src="paste-7d1661b37ebc51d59c54e68c0e319a11b2c3fdc0.jpg"></li><li>\(I_n\) =&nbsp;\([a_n, b_n]\)<br></li><li>\(I_{n+1}\)&nbsp;\(\subset\)&nbsp;\(I_n\)<br></li><li>Let x =&nbsp;\(\sup\)&nbsp;\({a_i}\) which exists because all intervals are bounded by&nbsp;\(b_1\)</li><li>\(x\)&nbsp;\(\geq\)&nbsp;\(a_i\) because it is the supremum<br></li><li>\(x\)&nbsp;\(\ &lt; \)&nbsp;\(b_n\) for all&nbsp;\(n\) because&nbsp;\(b_n\)&nbsp;\(\geq\)&nbsp;\(a_i\) for all i<br></li><li>Thus&nbsp;\(x\)&nbsp;\(\in\)&nbsp;\(I_n\) for all n and thus&nbsp;\(x\)&nbsp;\(\in\)&nbsp;\(\cap I_n\)</li></ul>
-
-============================================================
-
-Note ID: 1722176082869
-  Field: Text
-    Before:
-      Proof that R is uncountable:<br><ul><li>Pictographically: <img src="paste-475b17f8a3bfbb503aaa63e15f3725581f074712.jpg"></li><li>Suppose it were countable and we have a list of all elements&nbsp;\(\{x_i \}\)</li><li>We can choose&nbsp;\(I_1\) such that&nbsp;\(x_i\)&nbsp;\(\not \in\)&nbsp;\(I_1\)</li><li>Then choose&nbsp;\(I_n\)&nbsp;\(\subset\)&nbsp;\(I_{n-1}\) such that&nbsp;\(x_n\)&nbsp;\(\not \in\)&nbsp;\(I_n\)</li><li>By the nested interval property there exists an element in&nbsp;\(\cap_n\)&nbsp;\(I_n\) which is not on the original list</li></ul>
-
-    After:
-      Proof that R is uncountable:<br><ul><li>Pictographically: <img src="paste-475b17f8a3bfbb503aaa63e15f3725581f074712.jpg"></li><li>Suppose it were countable and we have a list of all elements&nbsp;\(\{x_i \}\)</li><li>We can choose&nbsp;\(I_1\) such that&nbsp;\(x_i\)&nbsp;\(\not \in\)&nbsp;\(I_1\)</li><li>Then choose&nbsp;\(I_n\)&nbsp;\(\subset\)&nbsp;\(I_{n-1}\) such that&nbsp;\(x_n\)&nbsp;\(\not \in\)&nbsp;\(I_n\)</li><li>By the nested interval property there exists an element in&nbsp;\(\cap_n\)&nbsp;\(I_n\) which is not on the original list</li></ul>
-
-============================================================
-
-Note ID: 1722182014869
-  Field: Text
-    Before:
-      Proof that any closed interval [a,b] in R is compact:<br><ul><li>Assume the interval is not compact</li><li>Then there exists an open cover \(O_{\alpha}\) without a finite subcover&nbsp;</li><li>Pictographically: <img src="paste-f454a7e35dc3ea3bcc0fdc1e9066cf9636efb0ae.jpg"></li><li>Divide the interval into&nbsp;\([a, c_1]\) and&nbsp;&nbsp;\([c_1, b]\)</li><li>Thus&nbsp;\(O_{\alpha}\) covers&nbsp;&nbsp;&nbsp;\([a, c_1]\)&nbsp; and \([c_1, b]\)</li><li>At least one of these intervals must also not have a finite subcover otherwise you coult just union two finite subcovers<br></li><li>Pick the one without a finite subcover&nbsp;\(I_1\) (WLOG \([a,c_1]\)</li><li>Then repeat the process splitting&nbsp;\(I_1\) into two halves by&nbsp;\(c_2\) to get&nbsp;\(I_2\)</li><li>Generally&nbsp;\(I_{n+1}\)&nbsp;\(\subset\)&nbsp;\(I_n\) created by difviding&nbsp;\(I_{n}\) by&nbsp;\(c_n\) such that we choose the </li><li>half without a finite subcover</li><li></li><li>By the nested interval property there exists \(x\) such that \(\cap_n I_n\) =&nbsp;\(x\)&nbsp;<br></li><li>Pictographically: <img src="paste-dd9c9e556ab016c8ec5ac318d70901050a90a0c3.jpg"></li><li>\(x\)&nbsp;\(\in\)&nbsp;\(O_{\alpha^\prime}\) for some&nbsp;\(\alpha^\prime\)<br></li><li>So there exists&nbsp;\(r\)&nbsp;\(&gt;\)&nbsp;\(0\) such that&nbsp;\(\exists\)&nbsp;\(N_{r}(x)\)&nbsp;\(\subset\)&nbsp;\(\)&nbsp;&nbsp;\(O_{\alpha^\prime}\)</li><li>Since the intervals get arbitrarily small, there exists&nbsp;\(I_m\)&nbsp;\(\subset \)&nbsp;&nbsp;\(N_{r}(x)\)&nbsp; meaning that&nbsp; \(O_{\alpha^\prime}\) covers \(I_m\)</li><li>Which contradicts \(I_m\) not having a finite subcover</li></ul>
-
-    After:
-      Proof that any closed interval [a,b] in R is compact:<br><ul><li>Assume the interval is not compact</li><li>Then there exists an open cover \(O_{\alpha}\) without a finite subcover&nbsp;</li><li>Pictographically: <img src="paste-f454a7e35dc3ea3bcc0fdc1e9066cf9636efb0ae.jpg"></li><li>Divide the interval into&nbsp;\([a, c_1]\) and&nbsp;&nbsp;\([c_1, b]\)</li><li>Thus&nbsp;\(O_{\alpha}\) covers&nbsp;&nbsp;&nbsp;\([a, c_1]\)&nbsp; and \([c_1, b]\)</li><li>At least one of these intervals must also not have a finite subcover otherwise you coult just union two finite subcovers<br></li><li>Pick the one without a finite subcover&nbsp;\(I_1\) (WLOG \([a,c_1]\)</li><li>Then repeat the process splitting&nbsp;\(I_1\) into two halves by&nbsp;\(c_2\) to get&nbsp;\(I_2\)</li><li>Generally&nbsp;\(I_{n+1}\)&nbsp;\(\subset\)&nbsp;\(I_n\) created by difviding&nbsp;\(I_{n}\) by&nbsp;\(c_n\) such that we choose the </li><li>half without a finite subcover</li><li></li><li>By the nested interval property there exists \(x\) such that \(\cap_n I_n\) =&nbsp;\(x\)&nbsp;<br></li><li>Pictographically: <img src="paste-dd9c9e556ab016c8ec5ac318d70901050a90a0c3.jpg"></li><li>\(x\)&nbsp;\(\in\)&nbsp;\(O_{\alpha^\prime}\) for some&nbsp;\(\alpha^\prime\)<br></li><li>So there exists&nbsp;\(r\)&nbsp;\(&gt;\)&nbsp;\(0\) such that&nbsp;\(\exists\)&nbsp;\(N_{r}(x)\)&nbsp;\(\subset\)&nbsp;\(\)&nbsp;&nbsp;\(O_{\alpha^\prime}\)</li><li>Since the intervals get arbitrarily small, there exists&nbsp;\(I_m\)&nbsp;\(\subset \)&nbsp;&nbsp;\(N_{r}(x)\)&nbsp; meaning that&nbsp; \(O_{\alpha^\prime}\) covers \(I_m\)</li><li>Which contradicts \(I_m\) not having a finite subcover</li></ul>
-
-============================================================
-
-Note ID: 1722183224988
-  Field: Text
-    Before:
-      Proof for the Heinel-Borel theorem in R: A set is K compact iff it is closed and bounded<br><ul><li>Backward direction, assume a set is closed and bounded</li><li>Since K is bounded&nbsp;\(K\)&nbsp;\(\subset\)&nbsp;\([-r, r]\) or of an n-cell if operating in higher dim spaces</li><ul><li>Since \([-r, r]\)&nbsp; is compact then K must be compact&nbsp;</li></ul></ul>
-
-    After:
-      Proof for the Heinel-Borel theorem in R: A set is K compact iff it is closed and bounded<br><ul><li>Backward direction, assume a set is closed and bounded</li><li>Since K is bounded&nbsp;\(K\)&nbsp;\(\subset\)&nbsp;\([-r, r]\) or of an n-cell if operating in higher dim spaces</li><ul><li>Since \([-r, r]\)&nbsp; is compact then K must be compact&nbsp;</li></ul></ul>
-
-============================================================
-
-Note ID: 1722188431839
-  Field: Text
-    Before:
-      Proof for Bolzano-Weirstrass: every infinite bounded subset of&nbsp;\(R^n\) has a limit point:<br><ul><li>&nbsp;If a subset&nbsp;\(E\) is bounded then&nbsp;\(E\) is in some compact&nbsp;\(K\)-cell&nbsp;</li><li>Since it is a bounded subset of a compact set, it has a limit point in the&nbsp;\(K\)-cell by the Heinel-Borel theorem</li></ul>
-
-    After:
-      Proof for Bolzano-Weirstrass: every infinite bounded subset of&nbsp;\(R^n\) has a limit point:<br><ul><li>&nbsp;If a subset&nbsp;\(E\) is bounded then&nbsp;\(E\) is in some compact&nbsp;\(K\)-cell&nbsp;</li><li>Since it is a bounded subset of a compact set, it has a limit point in the&nbsp;\(K\)-cell by the Heinel-Borel theorem</li></ul>
-
-============================================================
-
-Note ID: 1722189318493
-  Field: Text
-    Before:
-      What is Cantor's Finite Intersection property?<br><ul><li>A collection of compact subsets&nbsp;\(K_{\alpha}\) of some arbitrary metric space&nbsp;\(X\)</li><li>If any finite sub-collection has a non-empty intersection,&nbsp;\(\cap_{\alpha} K_{\alpha}\)&nbsp;\(\neq\)&nbsp;\(\emptyset\)</li><li>This is a generalization of the nested interval property to arbitrary metric spaces</li></ul><div>Proof:</div><div><ul><li>Assume that it is possible for a finite sub-collection to have a non-empty intersection but the intersection of all Ks to be empty</li><li>Let&nbsp;\(U_{\alpha}\) =&nbsp;\(K_{\alpha}^c\), which is open</li><li>Pick an arbitrary K in&nbsp;\(K_{\alpha}\)</li><li>If&nbsp;\(\cap_{\alpha}\)&nbsp;\(K_{\alpha}\) =&nbsp;\(\emptyset\) then&nbsp;\( \{ U_{\alpha} \}\) would cover&nbsp;\(K\) which we know to be compact</li><ul><li>This is because if no point is in the intersection than all points must be in the complement of one K</li></ul><li>Which implies there exists&nbsp;\(\{U_{\alpha_n} \}\) covering&nbsp;\(K\)</li><li>Implying that&nbsp; \( \cap_{i=1}^{n}\) \(K_{\alpha_i}\) \(\cap\) \(K\) =&nbsp;\(\emptyset\)</li></ul></div>
-
-    After:
-      What is Cantor's Finite Intersection property?<br><ul><li>A collection of compact subsets&nbsp;\(K_{\alpha}\) of some arbitrary metric space&nbsp;\(X\)</li><li>If any finite sub-collection has a non-empty intersection,&nbsp;\(\cap_{\alpha} K_{\alpha}\)&nbsp;\(\neq\)&nbsp;\(\emptyset\)</li><li>This is a generalization of the nested interval property to arbitrary metric spaces</li></ul><div>Proof:</div><div><ul><li>Assume that it is possible for a finite sub-collection to have a non-empty intersection but the intersection of all Ks to be empty</li><li>Let&nbsp;\(U_{\alpha}\) =&nbsp;\(K_{\alpha}^c\), which is open</li><li>Pick an arbitrary K in&nbsp;\(K_{\alpha}\)</li><li>If&nbsp;\(\cap_{\alpha}\)&nbsp;\(K_{\alpha}\) =&nbsp;\(\emptyset\) then&nbsp;\( \{ U_{\alpha} \}\) would cover&nbsp;\(K\) which we know to be compact</li><ul><li>This is because if no point is in the intersection than all points must be in the complement of one K</li></ul><li>Which implies there exists&nbsp;\(\{U_{\alpha_n} \}\) covering&nbsp;\(K\)</li><li>Implying that&nbsp; \( \cap_{i=1}^{n}\) \(K_{\alpha_i}\) \(\cap\) \(K\) =&nbsp;\(\emptyset\)</li></ul></div>
-
-============================================================
-
-Note ID: 1723310652604
-  Field: Text
-    Before:
-      Theorem: For \(\{K_{\alpha}\}\) compact subsets of a metric space X, if for any finite subcollection&nbsp;&nbsp;\(\cap_i\) \(K_{i}\)&nbsp;\(\neq\)&nbsp;\(\emptyset\) then&nbsp;&nbsp;\(\cap_\alpha\) \(K_{\alpha}\)&nbsp;\(\neq\)&nbsp;\(\emptyset\)&nbsp;
-
-    After:
-      Theorem: For \(\{K_{\alpha}\}\) compact subsets of a metric space X, if for any finite subcollection&nbsp;&nbsp;\(\cap_i\) \(K_{i}\)&nbsp;\(\neq\)&nbsp;\(\emptyset\) then&nbsp;&nbsp;\(\cap_\alpha\) \(K_{\alpha}\)&nbsp;\(\neq\)&nbsp;\(\emptyset\)&nbsp;
-
-============================================================
-
-Note ID: 1723386798926
-  Field: Text
-    Before:
-      &nbsp;A space X is compact iff any collection of closed sets satisfied the finite intersection property<br><br>Proof:<br><ul><li>Assume X is compact and that we have a collection of closed sets&nbsp;\(D_{\alpha}\) \(\subset X\)</li><li>Since these are closed subsets of a compact space, they are also compact</li><li>Since they are compact, it is necessarily true that any finite subcollection having non-empty intersection implies&nbsp;&nbsp;\(\cap_{\alpha}\) \(D_{\alpha} \)&nbsp;\(\neq\)&nbsp;\(\emptyset\)</li></ul>
-
-    After:
-      &nbsp;A space X is compact iff any collection of closed sets satisfied the finite intersection property<br><br>Proof:<br><ul><li>Assume X is compact and that we have a collection of closed sets&nbsp;\(D_{\alpha}\) \(\subset X\)</li><li>Since these are closed subsets of a compact space, they are also compact</li><li>Since they are compact, it is necessarily true that any finite subcollection having non-empty intersection implies&nbsp;&nbsp;\(\cap_{\alpha}\) \(D_{\alpha} \)&nbsp;\(\neq\)&nbsp;\(\emptyset\)</li></ul>
-
-============================================================
-
-Note ID: 1723387865343
-  Field: Text
-    Before:
-      &nbsp;A space X is compact iff any collection of closed sets satisfied the finite intersection property (FIP)<br><br>Proof by contradiction:<br><ul><li>Assume that any collection of closed sets satisfied the FIP</li><li>Then, for any open cover&nbsp;\(O_{\alpha}\) its complement&nbsp;&nbsp;\(O_{\alpha}^c\) is closed</li><li>Thus \(\cap_{i} \) \(O_{i}^c\) \(\neq\)&nbsp;\(\emptyset\) implies \(\cap_{\alpha}\) \(O_{\alpha}^c\)&nbsp;\(\neq\)&nbsp;\(\emptyset\)</li><li>Negate the entire expression via de-morgan</li><li>If \(\cup_{\alpha}\) \(O_{\alpha}\)&nbsp;\(=\)&nbsp;\(X\) then \(\cup_{i}\) \(O_{i}\)&nbsp;\(=\)&nbsp;\(X\)</li><li>Thus every open cover has a finite subcover</li></ul>
-
-    After:
-      &nbsp;A space X is compact iff any collection of closed sets satisfied the finite intersection property (FIP)<br><br>Proof by contradiction:<br><ul><li>Assume that any collection of closed sets satisfied the FIP</li><li>Then, for any open cover&nbsp;\(O_{\alpha}\) its complement&nbsp;&nbsp;\(O_{\alpha}^c\) is closed</li><li>Thus \(\cap_{i} \) \(O_{i}^c\) \(\neq\)&nbsp;\(\emptyset\) implies \(\cap_{\alpha}\) \(O_{\alpha}^c\)&nbsp;\(\neq\)&nbsp;\(\emptyset\)</li><li>Negate the entire expression via de-morgan</li><li>If \(\cup_{\alpha}\) \(O_{\alpha}\)&nbsp;\(=\)&nbsp;\(X\) then \(\cup_{i}\) \(O_{i}\)&nbsp;\(=\)&nbsp;\(X\)</li><li>Thus every open cover has a finite subcover</li></ul>
-
-============================================================
-
-Note ID: 1723391775615
-  Field: Text
-    Before:
-      What is a basis for the topology of a metric space?<br><ul><li>A basis is a collection&nbsp;\(\{V_{\alpha}\}\) such that<br></li><li>\(\forall\)&nbsp;\(x\)&nbsp;\(\in\)&nbsp;\(U\), where&nbsp;\(U\) is any open set<br></li><li>\(\exists\)&nbsp;\(V_{\beta}\) such that&nbsp;\(x\)&nbsp;\(\in\)&nbsp;\(\) \(V_{\beta}\) and \(V_{\beta}\)&nbsp;\(\subset\)&nbsp;\(U\)<br></li><li>Meaning that every open set is the union of base elements</li><li>Thus any set that that has a countable basis, must be small</li><li>And a compact metric space has a countable basis (countable dense subset)</li></ul>
-
-    After:
-      What is a basis for the topology of a metric space?<br><ul><li>A basis is a collection&nbsp;\(\{V_{\alpha}\}\) such that<br></li><li>\(\forall\)&nbsp;\(x\)&nbsp;\(\in\)&nbsp;\(U\), where&nbsp;\(U\) is any open set<br></li><li>\(\exists\)&nbsp;\(V_{\beta}\) such that&nbsp;\(x\)&nbsp;\(\in\)&nbsp;\(\) \(V_{\beta}\) and \(V_{\beta}\)&nbsp;\(\subset\)&nbsp;\(U\)<br></li><li>Meaning that every open set is the union of base elements</li><li>Thus any set that that has a countable basis, must be small</li><li>And a compact metric space has a countable basis (countable dense subset)</li></ul>
-
-============================================================
-
-Note ID: 1723392186211
-  Field: Text
-    Before:
-      What is a separated set?<br><br><ul><li>Picographically connected vs separated: <img src="paste-2f5fb451489476cc9349ed525cfa095a422d333d.jpg"></li><li>Two sets&nbsp;\(A\) and&nbsp;\(B \) in a metric space&nbsp;\(X\) are separated if</li><li>\(A \cap \bar{B}\) =&nbsp;\(\emptyset\)<br></li><li>\(\bar{A}&nbsp;\cap B\) =&nbsp;\(\emptyset\)</li><li>And thus no limit point of one is a limit point of the other</li></ul>
-
-    After:
-      What is a separated set?<br><br><ul><li>Picographically connected vs separated: <img src="paste-2f5fb451489476cc9349ed525cfa095a422d333d.jpg"></li><li>Two sets&nbsp;\(A\) and&nbsp;\(B \) in a metric space&nbsp;\(X\) are separated if</li><li>\(A \cap \bar{B}\) =&nbsp;\(\emptyset\)<br></li><li>\(\bar{A}&nbsp;\cap B\) =&nbsp;\(\emptyset\)</li><li>And thus no limit point of one is a limit point of the other</li></ul>
-
-============================================================
-
-Note ID: 1723392239938
-  Field: Text
-    Before:
-      When is a set E connected?<br><ul><li>A set E is connected if it is not the union of two separated sets</li></ul>
-
-    After:
-      When is a set E connected?<br><ul><li>A set E is connected if it is not the union of two separated sets</li></ul>
-
-============================================================
-
-Note ID: 1723393619709
-  Field: Text
-    Before:
-      Is the set of all pairs&nbsp; (x,y) of rational x,y connected? Why?<br><br><ul><li>Pictographically : <img src="paste-4a299da253efb63ed253fbc7e6d1895f3c9fb557.jpg"></li><li>Since we are only talking about rationals, dividing A and B by&nbsp;\(\pi\) means that neither contains &nbsp;\(\pi\) on the x-axis</li><li>Thus, while&nbsp;\(\pi\) is a limit point of both, the two sets are separated&nbsp;</li></ul>
-
-    After:
-      Is the set of all pairs&nbsp; (x,y) of rational x,y connected? Why?<br><br><ul><li>Pictographically : <img src="paste-4a299da253efb63ed253fbc7e6d1895f3c9fb557.jpg"></li><li>Since we are only talking about rationals, dividing A and B by&nbsp;\(\pi\) means that neither contains &nbsp;\(\pi\) on the x-axis</li><li>Thus, while&nbsp;\(\pi\) is a limit point of both, the two sets are separated&nbsp;</li></ul>
-
-============================================================
-
-Note ID: 1723394324795
-  Field: Text
-    Before:
-      Proof that the any interval [a,b] is connected<br><ul><li>Assume there exists a separation A,B</li><li>with&nbsp;\(a\)&nbsp;\(\in A\) and&nbsp;\(b\)&nbsp;\(\in\)&nbsp;\(B\)</li><li>Then&nbsp;\(\bar{A} \cap B\)&nbsp;\(\neq\)&nbsp;\(\emptyset\) and vice-versa</li><li>Lets consider&nbsp;\(s\) \(=\) \(\sup\)&nbsp;\(A\)</li><li>Then \(s\)&nbsp;\(\in\)&nbsp;\(\bar{A}\) because it is either in the set or a limit point</li><li>Thus&nbsp;&nbsp;\(s\) \(\not \in\)&nbsp;\(B\)</li><li>Since \([a,b]\) =&nbsp;\(A \cup B\), then&nbsp;\(s\) must be in&nbsp;\(A\)</li><li>If&nbsp;\(s\)&nbsp;\(\in\)&nbsp;\(A\) then&nbsp;&nbsp;\(s\)&nbsp;\(\not \in\)&nbsp;\(\bar{B}\)</li><li>Then \(\exists \epsilon\) such that&nbsp;\(V_{\epsilon}(s)\)&nbsp;\(\not \in \)&nbsp;\(B\)&nbsp;</li><li>Since&nbsp;&nbsp;\([a,b]\) =&nbsp;\(A \cup B\),&nbsp; \(V_{\epsilon}(s)\)&nbsp;\(\in\)&nbsp;\(A\)</li><li>Which contradicts&nbsp;\(s\) being the supremum as a larger number exists in the nighbourhood.</li><li>If&nbsp;\(s\) is one of the endpoints we can use the infimum of the other set</li></ul>
-
-    After:
-      Proof that the any interval [a,b] is connected<br><ul><li>Assume there exists a separation A,B</li><li>with&nbsp;\(a\)&nbsp;\(\in A\) and&nbsp;\(b\)&nbsp;\(\in\)&nbsp;\(B\)</li><li>Then&nbsp;\(\bar{A} \cap B\)&nbsp;\(\neq\)&nbsp;\(\emptyset\) and vice-versa</li><li>Lets consider&nbsp;\(s\) \(=\) \(\sup\)&nbsp;\(A\)</li><li>Then \(s\)&nbsp;\(\in\)&nbsp;\(\bar{A}\) because it is either in the set or a limit point</li><li>Thus&nbsp;&nbsp;\(s\) \(\not \in\)&nbsp;\(B\)</li><li>Since \([a,b]\) =&nbsp;\(A \cup B\), then&nbsp;\(s\) must be in&nbsp;\(A\)</li><li>If&nbsp;\(s\)&nbsp;\(\in\)&nbsp;\(A\) then&nbsp;&nbsp;\(s\)&nbsp;\(\not \in\)&nbsp;\(\bar{B}\)</li><li>Then \(\exists \epsilon\) such that&nbsp;\(V_{\epsilon}(s)\)&nbsp;\(\not \in \)&nbsp;\(B\)&nbsp;</li><li>Since&nbsp;&nbsp;\([a,b]\) =&nbsp;\(A \cup B\),&nbsp; \(V_{\epsilon}(s)\)&nbsp;\(\in\)&nbsp;\(A\)</li><li>Which contradicts&nbsp;\(s\) being the supremum as a larger number exists in the nighbourhood.</li><li>If&nbsp;\(s\) is one of the endpoints we can use the infimum of the other set</li></ul>
-
-============================================================
-
-Note ID: 1723398212295
-  Field: Text
-    Before:
-      The matchin problem continued:<br><ul><li>A deck of N cards</li><li>The event \(A_i\) is the event that the i-th card matches, i.e the i-th card in the deck has the number i on it</li><li>To compute \(P\)&nbsp;\(\cup_i A_i\) we need the probabilities</li><li>\(P\) ( \(\cap^k_{i=1} A_i\) ) for all k, by symmetry<br></li><li>\(P\) ( \(\cap^k_{i=1} A_i\) ) =&nbsp;\((n-k)!\)&nbsp;\(/\)&nbsp;\(n!\) by naive definition because we are assuming all orders of the n-k cards are equally likely while the first k are constrained<br></li><li>Importantly, there are&nbsp;\(\binom{n}{k}\) terms representing intersections of k events, all of which are the same by symmetry</li><li>By inclusion-exclusion</li><li>\(P\)&nbsp;\(\cup_i A_i\) =&nbsp;\(\sum_{k=1}\)&nbsp;\((-1)^{k-+1}\)\(\frac{n!}{(n-k)! k!}\) \(\frac{(n-k)!}{n!} \) = \(\sum_{k=1}\)&nbsp;\((-1)^{k+1}\)&nbsp;&nbsp;\(\frac{1}{k!}\)<br></li></ul>
-
-    After:
-      The matchin problem continued:<br><ul><li>A deck of N cards</li><li>The event \(A_i\) is the event that the i-th card matches, i.e the i-th card in the deck has the number i on it</li><li>To compute \(P\)&nbsp;\(\cup_i A_i\) we need the probabilities</li><li>\(P\) ( \(\cap^k_{i=1} A_i\) ) for all k, by symmetry<br></li><li>\(P\) ( \(\cap^k_{i=1} A_i\) ) =&nbsp;\((n-k)!\)&nbsp;\(/\)&nbsp;\(n!\) by naive definition because we are assuming all orders of the n-k cards are equally likely while the first k are constrained<br></li><li>Importantly, there are&nbsp;\(\binom{n}{k}\) terms representing intersections of k events, all of which are the same by symmetry</li><li>By inclusion-exclusion</li><li>\(P\)&nbsp;\(\cup_i A_i\) =&nbsp;\(\sum_{k=1}\)&nbsp;\((-1)^{k-+1}\)\(\frac{n!}{(n-k)! k!}\) \(\frac{(n-k)!}{n!} \) = \(\sum_{k=1}\)&nbsp;\((-1)^{k+1}\)&nbsp;&nbsp;\(\frac{1}{k!}\)<br></li></ul>
-
-============================================================
-
-Note ID: 1723416117017
-  Field: Text
-    Before:
-      When are three events independent?<br><ul><li>\(P(A,B)\)&nbsp; =&nbsp;\(P(A)\)&nbsp;\(P(B)\)<br></li><li>\(P(A,C)\)&nbsp; =&nbsp;\(P(A)\)&nbsp;\(P(C)\)</li><li>\(P(B,C)\)&nbsp; =&nbsp;\(P(B)\)&nbsp;\(P(C)\)</li><li>\(P(A,B, C)\)&nbsp; =&nbsp;\(P(A)\)&nbsp;\(P(B)\) \(P(C)\)</li></ul>
-
-    After:
-      When are three events independent?<br><ul><li>\(P(A,B)\)&nbsp; =&nbsp;\(P(A)\)&nbsp;\(P(B)\)<br></li><li>\(P(A,C)\)&nbsp; =&nbsp;\(P(A)\)&nbsp;\(P(C)\)</li><li>\(P(B,C)\)&nbsp; =&nbsp;\(P(B)\)&nbsp;\(P(C)\)</li><li>\(P(A,B, C)\)&nbsp; =&nbsp;\(P(A)\)&nbsp;\(P(B)\) \(P(C)\)</li></ul>
-
-============================================================
-
-Note ID: 1723416492358
-  Field: Text
-    Before:
-      When are N events independent?<br><ul><li>Definition 2.5.6 (Independence of many events). For \(n\) events \(A_{1}, A_{2}, \ldots, A_{n}\) to be independent, we require any pair to satisfy \(P\) \(\left(A_{i} \cap A_{j}\right)\) = \(P\left(A_{i}\right) P\left(A_{j}\right)\) (for \(i \neq j\) ), any triplet to satisfy P\(\left(A_{i} \cap A_{j} \cap A_{k}\right)\)=\(P\left(A_{i}\right) P\left(A_{j}\right) P\left(A_{k}\right)\) (for \(i, j, k\) distinct), and similarly for all quadruplets, quintuplets, and so on.</li></ul>
-
-    After:
-      When are N events independent?<br><ul><li>Definition 2.5.6 (Independence of many events). For \(n\) events \(A_{1}, A_{2}, \ldots, A_{n}\) to be independent, we require any pair to satisfy \(P\) \(\left(A_{i} \cap A_{j}\right)\) = \(P\left(A_{i}\right) P\left(A_{j}\right)\) (for \(i \neq j\) ), any triplet to satisfy P\(\left(A_{i} \cap A_{j} \cap A_{k}\right)\)=\(P\left(A_{i}\right) P\left(A_{j}\right) P\left(A_{k}\right)\) (for \(i, j, k\) distinct), and similarly for all quadruplets, quintuplets, and so on.</li></ul>
-
-============================================================
-
-Note ID: 1723452922911
-  Field: Text
-    Before:
-      Newton Pepys problem: which is more likely<br><ol><li>(A) At least one 6 with 6 dice</li><li>(B) At least two 6s with 12 dice</li><li>(C) At least three 6s with 18 dice</li></ol><div>Solution:</div><div><ul><li><br></li><li>P( \(\cup C_i\) ) = 1 - P(&nbsp;\(\cap C_i\) &nbsp;) = 1 -&nbsp;\(\sum_{k=0}^{3}\)&nbsp;\(\binom{18}{k}\) \(\frac{1}{6^k}\) \((\frac{5}{6})^{18-k}\)<br></li><ul><li>\(\binom{18}{k}\) means the number of ways to choose k dice<br></li><li>\(&nbsp;\frac{1}{6^k}\) is the probability of getting that many sixes<br></li><li>&nbsp;\( (\frac{5}{6})^{18-k} \) is the probability that the other dice are non-sixes<br></li></ul><li>Pictographically: <img src="paste-26d144180ab020f962d9426d425c27fa1fc8973f.jpg"></li><li>This is called a binomial probability, replace 18 with n for the general case of n dice and change the 3 to i for any number of sixes</li></ul></div>
-
-    After:
-      Newton Pepys problem: which is more likely<br><ol><li>(A) At least one 6 with 6 dice</li><li>(B) At least two 6s with 12 dice</li><li>(C) At least three 6s with 18 dice</li></ol><div>Solution:</div><div><ul><li><br></li><li>P( \(\cup C_i\) ) = 1 - P(&nbsp;\(\cap C_i\) &nbsp;) = 1 -&nbsp;\(\sum_{k=0}^{3}\)&nbsp;\(\binom{18}{k}\) \(\frac{1}{6^k}\) \((\frac{5}{6})^{18-k}\)<br></li><ul><li>\(\binom{18}{k}\) means the number of ways to choose k dice<br></li><li>\(&nbsp;\frac{1}{6^k}\) is the probability of getting that many sixes<br></li><li>&nbsp;\( (\frac{5}{6})^{18-k} \) is the probability that the other dice are non-sixes<br></li></ul><li>Pictographically: <img src="paste-26d144180ab020f962d9426d425c27fa1fc8973f.jpg"></li><li>This is called a binomial probability, replace 18 with n for the general case of n dice and change the 3 to i for any number of sixes</li></ul></div>
-
-============================================================
-
-Note ID: 1723454519101
-  Field: Text
-    Before:
-      What is the probability&nbsp;\(P(A \cap B)\), using conditional probability?<br><br><ul><li>\(P(A \cap B)\) = \(P(A | B)\)&nbsp;\(P(B)\)</li></ul>
-
-    After:
-      What is the probability&nbsp;\(P(A \cap B)\), using conditional probability?<br><br><ul><li>\(P(A \cap B)\) = \(P(A | B)\)&nbsp;\(P(B)\)</li></ul>
-
-============================================================
-
-Note ID: 1723504460349
-  Field: Text
-    Before:
-      <ul><li>A collection of sets has the Finite Intersection Condition if every finite subcollection has a non-empty intersection</li><li>A set is compact iff any collection of closed sets&nbsp;\(K_{\alpha}\) that has the finite intersection condition has a non-empty intersection</li></ul>
-
-    After:
-      <ul><li>A collection of sets has the Finite Intersection Condition if every finite subcollection has a non-empty intersection</li><li>A set is compact iff any collection of closed sets&nbsp;\(K_{\alpha}\) that has the finite intersection condition has a non-empty intersection</li></ul>
-
-============================================================
-
-Note ID: 1723505887015
-  Field: Text
-    Before:
-      <ul><li>A sequence&nbsp;\(p_n\) in a metric space \(X\) converges if&nbsp;\(\exists\)&nbsp;\(p\)&nbsp;\(\in\)&nbsp;\(X\) such that&nbsp;\(\forall\)&nbsp;\(\epsilon\)&nbsp;\(&gt;\)&nbsp;\(0\),&nbsp;\(\exists\)&nbsp;\(N\) such that&nbsp;\(\forall\)&nbsp;\(n\)&nbsp;\(&gt;\)&nbsp;\(N\) implies \(d\)\((p_n,p)\)&nbsp;\(&lt;\)&nbsp;\(\epsilon\)</li></ul>
-
-    After:
-      <ul><li>A sequence&nbsp;\(p_n\) in a metric space \(X\) converges if&nbsp;\(\exists\)&nbsp;\(p\)&nbsp;\(\in\)&nbsp;\(X\) such that&nbsp;\(\forall\)&nbsp;\(\epsilon\)&nbsp;\(&gt;\)&nbsp;\(0\),&nbsp;\(\exists\)&nbsp;\(N\) such that&nbsp;\(\forall\)&nbsp;\(n\)&nbsp;\(&gt;\)&nbsp;\(N\) implies \(d\)\((p_n,p)\)&nbsp;\(&lt;\)&nbsp;\(\epsilon\)</li></ul>
-
-============================================================
-
-Note ID: 1723533259765
-  Field: Text
-    Before:
-      <div><br></div><div><ul><li>\(p_n\)&nbsp;\(\to\)&nbsp;\(p\)<br></li><li>\(\lim_{n \to \infty}\)&nbsp;\(p_n\)&nbsp;\(=\)&nbsp;\(p\)</li><li>\(p_n\) converges to p<br></li><li>\(p\) is the limit of&nbsp;\(p_n\)<br></li></ul></div>
-
-    After:
-      <div><br></div><div><ul><li>\(p_n\)&nbsp;\(\to\)&nbsp;\(p\)<br></li><li>\(\lim_{n \to \infty}\)&nbsp;\(p_n\)&nbsp;\(=\)&nbsp;\(p\)</li><li>\(p_n\) converges to p<br></li><li>\(p\) is the limit of&nbsp;\(p_n\)<br></li></ul></div>
-
-============================================================
-
-Note ID: 1723534538215
-  Field: Text
-    Before:
-      <div><div>What does&nbsp;\(p_n = \frac{n+1}{n}\) converge to in R?</div></div><div><ul><li><br></li><li>\(\forall\)&nbsp;\( \epsilon \)&nbsp;\(&gt;\)&nbsp;\(0\)<br></li><li>\(| \frac{n+1}{n} - 1|\)&nbsp;\(&lt;\)&nbsp;\(\epsilon\)<br></li><li>\(\forall\)&nbsp;\(n\)&nbsp;\(&gt;\) \(N\)<br></li><li>Because we can pick</li><li>\(N\)&nbsp;\(&gt;\)&nbsp;\(\frac{1}{\epsilon}\)<br></li><li>For&nbsp; example&nbsp;</li><li>\(N\)&nbsp;\(=\)&nbsp;\(\lceil \frac{1}{\epsilon} \rceil \)&nbsp; + \(1\)</li><li>If&nbsp;\(n\)&nbsp;\(\geq\)&nbsp;\(N\) then&nbsp;\(n\)&nbsp;\(&gt;\)&nbsp;\(\frac{1}{\epsilon}\) hence&nbsp;</li><li>\( |\frac{1}{n}| \)&nbsp;\(&lt;\)&nbsp;\(\frac{1}{\epsilon}\)<br></li></ul></div>
-
-    After:
-      <div><div>What does&nbsp;\(p_n = \frac{n+1}{n}\) converge to in R?</div></div><div><ul><li><br></li><li>\(\forall\)&nbsp;\( \epsilon \)&nbsp;\(&gt;\)&nbsp;\(0\)<br></li><li>\(| \frac{n+1}{n} - 1|\)&nbsp;\(&lt;\)&nbsp;\(\epsilon\)<br></li><li>\(\forall\)&nbsp;\(n\)&nbsp;\(&gt;\) \(N\)<br></li><li>Because we can pick</li><li>\(N\)&nbsp;\(&gt;\)&nbsp;\(\frac{1}{\epsilon}\)<br></li><li>For&nbsp; example&nbsp;</li><li>\(N\)&nbsp;\(=\)&nbsp;\(\lceil \frac{1}{\epsilon} \rceil \)&nbsp; + \(1\)</li><li>If&nbsp;\(n\)&nbsp;\(\geq\)&nbsp;\(N\) then&nbsp;\(n\)&nbsp;\(&gt;\)&nbsp;\(\frac{1}{\epsilon}\) hence&nbsp;</li><li>\( |\frac{1}{n}| \)&nbsp;\(&lt;\)&nbsp;\(\frac{1}{\epsilon}\)<br></li></ul></div>
-
-============================================================
-
-Note ID: 1723536739841
-  Field: Text
-    Before:
-      <div>Why a sequence of a metric space&nbsp;\(X,d\) must have a unique limit</div><div><ul><li>Pictographically: <img src="paste-f588701d705dd4d9983372cdecf8b04e18d959f4.jpg"></li><li>Assume&nbsp;\(p_n\) converges to&nbsp;\(p\) and&nbsp;\(q\)</li><li>Let&nbsp;\(\epsilon\) =&nbsp;\(d(p,q)\)</li><li>\(\exists\)&nbsp;\(N_p\) such that&nbsp;\(n\) &gt;&nbsp;\(N_p\) implies&nbsp;\(d(p_n,p)\)&nbsp;\(&lt;\)&nbsp;\(\frac{\epsilon}{2}\)<br></li><li>Also</li><li>\(\exists\)&nbsp;\(N_q\) such that&nbsp;\(n\) &gt;&nbsp;\(N_q\) implies&nbsp;\(d(p_n,q)\)&nbsp;\(&lt;\)&nbsp;\(\frac{\epsilon}{2}\)</li><li>Finally,</li><li>Let&nbsp;\(N\) =&nbsp;\(\max\) ( \(N_p, N_q\) )</li><li>Then, \(\forall\)&nbsp;\(n\)&nbsp;\(&gt;\)&nbsp;\(N\)&nbsp;<br></li><ul><li>\(\epsilon\) = \(d(p,q)\)&nbsp;\(&lt;\) \(d(p,p_n)\) + \(d(p_n,q)\)&nbsp;\(&lt;\) \( \frac{\epsilon}{2}\) 7 \( \frac{\epsilon}{2}\) =&nbsp;\(\epsilon\)<br></li></ul><li>This proof could also have been done by showing that the distance between p,q must always be less than every epsilon, thus 0</li></ul></div>
-
-    After:
-      <div>Why a sequence of a metric space&nbsp;\(X,d\) must have a unique limit</div><div><ul><li>Pictographically: <img src="paste-f588701d705dd4d9983372cdecf8b04e18d959f4.jpg"></li><li>Assume&nbsp;\(p_n\) converges to&nbsp;\(p\) and&nbsp;\(q\)</li><li>Let&nbsp;\(\epsilon\) =&nbsp;\(d(p,q)\)</li><li>\(\exists\)&nbsp;\(N_p\) such that&nbsp;\(n\) &gt;&nbsp;\(N_p\) implies&nbsp;\(d(p_n,p)\)&nbsp;\(&lt;\)&nbsp;\(\frac{\epsilon}{2}\)<br></li><li>Also</li><li>\(\exists\)&nbsp;\(N_q\) such that&nbsp;\(n\) &gt;&nbsp;\(N_q\) implies&nbsp;\(d(p_n,q)\)&nbsp;\(&lt;\)&nbsp;\(\frac{\epsilon}{2}\)</li><li>Finally,</li><li>Let&nbsp;\(N\) =&nbsp;\(\max\) ( \(N_p, N_q\) )</li><li>Then, \(\forall\)&nbsp;\(n\)&nbsp;\(&gt;\)&nbsp;\(N\)&nbsp;<br></li><ul><li>\(\epsilon\) = \(d(p,q)\)&nbsp;\(&lt;\) \(d(p,p_n)\) + \(d(p_n,q)\)&nbsp;\(&lt;\) \( \frac{\epsilon}{2}\) 7 \( \frac{\epsilon}{2}\) =&nbsp;\(\epsilon\)<br></li></ul><li>This proof could also have been done by showing that the distance between p,q must always be less than every epsilon, thus 0</li></ul></div>
-
-============================================================
-
-Note ID: 1723536977905
-  Field: Text
-    Before:
-      A convergent sequence must be bounded because:<br><ul><li>&nbsp;For&nbsp;\(\epsilon\) =&nbsp;\(1\)</li><li>\(\exists\)&nbsp;\(N\) sucht that&nbsp;\(d\)&nbsp;\((p_n,p)\)&nbsp;\(&lt;\)&nbsp;\(1\)<br></li><li>Let&nbsp;\(R\) =&nbsp;\(\max\)&nbsp;\((*d(p_n,p)\forall n\))</li><li>So all&nbsp;\(p_n\) are in&nbsp;\(\mathcal{N}_R(p)\)</li></ul>
-
-    After:
-      A convergent sequence must be bounded because:<br><ul><li>&nbsp;For&nbsp;\(\epsilon\) =&nbsp;\(1\)</li><li>\(\exists\)&nbsp;\(N\) sucht that&nbsp;\(d\)&nbsp;\((p_n,p)\)&nbsp;\(&lt;\)&nbsp;\(1\)<br></li><li>Let&nbsp;\(R\) =&nbsp;\(\max\)&nbsp;\((*d(p_n,p)\forall n\))</li><li>So all&nbsp;\(p_n\) are in&nbsp;\(\mathcal{N}_R(p)\)</li></ul>
-
-============================================================
-
-Note ID: 1723765778857
-  Field: Text
-    Before:
-      Suppose we have a sequence \(s_n\in C\) if&nbsp;\(s_n \to s\) then&nbsp;\(c\)\(s_n\)&nbsp;\(\to\)&nbsp;\(c\)\(s\)<br><ul><li>For all&nbsp;\(\epsilon &gt;0\),&nbsp;\(\exists\)&nbsp;\(N\) such that&nbsp;\(\forall n &gt; N\),&nbsp; \(|\)&nbsp;\(s_n - s\) \(|\)&nbsp;\(&lt;\)&nbsp;\(\frac{\epsilon}{|c|}\)</li><li>Then</li><li>&nbsp;\(|\)&nbsp;\(c s_n - c s\) \(|\)<br></li><li>= \(|\)&nbsp;\(c (s_n - s)\) \(|\)</li><li>=&nbsp;&nbsp;\(|c|\)\(|\)&nbsp;\(s_n - s\) \(|\)</li><li>\(&lt;\)&nbsp;\(|c|\)\(\frac{\epsilon}{|c|}\)<br></li><li>\(&lt;\)&nbsp;\(\epsilon\)<br></li></ul>
-
-    After:
-      Suppose we have a sequence \(s_n\in C\) if&nbsp;\(s_n \to s\) then&nbsp;\(c\)\(s_n\)&nbsp;\(\to\)&nbsp;\(c\)\(s\)<br><ul><li>For all&nbsp;\(\epsilon &gt;0\),&nbsp;\(\exists\)&nbsp;\(N\) such that&nbsp;\(\forall n &gt; N\),&nbsp; \(|\)&nbsp;\(s_n - s\) \(|\)&nbsp;\(&lt;\)&nbsp;\(\frac{\epsilon}{|c|}\)</li><li>Then</li><li>&nbsp;\(|\)&nbsp;\(c s_n - c s\) \(|\)<br></li><li>= \(|\)&nbsp;\(c (s_n - s)\) \(|\)</li><li>=&nbsp;&nbsp;\(|c|\)\(|\)&nbsp;\(s_n - s\) \(|\)</li><li>\(&lt;\)&nbsp;\(|c|\)\(\frac{\epsilon}{|c|}\)<br></li><li>\(&lt;\)&nbsp;\(\epsilon\)<br></li></ul>
-
-============================================================
-
-Note ID: 1723766593590
-  Field: Text
-    Before:
-      Suppose we have twos sequencees \(s_n, t_n \in C\) if&nbsp;\(s_n \to s\) and \(t_n \to t\), then&nbsp;\((s_n t_n) \to (st)\)<br><ul><li>Both sequences must have a maximum, pick &nbsp;\(|M|\) =&nbsp;\(\max\) \((|M_s|,|M_t|)\)</li><li>\(\forall\)&nbsp;\(\epsilon &gt;0 \),&nbsp;\(\exists\)&nbsp;\(N_s\) such that&nbsp;\(|s_n -s| \)\(&lt;\)&nbsp;\(\frac{\epsilon}{|M|}\)<br></li><li>Analagous for&nbsp;\(t_n\), pick&nbsp;\(N\) =&nbsp;\(\max\) \((N_s,N_t)\)</li><li>\(| s_n t_n - st| \)<br></li><li>= \(| s_n t_n + s_n t - s_n t - st| \)</li><li>= \(| s_n (t_n - t)&nbsp; + t(s_n - s)| \)<br></li><li>\(\leq\)&nbsp;\(| s_n (t_n - t) |\) +&nbsp;\(|t(s_n - s)|\)<br></li><li>\(\leq\) \(| M (t_n - t) |\) +&nbsp;\(| M(s_n - s)|\)<br></li><li>\(\leq \)&nbsp;\(\epsilon\)<br></li></ul>
-
-    After:
-      Suppose we have twos sequencees \(s_n, t_n \in C\) if&nbsp;\(s_n \to s\) and \(t_n \to t\), then&nbsp;\((s_n t_n) \to (st)\)<br><ul><li>Both sequences must have a maximum, pick &nbsp;\(|M|\) =&nbsp;\(\max\) \((|M_s|,|M_t|)\)</li><li>\(\forall\)&nbsp;\(\epsilon &gt;0 \),&nbsp;\(\exists\)&nbsp;\(N_s\) such that&nbsp;\(|s_n -s| \)\(&lt;\)&nbsp;\(\frac{\epsilon}{|M|}\)<br></li><li>Analagous for&nbsp;\(t_n\), pick&nbsp;\(N\) =&nbsp;\(\max\) \((N_s,N_t)\)</li><li>\(| s_n t_n - st| \)<br></li><li>= \(| s_n t_n + s_n t - s_n t - st| \)</li><li>= \(| s_n (t_n - t)&nbsp; + t(s_n - s)| \)<br></li><li>\(\leq\)&nbsp;\(| s_n (t_n - t) |\) +&nbsp;\(|t(s_n - s)|\)<br></li><li>\(\leq\) \(| M (t_n - t) |\) +&nbsp;\(| M(s_n - s)|\)<br></li><li>\(\leq \)&nbsp;\(\epsilon\)<br></li></ul>
-
-============================================================
-
-Note ID: 1723767194714
-  Field: Text
-    Before:
-      Suppose we have twos sequencees \(s_n, t_n \in C\) if&nbsp;\(s_n \to s\) and \(t_n \to t\), then&nbsp;\((s_n t_n) \to (st)\)<br><ul><li>Let&nbsp;\(K\) =&nbsp;\(\max\) &nbsp;\((s,t,1)\), the 1 is necesasry for later expansion</li><li>\(\forall \epsilon &gt; 0\),\(\exists\)&nbsp;\(N_1, N_2\) such that&nbsp;\(n\)&nbsp;\(&gt;\)&nbsp;\(N-1\) implies that&nbsp;\(s_n - s\)&nbsp;\(&lt;\)&nbsp;\(\frac{\epsilon}{3K}\), same for&nbsp;\(t_n\) and&nbsp;\(N_2\)<br></li><li>Pick&nbsp;\(N\) =&nbsp;\(\max\)&nbsp;\((N_1,N_2)\)</li><li>\(| s_n t_n - st|\)&nbsp;</li><li>\(\leq\)&nbsp;\(|\)\((s_n-s)\) \((t_n-t)\) + \(s(t_n-t)\) +&nbsp;\(t(s_n-s)\)&nbsp;\(|\)<br></li><li>\(&lt;\)&nbsp;\(\frac{\epsilon^2}{9K^2}\) + \(\frac{\epsilon}{3K}\) + \(\frac{\epsilon}{3K}\)<br></li><li>\(&lt;\)&nbsp;&nbsp;\(\frac{\epsilon}{9K}\) + \(\frac{\epsilon}{3K}\) + \(\frac{\epsilon}{3K}\)<br></li></ul><div><br></div>
-
-    After:
-      Suppose we have twos sequencees \(s_n, t_n \in C\) if&nbsp;\(s_n \to s\) and \(t_n \to t\), then&nbsp;\((s_n t_n) \to (st)\)<br><ul><li>Let&nbsp;\(K\) =&nbsp;\(\max\) &nbsp;\((s,t,1)\), the 1 is necesasry for later expansion</li><li>\(\forall \epsilon &gt; 0\),\(\exists\)&nbsp;\(N_1, N_2\) such that&nbsp;\(n\)&nbsp;\(&gt;\)&nbsp;\(N-1\) implies that&nbsp;\(s_n - s\)&nbsp;\(&lt;\)&nbsp;\(\frac{\epsilon}{3K}\), same for&nbsp;\(t_n\) and&nbsp;\(N_2\)<br></li><li>Pick&nbsp;\(N\) =&nbsp;\(\max\)&nbsp;\((N_1,N_2)\)</li><li>\(| s_n t_n - st|\)&nbsp;</li><li>\(\leq\)&nbsp;\(|\)\((s_n-s)\) \((t_n-t)\) + \(s(t_n-t)\) +&nbsp;\(t(s_n-s)\)&nbsp;\(|\)<br></li><li>\(&lt;\)&nbsp;\(\frac{\epsilon^2}{9K^2}\) + \(\frac{\epsilon}{3K}\) + \(\frac{\epsilon}{3K}\)<br></li><li>\(&lt;\)&nbsp;&nbsp;\(\frac{\epsilon}{9K}\) + \(\frac{\epsilon}{3K}\) + \(\frac{\epsilon}{3K}\)<br></li></ul><div><br></div>
-
-============================================================
-
-Note ID: 1723810957675
-  Field: Text
-    Before:
-      Suppose&nbsp;\(\{p_n\}\) is a sequence, let&nbsp;\(n_1 \)&nbsp;\(&lt;\)&nbsp;\(n_2\)&nbsp;\(&lt;\)&nbsp;\(n_3\)&nbsp;\(\cdots\) in&nbsp;\(N\) be an increasing sequence, then&nbsp;\(\{p_{n_i}\}\) is a subsequence
-
-    After:
-      Suppose&nbsp;\(\{p_n\}\) is a sequence, let&nbsp;\(n_1 \)&nbsp;\(&lt;\)&nbsp;\(n_2\)&nbsp;\(&lt;\)&nbsp;\(n_3\)&nbsp;\(\cdots\) in&nbsp;\(N\) be an increasing sequence, then&nbsp;\(\{p_{n_i}\}\) is a subsequence
-
-============================================================
-
-Note ID: 1723822690735
-  Field: Text
-    Before:
-      A sequence which does not converge may have a convergent subsequence.<br><br>For example:<br><ul><li>\(1,\pi, \frac{1}{2}, \pi, \frac{1}{4}, \cdots\)<br></li><li>Does not converge but has at least two convergent subsequences</li><li>\(\pi \cdots\)<br></li><li>And</li><li>\(\frac{1}{n}\)<br></li></ul>
-
-    After:
-      A sequence which does not converge may have a convergent subsequence.<br><br>For example:<br><ul><li>\(1,\pi, \frac{1}{2}, \pi, \frac{1}{4}, \cdots\)<br></li><li>Does not converge but has at least two convergent subsequences</li><li>\(\pi \cdots\)<br></li><li>And</li><li>\(\frac{1}{n}\)<br></li></ul>
-
-============================================================
-
-Note ID: 1723823462935
-  Field: Text
-    Before:
-      Every compact set is also sequentially compact
-
-    After:
-      Every compact set is also sequentially compact
-
-============================================================
-
-Note ID: 1723823689013
-  Field: Text
-    Before:
-      In a compact metric space every sequence has a convergent subsequence with a limit in X, which means that if&nbsp;\(X\) is compact then&nbsp;\(X\) is sequentially compact
-
-    After:
-      In a compact metric space every sequence has a convergent subsequence with a limit in X, which means that if&nbsp;\(X\) is compact then&nbsp;\(X\) is sequentially compact
-
-============================================================
-
-Note ID: 1723823764728
-  Field: Text
-    Before:
-      Converging to a point inside a metric space is crucial for understanding sequential compactness, this is why a series approaching&nbsp;\(\pi\) in&nbsp;\(Q\) does not converge to&nbsp;\(\pi\) since it is not in&nbsp;\(Q\)
-
-    After:
-      Converging to a point inside a metric space is crucial for understanding sequential compactness, this is why a series approaching&nbsp;\(\pi\) in&nbsp;\(Q\) does not converge to&nbsp;\(\pi\) since it is not in&nbsp;\(Q\)
-
-============================================================
-
-Note ID: 1723824100900
-  Field: Text
-    Before:
-      Every bounded sequence in&nbsp;\(R^K\) contains a convergent subsequence <br>Proof:<br><ul><li>Despite&nbsp;\(R^K\) not being compact, since the bounded sequence lives in a compact subset of \(R^K\) and thus a compact subset of the metric space meaning it has a convergent subsequence in the compact subset and thus in \(R^K\)</li></ul>
-
-    After:
-      Every bounded sequence in&nbsp;\(R^K\) contains a convergent subsequence <br>Proof:<br><ul><li>Despite&nbsp;\(R^K\) not being compact, since the bounded sequence lives in a compact subset of \(R^K\) and thus a compact subset of the metric space meaning it has a convergent subsequence in the compact subset and thus in \(R^K\)</li></ul>
-
-============================================================
-
-Note ID: 1723831147136
-  Field: Text
-    Before:
-      <ul><li>A sequence&nbsp;\(\{p_n\}\) is Cacuchy in an abstract metric space&nbsp;\(X,d\) iff&nbsp;\(\forall\)&nbsp;\(\epsilon\)&nbsp;\(&gt;\)&nbsp;\(0\),&nbsp;\(\exists\)&nbsp;\(N\) such that&nbsp;\(\forall\)&nbsp;\(m,n\)&nbsp;\(&gt;\)&nbsp;\(N\),&nbsp;\(d\) \((p_m,p_n)\)&nbsp;\(&lt;\)&nbsp;\(\epsilon\)</li></ul>
-
-    After:
-      <ul><li>A sequence&nbsp;\(\{p_n\}\) is Cacuchy in an abstract metric space&nbsp;\(X,d\) iff&nbsp;\(\forall\)&nbsp;\(\epsilon\)&nbsp;\(&gt;\)&nbsp;\(0\),&nbsp;\(\exists\)&nbsp;\(N\) such that&nbsp;\(\forall\)&nbsp;\(m,n\)&nbsp;\(&gt;\)&nbsp;\(N\),&nbsp;\(d\) \((p_m,p_n)\)&nbsp;\(&lt;\)&nbsp;\(\epsilon\)</li></ul>
-
-============================================================
-
-Note ID: 1723831323297
-  Field: Text
-    Before:
-      <div><ul><li>If&nbsp;\(\{p_n\}\) converges then&nbsp;\(\{p_n\}\) is also Cauchy</li></ul><div>Proof Idea:</div></div><div><ul><li>Bound&nbsp;\(d\)\((p_n,p_m)\) based on the distances&nbsp;\(d\)\((p_n,p)\) and \(d\)\((p_m,p)\) using the triangle inequality</li></ul></div>
-
-    After:
-      <div><ul><li>If&nbsp;\(\{p_n\}\) converges then&nbsp;\(\{p_n\}\) is also Cauchy</li></ul><div>Proof Idea:</div></div><div><ul><li>Bound&nbsp;\(d\)\((p_n,p_m)\) based on the distances&nbsp;\(d\)\((p_n,p)\) and \(d\)\((p_m,p)\) using the triangle inequality</li></ul></div>
-
-============================================================
-
-Note ID: 1723831623945
-  Field: Text
-    Before:
-      <div><div><br></div><div>Proof that convegent sequences are Cauchy:</div></div><div><ul><li>Given&nbsp;\(\epsilon\)&nbsp;\(&gt;\)&nbsp;\(0\),&nbsp;\(\exists\)&nbsp;\(N\) such that&nbsp;\(d(p_n, p) \)&nbsp;\(&lt;\)&nbsp;\(\frac{\epsilon}{2}\) for all&nbsp;\(n\)&nbsp;\(&gt;\)&nbsp;\(N\)</li><li>For&nbsp;\(m,n\)&nbsp;\(&gt;\)&nbsp;\(N\):</li><ul><li>\(d\)\((p_n,p_m)\)&nbsp;</li><li>\(\leq\) \(d\)\((p_n,p)\) + \(d\)\((p_m,p)\) by triangle inequality<br></li><li>\(&lt;\)&nbsp;\(\frac{\epsilon}{2}\) +&nbsp;\(\frac{\epsilon}{2}\)<br></li><li>\(&lt;\)&nbsp;\(\epsilon\)<br></li></ul></ul></div>
-
-    After:
-      <div><div><br></div><div>Proof that convegent sequences are Cauchy:</div></div><div><ul><li>Given&nbsp;\(\epsilon\)&nbsp;\(&gt;\)&nbsp;\(0\),&nbsp;\(\exists\)&nbsp;\(N\) such that&nbsp;\(d(p_n, p) \)&nbsp;\(&lt;\)&nbsp;\(\frac{\epsilon}{2}\) for all&nbsp;\(n\)&nbsp;\(&gt;\)&nbsp;\(N\)</li><li>For&nbsp;\(m,n\)&nbsp;\(&gt;\)&nbsp;\(N\):</li><ul><li>\(d\)\((p_n,p_m)\)&nbsp;</li><li>\(\leq\) \(d\)\((p_n,p)\) + \(d\)\((p_m,p)\) by triangle inequality<br></li><li>\(&lt;\)&nbsp;\(\frac{\epsilon}{2}\) +&nbsp;\(\frac{\epsilon}{2}\)<br></li><li>\(&lt;\)&nbsp;\(\epsilon\)<br></li></ul></ul></div>
-
-============================================================
-
-Note ID: 1723831803863
-  Field: Text
-    Before:
-      <div><div>Cauchy sequences are not always convergent on all metric spaces, for example a series approximating&nbsp;\(\pi\) in&nbsp;\(Q\) is Cauchy but not convergent because while the numbers get close to each other the limit cannot be&nbsp;\(\pi\) since&nbsp;\(\pi\)&nbsp;\(\not \in\) \(Q\)</div></div>
-
-    After:
-      <div><div>Cauchy sequences are not always convergent on all metric spaces, for example a series approximating&nbsp;\(\pi\) in&nbsp;\(Q\) is Cauchy but not convergent because while the numbers get close to each other the limit cannot be&nbsp;\(\pi\) since&nbsp;\(\pi\)&nbsp;\(\not \in\) \(Q\)</div></div>
-
-============================================================
-
-Note ID: 1723835029967
-  Field: Text
-    Before:
-      Proof the compact metric spaces are complete:<br><ul><li>Let&nbsp;\(\{x_n\}\) be a Cauchy sequence in the space&nbsp;\(X\)</li><li>&nbsp;Since&nbsp;\(X\) is compact it is also sequentially compact, meaning that every sequence has a convergent subsequence&nbsp;</li><li>As such there exists \(x_{n_k}\)&nbsp;\(\to\)&nbsp;\(x\)&nbsp;\(\in\)&nbsp;\(X\)</li><li>Pictographically: <img src="paste-22571f69de45ed76447262771338cf6e10c6c260.jpg"></li><li>Fix&nbsp;\(\epsilon\)&nbsp;\(&gt;\)&nbsp;\(0\)</li><li>\(x_n\) being Cauchy implies&nbsp;\(\exists\)&nbsp;\(N_1\) such that&nbsp;\(\forall i,j\)&nbsp;\(&gt;\)&nbsp;\(N\) then&nbsp;\(d\)\((x_i,x_j)\)&nbsp;\(&lt;\)&nbsp;\(\frac{\epsilon}{2}\)<br></li><li>\(\{x_{n_k}\}\)&nbsp;\(\to\)&nbsp;\(x\) implies that&nbsp;\(\forall\)&nbsp;\(n_k\)&nbsp;\(&gt;\)&nbsp;\(N_2\),&nbsp;\(d\)&nbsp;\((x_{n_k},x)\)&nbsp;\(&lt;\)&nbsp;\(\frac{\epsilon}{2}\)<br></li><li>Let \(N\) =&nbsp;\(\max\)&nbsp;\((N_1,N_2)\)</li><li>\(\forall\)&nbsp;\(n\)&nbsp;\(&gt;N\):<br></li><ul><li>\(d\)\((x_n,x)\)&nbsp;</li><li>\(\leq\)&nbsp;\(d\)&nbsp;\((x_n,x_{n_k})\) +&nbsp;&nbsp;\(d\)&nbsp;\((x_{n_k},x)\) for any&nbsp;\(n_k\)&nbsp;\(&gt;\)&nbsp;\(N\) , fix one</li><li>\(&lt;\)&nbsp;\(\frac{\epsilon}{2}\) <span style="font-size: 16.6667px;">+</span><span style="font-size: 16.6667px;">&nbsp;</span>\(\frac{\epsilon}{2}\)</li><li>\(&lt;\)&nbsp;\(\epsilon\)<br></li></ul><li>Since&nbsp;\(\epsilon\) was arbitrary, \(X\) is complete</li></ul>
-
-    After:
-      Proof the compact metric spaces are complete:<br><ul><li>Let&nbsp;\(\{x_n\}\) be a Cauchy sequence in the space&nbsp;\(X\)</li><li>&nbsp;Since&nbsp;\(X\) is compact it is also sequentially compact, meaning that every sequence has a convergent subsequence&nbsp;</li><li>As such there exists \(x_{n_k}\)&nbsp;\(\to\)&nbsp;\(x\)&nbsp;\(\in\)&nbsp;\(X\)</li><li>Pictographically: <img src="paste-22571f69de45ed76447262771338cf6e10c6c260.jpg"></li><li>Fix&nbsp;\(\epsilon\)&nbsp;\(&gt;\)&nbsp;\(0\)</li><li>\(x_n\) being Cauchy implies&nbsp;\(\exists\)&nbsp;\(N_1\) such that&nbsp;\(\forall i,j\)&nbsp;\(&gt;\)&nbsp;\(N\) then&nbsp;\(d\)\((x_i,x_j)\)&nbsp;\(&lt;\)&nbsp;\(\frac{\epsilon}{2}\)<br></li><li>\(\{x_{n_k}\}\)&nbsp;\(\to\)&nbsp;\(x\) implies that&nbsp;\(\forall\)&nbsp;\(n_k\)&nbsp;\(&gt;\)&nbsp;\(N_2\),&nbsp;\(d\)&nbsp;\((x_{n_k},x)\)&nbsp;\(&lt;\)&nbsp;\(\frac{\epsilon}{2}\)<br></li><li>Let \(N\) =&nbsp;\(\max\)&nbsp;\((N_1,N_2)\)</li><li>\(\forall\)&nbsp;\(n\)&nbsp;\(&gt;N\):<br></li><ul><li>\(d\)\((x_n,x)\)&nbsp;</li><li>\(\leq\)&nbsp;\(d\)&nbsp;\((x_n,x_{n_k})\) +&nbsp;&nbsp;\(d\)&nbsp;\((x_{n_k},x)\) for any&nbsp;\(n_k\)&nbsp;\(&gt;\)&nbsp;\(N\) , fix one</li><li>\(&lt;\)&nbsp;\(\frac{\epsilon}{2}\) <span style="font-size: 16.6667px;">+</span><span style="font-size: 16.6667px;">&nbsp;</span>\(\frac{\epsilon}{2}\)</li><li>\(&lt;\)&nbsp;\(\epsilon\)<br></li></ul><li>Since&nbsp;\(\epsilon\) was arbitrary, \(X\) is complete</li></ul>
-
-============================================================
-
-Note ID: 1723835099362
-  Field: Text
-    Before:
-      [0,1] is a complete metric space since it is compact, similarly k-cells in&nbsp;\(R^n\) are complete
-
-    After:
-      [0,1] is a complete metric space since it is compact, similarly k-cells in&nbsp;\(R^n\) are complete
-
-============================================================
-
-Note ID: 1723835277501
-  Field: Text
-    Before:
-      Since closed subsets of compact metric spaces are also compact, they are in fact complete. This surprisingly implies that the cantor set is complete
-
-    After:
-      Since closed subsets of compact metric spaces are also compact, they are in fact complete. This surprisingly implies that the cantor set is complete
-
-============================================================
-
-Note ID: 1723835639530
-  Field: Text
-    Before:
-      \(R^n\) is complete proof:<br><ul><li>If&nbsp;\(x_n\) is Cauchy it is also bounded because</li><ul><li>&nbsp;for&nbsp;\(\epsilon\) =&nbsp;\(1\) there exists&nbsp;\(N\) such that&nbsp;\(d\)&nbsp;\((x_n,x_m)\)&nbsp;&nbsp;\(&lt;\)&nbsp;\(1\)&nbsp;\(\forall n,m &gt; N\).&nbsp;</li><li>Let&nbsp;\(R\) =&nbsp;\(\max\)&nbsp;\((d(x_n,x_m),1)\)&nbsp;\(\forall\)&nbsp;\(n,m\)</li><li>Then the sequence is bounded by &nbsp;\(N_{R}(x_n)\)</li></ul><li>Due to boundedness, \(N_{R}(x_n)\) is inside some k-cell in&nbsp;\(R^n\), thus the ball is complete</li><li>Thus&nbsp;\(x_n\) converges because the k-cell is complete</li></ul>
-
-    After:
-      \(R^n\) is complete proof:<br><ul><li>If&nbsp;\(x_n\) is Cauchy it is also bounded because</li><ul><li>&nbsp;for&nbsp;\(\epsilon\) =&nbsp;\(1\) there exists&nbsp;\(N\) such that&nbsp;\(d\)&nbsp;\((x_n,x_m)\)&nbsp;&nbsp;\(&lt;\)&nbsp;\(1\)&nbsp;\(\forall n,m &gt; N\).&nbsp;</li><li>Let&nbsp;\(R\) =&nbsp;\(\max\)&nbsp;\((d(x_n,x_m),1)\)&nbsp;\(\forall\)&nbsp;\(n,m\)</li><li>Then the sequence is bounded by &nbsp;\(N_{R}(x_n)\)</li></ul><li>Due to boundedness, \(N_{R}(x_n)\) is inside some k-cell in&nbsp;\(R^n\), thus the ball is complete</li><li>Thus&nbsp;\(x_n\) converges because the k-cell is complete</li></ul>
-
-============================================================
-
-Note ID: 1723836207695
-  Field: Text
-    Before:
-      Proof that the harmonic series&nbsp;\(x_n\)&nbsp;\(=\)&nbsp;\(\sum_{k=1}^n\)&nbsp;\(\frac{1}{k}\) does not converge using cauchyness:<br><ul><li>Consider, for&nbsp;\(n&gt;m\), \(| x_n - x_m|\)&nbsp;</li><li>=&nbsp;\(\sum_{k=n+1}^m\)\(\frac{1}{k}\)&nbsp;</li><li>\(&gt;\) \(\sum_{k=n+1}^m\)\(\frac{1}{n}\)&nbsp;</li><li>\(=\)&nbsp;\(\frac{n-m}{n}\)</li><li>\(=\)&nbsp;\(1 - \frac{n}{m}\)<br></li><li>Let&nbsp;\(n\) =&nbsp;\(2m\), then&nbsp;\(|x_{2n} - x_{m}|\)&nbsp;\(&gt;\)&nbsp;\(\frac{1}{2}\)</li><li>Thus, considering&nbsp;\(\epsilon\)&nbsp;\(&lt;\)&nbsp;\(\frac{1}{2}\) the sequence can never be cauchy so it does not converge</li><li>This show that we do not need to propose a limit to determine if a sequence converges in a complete metric space</li></ul>
-
-    After:
-      Proof that the harmonic series&nbsp;\(x_n\)&nbsp;\(=\)&nbsp;\(\sum_{k=1}^n\)&nbsp;\(\frac{1}{k}\) does not converge using cauchyness:<br><ul><li>Consider, for&nbsp;\(n&gt;m\), \(| x_n - x_m|\)&nbsp;</li><li>=&nbsp;\(\sum_{k=n+1}^m\)\(\frac{1}{k}\)&nbsp;</li><li>\(&gt;\) \(\sum_{k=n+1}^m\)\(\frac{1}{n}\)&nbsp;</li><li>\(=\)&nbsp;\(\frac{n-m}{n}\)</li><li>\(=\)&nbsp;\(1 - \frac{n}{m}\)<br></li><li>Let&nbsp;\(n\) =&nbsp;\(2m\), then&nbsp;\(|x_{2n} - x_{m}|\)&nbsp;\(&gt;\)&nbsp;\(\frac{1}{2}\)</li><li>Thus, considering&nbsp;\(\epsilon\)&nbsp;\(&lt;\)&nbsp;\(\frac{1}{2}\) the sequence can never be cauchy so it does not converge</li><li>This show that we do not need to propose a limit to determine if a sequence converges in a complete metric space</li></ul>
-
-============================================================
-
-Note ID: 1723836908421
-  Field: Text
-    Before:
-      Theorem: Every metric space&nbsp;\((X,d)\) has a completion, denoted ( \(X^*\),\(d^*\) ) such that&nbsp;\(X\)&nbsp;\(\subset\)\(X^*\) and&nbsp;\(d^*_{|X}\) =&nbsp;\(d\)
-
-    After:
-      Theorem: Every metric space&nbsp;\((X,d)\) has a completion, denoted ( \(X^*\),\(d^*\) ) such that&nbsp;\(X\)&nbsp;\(\subset\)\(X^*\) and&nbsp;\(d^*_{|X}\) =&nbsp;\(d\)
-
-============================================================
-
-Note ID: 1734022442790
-  Field: Text
-    Before:
-      Theorem 3.4.5. The \(\operatorname{HGeom}(w, b, n)\) and \(\operatorname{HGeom}\) (n, w+b-n, w) distributions are identical. That is, if \(X \sim \operatorname{HGeom}(w, b, n)\) and \(Y \sim \operatorname{HGeom}\) (n, w+b-n, w), then \(X\) and \(Y\) have the same distribution.
-
-    After:
-      Theorem 3.4.5. The \(\operatorname{HGeom}(w, b, n)\) and \(\operatorname{HGeom}\) (n, w+b-n, w) distributions are identical. That is, if \(X \sim \operatorname{HGeom}(w, b, n)\) and \(Y \sim \operatorname{HGeom}\) (n, w+b-n, w), then \(X\) and \(Y\) have the same distribution.
-
-============================================================
-
-Note ID: 1734406959248
-  Field: Text
-    Before:
-      A closed subset of a compact set must be complete since closed subsets of compact sets are in fact compact, e.g, perhaps surprisingly the cantor set is complete
-
-    After:
-      A closed subset of a compact set must be complete since closed subsets of compact sets are in fact compact, e.g, perhaps surprisingly the cantor set is complete
-
-============================================================
-
-Note ID: 1734515001623
-  Field: Text
-    Before:
-      Proof that every metric space has a completion:<br><ol><li>Given&nbsp;\(X\), let&nbsp;\(X^*\) be the set of all cauchy sequences in&nbsp;\(X\) under an equivalence relation&nbsp;\(\approx\) where&nbsp;\(\{p_n\}\)&nbsp;\(\approx\)&nbsp;\(\{q_n\}\) iff&nbsp;\(\lim_{n \infty}\) \(d(p_n,q_n)\) =&nbsp;\(0\)</li><li>Denote arbitray equivalence classes \(E\) and \(R\) with&nbsp;\(E,R\)&nbsp;\(\in\)&nbsp;\(X^*\)</li><li>For example, we can pick&nbsp;\(\{e_n\}\) and&nbsp;\(\{r_n\}\) as representatives of the equivalence classes&nbsp;\(E,R\)</li><li>Define a distance between them:&nbsp;\(\Delta(E,R)\) =&nbsp;\(\lim_{n \to \infty}\)&nbsp;\(d(e_n,r_n)\) for two representatives where this limit is meaningful since the metric is a real number and thus we are taking the limit in R which is complete</li><li>The limit of the distances is guaranteed to exist because the sequences are cauchy</li></ol>
-
-    After:
-      Proof that every metric space has a completion:<br><ol><li>Given&nbsp;\(X\), let&nbsp;\(X^*\) be the set of all cauchy sequences in&nbsp;\(X\) under an equivalence relation&nbsp;\(\approx\) where&nbsp;\(\{p_n\}\)&nbsp;\(\approx\)&nbsp;\(\{q_n\}\) iff&nbsp;\(\lim_{n \infty}\) \(d(p_n,q_n)\) =&nbsp;\(0\)</li><li>Denote arbitray equivalence classes \(E\) and \(R\) with&nbsp;\(E,R\)&nbsp;\(\in\)&nbsp;\(X^*\)</li><li>For example, we can pick&nbsp;\(\{e_n\}\) and&nbsp;\(\{r_n\}\) as representatives of the equivalence classes&nbsp;\(E,R\)</li><li>Define a distance between them:&nbsp;\(\Delta(E,R)\) =&nbsp;\(\lim_{n \to \infty}\)&nbsp;\(d(e_n,r_n)\) for two representatives where this limit is meaningful since the metric is a real number and thus we are taking the limit in R which is complete</li><li>The limit of the distances is guaranteed to exist because the sequences are cauchy</li></ol>
-
-============================================================
-
-Note ID: 1734517645048
-  Field: Text
-    Before:
-      Proof that bounded monotonically increasing sequences in R converge to their supremum:<br><ul><li>Since the sequnce is bounded,&nbsp;\(s_n\)&nbsp;\(\leq\)&nbsp;\(M\)</li><li>The elements of the sequence form a bounded nonempty subset of R</li><li>By the axiom of completeness, a supremum exists&nbsp;\(s\) =&nbsp;\(\sup(\{\mathrm{range}(s_n)\})\)</li><li>For any&nbsp;\(s_n\), \(\exists\)&nbsp;\(\epsilon^\prime &gt;0\) such that&nbsp;\(s - \epsilon^\prime \) &lt; \(s_n\) which implies that we&nbsp;\(|s - s_n|\)&nbsp;\(&lt;\)&nbsp;\(\epsilon^\prime\)</li><li>Now, given that&nbsp;\(\epsilon^\prime\) exists, we need to prove convergence for any&nbsp;\(\epsilon\) unconditional on picking an&nbsp;\(s_n\)</li><li>For any&nbsp;\(\epsilon\)&nbsp;\(&gt;\)&nbsp;\(0\), choose&nbsp;\(N\) such that&nbsp;\(|s_N - s|\)&nbsp;\(&lt;\)&nbsp;\(\epsilon\)</li><li>Since the sequence is monotonically increasing,&nbsp;\(\forall\)&nbsp;\(n \geq N\),&nbsp;\(|s_n - s|\)&nbsp;\(&lt;\)&nbsp;\(\epsilon\)&nbsp;</li></ul>
-
-    After:
-      Proof that bounded monotonically increasing sequences in R converge to their supremum:<br><ul><li>Since the sequnce is bounded,&nbsp;\(s_n\)&nbsp;\(\leq\)&nbsp;\(M\)</li><li>The elements of the sequence form a bounded nonempty subset of R</li><li>By the axiom of completeness, a supremum exists&nbsp;\(s\) =&nbsp;\(\sup(\{\mathrm{range}(s_n)\})\)</li><li>For any&nbsp;\(s_n\), \(\exists\)&nbsp;\(\epsilon^\prime &gt;0\) such that&nbsp;\(s - \epsilon^\prime \) &lt; \(s_n\) which implies that we&nbsp;\(|s - s_n|\)&nbsp;\(&lt;\)&nbsp;\(\epsilon^\prime\)</li><li>Now, given that&nbsp;\(\epsilon^\prime\) exists, we need to prove convergence for any&nbsp;\(\epsilon\) unconditional on picking an&nbsp;\(s_n\)</li><li>For any&nbsp;\(\epsilon\)&nbsp;\(&gt;\)&nbsp;\(0\), choose&nbsp;\(N\) such that&nbsp;\(|s_N - s|\)&nbsp;\(&lt;\)&nbsp;\(\epsilon\)</li><li>Since the sequence is monotonically increasing,&nbsp;\(\forall\)&nbsp;\(n \geq N\),&nbsp;\(|s_n - s|\)&nbsp;\(&lt;\)&nbsp;\(\epsilon\)&nbsp;</li></ul>
-
-============================================================
-
-Note ID: 1734623101096
-  Field: Text
-    Before:
-      What are the two comparison tests for series?<br><ol><li>If&nbsp;\(|a_n|\)&nbsp;\(\leq\)&nbsp;\(c_n\),&nbsp;\(\forall\)&nbsp;\(n\)&nbsp;\(\geq\)&nbsp;\(N\)-constant::condition for n::condition for n::condition for n::condition for n</li><ol><li>And&nbsp;\(\sum c_n\) converges, then&nbsp;\(\sum a_n\) converges</li></ol><li>If&nbsp;\(a_n\)&nbsp;\(&gt;\)&nbsp;&nbsp;\(d_n\)&nbsp;\(\geq\)&nbsp;\(0\),&nbsp;\(\forall\)&nbsp;\(n\)&nbsp;\(\geq\)&nbsp;\(N\)-constant::condition for n::condition for n::condition for n::condition for n</li><ol><li>If&nbsp;\(\sum d_n\) diverges, then&nbsp;\(\sum a_n\) diverges</li></ol></ol>
-
-    After:
-      What are the two comparison tests for series?<br><ol><li>If&nbsp;\(|a_n|\)&nbsp;\(\leq\)&nbsp;\(c_n\),&nbsp;\(\forall\)&nbsp;\(n\)&nbsp;\(\geq\)&nbsp;\(N\)-constant::condition for n::condition for n::condition for n::condition for n</li><ol><li>And&nbsp;\(\sum c_n\) converges, then&nbsp;\(\sum a_n\) converges</li></ol><li>If&nbsp;\(a_n\)&nbsp;\(&gt;\)&nbsp;&nbsp;\(d_n\)&nbsp;\(\geq\)&nbsp;\(0\),&nbsp;\(\forall\)&nbsp;\(n\)&nbsp;\(\geq\)&nbsp;\(N\)-constant::condition for n::condition for n::condition for n::condition for n</li><ol><li>If&nbsp;\(\sum d_n\) diverges, then&nbsp;\(\sum a_n\) diverges</li></ol></ol>
-
-============================================================
-
-Note ID: 1734624964293
-  Field: Text
-    Before:
-      <div><ol><li>If&nbsp;\(a_n\)&nbsp;\(&gt;\)&nbsp;&nbsp;\(d_n\)&nbsp;\(\geq\)&nbsp;\(0\),&nbsp;\(\forall\)&nbsp;\(n\)&nbsp;\(\geq\)&nbsp;\(N\)-constant</li><ol><li>If&nbsp;\(\sum d_n\) diverges, then&nbsp;\(\sum a_n\) diverges</li></ol></ol><div>Proof:</div></div><div><ol><li>We can prove the contrapositive by showing that&nbsp;\(\sum a_n\) convergent implies&nbsp;\(\sum\)&nbsp;\(d_n\) convergent, using the standard comparison test&nbsp;</li></ol></div><div><ol></ol></div>
-
-    After:
-      <div><ol><li>If&nbsp;\(a_n\)&nbsp;\(&gt;\)&nbsp;&nbsp;\(d_n\)&nbsp;\(\geq\)&nbsp;\(0\),&nbsp;\(\forall\)&nbsp;\(n\)&nbsp;\(\geq\)&nbsp;\(N\)-constant</li><ol><li>If&nbsp;\(\sum d_n\) diverges, then&nbsp;\(\sum a_n\) diverges</li></ol></ol><div>Proof:</div></div><div><ol><li>We can prove the contrapositive by showing that&nbsp;\(\sum a_n\) convergent implies&nbsp;\(\sum\)&nbsp;\(d_n\) convergent, using the standard comparison test&nbsp;</li></ol></div><div><ol></ol></div>
-
-============================================================
-
-Note ID: 1734694653923
-  Field: Text
-    Before:
-      <div><ol><li>For&nbsp;\(a_1\)&nbsp;\(\geq\)&nbsp;\(a_2\)\(\ldots\)\(\geq\)\(a_n\)\(\geq\)\(0\), so monotone decreasing to 0</li><li>Then&nbsp;\(\sum a_k\) converges iff&nbsp;\(\sum 2^k a_{2^k}\)</li></ol><div>Proof idea:</div></div><div><ul><li>Write&nbsp;\(s_n\) =&nbsp;\(\sum_{k=1}^n a_k\) and&nbsp;\(t_n\) = \(\sum_{k=1}^n 2^k a_{2^k}\)<br></li><li>\(2\) \(s_n\) =&nbsp;\(2\)&nbsp;\(a_1\) <span style="font-size: 16.6667px;">+&nbsp;</span>\(2\)&nbsp;\(a_2\)+&nbsp;\(2\) (\(a_3+a_4\)) +&nbsp;\(2\) (\(a_5+a_6+a_7+a_8\))&nbsp;+&nbsp;\(\ldots\)<br></li><li>\(t_n\) =&nbsp;\(a_1\) + (\(a_2+a_2\)) +&nbsp;\(4 a_4\) +&nbsp;\(8 a_8\)+&nbsp;\(\ldots\)<br></li><li>Thus for&nbsp;\(n\)&nbsp;\(&gt;\)&nbsp;\(2^k\), then&nbsp;\(t_n\)&nbsp;\(&lt;\)&nbsp;\(2\)&nbsp;\(s_n\), since&nbsp;\(a_1\)&nbsp;\(&lt;\)&nbsp;\(2 a_1\),&nbsp;\(4 a_4\)&nbsp;\(&lt;\)\(2\) (\(a_3+a_4\)) since&nbsp;\(a_4\)&nbsp;\(&lt;\)&nbsp;\(a_3\) and so on</li><ul><li>If&nbsp;\(t_n\)&nbsp;\(&lt;\)&nbsp;\(2 s_n\) and&nbsp;\(t_n\) converges so does&nbsp;\(2\)&nbsp;\(s_n\) by comparison test</li><li>If&nbsp;\(2 s_n\) converges so does&nbsp;\(s_n\)</li></ul></ul></div>
-
-    After:
-      <div><ol><li>For&nbsp;\(a_1\)&nbsp;\(\geq\)&nbsp;\(a_2\)\(\ldots\)\(\geq\)\(a_n\)\(\geq\)\(0\), so monotone decreasing to 0</li><li>Then&nbsp;\(\sum a_k\) converges iff&nbsp;\(\sum 2^k a_{2^k}\)</li></ol><div>Proof idea:</div></div><div><ul><li>Write&nbsp;\(s_n\) =&nbsp;\(\sum_{k=1}^n a_k\) and&nbsp;\(t_n\) = \(\sum_{k=1}^n 2^k a_{2^k}\)<br></li><li>\(2\) \(s_n\) =&nbsp;\(2\)&nbsp;\(a_1\) <span style="font-size: 16.6667px;">+&nbsp;</span>\(2\)&nbsp;\(a_2\)+&nbsp;\(2\) (\(a_3+a_4\)) +&nbsp;\(2\) (\(a_5+a_6+a_7+a_8\))&nbsp;+&nbsp;\(\ldots\)<br></li><li>\(t_n\) =&nbsp;\(a_1\) + (\(a_2+a_2\)) +&nbsp;\(4 a_4\) +&nbsp;\(8 a_8\)+&nbsp;\(\ldots\)<br></li><li>Thus for&nbsp;\(n\)&nbsp;\(&gt;\)&nbsp;\(2^k\), then&nbsp;\(t_n\)&nbsp;\(&lt;\)&nbsp;\(2\)&nbsp;\(s_n\), since&nbsp;\(a_1\)&nbsp;\(&lt;\)&nbsp;\(2 a_1\),&nbsp;\(4 a_4\)&nbsp;\(&lt;\)\(2\) (\(a_3+a_4\)) since&nbsp;\(a_4\)&nbsp;\(&lt;\)&nbsp;\(a_3\) and so on</li><ul><li>If&nbsp;\(t_n\)&nbsp;\(&lt;\)&nbsp;\(2 s_n\) and&nbsp;\(t_n\) converges so does&nbsp;\(2\)&nbsp;\(s_n\) by comparison test</li><li>If&nbsp;\(2 s_n\) converges so does&nbsp;\(s_n\)</li></ul></ul></div>
-
-============================================================
-
-Note ID: 1735161071198
-  Field: Text
-    Before:
-      <ol><ol><li>Given a series&nbsp;\(\sum a_n\), then let&nbsp;\(\alpha\) =&nbsp;\(\lim\)&nbsp;\(\sup\)&nbsp;\(\sqrt[n]{a_n}\), which can be calculated in the extended reals</li><li>Then:</li><ol><li>\(\alpha\)&nbsp;\(\leq\)&nbsp;\(1\)&nbsp;\(\implies\)&nbsp;\(\sum a_n\) converges<br></li><li>\(\alpha\)&nbsp;\(&gt;\)&nbsp;\(1\)&nbsp;\(\implies\)&nbsp;\(\sum a_n\) diverges</li><li>\(\alpha\)&nbsp;\(=\)&nbsp;\(1\)&nbsp;\(\implies\) test inconclusive</li></ol></ol><div>Proof:</div><div><ol><li>If&nbsp;\(\alpha\)&nbsp;\(&gt;\)&nbsp;\(1\)</li><ol><li>There exists a subsequence \(\sqrt[n_k]{a_{n_k} }\)&nbsp; which converges to&nbsp;\(\alpha\)&nbsp;\(&gt;\)&nbsp;\(1\) which implies&nbsp;\(|a_{n_k}|\)&nbsp;\(&gt;\)&nbsp;\(1\) for infinitely many terms so \(a_n\) \(\not \to\)&nbsp;\(0\) meaning the series does not converge<br></li></ol></ol></div></ol>
-
-    After:
-      <ol><ol><li>Given a series&nbsp;\(\sum a_n\), then let&nbsp;\(\alpha\) =&nbsp;\(\lim\)&nbsp;\(\sup\)&nbsp;\(\sqrt[n]{a_n}\), which can be calculated in the extended reals</li><li>Then:</li><ol><li>\(\alpha\)&nbsp;\(\leq\)&nbsp;\(1\)&nbsp;\(\implies\)&nbsp;\(\sum a_n\) converges<br></li><li>\(\alpha\)&nbsp;\(&gt;\)&nbsp;\(1\)&nbsp;\(\implies\)&nbsp;\(\sum a_n\) diverges</li><li>\(\alpha\)&nbsp;\(=\)&nbsp;\(1\)&nbsp;\(\implies\) test inconclusive</li></ol></ol><div>Proof:</div><div><ol><li>If&nbsp;\(\alpha\)&nbsp;\(&gt;\)&nbsp;\(1\)</li><ol><li>There exists a subsequence \(\sqrt[n_k]{a_{n_k} }\)&nbsp; which converges to&nbsp;\(\alpha\)&nbsp;\(&gt;\)&nbsp;\(1\) which implies&nbsp;\(|a_{n_k}|\)&nbsp;\(&gt;\)&nbsp;\(1\) for infinitely many terms so \(a_n\) \(\not \to\)&nbsp;\(0\) meaning the series does not converge<br></li></ol></ol></div></ol>
-
-============================================================
-
-Note ID: 1735173298900
-  Field: Text
-    Before:
-      If a series&nbsp;\(\sum\)&nbsp;\(a_n\) converges absolutely then it converges
-
-    After:
-      If a series&nbsp;\(\sum\)&nbsp;\(a_n\) converges absolutely then it converges
-
-============================================================
-
-Note ID: 1735248827610
-  Field: Text
-    Before:
-      Which of the following are continous/not and why?<br><img src="paste-1e90975d470adf571e57eb17cb93685201d5a13d.jpg"><br><ol><li>Continous, no gaps</li><li>Not continous, f(p) very disimilar to the limits from left and right</li><li>Not continous, discontinuity at p</li></ol>
-
-    After:
-      Which of the following are continous/not and why?<br><img src="paste-1e90975d470adf571e57eb17cb93685201d5a13d.jpg"><br><ol><li>Continous, no gaps</li><li>Not continous, f(p) very disimilar to the limits from left and right</li><li>Not continous, discontinuity at p</li></ol>
-
-============================================================
-
-Note ID: 1735250443222
-  Field: Text
-    Before:
-      Sums and products of continous functions are continous when the domain is R
-
-    After:
-      Sums and products of continous functions are continous when the domain is R
-
-============================================================
-
-Note ID: 1735250492617
-  Field: Text
-    Before:
-      Quotients of continous functions with domain R \(\frac{f}{g}\) are also continous&nbsp; when&nbsp;\(g\)&nbsp;\(\neq\)&nbsp;\(0\)
-
-    After:
-      Quotients of continous functions with domain R \(\frac{f}{g}\) are also continous&nbsp; when&nbsp;\(g\)&nbsp;\(\neq\)&nbsp;\(0\)
-
-============================================================
-
-Note ID: 1735253553382
-  Field: Text
-    Before:
-      For&nbsp;\(f,g\)&nbsp;\(\in\)&nbsp;\(X\)&nbsp;\(\to\)&nbsp;\(R^k\) , which can be represented as&nbsp;\(f\) =&nbsp;\((f_1,\ldots,f_n)\) we can check if&nbsp;\(f\) is continous by checking if each indepdendent&nbsp;\(f_j\) is continous
-
-    After:
-      For&nbsp;\(f,g\)&nbsp;\(\in\)&nbsp;\(X\)&nbsp;\(\to\)&nbsp;\(R^k\) , which can be represented as&nbsp;\(f\) =&nbsp;\((f_1,\ldots,f_n)\) we can check if&nbsp;\(f\) is continous by checking if each indepdendent&nbsp;\(f_j\) is continous
-
-============================================================
-
-Note ID: 1735303671022
-  Field: Text
-    Before:
-      &nbsp;\(f\)&nbsp;\(:\)&nbsp;\(X\)&nbsp;\(\to\)&nbsp;\(Y\) is continous iff&nbsp;\(\forall\) open sets&nbsp;\(U\) in&nbsp;\(Y\),&nbsp;\(f^{-1}\)\((U)\) is open in&nbsp;\(X\)<br>Proof<br><ol><li><img src="paste-c034d751b0d8adaa299738bc65c8aa07760b1800.jpg"><br></li><li>For any \(U\)&nbsp;\(\in Y\) open, pick any point&nbsp;\(x\)&nbsp;\(\in\)&nbsp;\(f^{-1}(U)\), we want to show&nbsp;\(x\) is interior to \(f^{-1}(U)\)</li><li>Since U is open,&nbsp;\(f(x)\) is interior so there exists a neighbourhood \(N_{\epsilon}(f(x))\) =&nbsp;\(\{y \in Y, d(f(x),y) &lt; \epsilon\}\) with \(N_{\epsilon}(f(x))\)&nbsp;&nbsp;\(\subset\)&nbsp;\(U\)</li><li>By continuity of&nbsp;\(f\),&nbsp;\(\exists\) a \(\delta\)-ball&nbsp;\(N_{\delta}(x)\)&nbsp; such that \(N_{\delta}(x)\) is mapped into \(N_{\epsilon}(f(x))\)</li><li>Because \(N_{\epsilon}(f(x))\)&nbsp;\(\subset\)&nbsp;\(U\), this means \(N_{\delta}(x)\)&nbsp;\(\subset\)&nbsp;\(f^{-1}(U)\)</li><li>Which means&nbsp;\(x\) is an interior point of \(f^{-1}(U)\)&nbsp;</li></ol>
-
-    After:
-      &nbsp;\(f\)&nbsp;\(:\)&nbsp;\(X\)&nbsp;\(\to\)&nbsp;\(Y\) is continous iff&nbsp;\(\forall\) open sets&nbsp;\(U\) in&nbsp;\(Y\),&nbsp;\(f^{-1}\)\((U)\) is open in&nbsp;\(X\)<br>Proof<br><ol><li><img src="paste-c034d751b0d8adaa299738bc65c8aa07760b1800.jpg"><br></li><li>For any \(U\)&nbsp;\(\in Y\) open, pick any point&nbsp;\(x\)&nbsp;\(\in\)&nbsp;\(f^{-1}(U)\), we want to show&nbsp;\(x\) is interior to \(f^{-1}(U)\)</li><li>Since U is open,&nbsp;\(f(x)\) is interior so there exists a neighbourhood \(N_{\epsilon}(f(x))\) =&nbsp;\(\{y \in Y, d(f(x),y) &lt; \epsilon\}\) with \(N_{\epsilon}(f(x))\)&nbsp;&nbsp;\(\subset\)&nbsp;\(U\)</li><li>By continuity of&nbsp;\(f\),&nbsp;\(\exists\) a \(\delta\)-ball&nbsp;\(N_{\delta}(x)\)&nbsp; such that \(N_{\delta}(x)\) is mapped into \(N_{\epsilon}(f(x))\)</li><li>Because \(N_{\epsilon}(f(x))\)&nbsp;\(\subset\)&nbsp;\(U\), this means \(N_{\delta}(x)\)&nbsp;\(\subset\)&nbsp;\(f^{-1}(U)\)</li><li>Which means&nbsp;\(x\) is an interior point of \(f^{-1}(U)\)&nbsp;</li></ol>
-
-============================================================
-
-Note ID: 1735306866611
-  Field: Text
-    Before:
-      For&nbsp;\(f,g\) continous,&nbsp;\(f:\) \(X \to&nbsp; Y\) and&nbsp;\(g:\) \(Y \to Z\),&nbsp;\(g \circ f\) is continous
-
-    After:
-      For&nbsp;\(f,g\) continous,&nbsp;\(f:\) \(X \to&nbsp; Y\) and&nbsp;\(g:\) \(Y \to Z\),&nbsp;\(g \circ f\) is continous
-
-============================================================
-
-Note ID: 1735307615435
-  Field: Text
-    Before:
-      <ol><li>\(f:\)\(X\)&nbsp;\(\to\)\(Y\) is continous iff&nbsp;\(\forall\)&nbsp;\(K\) closed in&nbsp;\(Y\),&nbsp;\(f^{-1}\)\((K)\) is closed in&nbsp;\(X\)</li></ol>
-
-    After:
-      <ol><li>\(f:\)\(X\)&nbsp;\(\to\)\(Y\) is continous iff&nbsp;\(\forall\)&nbsp;\(K\) closed in&nbsp;\(Y\),&nbsp;\(f^{-1}\)\((K)\) is closed in&nbsp;\(X\)</li></ol>
-
-============================================================
-
-Note ID: 1735316266082
-  Field: Text
-    Before:
-      If \(f\)\(:\)&nbsp;\(X \to Y\) is continous and&nbsp;\(X\) is compact, then \(f\)\((X)\) is compact
-
-    After:
-      If \(f\)\(:\)&nbsp;\(X \to Y\) is continous and&nbsp;\(X\) is compact, then \(f\)\((X)\) is compact
-
-============================================================
-
-Note ID: 1735332107444
-  Field: Text
-    Before:
-      For&nbsp;\(f\)\(:\)\(X\)\(\to\)\(Y\) continous, if&nbsp;\(E\) is a connected subset of&nbsp;\(X\), then&nbsp;\(f\)\((E)\) is connected
-
-    After:
-      For&nbsp;\(f\)\(:\)\(X\)\(\to\)\(Y\) continous, if&nbsp;\(E\) is a connected subset of&nbsp;\(X\), then&nbsp;\(f\)\((E)\) is connected
-
-============================================================
-
-Note ID: 1735338731021
-  Field: Text
-    Before:
-      For a function&nbsp;\(f:\)\((a,b)\)&nbsp;\(\to\)&nbsp;\(R\)<br><ol><li>If&nbsp;\(\forall\) sequences \(\{t_n\}\) \(\in\) \((x,b)\) with&nbsp;\(t_n\)&nbsp;\(\to\)&nbsp;\(x\),&nbsp;\(f\)\((t_n)\)&nbsp;\(\to\)&nbsp;\(q\), then&nbsp;\(f(x^+)\)&nbsp;\(=\)&nbsp;\(q\) =&nbsp;\(\lim_{t \to x^+}\) \(f(t) \) is the right-hand limit of&nbsp;\(f\)</li><li>Mutatis mulandis for left-hand limit&nbsp;&nbsp;\(f(x^-)\)&nbsp;\(=\)&nbsp;\(p\) =&nbsp;\(\lim_{t \to x^-}\) \(f(t) \)</li><li>If&nbsp;\(\lim_{t \to x}\) exists then \(\lim_{t \to x}\) = \(\lim_{t \to x^-}\) \(f(t) \) = \(\lim_{t \to x^+}\) \(f(t) \)&nbsp;</li></ol>
-
-    After:
-      For a function&nbsp;\(f:\)\((a,b)\)&nbsp;\(\to\)&nbsp;\(R\)<br><ol><li>If&nbsp;\(\forall\) sequences \(\{t_n\}\) \(\in\) \((x,b)\) with&nbsp;\(t_n\)&nbsp;\(\to\)&nbsp;\(x\),&nbsp;\(f\)\((t_n)\)&nbsp;\(\to\)&nbsp;\(q\), then&nbsp;\(f(x^+)\)&nbsp;\(=\)&nbsp;\(q\) =&nbsp;\(\lim_{t \to x^+}\) \(f(t) \) is the right-hand limit of&nbsp;\(f\)</li><li>Mutatis mulandis for left-hand limit&nbsp;&nbsp;\(f(x^-)\)&nbsp;\(=\)&nbsp;\(p\) =&nbsp;\(\lim_{t \to x^-}\) \(f(t) \)</li><li>If&nbsp;\(\lim_{t \to x}\) exists then \(\lim_{t \to x}\) = \(\lim_{t \to x^-}\) \(f(t) \) = \(\lim_{t \to x^+}\) \(f(t) \)&nbsp;</li></ol>
-
-============================================================
-
-Note ID: 1736250075071
-  Field: Text
-    Before:
-      If \(a\) \(&lt;\) \(b\) and \(c\) \(&lt;\) \(0\) then \(a c\) \(&gt;\) \(b c\) and \(\frac{a}{c}\) \(&gt;\) \(\frac{b}{c}\)
-
-    After:
-      If \(a\) \(&lt;\) \(b\) and \(c\) \(&lt;\) \(0\) then \(a c\) \(&gt;\) \(b c\) and \(\frac{a}{c}\) \(&gt;\) \(\frac{b}{c}\)
-
-============================================================
-
-Note ID: 1736256944851
-  Field: Text
-    Before:
-      If \(b\) is a positive number<br><ol><li>\(|p|\) =\(b\) \(\Rightarrow\)&nbsp; \(p\) = \(-b\) or \(p\) =\(b\)<br></li></ol>
-
-    After:
-      If \(b\) is a positive number<br><ol><li>\(|p|\) =\(b\) \(\Rightarrow\)&nbsp; \(p\) = \(-b\) or \(p\) =\(b\)<br></li></ol>
-
-============================================================
-
-Note ID: 1736257041156
-  Field: Text
-    Before:
-      <div>If \(b\) is a positive number<br></div><div><ol><li>\(|p|\) \(&lt;\) \(b\) \(\Rightarrow\) \(-b\) \(&lt;\) \(p\) \(&lt;\) \(b\)<br></li></ol></div>
-
-    After:
-      <div>If \(b\) is a positive number<br></div><div><ol><li>\(|p|\) \(&lt;\) \(b\) \(\Rightarrow\) \(-b\) \(&lt;\) \(p\) \(&lt;\) \(b\)<br></li></ol></div>
-
-============================================================
-
-Note ID: 1736257142252
-  Field: Text
-    Before:
-      If \(b\) is a positive number<br><ol><li>\(|p|\)\(&gt;\)\(b\) \(\Rightarrow\) \(p\)\(&lt;\)\(-b\) or \(p\)\(&gt;\)\(b\)<br></li></ol>
-
-    After:
-      If \(b\) is a positive number<br><ol><li>\(|p|\)\(&gt;\)\(b\) \(\Rightarrow\) \(p\)\(&lt;\)\(-b\) or \(p\)\(&gt;\)\(b\)<br></li></ol>
-
-============================================================
-
-Note ID: 1736354566042
-  Field: Text
-    Before:
-      <ol><li>If \(r\) \(&gt;\) \(0\) then \(\lim _{x \rightarrow \infty}\) \(\frac{b}{x^{r}}\) = \(0\)<br></li></ol>
-
-    After:
-      <ol><li>If \(r\) \(&gt;\) \(0\) then \(\lim _{x \rightarrow \infty}\) \(\frac{b}{x^{r}}\) = \(0\)<br></li></ol>
-
-============================================================
-
-Note ID: 1736354611016
-  Field: Text
-    Before:
-      <ol><li>If \(r\) \(&gt;\) \(0\) and \(x^{r}\) is real for negative \(x\) then \(\lim _{x \rightarrow-\infty}\) \(\frac{b}{x^{r}}\) = \(0\)<br></li></ol>
-
-    After:
-      <ol><li>If \(r\) \(&gt;\) \(0\) and \(x^{r}\) is real for negative \(x\) then \(\lim _{x \rightarrow-\infty}\) \(\frac{b}{x^{r}}\) = \(0\)<br></li></ol>
-
-============================================================
-
-Note ID: 1736355594097
-  Field: Text
-    Before:
-      <div>Note: \(\operatorname{sgn}\) (\(a\)) = \(1\) if \(a\) \(&gt;\) \(0\) and \(\operatorname{sgn}\)(\(a\)) = \(-1\) if \(a\) \(&lt;\) \(0\).</div>
-
-    After:
-      <div>Note: \(\operatorname{sgn}\) (\(a\)) = \(1\) if \(a\) \(&gt;\) \(0\) and \(\operatorname{sgn}\)(\(a\)) = \(-1\) if \(a\) \(&lt;\) \(0\).</div>
-
-============================================================
-
-Note ID: 1736356837024
-  Field: Text
-    Before:
-      <ol><li>\(f(x)\) is continuous at \(b\) and \(\lim _{x \rightarrow a} g(x)\) = \(b\) then</li><ol><li>\(\lim _{x \rightarrow a} f(g(x))\)&nbsp;</li><li>= \(f\left(\lim _{x \rightarrow a} g(x)\right)\)</li><li>= \(f(b)\)</li></ol></ol>
-
-    After:
-      <ol><li>\(f(x)\) is continuous at \(b\) and \(\lim _{x \rightarrow a} g(x)\) = \(b\) then</li><ol><li>\(\lim _{x \rightarrow a} f(g(x))\)&nbsp;</li><li>= \(f\left(\lim _{x \rightarrow a} g(x)\right)\)</li><li>= \(f(b)\)</li></ol></ol>
-
-============================================================
-
-Note ID: 1736424637639
-  Field: Text
-    Before:
-      <ol><li>\(\frac{d}{d x}\)(\(\sec\) (\(x\)))=\(\sec\) (\(x\)) \(\tan\) (\(x\))</li></ol>
-
-    After:
-      <ol><li>\(\frac{d}{d x}\)(\(\sec\) (\(x\)))=\(\sec\) (\(x\)) \(\tan\) (\(x\))</li></ol>
-
-============================================================
-
-Note ID: 1736425611697
-  Field: Text
-    Before:
-      <ul><li>\(\frac{d}{d x}\)\(a^x\)=\(a^x\) \(\ln (a)\)<br></li></ul>
-
-    After:
-      <ul><li>\(\frac{d}{d x}\)\(a^x\)=\(a^x\) \(\ln (a)\)<br></li></ul>
-
-============================================================
-
-Note ID: 1736694561774
-  Field: Text
-    Before:
-      <ol><li>\(\frac{d}{d x}\)\(\mathbf{e}^{f(x)}\)=\(\mathbf{e}^{f(x)}\)\(f^{\prime}(x)\)<br></li></ol>
-
-    After:
-      <ol><li>\(\frac{d}{d x}\)\(\mathbf{e}^{f(x)}\)=\(\mathbf{e}^{f(x)}\)\(f^{\prime}(x)\)<br></li></ol>
-
-============================================================
-
-Note ID: 1736694967335
-  Field: Text
-    Before:
-      <ol><li>\(\frac{d}{d x}\) \(\sec\) [\(f(x)\)] = \(f^{\prime}(x)\) \(\sec\) [\(f(x)\)] \(\tan\) [\(f(x)\)]<br></li></ol>
-
-    After:
-      <ol><li>\(\frac{d}{d x}\) \(\sec\) [\(f(x)\)] = \(f^{\prime}(x)\) \(\sec\) [\(f(x)\)] \(\tan\) [\(f(x)\)]<br></li></ol>
-
-============================================================
-
-Note ID: 1736696468981
-  Field: Text
-    Before:
-      Implicit diferentiation of&nbsp;\(\mathbf{e}^{2 x-9 y}+x^{3} y^{2}=\sin (y)+11 x\)<br><ol><li>\(\mathbf{e}^{2x - 9y}\)\(\left(2 - 9\frac{dy}{dx}\right)\) + \(3x^2y^2\) + \(2x^3y\frac{dy}{dx}&nbsp;\) = \(\cos(y)\)\(\frac{dy}{dx}\) + \(11\)&nbsp;</li><li>From this, put all terms with&nbsp;\(\frac{dy}{dx}\) on one side and then resolve&nbsp;\(\frac{dy}{dx}\), the final fraction may include both x and&nbsp;\(y\)</li><li>\(&nbsp;y^{\prime}=\frac{11-2 \mathbf{e}^{2 x-9 y}-3 x^2 y^2}{2 x^3 y-9 \mathbf{e}^{2 x-9 y}-\cos (y)}\)</li></ol>
-
-    After:
-      Implicit diferentiation of&nbsp;\(\mathbf{e}^{2 x-9 y}+x^{3} y^{2}=\sin (y)+11 x\)<br><ol><li>\(\mathbf{e}^{2x - 9y}\)\(\left(2 - 9\frac{dy}{dx}\right)\) + \(3x^2y^2\) + \(2x^3y\frac{dy}{dx}&nbsp;\) = \(\cos(y)\)\(\frac{dy}{dx}\) + \(11\)&nbsp;</li><li>From this, put all terms with&nbsp;\(\frac{dy}{dx}\) on one side and then resolve&nbsp;\(\frac{dy}{dx}\), the final fraction may include both x and&nbsp;\(y\)</li><li>\(&nbsp;y^{\prime}=\frac{11-2 \mathbf{e}^{2 x-9 y}-3 x^2 y^2}{2 x^3 y-9 \mathbf{e}^{2 x-9 y}-\cos (y)}\)</li></ol>
-
-============================================================
-
-Note ID: 1736696740023
-  Field: Text
-    Before:
-      \section*{Increasing/Decreasing}<br><ol><li>If \(f^{\prime}(x)\) &gt; \(0\) for all \(x\) in an interval \(I\) then \(f(x)\) is increasing on the interval \(I\).</li><li>If \(f^{\prime}(x)\) &lt; \(0\) for all \(x\) in an interval \(I\) then \(f(x)\) is decreasing on the interval \(I\).</li><li>If \(f^{\prime}(x)\) = \(0\) for all \(x\) in an interval \(I\) then \(f(x)\) is constant on the interval \(I\).<br></li></ol>
-
-    After:
-      \section*{Increasing/Decreasing}<br><ol><li>If \(f^{\prime}(x)\) &gt; \(0\) for all \(x\) in an interval \(I\) then \(f(x)\) is increasing on the interval \(I\).</li><li>If \(f^{\prime}(x)\) &lt; \(0\) for all \(x\) in an interval \(I\) then \(f(x)\) is decreasing on the interval \(I\).</li><li>If \(f^{\prime}(x)\) = \(0\) for all \(x\) in an interval \(I\) then \(f(x)\) is constant on the interval \(I\).<br></li></ol>
-
-============================================================
-
-Note ID: 1736697014223
-  Field: Text
-    Before:
-      <br>1. If \(f^{\prime \prime}(x)\) \(&gt;\) \(0\) for all \(x\) in an interval \(I\) then \(f(x)\) is convex on the interval \(I\).<br>2. If \(f^{\prime \prime}(x)\) \(&lt;\) \(0\) for all \(x\) in an interval \(I\) then \(f(x)\) is concave on the interval \(I\).
-
-    After:
-      <br>1. If \(f^{\prime \prime}(x)\) \(&gt;\) \(0\) for all \(x\) in an interval \(I\) then \(f(x)\) is convex on the interval \(I\).<br>2. If \(f^{\prime \prime}(x)\) \(&lt;\) \(0\) for all \(x\) in an interval \(I\) then \(f(x)\) is concave on the interval \(I\).
-
-============================================================
-
-Note ID: 1736706792454
-  Field: Text
-    Before:
-      \section*{\(1^{\text {st }}\) Derivative Test}<br><br>If \(x=c\) is a critical point of \(f(x)\) then \(x=c\) is<br><ol><li>a relative maximum of \(f(x)\) if \(f^{\prime}(x)\) \(&gt;\) \(0\) to the left of \(x\) = \(c\) and \(f^{\prime}(x)\) \(&lt;\) \(0\) to the right of \(x=c\).</li><li>a relative minimum of \(f(x)\) if \(f^{\prime}(x)\) \(&lt;\) \(0\) to the left of \(x=c\) and \(f^{\prime}(x)\) \(&gt;\) \(0\) to the right of \(x=c\).</li><li>not a relative extrema of \(f(x)\) if \(f^{\prime}(x\) is the same sign on both sides of \(x=c\).</li></ol>
-
-    After:
-      \section*{\(1^{\text {st }}\) Derivative Test}<br><br>If \(x=c\) is a critical point of \(f(x)\) then \(x=c\) is<br><ol><li>a relative maximum of \(f(x)\) if \(f^{\prime}(x)\) \(&gt;\) \(0\) to the left of \(x\) = \(c\) and \(f^{\prime}(x)\) \(&lt;\) \(0\) to the right of \(x=c\).</li><li>a relative minimum of \(f(x)\) if \(f^{\prime}(x)\) \(&lt;\) \(0\) to the left of \(x=c\) and \(f^{\prime}(x)\) \(&gt;\) \(0\) to the right of \(x=c\).</li><li>not a relative extrema of \(f(x)\) if \(f^{\prime}(x\) is the same sign on both sides of \(x=c\).</li></ol>
-
-============================================================
-
-Note ID: 1736707031062
-  Field: Text
-    Before:
-      \(2^{\text {nd \)}} Derivative Test<br><br>If \(x=c\) is a critical point of \(f(x)\) such that \(f^{\prime}(c)\) = \(0\) then \(x=c\)<br><ol><li>is a relative maximum of \(f(x)\) if \(f^{\prime \prime}(c)\) \(&lt;\) \(0\).</li><li>is a relative minimum of \(f(x)\) if \(f^{\prime \prime}(c)\) \(&gt;\) \(0\).</li><li>may be a relative maximum, relative minimum, or neither if \(f^{\prime \prime}(c)\) = \(0\).</li></ol>
-
-    After:
-      \(2^{\text {nd \)}} Derivative Test<br><br>If \(x=c\) is a critical point of \(f(x)\) such that \(f^{\prime}(c)\) = \(0\) then \(x=c\)<br><ol><li>is a relative maximum of \(f(x)\) if \(f^{\prime \prime}(c)\) \(&lt;\) \(0\).</li><li>is a relative minimum of \(f(x)\) if \(f^{\prime \prime}(c)\) \(&gt;\) \(0\).</li><li>may be a relative maximum, relative minimum, or neither if \(f^{\prime \prime}(c)\) = \(0\).</li></ol>
-
-============================================================
-
-Note ID: 1736707590657
-  Field: Text
-    Before:
-      <ol><li>ab</li><li>ba</li></ol>
-
-    After:
-      <ol><li>ab</li><li>ba</li></ol>
-
-============================================================
-
-Note ID: 1736709697269
-  Field: Text
-    Before:
-      <ol><li>\(\int_a^b\) \(c\) \(d\) \(x\)=\(c\)\((b-a)\), if \(c \text { is a constant }\)<br></li></ol>
-
-    After:
-      <ol><li>\(\int_a^b\) \(c\) \(d\) \(x\)=\(c\)\((b-a)\), if \(c \text { is a constant }\)<br></li></ol>
-
-============================================================
-
-Note ID: 1736709985962
-  Field: Text
-    Before:
-      <div><ol><li>If \(f(x)\) \(\geq\) \(g(x)\) on \(a \leq x \leq b\) then \(\int_{a}^{b}\) f(x) \(d x\) \(\geq\) \(\int_{a}^{b}\) \(g(x)\) \(d x\)</li></ol></div>
-
-    After:
-      <div><ol><li>If \(f(x)\) \(\geq\) \(g(x)\) on \(a \leq x \leq b\) then \(\int_{a}^{b}\) f(x) \(d x\) \(\geq\) \(\int_{a}^{b}\) \(g(x)\) \(d x\)</li></ol></div>
-
-============================================================
-
-Note ID: 1736710043639
-  Field: Text
-    Before:
-      <ol><li>If \(f(x)\) \(\geq\) \(0\) on \(a \leq x \leq b\) then \(\int_{a}^{b}\) \(f(x)\) \(d x\) \(\geq\) \(0\)<br></li></ol>
-
-    After:
-      <ol><li>If \(f(x)\) \(\geq\) \(0\) on \(a \leq x \leq b\) then \(\int_{a}^{b}\) \(f(x)\) \(d x\) \(\geq\) \(0\)<br></li></ol>
-
-============================================================
-
-Note ID: 1736711022244
-  Field: Text
-    Before:
-      <ol><li>\(\int\) \(k\) \(d x\)=\(k\) \(x\)\(+\)\(c\)<br></li></ol>
-
-    After:
-      <ol><li>\(\int\) \(k\) \(d x\)=\(k\) \(x\)\(+\)\(c\)<br></li></ol>
-
-============================================================
-
-Note ID: 1736711066783
-  Field: Text
-    Before:
-      <ol><li>\(\int\) \(\mathbf{e}^u\) \(d u\)=\(\mathbf{e}^u\)\(+c\)<br></li></ol>
-
-    After:
-      <ol><li>\(\int\) \(\mathbf{e}^u\) \(d u\)=\(\mathbf{e}^u\)\(+c\)<br></li></ol>
-
-============================================================
-
-Note ID: 1736711629984
-  Field: Text
-    Before:
-      <ol><li>\(\int\) \(\csc\)(\(u\))\(\cot\)(\(u\)) \(d u\)=\(-\)\(\csc\)(\(u\))\(+c\)<br></li></ol>
-
-    After:
-      <ol><li>\(\int\) \(\csc\)(\(u\))\(\cot\)(\(u\)) \(d u\)=\(-\)\(\csc\)(\(u\))\(+c\)<br></li></ol>
-
-============================================================
-
-Note ID: 1736711902332
-  Field: Text
-    Before:
-      <div><ol><li>\(\int\) \(\ln\)(\(u\)) \(d u\)=\(u\)\(\ln\)(\(u\))\(-\)\(u\)\(+c\)</li></ol></div>
-
-    After:
-      <div><ol><li>\(\int\) \(\ln\)(\(u\)) \(d u\)=\(u\)\(\ln\)(\(u\))\(-\)\(u\)\(+c\)</li></ol></div>
-
-============================================================
-
-Note ID: 1736722322172
-  Field: Text
-    Before:
-      <br>3. \(\int_{-\infty}^{\infty}\) \(f(x) d x\)=\(\int_{-\infty}^{c}\) \(f(x)\) \(d x\)\(+\)\(\int_{c}^{\infty}\) \(f(x)\) \(d x\) provided both integrals are convergent.
-
-    After:
-      <br>3. \(\int_{-\infty}^{\infty}\) \(f(x) d x\)=\(\int_{-\infty}^{c}\) \(f(x)\) \(d x\)\(+\)\(\int_{c}^{\infty}\) \(f(x)\) \(d x\) provided both integrals are convergent.
-
-============================================================
-
-Note ID: 1736722836249
-  Field: Text
-    Before:
-      Comparison Test for Improper Integrals: If \(f(x)\) \(\geq\) \(g(x)\) \(\geq\) \(0\) on \([a, \infty)\) then,<br><ol><li>If \(\int_{a}^{\infty} f(x) d x\) is convergent then \(\int_{a}^{\infty} g(x) d x\) is convergent (if larger converges so does the smaller).<br></li></ol>
-
-    After:
-      Comparison Test for Improper Integrals: If \(f(x)\) \(\geq\) \(g(x)\) \(\geq\) \(0\) on \([a, \infty)\) then,<br><ol><li>If \(\int_{a}^{\infty} f(x) d x\) is convergent then \(\int_{a}^{\infty} g(x) d x\) is convergent (if larger converges so does the smaller).<br></li></ol>
-
-============================================================
-
-Note ID: 1736722901856
-  Field: Text
-    Before:
-      <div>Comparison Test for Improper Integrals: If \(f(x)\) \(\geq\) \(g(x)\) \(\geq\) \(0\) on \([a, \infty)\) then,<br></div><div><ol><li>If \(\int_{a}^{\infty} g(x) d x\) is divergent then \(\int_{a}^{\infty} f(x) d x\) is divergent (if smaller diverges so does the larger).<br></li></ol></div>
-
-    After:
-      <div>Comparison Test for Improper Integrals: If \(f(x)\) \(\geq\) \(g(x)\) \(\geq\) \(0\) on \([a, \infty)\) then,<br></div><div><ol><li>If \(\int_{a}^{\infty} g(x) d x\) is divergent then \(\int_{a}^{\infty} f(x) d x\) is divergent (if smaller diverges so does the larger).<br></li></ol></div>
-
-============================================================
-
-Note ID: 1736724263358
-  Field: Text
-    Before:
-      <div><div>Useful fact: If \(a\) \(&gt;\) \(0\) then \(\int_{a}^{\infty}\) \(\frac{1}{x^{p} }\) \(d x\) converges if \(p\)\(&gt;\) \(1\) and diverges for \(p\) \(\leq\) \(1\).<br></div></div>
-
-    After:
-      <div><div>Useful fact: If \(a\) \(&gt;\) \(0\) then \(\int_{a}^{\infty}\) \(\frac{1}{x^{p} }\) \(d x\) converges if \(p\)\(&gt;\) \(1\) and diverges for \(p\) \(\leq\) \(1\).<br></div></div>
-
-============================================================
-
-Note ID: 1736804982531
-  Field: Text
-    Before:
-      Definition 2.1.3 We define \(^3 1\) to be the number \(0++\), \(2\) to be the number \((0++)++\), 3 to be the number \(((0++)++)++\), etc. (In other words, \(1:=\)\(0++\) , \(2:\)=\(1++\), \(3\):= \(2++\), etc.) In this text I use " \(x\)\(:=\)\(y\) " to denote the statement that \(x\) is defined to equal \(y\).)
-
-    After:
-      Definition 2.1.3 We define \(^3 1\) to be the number \(0++\), \(2\) to be the number \((0++)++\), 3 to be the number \(((0++)++)++\), etc. (In other words, \(1:=\)\(0++\) , \(2:\)=\(1++\), \(3\):= \(2++\), etc.) In this text I use " \(x\)\(:=\)\(y\) " to denote the statement that \(x\) is defined to equal \(y\).)
-
-============================================================
-
-Note ID: 1736806205434
-  Field: Text
-    Before:
-      Proposition Template 2.1.11 A certain property \(P(n)\) is true for every natural number \(n\).<br><br>Proof Template:<br><ol><li>&nbsp;We use induction. We first verify the base case \(n=0\),</li><li>&nbsp;Now suppose inductively that \(n\) is a natural number, and \(P(n)\) has already been proven.&nbsp;</li><li>We now prove \(P(n++)\). , assuming that \(P(n)\) is true</li><li>&nbsp;This closes the induction, and thus \(P(n)\) is true for all numbers \(n\).</li></ol>
-
-    After:
-      Proposition Template 2.1.11 A certain property \(P(n)\) is true for every natural number \(n\).<br><br>Proof Template:<br><ol><li>&nbsp;We use induction. We first verify the base case \(n=0\),</li><li>&nbsp;Now suppose inductively that \(n\) is a natural number, and \(P(n)\) has already been proven.&nbsp;</li><li>We now prove \(P(n++)\). , assuming that \(P(n)\) is true</li><li>&nbsp;This closes the induction, and thus \(P(n)\) is true for all numbers \(n\).</li></ol>
-
-============================================================
-
-Note ID: 1736808209528
-  Field: Text
-    Before:
-      Proposition 2.2.6 (Cancellation law). Let \(a, b, c\) be natural numbers such that \(a+\) \(b\)=\(a+c\). Then we have \(b\)=\(c\).
-
-    After:
-      Proposition 2.2.6 (Cancellation law). Let \(a, b, c\) be natural numbers such that \(a+\) \(b\)=\(a+c\). Then we have \(b\)=\(c\).
-
-============================================================
-
-Note ID: 1736808328877
-  Field: Text
-    Before:
-      Proposition 2.2.8 If a is a positive natural number, and \(b\) is a natural number, then \(a+b\) is positive&nbsp;
-
-    After:
-      Proposition 2.2.8 If a is a positive natural number, and \(b\) is a natural number, then \(a+b\) is positive&nbsp;
-
-============================================================
-
-Note ID: 1736808388988
-  Field: Text
-    Before:
-      Corollary 2.2.9 If \(a\) and \(b\) are natural numbers such that \(a+b\)=\(0\), then \(a\)=\(0\) and \(b\)=\(0\).
-
-    After:
-      Corollary 2.2.9 If \(a\) and \(b\) are natural numbers such that \(a+b\)=\(0\), then \(a\)=\(0\) and \(b\)=\(0\).
-
-============================================================
-
-Note ID: 1736811028855
-  Field: Text
-    Before:
-      <li><br class="Apple-interchange-newline">Proposition 2.2.12 (Basic properties of order for natural numbers). Let \(a, b, c\) be natural numbers. Then</li><li>(a) (Order is reflexive) \(a \geq a\).</li><li>(b) (Order is transitive) If \(a \geq b\) and \(b \geq c\), then \(a \geq c\).</li><li>(c) (Order is antisymmetric) If \(a \geq b\) and \(b \geq a\), then \(a=b\).</li><li>(d) (Addition preserves order) \(a \geq b\) if and only if \(a+c \geq b+c\).</li><li>(e) \(a&lt;b\) if and only if \(a+1 \leq b\).</li><li>(f) \(a&lt;b\) if and only if \(b=a+d\) for some positive number \(d\).</li>
-
-    After:
-      <li><br class="Apple-interchange-newline">Proposition 2.2.12 (Basic properties of order for natural numbers). Let \(a, b, c\) be natural numbers. Then</li><li>(a) (Order is reflexive) \(a \geq a\).</li><li>(b) (Order is transitive) If \(a \geq b\) and \(b \geq c\), then \(a \geq c\).</li><li>(c) (Order is antisymmetric) If \(a \geq b\) and \(b \geq a\), then \(a=b\).</li><li>(d) (Addition preserves order) \(a \geq b\) if and only if \(a+c \geq b+c\).</li><li>(e) \(a&lt;b\) if and only if \(a+1 \leq b\).</li><li>(f) \(a&lt;b\) if and only if \(b=a+d\) for some positive number \(d\).</li>
-
-============================================================
-
-Note ID: 1736811886483
-  Field: Text
-    Before:
-      Definition 2.3.11 (Exponentiation for natural numbers). Let \(m\) be a natural number. To raise \(m\) to the power 0 , we define \(m^0\)\(:=\)\(1\); in particular, we define \(0^0\)\(:=\)\(1\). Now suppose recursively that \(m^n\) has been defined for some natural number \(n\), then we define \(m^{n++}\)\(:=\)\(m^n \times m\).
-
-    After:
-      Definition 2.3.11 (Exponentiation for natural numbers). Let \(m\) be a natural number. To raise \(m\) to the power 0 , we define \(m^0\)\(:=\)\(1\); in particular, we define \(0^0\)\(:=\)\(1\). Now suppose recursively that \(m^n\) has been defined for some natural number \(n\), then we define \(m^{n++}\)\(:=\)\(m^n \times m\).
-
-============================================================
-
-Note ID: 1736854921606
-  Field: Text
-    Before:
-      Axiom 3.3 (Empty set). There exists a set \(\emptyset\), known as the empty set, which contains no elements, i.e., for every object \(x\) we have \(x\) \(\notin\) \(\varnothing\).
-
-    After:
-      Axiom 3.3 (Empty set). There exists a set \(\emptyset\), known as the empty set, which contains no elements, i.e., for every object \(x\) we have \(x\) \(\notin\) \(\varnothing\).
-
-============================================================
-
-Note ID: 1736855115749
-  Field: Text
-    Before:
-      Lemma 3.1.5 (Single choice). Let A be a non-empty set. Then there exists an object \(x\) such that \(x\) \(\in\) \(A\).
-
-    After:
-      Lemma 3.1.5 (Single choice). Let A be a non-empty set. Then there exists an object \(x\) such that \(x\) \(\in\) \(A\).
-
-============================================================
-
-Note ID: 1736855947662
-  Field: Text
-    Before:
-      Axiom 3.4 (Singleton sets). <br><ol><li>If \(a\) is an object, then there exists a set \(\{a\}\) whose only element is \(a\), i.e., for every object \(y\), we have \(y\) \(\in\{a\}\) if and only if \(y\)=\(a\); we refer to \(\{a\}\) as the singleton set whose element is \(a\).&nbsp;</li></ol>
-
-    After:
-      Axiom 3.4 (Singleton sets). <br><ol><li>If \(a\) is an object, then there exists a set \(\{a\}\) whose only element is \(a\), i.e., for every object \(y\), we have \(y\) \(\in\{a\}\) if and only if \(y\)=\(a\); we refer to \(\{a\}\) as the singleton set whose element is \(a\).&nbsp;</li></ol>
-
-============================================================
-
-Note ID: 1736856056526
-  Field: Text
-    Before:
-      <ol><li>Furthermore, if \(a\) and \(b\) are objects, then there exists a set \(\{a, b\}\) whose only elements are \(a\) and \(b\); i.e., for every object \(y\), we have \(y\) \(\in\)\(\{a, b\}\) if and only if \(y\)=\(a\) or \(y\)=\(b\); we refer to this set as the pair set formed by \(a\) and \(b\).</li></ol>
-
-    After:
-      <ol><li>Furthermore, if \(a\) and \(b\) are objects, then there exists a set \(\{a, b\}\) whose only elements are \(a\) and \(b\); i.e., for every object \(y\), we have \(y\) \(\in\)\(\{a, b\}\) if and only if \(y\)=\(a\) or \(y\)=\(b\); we refer to this set as the pair set formed by \(a\) and \(b\).</li></ol>
-
-============================================================
-
-Note ID: 1736857955428
-  Field: Text
-    Before:
-      Lemma 3.1.12 If \(a\) and \(b\) are objects, then \(\{a, b\}\)=\(\{a\} \cup\{b\}\). If \(A, B, C\) are sets, then the union operation is commutative and associative. Also, we have \(A\) \(\cup\) \(A\)=\(A\) \(\cup\) \(\emptyset\)=\(\emptyset\) \(\cup\) \(A\)=\(A\).

=== Note ID: 1687277457209 (Block 1) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Why is clients starting from different initialisations bad?<br><ul><li>Causes weight divergence even if the data is IID</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1693264851824 (Block 2) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       5 (a) Show that if we think of \(\mathbf{C}\) as a vector space over \(\mathbf{R}\), then the list \((1+i, 1-i)\) is linearly independent.<br><br>(b) Show that if we think of \(\mathbf{C}\) as a vector space over \(\mathbf{C}\), then the list \((1+i, 1-i)\) is linearly dependent.
+
+============================================================
----------------------------

=== Note ID: 1693264905846 (Block 3) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       1 Suppose \(v_{1}, v_{2}, v_{3}, v_{4}\) spans \(V\). Prove that the list<br><br>\[<br>v_{1}-v_{2}, v_{2}-v_{3}, v_{3}-v_{4}, v_{4}<br>\]<br><br>also spans \(V\).
+
+============================================================
----------------------------

=== Note ID: 1693265369941 (Block 4) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose \(v_{1}, \ldots, v_{m}\) is linearly independent in \(V\) and \(w \in V\). <br><ul><li>Show that \(v_{1}, \ldots, v_{m}, w\) is linearly independent if and only if</li></ul><br>\[<br>w \notin \operatorname{span}\left(v_{1}, \ldots, v_{m}\right) .<br>\]<br>
+
+============================================================
----------------------------

=== Note ID: 1693304890148 (Block 5) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>Suppose \(T \in \mathcal{L}(V, W)\) is injective and \(v_{1}, \ldots, v_{n}\) is linearly independent in \(V\).&nbsp;</li><li>Prove that \(T v_{1}, \ldots, T v_{n}\) is linearly independent in \(W\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1693305017320 (Block 6) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       10 <br><br><ul><li>Suppose \(v_{1}, \ldots, v_{n}\) spans \(V\) and \(T \in \mathcal{L}(V, W)\).&nbsp;</li><li>Prove that the list \(T v_{1}, \ldots, T v_{n}\) spans range \(T\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1693305064083 (Block 7) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
-      <ul><li>Suppose \(S_{1}, \ldots, S_{n}\) are injective linear maps such that \(S_{1} S_{2} \cdots S_{n}\) makes sense.&nbsp;</li><li>Prove that \(S_{1} S_{2} \cdots S_{n}\) is injective.</li></ul>
+      <ul><li>Suppos e \(S_{1}, \ldots, S_{n}\) are injective linear maps such that \(S_{1} S_{2} \cdots S_{n}\) makes sense.&nbsp;</li><li>Prove that \(S_{1} S_{2} \cdots S_{n}\) is injective.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1693306042543 (Block 8) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose there exists a linear map on \(V\) whose null space and range are both finite-dimensional. Prove that \(V\) is finite-dimensional.
+
+============================================================
----------------------------

=== Note ID: 1693307869548 (Block 9) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       11 <br>Suppose \(a=\left(\begin{array}{lll}a_{1} &amp; \cdots &amp; a_{n}\end{array}\right)\) is a 1-by- \(n\) matrix and \(C\) is an \(n\)-by- \(p\) matrix. <br><ul><li>Prove that</li><li>\(a C\)= \(a_{1} C_{1, \cdot}+\cdots+a_{n} C_{n, \cdots}\)</li><li>In other words, show that \(a C\) is a linear combination of the rows of \(C\), with the scalars that multiply the rows coming from \(a\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1694674357653 (Block 10) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       1 <br><br>Suppose \(T \in \mathcal{L}(U, V)\) and \(S \in \mathcal{L}(V, W)\) are both invertible linear maps. <br><ul><li>Prove that \(S T \in \mathcal{L}(U, W)\) is invertible&nbsp;</li><li>And that \((S T)^{-1}\) = \(T^{-1} S^{-1}\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1694675926329 (Block 11) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       10 <br>Suppose \(V\) is finite-dimensional and \(S, T \in \mathcal{L}(V)\). <br><ul><li>Prove that \(S T\) = \(I\) if and only if \(T S\) = \(I\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1694774847509 (Block 12) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       4. (3 points) The second edition Axler, page 12 , exercise 5 . <br><br>For each of the following subsets of \(\mathbf{F}^{3}\), determine whether it is a subspace of \(F^3\)<ul><li>(a) \(\left\{\left(x_{1}, x_{2}, x_{3}\right) \in \mathbf{F}^{3}: x_{1}+2 x_{2}+3 x_{3}=0\right\}\).&nbsp;</li><ul><li>Addition \( (x_1+y_1) + 2(x_2+y_2) + 3(x_3+y_3) \) = 0</li><li>Mult:&nbsp;\(\lambda x_1 + 2 (\lambda x_2) + 3(\lambda x_3)\) = 0</li></ul><li>(b) \(\left\{\left(x_{1}, x_{2}, x_{3}\right) \in \mathbf{F}^{3}: x_{1}+2 x_{2}+3 x_{3}=4\right\}\). </li><ul><li>No origin</li></ul><li>(c) \(\left\{\left(x_{1}, x_{2}, x_{3}\right) \in \mathbf{F}^{3}: x_{1} x_{2} x_{3}=0\right\}\).</li><ul><li>&nbsp;Addition:&nbsp; &nbsp;\( \prod x_i = 0 \land \prod y_i = 0\) \(\not \implies\) \(&nbsp; \prod (x_i+y_i) = 0 \)&nbsp;</li></ul><li>(d) \(\left\{\left(x_{1}, x_{2}, x_{3}\right) \in \mathbf{F}^{3}: x_{1}=5 x_{3}\right\}\).</li><ul><li>Addition:&nbsp;\(x_1 + y_1\) = \(5x_3 + 5 y_3\) =&nbsp;\(5 (x_3+y_3)\)</li><li>Mult:&nbsp;\(&nbsp; \lambda x_1\) =&nbsp;\(5 \lambda x_3\)</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1694777496098 (Block 13) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       (3 points) The second edition Axler, page 35, exercise 3.<br><br>Suppose \(\left(v_{1}, \ldots, v_{n}\right)\) is linearly independent in \(V\) and \(w \in V\). <br><br>Prove that if \(\left(v_{1}+\right.\) \(\left.w, \ldots, v_{n}+w\right)\) is linearly dependent, then \(w \in \operatorname{span}\left(v_{1}, \ldots, v_{n}\right)\).<br><br>Solution:<br><ul><li>\(\vec{c} \cdot \vec{v} = 0\)&nbsp;\(\implies\)&nbsp;\( \vec{c} = \vec{0}\)<br></li><li>Not linearly indp implies&nbsp;\(\vec{c} \neq \vec{0}\)<br></li><ul><li>\(\vec{c} \cdot (\vec{v} + w \cdot \vec{1})\)<br></li><li>=&nbsp;\(\vec{c} \cdot \vec{v}\) + \(w \times \vec{c} \cdot \vec{1}\)</li><li>=&nbsp;\(\vec{c} \cdot \vec{v}\)&nbsp; + \(w \sum c_i\)</li></ul><li>Which implies:</li><ul><li>w =&nbsp;\(\frac{\vec{c} \cdot \vec{v} }{\sum c_i}\)</li><li>w&nbsp;\(\in\)&nbsp;\(\operatorname{span} \vec{v}\)</li></ul></ul><br>
+
+============================================================
----------------------------

=== Note ID: 1694789939193 (Block 14) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       2. (3 points) Let \(V\) be the vector space of polynomials of degree at most 999 with real coefficients. Define a linear map<br><br>\[<br>T: V \rightarrow \mathbb{R}^{100}, \quad T(p)=(p(1), p(2), \ldots, p(100)) .<br>\]<br><br><ul><li>a) Find the dimension of the null space of \(T\)</li><ul><li>Since p must be divisible by the polynomial \(z(x)\) =&nbsp;\(\prod_{i=1}^{100} (x-i)\) with degree 100&nbsp;</li><li>A basis of null space consists of the polynomials&nbsp;</li><ul><li>\( (z(x), x z(x),....,x^{899} z(x) )\)<br></li></ul><li>Meaning that the null space has dimension 900&nbsp;</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1694790432109 (Block 15) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       3. (6 points) Let \(V\) be the vector space of polynomials of degree at most 99 with real coefficients. Define a linear map<br><br>\[<br>T: V \rightarrow \mathbb{R}^{1000}, \quad T(p)=(p(1), p(2), \ldots, p(1000)) .<br>\]<br><br><ul><li>a) Find the dimension of the null space:</li><ul><li>Since for a polynomial to be in the null space it must vanish for 1,...,1000 then it must be of at degree 1000 at least</li><li>Since the polynomials of V have degree at most 99, the null space must be have dimension 0</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1695047324471 (Block 16) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       4. (3 points)<br><br>Suppose \(V\) and \(W\) are vector spaces over \(F\); that \(T \in\) \(\mathcal{L}(V, W)\); that \(\left(v_{1}, \ldots, v_{n}\right)\) is any list of vectors in \(V\); and that \(\left(T v_{1}, \ldots, T v_{n}\right)\) is linearly independent in \(W\).<ol><li>Prove that \(\left(v_{1}, \ldots, v_{n}\right)\) is linearly independent in \(V\).</li></ol><div>Solution:</div><div><ol><li>\(\forall x \in V\)&nbsp;\(x = \vec{v} \cdot \vec{v}\)<br></li><li>\(T(x)\) =&nbsp;\(T(\vec{c} \cdot \vec{v}) \)<br></li><li>Assume that&nbsp;\(\vec{v}\) is not lin indp:</li><ol><li>Meaning \(\exists \vec{b}\) such that :</li><ol><li>\(\vec{b} \cdot \vec{v} = 0\)</li><li>And&nbsp;\(\vec{b} \neq \vec{0}\)</li></ol><li>Which implies&nbsp;</li><ol><li>\(T(\vec{b}\cdot \vec{v}) \) = \( 0\)</li><li>&nbsp;Thus \(\sum b_i Tv_i = 0\) and&nbsp;\(\vec{b} \neq \vec{0}\)</li><li>Which would imply that&nbsp;\(\vec{Tv}\) was not linearly independent, which would be a contradiction</li></ol></ol></ol></div>
+
+============================================================
----------------------------

=== Note ID: 1695200542904 (Block 17) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       6 <br>For \(n\) a positive integer, define \(V^{n}\) by<br><br><ul><li>\(V^{n}\) = \(\underbrace{V \times \cdots \times V}_{n \text { times } } .\)</li></ul><br>Prove that \(V^{n}\) and \(\mathcal{L}\)\(\left(\mathbf{F}^{n}, V\right)\) are isomorphic vector spaces.<br>
+
+============================================================
----------------------------

=== Note ID: 1695202241388 (Block 18) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       11 Suppose \(v_{1}, \ldots, v_{m} \in V\). Let<br><br>\(A=\left\{\lambda_{1} v_{1}+\cdots+\lambda_{m} v_{m}: \lambda_{1}, \ldots, \lambda_{m} \in \mathbf{F}\right.\) and \(\left.\lambda_{1}+\cdots+\lambda_{m}=1\right\}\).<br><br><ul><li>(a) Prove that \(A\) is an affine subset of \(V\).</li><li><br></li><li>(b) Prove that every affine subset of \(V\) that contains \(v_{1}, \ldots, v_{m}\) also contains \(A\).</li><li><br></li><li>(c) Prove that \(A\) = \(v+U\) for some \(v \in V\) and some subspace \(U\) of \(V\) with \(\operatorname{dim} U \leq m-1\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1695204113618 (Block 19) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       9 <br><ul><li>Suppose \(v_{1}, \ldots, v_{n}\) is a basis of \(V\) and \(\varphi_{1}, \ldots, \varphi_{n}\) is the corresponding dual basis of \(V^{\prime}\).</li><li>Suppose \(\psi\) \(\in\) \(V^{\prime}\). </li><li><b>Prove that</b></li><ul><li>\(\psi\) = \(\psi\left(v_{1}\right) \varphi_{1}\) + \(\cdots\) + \(\psi\left(v_{n}\right) \varphi_{n} .\)</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1695204272427 (Block 20) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       12 Show that the dual map of the identity map on \(V\) is the identity map on \(V^{\prime}\).
+
+============================================================
----------------------------

=== Note ID: 1695204319746 (Block 21) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       15<br><br>&nbsp;Suppose \(W\) is finite-dimensional and \(T \in \mathcal{L}(V, W)\). <br><ul><li>Prove that \(T^{\prime}\) = \(0\) if and only if \(T\) =\(0\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1695234295671 (Block 22) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition&nbsp;</b>\(\operatorname{Re} z\)<b>&nbsp;,&nbsp;</b>\(\operatorname{Im} z\)<br><br>Suppose \(z\) = \(a+b i\), where \(a\) and \(b\) are real numbers.<br><ul><li>- The real part of \(z\), denoted \(\operatorname{Re} z\), is defined by \(\operatorname{Re} z\) = \(a\).<br></li><li><br></li><li>- The imaginary part of \(z\), denoted \(\operatorname{Im} z\), is defined by \(\operatorname{Im} z\) = \(b\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1695234542611 (Block 23) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition complex conjugate, absolute value</b><br><br>Suppose \(z \in \mathbf{C}\).<br><br>- The complex conjugate of \(z \in \mathbf{C}\), denoted \(\bar{z}\), is defined by<br><br><ul><li>\(\bar{z}\) = \(\operatorname{Re} z-(\operatorname{Im} z) i .\)</li></ul><br>- The absolute value of a complex number \(z\), denoted \(|z|\), is defined by<br><br><ul><li>\(|z|\) = \(\sqrt{ (\operatorname{Re} z)^{2}+(\operatorname{Im} z)^{2} }&nbsp;.\)  </li></ul>
+
+============================================================
----------------------------

=== Note ID: 1695236931274 (Block 24) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition&nbsp;</b>factor<br><br>A polynomial \(s \in \mathcal{P}(\mathbf{F})\) is called a factor of \(p \in \mathcal{P}(\mathbf{F})\) if:<br><ul><li>There exists a polynomial \(q \in \mathcal{P}(\mathbf{F})\) such that \(p=s q\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1695237528795 (Block 25) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Polynomials with real coefficients have zeros in pairs<br><ul><li>Suppose \(p \in \mathcal{P}(\mathbf{C})\) is a polynomial with real coefficients. If \(\lambda \in \mathbf{C}\) is a zero of \(p\), then so is \(\bar{\lambda}\).<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1695237971317 (Block 26) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       To show why we can factorise real-coefficient polynomials as quadratic + linear polynomials:<br><ul><li>Because:&nbsp;\( (x-\lambda) (x-\bar{\lambda})\) =<br>\( \left(x^{2}-2(\operatorname{Re} \lambda) x+|\lambda|^{2}\right)<br>\)<br></li><li>Gives us a quadratic term of the required form</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1695238227321 (Block 27) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Factorization of a polynomial over \(\mathbf{R}\)</b><br><br>Suppose \(p \in \mathcal{P}(\mathbf{R})\) is a nonconstant polynomial. Then \(p\) has a unique factorization (except for the order of the factors) of the form<br><ul><li>\(p(x)\) = \(c\) \((x-\lambda_{1})\) \(\cdots\) \((x-\lambda_{m})\) \((x^{2}+b_{1} x+c_{1})\) \(\cdots\) \((x^{2}+b_{M} x+c_{M}),\)</li><li>where \(c, \lambda_{1}, \ldots, \lambda_{m}, b_{1}, \ldots, b_{M}, c_{1}, \ldots, c_{M} \in \mathbf{R}\), with \(b_{j}{ }^{2}&lt;4 c_{j}\) for each \(j\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1695307250073 (Block 28) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition eigenvalue<br><br>Suppose \(T \in \mathcal{L}(V)\). A number \(\lambda \in \mathbf{F}\) is called an eigenvalue of \(T\) if there exists \(v \in V\) such that \(v \neq 0\) and \(T v=\lambda v\).
+
+============================================================
----------------------------

=== Note ID: 1695307399510 (Block 29) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Equivalent conditions to be an eigenvalue</b><br><br>Suppose \(V\) is finite-dimensional, \(T \in \mathcal{L}(V)\), and \(\lambda \in F\). Then the following are equivalent:<br><br><ul><li>(a) \(\lambda\) is an eigenvalue of \(T\);</li><li>(b) \(T-\lambda I\) is not injective;</li><li>(c) \(T-\lambda I\) is not surjective;</li><li>(d) \(T-\lambda I\) is not invertible.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1695308032274 (Block 30) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition</b> eigenvector<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(\lambda \in \mathbf{F}\) is an eigenvalue of \(T\). A vector \(v \in V\) is called an eigenvector of \(T\) corresponding to \(\lambda\) if \(v \neq 0\) and \(T v=\lambda v\).
+
+============================================================
----------------------------

=== Note ID: 1695308642410 (Block 31) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Proof that a vector space V has at most dim V distinct eigenvalues<br><ul><li>Because eigenvectors form a linearly independent list</li><li>The maximum length of an eigenvectors list is dim V</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1695308945769 (Block 32) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition \(\left.T\right|_{U}\) and \(T / U\)<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(U\) is a subspace of \(V\) invariant under \(T\).<br><br><ul><li>The restriction operator \(\left.T\right|_{U}\)&nbsp; \(\in\) \( \mathcal{L}(U)\) is defined by</li><ul><li>\(\left.T\right|_{U}\) (u)= \(T u\)</li><li>for \(u \in U\).</li></ul></ul><ul><li>The quotient operator \(T / U\)&nbsp; \(\in\) \( \mathcal{L}(V / U)\) is defined by</li><ul><li>\((T / U)\) (v+U)= \(T v+U\)</li><li>for \(v \in V\).</li></ul></ul><br>
+
+============================================================
----------------------------

=== Note ID: 1695309770149 (Block 33) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       &nbsp;1 Suppose \(T \in \mathcal{L}(V)\) and \(U\) is a subspace of \(V\).<br><br><ul><li>(a) Prove that if \(U\) \(\subset\) null \(T\), then \(U\) is invariant under \(T\).</li><li><br></li><li>(b) Prove that if range \(T\)&nbsp; \(\subset\)&nbsp; \(U\), then \(U\) is invariant under \(T\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1695309958682 (Block 34) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       4 Suppose that \(T \in \mathcal{L}(V)\) and \(U_{1}, \ldots, U_{m}\) are subspaces of \(V\) invariant under \(T\). Prove that \(U_{1}+\cdots+U_{m}\) is invariant under \(T\).
+
+============================================================
----------------------------

=== Note ID: 1695310009569 (Block 35) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       5 Suppose \(T \in \mathcal{L}(V)\). Prove that the intersection of every collection of subspaces of \(V\) invariant under \(T\) is invariant under \(T\).
+
+============================================================
----------------------------

=== Note ID: 1695310357147 (Block 36) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       21 Suppose \(T \in \mathcal{L}(V)\) is invertible.<br><br><ul><li>(a) Suppose \(\lambda \in \mathbf{F}\) with \(\lambda \neq 0\).&nbsp;</li><ul><li>Prove that \(\lambda\) is an eigenvalue of \(T\) if and only if \(\frac{1}{\lambda}\) is an eigenvalue of \(T^{-1}\).</li></ul><li>(b) Prove that \(T\) and \(T^{-1}\) have the same eigenvectors.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1695310449694 (Block 37) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       24 <br><br>Suppose \(A\) is an \(n\)-by- \(n\) matrix with entries in \(\mathbf{F}\). Define \(T \in \mathcal{L}\left(\mathbf{F}^{n}\right)\) by \(T x=A x\), where elements of \(\mathbf{F}^{n}\) are thought of as \(n\)-by-1 column vectors.<br><br><ol><li>(a) Suppose the sum of the entries in each row of \(A\) equals 1 . Prove that 1 is an eigenvalue of \(T\).</li><li>(b) Suppose the sum of the entries in each column of \(A\) equals 1 . Prove that 1 is an eigenvalue of \(T\).</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1695396306003 (Block 38) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition</b> \(T^{m}\)<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(m\) is a positive integer.<ul><li>- \(T^{m}\) is defined by</li><ul><li>\(T^{m}\)  = \(\underbrace{T \cdots T}_{m \text { times } } \text {. }\)</li></ul><li>- \(T^{0}\) is defined to be the identity operator \(I\) on \(V\).</li><li>- If \(T\) is invertible with inverse \(T^{-1}\), then \(T^{-m}\) is defined by</li><ul><li>\(T^{-m}\) = \(\left(T^{-1}\right)^{m} \text {. }\)</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1695396526873 (Block 39) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition</b> \(p(T)\)<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(p \in \mathcal{P}(\mathbf{F})\) is a polynomial given by<br><br><ul><li>\(p(z)\) = \(a_{0}+a_{1} z+a_{2} z^{2}+\cdots+a_{m} z^{m}\)</li><li>for \(z \in \mathbf{F}\).&nbsp;</li><li>Then \(p(T)\) is the operator defined by<br></li><ul><li>\(p(T)\) = \(a_{0} I+a_{1} T+a_{2} T^{2}+\cdots+a_{m} T^{m} .\)</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1695396685144 (Block 40) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Multiplicative properties of polynomials of operartors<br><br>Suppose \(p, q \in \mathcal{P}(\mathbf{F})\) and \(T \in \mathcal{L}(V)\). Then<br><ul><li>(a) \((p q)(T)\) = \(p(T) q(T)\);</li><li>(b) \(p(T) q(T)\) = \(q(T) p(T)\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1695396788479 (Block 41) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Operators on complex vector spaces have an eigenvalue<br><br>Every operator on a finite-dimensional, nonzero, complex vector space has an eigenvalue.
+
+============================================================
----------------------------

=== Note ID: 1695397195775 (Block 42) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition matrix of an operator, \(\mathcal{M}(T)\)<br><br>Suppose \(T\) \(\in\) \(\mathcal{L}(V)\) and \(v_{1}, \ldots, v_{n}\) is a basis of \(V\). The matrix of \(T\) with respect to this basis is the \(n\)-by- \(n\) matrix<br><br>\[<br>\mathcal{M}(T)=\left(\begin{array}{ccc}<br>A_{1,1} &amp; \ldots &amp; A_{1, n} \\<br>\vdots &amp; &amp; \vdots \\<br>A_{n, 1} &amp; \ldots &amp; A_{n, n}<br>\end{array}\right)<br>\]<br><br>whose entries \(A_{j, k}\) are defined by<br><br><ul><li>\(T v_{k}\) = \(A_{1, k} v_{1}+\cdots+A_{n, k} v_{n} .\)</li></ul><br>
+
+============================================================
----------------------------

=== Note ID: 1695648094768 (Block 43) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition diagonal of a matrix<br><br>The diagonal of a square matrix consists of the entries along the line from the upper left corner to the bottom right corner.
+
+============================================================
----------------------------

=== Note ID: 1695648515294 (Block 44) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Over \(\mathbf{C}\), every operator has an upper-triangular matrix<br><br>Suppose \(V\) is a finite-dimensional complex vector space and \(T \in \mathcal{L}(V)\). Then \(T\) has an upper-triangular matrix with respect to some basis of \(V\).
+
+============================================================
----------------------------

=== Note ID: 1695649167203 (Block 45) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Determination of invertibility from upper-triangular matrix<br><br>Suppose \(T \in \mathcal{L}(V)\) has an upper-triangular matrix with respect to some basis of \(V\). Then \(T\) is invertible if and only if all the entries on the diagonal of that upper-triangular matrix are nonzero.
+
+============================================================
----------------------------

=== Note ID: 1695649225306 (Block 46) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Determination of eigenvalues from upper-triangular matrix<br><br>Suppose \(T \in \mathcal{L}(V)\) has an upper-triangular matrix with respect to some basis of \(V\). Then the eigenvalues of \(T\) are precisely the entries on the diagonal of that upper-triangular matrix.
+
+============================================================
----------------------------

=== Note ID: 1695649821494 (Block 47) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       7 Suppose \(T \in \mathcal{L}(V)\). Prove that 9 is an eigenvalue of \(T^{2}\) if and only if 3 or -3 is an eigenvalue of \(T\).
+
+============================================================
----------------------------

=== Note ID: 1695650125485 (Block 48) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition</b> diagonal <b>matrix</b><br><br>A diagonal matrix is a square matrix that is 0 everywhere except possibly along the diagonal.
+
+============================================================
----------------------------

=== Note ID: 1695650171001 (Block 49) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If an operator has a diagonal or upper-triangular matrix with respect to some basis, then the entries along the diagonal are precisely the eigenvalues of the operator
+
+============================================================
----------------------------

=== Note ID: 1695650277938 (Block 50) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition eigenspace, \(E(\lambda, T)\)<br><br><ul><li>Suppose \(T \in \mathcal{L}(V)\) and \(\lambda \in \mathbf{F}\). The eigenspace of \(T\) corresponding to \(\lambda\), denoted \(E(\lambda, T)\), is defined by&nbsp;</li><ul><li>\(E(\lambda, T)\) = \(\operatorname{null}(T-\lambda I) .\)</li></ul><li>In other words, \(E(\lambda, T)\) is the set of all eigenvectors of \(T\) corresponding to \(\lambda\), along with the 0 vector.<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1695651052561 (Block 51) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Sum of eigenspaces is a direct sum<br><br>Suppose \(V\) is finite-dimensional and \(T \in \mathcal{L}(V)\). Suppose also that \(\lambda_{1}, \ldots, \lambda_{m}\) are distinct eigenvalues of \(T\). Then<br><br><ul><li>\(E\left(\lambda_{1}, T\right)+\cdots+E\left(\lambda_{m}, T\right)\)</li><ul><li>is a direct sum,</li></ul><li>Furthermore</li><li>\(\operatorname{dim} E\left(\lambda_{1}, T\right)+\cdots+\operatorname{dim} E\left(\lambda_{m}, T\right)\)&nbsp; \(\leq\) \( \operatorname{dim} V\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1695651156316 (Block 52) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition</b> diagonalizable<br><br>An operator \(T \in \mathcal{L}(V)\) is called diagonalizable if it has a diagonal matrix with respect to some basis of \(V\).
+
+============================================================
----------------------------

=== Note ID: 1695652010098 (Block 53) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-56c6ac480fcee3254347c9d29a6a8837480fd05f.jpg"><br>Proof Suppose \(T \in \mathcal{L}(V)\) has \(\operatorname{dim} V\) distinct eigenvalues \(\lambda_{1}, \ldots, \lambda_{\operatorname{dim} V}\). <br><ul><li>For each \(j\), let \(v_{j} \in V\) be an eigenvector corresponding to the eigenvalue \(\lambda_{j}\).&nbsp;</li><li>Because eigenvectors corresponding to distinct eigenvalues are linearly independent \(v_{1}, \ldots, v_{\operatorname{dim} V}\) is linearly independent</li><li>. A linearly independent list of \(\operatorname{dim} V\) vectors in \(V\) is a basis of \(V\)&nbsp; thus \(v_{1}, \ldots, v_{\operatorname{dim}} V\) is a basis of \(V\). With respect to this basis consisting of eigenvectors, \(T\) has a diagonal matrix.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1695654120373 (Block 54) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       16 The Fibonacci sequence \(F_{1}, F_{2}, \ldots\) is defined by<br><br>\[<br>F_{1}=1, F_{2}=1, \quad \text { and } F_{n}=F_{n-2}+F_{n-1} \text { for } n \geq 3 .<br>\]<br><br>Define \(T \in \mathcal{L}\left(\mathbf{R}^{2}\right)\) by \(T(x, y)=(y, x+y)\).<ul><li>(a) Show that \(T^{n}(0,1)\) = \(\left(F_{n}, F_{n+1}\right)\) for each positive integer \(n\).</li><li>(b) Find the eigenvalues of \(T\).</li><li>(c) Find a basis of \(\mathbf{R}^{2}\) consisting of eigenvectors of \(T\).</li><li>(d) Use the solution to part (c) to compute \(T^{n}(0,1)\). Conclude that</li><ul><li>\(F_{n}\) = \(\frac{1}{\sqrt{5} }\) \(\left[\left(\frac{1+\sqrt{5} }{2}\right)^{n}-\left(\frac{1-\sqrt{5} }{2}\right)^{n}\right]\)</li><li>for each positive integer \(n\).</li></ul><li>(e) Use part (d) to conclude that for each positive integer \(n\), the Fibonacci number \(F_{n}\) is the integer that is closest to</li><ul><li>\(\frac{1}{\sqrt{5} }\) \(\left(\frac{1+\sqrt{5} }{2}\right)^{n} \text {. }\)</li></ul></ul><br>
+
+============================================================
----------------------------

=== Note ID: 1695739236158 (Block 55) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       A polynomial must be a finite sum meaning that its degree must be finite&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1695831249270 (Block 56) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       1. (15 points) Let \(E\) consist of all real numbers of the form \(a+b \sqrt{2}\), with \(a\) and \(b\) rational numbers.<br><br><ul><li>a) Is \(E\) closed under addition (of real numbers)?&nbsp;</li><ul><li>Yes</li></ul><li>b) Is \(E\) closed under multiplication (of real numbers)? Yes</li><li>c) Does \((5+4 \sqrt{2})^{-1}\) belong to \(E\) ?</li><ul><li>Yes, it's equal to \(\underline{\frac{1}{7}(4 \sqrt{2}-5)}\).</li></ul><li>d) Is \(E\) a field?</li><ul><li>Yes, we can check the multiplication and the addition is easier.</li><ul><li>\((a+b \sqrt{2})(c+d \sqrt{2}) \) = \((a c+2 b d)+(a d+b c) \sqrt{2},\)</li></ul><li>and</li><ul><li>\(1 /(a+b \sqrt{2})\) = \(\frac{a-b \sqrt{2} }{a^{2}-2 b^{2} } .\)</li></ul></ul><ul><li>All expressions are rational numbers as long as \(a, b, c, d\) are rational numbers.</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1695905516351 (Block 57) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Proposition 2.7. Suppose \(A\) is an \(m \times n\) matrix.<br><br><ul><li>1. The row rank and the column rank of \(A\) are equal, and equal to the dimension of the range of \(A\) :</li><ul><li>\(\mathrm{r}-\operatorname{rank}(A)\) = \(\operatorname{c}-\operatorname{rank}(A)=\operatorname{dim} \operatorname{Range}(A) .\)</li><li>Their common value is called the rank of \(A\), and written \(\operatorname{rank}(A)\).</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1695910925645 (Block 58) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The row-echelon matrix \(A\) is said to be in reduced row-echelon form if in addition to the row-echelon&nbsp;form:<br><br>1. each pivot entry is equal to 1 , and<br><br>2. all the other entries in the column of a pivot are equal to zero.
+
+============================================================
----------------------------

=== Note ID: 1695911155167 (Block 59) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose that the row-echelon matrix \(A\) has pivots in the first \(r\) rows, in columns<br><br>\[<br>1 \leq j(1)&lt;j(2)&lt;\cdots&lt;j(r) \leq n .<br>\]<br><br>We call \(x_{1}, x_{2}, \ldots, x_{n}\) the variables, . The \(r\) variables \(x_{j(i)}\) corresponding to the pivot columns are called pivot variables. The remaining \(n-r\) variables are called free variables.
+
+============================================================
----------------------------

=== Note ID: 1695912073329 (Block 60) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Proposition 2.11. Suppose that \(A\) is in reduced row-echelon form, with \(r\) pivots in the entries \(\{(i, j(i)) \mid 1 \leq i \leq r\}\).<br><br><ul><li>The equation \(A x=b\) (see (2.5)) has a solution if and only if \(b_{i}=0\) for all \(i&gt;r\). In that case, one solution is</li><li>\(x_{j(i)}\) = \(b_{i} \quad(1 \leq i \leq r)\)&nbsp;</li><li>\( \quad x_{j}\) = 0 \(\quad\left(x_{j} \text { free variable }\right) .\)</li></ul><br>5. Still assuming that \(b_{i}=0\) for all \(i&gt;r\), the most general solution of \(A x=b\) has arbitrary values \(x_{j}\) for the \(n-r\) free variables, and<br><br><ul><li>\(x_{j(i)}\) = \(b_{i}-\sum_{j \text { free } } a_{i j} x_{j}\) for \(\quad(1 \leq i \leq r) .\)</li></ul><br>That is, we choose the \(n-r\) free variables, and then define the \(r\) pivot variables by the equation above.<br><br>
+
+============================================================
----------------------------

=== Note ID: 1695994677630 (Block 61) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The first part of Gaussian elimination finds (in succession) \(r\) special entries<br><br><ul><li>\( (i(1), j(1)),(i(2), j(2)), \ldots,(i(r), j(r)), \)</li><ul><li>such that</li><li>&nbsp;\(1 \leq j(1)&lt;j(2)&lt;\cdots&lt;j(r) \leq n, \)</li><li>\( 1 \leq i(p) \leq m \text { all distinct }\)</li></ul></ul>These entries will become the pivots in the row-echelon form&nbsp;<br>
+
+============================================================
----------------------------

=== Note ID: 1696002130682 (Block 62) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1,3 +1,5 @@
       Theorem 4.8. Suppose \(A^{\prime}\) is an \(m \times n\) matrix with entries in a field \(F\). Then we can perform a finite sequence of elementary row operations on \(A^{\prime}\) to obtain a new \(m \times n\) matrix \(A^{\prime}\) in reduced row-echelon form. More precisely, we perform<br><br><ol><li>at most \(m\) row operations of type \(M\) (multiply a row by a nonzero scalar)  interspersed with at most
 
 &nbsp;&nbsp;&nbsp;\(m(m-1) / 2\) operations of type \(L\) (add a multiple of a row to a later row); then</li><li>at most \(m(m-1) / 2\) operations of type \(E\) (exchange two rows); then</li><li>at most \(m(m-1) / 2\) operations of type \(U\) (add a multiple of a row to an earlier row).</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1696002567366 (Block 63) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>We can write Gaussian Elimination as&nbsp;</li><ul><li>\(A^{\prime}=U E L A\) where \(A^{\prime}\) is A but&nbsp; in reduced row-echalon form</li><li>\(\quad A=L^{-1} E^{-1} U^{-1} A^{\prime} .\)</li></ul><li>Here:</li><ul><li>&nbsp;\(L\) and \(L^{-1}\) are \(m \times m\) invertible lower-triangular::shape?::shape?::shape?::shape? matrices;&nbsp;</li><li>\(E\) and \(E^{-1}\) are invertible \(m \times m\) permutation matrices;&nbsp;</li><li>and \(U\) and \(U^{-1}\) are invertible \(m \times m\) upper-triangular matrices with ones on the diagonal.&nbsp;</li></ul><li>The reduced row echelon matrix \(A\) is unique (independent of how the row reduction is performed).<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1696240176900 (Block 64) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1,3 +1,5 @@
       What are the three primary benefits
      of previous DAG dataflow systems that the authors enumerate and wish to
      ensure for CIEL?<ul><li>Task-level parallelism</li><li>Transparent fault-tolerance</li><li>Transparent scaling</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1696240435973 (Block 65) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 1: Original Curriculum Learning [6]. A curriculum is a sequence of training criteria over \(T\) training steps: \(\mathcal{C}=\left\langle Q_1, \ldots, Q_t, \ldots, Q_T\right\rangle\). Each criterion \(Q_t\) is a reweighting of the target training distribution \(P(z)\) :<br>\[<br>Q_t(z) \propto W_t(z) P(z) \quad \forall \text { example } z \in \text { training set } D,<br>\]<br>such that the following three conditions are satisfied:<br>- 1) The entropy of distributions gradually increases, i.e., \(H\left(Q_t\right)&lt;H\left(Q_{t+1}\right)\).<br>- 2) The weight for any example increases, i.e., \(W_t(z) \leq W_{t+1}(z) \quad \forall z \in D\).<br>- 3) \(Q_T(z)=P(z)\).
+
+============================================================
----------------------------

=== Note ID: 1696244727942 (Block 66) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 5.2. Suppose \(n\) and \(r\) are nonnegative integers. There is a oneto-one correspondence between \(r\)-dimensional subspaces \(U \subset F^{n}\), and \(r \times n\) matrices \(A\) in reduced row-echelon form, with one pivot in each row; that is, with no rows equal to zero. The correspondence sends the matrix \(A\) to the span \(\operatorname{Row}(A)\) of the rows of \(A\). To go in the other direction, suppose \(U\) is an \(r\)-dimensional subspace of \(F_{n}\). Choose a basis \(\left(u_{1}, \ldots, u_{r}\right)\) of \(U\), and let \(A^{\prime}\) be the \(r \times n\) matrix with rows \(u_{i}\). Perform Gaussian elimination on \(A^{\prime}\), getting an \(r \times n\) matrix \(A\) in reduced row echelon form; this is the matrix corresponding to the subspace \(U\).<br><br>Sketch of proof. A matrix \(A\) of the desired form clearly has \(r\) pivots, and so has rank \(r\) (Proposition 2.7). Therefore the row space \(\operatorname{Row}(A)\) is indeed an \(r\)-dimensional subspace of \(F^{n}\). <br><ul><li>Conversely, given an \(r\)-dimensional \(U\), the construction in the theorem produces an \(r \times n\)::size? matrix \(A^{\prime}\) with \(\operatorname{Row}\left(A^{\prime}\right)\) = \(U\). Now perform Gaussian elimination on \(A^{\prime}\) (Theorem 4.8), obtaining a reduced row echelon matrix \(A\) with \(\operatorname{Row}(A)=\operatorname{Row}\left(A^{\prime}\right)=U\), as desired.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1696319964400 (Block 67) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       2. (3 points) Suppose \(A\) is an \(m \times n\) matrix, and that \(m&lt;n\). For each statement below, tell whether it is true or false; give a counterexample if it is false; and explain why if it is true.<br><br>b) There is a constant \(C\) in \(\mathbb{R}^{m}\) so that the equation \(A X=C\) has no solutions.<br><br><ul><li>This depepends on weather the linear map is surjective</li><li>It the linear map is surjective there is always a solution</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1696321573025 (Block 68) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       4. (3 points) (Based on Axler, 2nd edition page 60, exercise 16 or 3rd edition page 69 , exercise 22). Suppose \(U\) is a finite-dimensional vector spaces, that \(S \in \mathcal{L}(V, W)\), and that \(T \in \mathcal{L}(U, V)\). Prove that<br><br>\[<br>\operatorname{dim} \operatorname{Null}(S T)=\operatorname{dim} \operatorname{Null}(T)+\operatorname{dim}(\operatorname{Range}(T) \cap \operatorname{Null}(S)) .<br>\]<br><br>Original proof:<br><ul><li>E = ST, E in L(U,W)</li><li>dim null E = dim U - dim range E</li><li>dim U = dim range T + dim null T</li><li>Thus dim null E = dim range T + dim&nbsp; null T - dim range E</li><li>As such, the question comes down to dim range E</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1696322644237 (Block 69) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       6. (3 points) Suppose \(T \in \mathcal{L}(V, W)\), and that \(V\) is finite-dimensional.<br><br>a) Prove that \(\operatorname{Null}(T)=\{0\}\) if and only if for every linearly independent list \(\left(v_{1}, v_{2}, \ldots, v_{p}\right)\) in \(V,\left(T v_{1}, \ldots, T v_{p}\right)\) is linearly independent in \(W\).<br><br><ul><li>Two steps:</li><ul><li>If null T = {0} then mapping a lin indp list in V via T can only result in a lin indp list in W since no&nbsp;\(Tv_i\) can be 0&nbsp;</li><li>\(T(\vec{c} \cdot \vec{v})\) = 0&nbsp;</li><li>\(\vec{c} \cdot T(\vec{v})\) = 0 implies&nbsp;\(\vec{c} = \vec{0}\)<br></li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1696323204708 (Block 70) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       6. (3 points) Suppose \(T \in \mathcal{L}(V, W)\), and that \(V\) is finite-dimensional.<br>Background:<br><ul><li>a) Prove that \(\operatorname{Null}(T)=\{0\}\) if and only if for every linearly independent list \(\left(v_{1}, v_{2}, \ldots, v_{p}\right)\) in \(V,\left(T v_{1}, \ldots, T v_{p}\right)\) is linearly independent in \(W\).</li><li>b) Prove that Range \((T)=W\) if and only if for every spanning list \(\left(v_{1}, v_{2}, \ldots, v_{q}\right)\) in \(V,\left(T v_{1}, \ldots, T v_{q}\right)\) is a spanning list in \(W\).</li></ul><b>c) Prove \(T\) is invertible if and only if \(T\) takes each basis of \(V\) to a basis of \(W\).</b><br><br>Proof: suppose it is invertible then:<ul><li>if T takes each basis of V to a basis of W:</li><ul><li>Suppose&nbsp;\(\vec{v}_{1\to p}\) is lin indp, then we can extend it to a basis&nbsp;\(\vec{v}\)</li><li>Since any sublist of a basis must be linearly independent and&nbsp;\(T(\vec{v})\) is lin indp, then&nbsp;\(T(\vec{v}_{1\to p})\) must also be lin indp so null T = 0</li></ul><li>The same method can be applied for spanning lists to prove surjectivity</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1696328093395 (Block 71) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       2. (20 points) List all \(3 \times 2\) matrices over \(\mathbb{R}\) which are in reduced row-echelon form. Point out the rank of each one.<br><br>Solution:<br><br><ul><li>\(\left(\begin{array}{ll}1 &amp; 0 \\ 0 &amp; 1 \\ 0 &amp; 0\end{array}\right)\)</li><ul><li>with rank 2</li></ul><li>\(\left(\begin{array}{ll}1 &amp; a \\ 0 &amp; 0 \\ 0 &amp; 0\end{array}\right)\)<br></li><ul><li>with rank 1</li></ul><li>\(\left(\begin{array}{ll}0 &amp; 1 \\ 0 &amp; 0 \\ 0 &amp; 0\end{array}\right)\)<br></li><ul><li>with rank 1</li></ul><li>\(\left(\begin{array}{ll}0 &amp; 0 \\ 0 &amp; 0 \\ 0 &amp; 0\end{array}\right)\)</li><ul><li>with rank 0</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1696329111006 (Block 72) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       5.(20 points) Let \(T(p)=\left(\left(x^{2}+1\right) p\right)^{\prime \prime}\) be a map on \(\mathcal{P}(z)\) (the polynomials with complex coefficients of any degree) to itself. Prove \(T\) is surjective.<br><ul><li>T sends all polynomials to polynomials of degree equal to themselves<br></li><li>T sends only 0 to 0 so it is injective</li><li>Meaning it is surjective for&nbsp;\(P_m\)&nbsp;\(\forall m\), meaning it is surjective over&nbsp;\(P(z)\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1696345244130 (Block 73) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       &nbsp;If \(T\) has both a left and a right inverse, then the left and right inverses are unique and equal to each other.&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1696404819154 (Block 74) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Example:<br>If \(T \in \mathcal{L}\left(\mathbf{C}^2\right)\) is defined by \(T(w, z)=(-z, w)\), then \((1,-i)\) is an eigenvector corresponding to the eigenvalue \(i\) because<br><ul><li>\(T \) \((1,-i)\) = \(i\) \((1,-i)\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1696404886391 (Block 75) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Any nonzero scalar multiple of an eigenvector is an eigenvector
+
+============================================================
----------------------------

=== Note ID: 1696408424794 (Block 76) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       How do the usual proofs of the existance of an eigenvalue go?<br><ul><li>Build&nbsp;\(det (\lambda I -&nbsp; T)\) which is called the characteristic polynomial of T</li><li>This polynomial is only equal to 0 if&nbsp;\(\lambda\) is an eigenvalue</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1696491872632 (Block 77) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Composition of the same operator is obviously comutative, which is part of the reason why multiplying polynomials of the same operator is comutative
+
+============================================================
----------------------------

=== Note ID: 1696494685416 (Block 78) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The spans of column vectors in upper triangular matrices can be said to be nested, with a total of dim V such nested subspaces
+
+============================================================
----------------------------

=== Note ID: 1696495948966 (Block 79) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition: trace<br><br>Let&nbsp;\(T \in L(V)\) be an operator and&nbsp;\(B\) be a basis making the matrix \(M_B(T)\) upper triangular.<br><ul><li>The trace, written \(tr(T)\), is the sum of the diagonal entries of the upper-triangular matrix</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1696496137052 (Block 80) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition</b>: trace, determinant<br><br>Let&nbsp;\(T \in L(V)\) be an operator and&nbsp;\(B\) a basis w.r.t which the matrix&nbsp;\(M_B(T)\) is upper-triangular:<br><ul><li>The trace,written \(tr(T)\) is the sum of the diagonal entries of the matrix&nbsp;\(M_B(T)\)</li><li>The determinant, written&nbsp;\(det(T)\) is the product ot the diagonal entries of the matrix&nbsp;\(M_B(T)\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1696496843722 (Block 81) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose an operator T has an upper-triangular matrix w.r.t some basis&nbsp;\(B\).<br><br><ul><li>Then T is invertible iff all diagonal of&nbsp;\(M_B(T)\) entries are nonzero</li><li>Furthermore, since a list of field elements is nonzero iff their product is nonzero this implies that T is invertible iiff its determinant&nbsp;\(det(T)\)&nbsp; \( \neq\)&nbsp; \(0\)&nbsp;</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1696497883811 (Block 82) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Since the elements on the diagonal of an upper-triangular matrix are eigenvalues, the trace is the sum of eigenvalues while the determinant is the product of eigenvalues
+
+============================================================
----------------------------

=== Note ID: 1696499593298 (Block 83) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Proof that the elements of the diagonal of an upper-triangular matrix of T, w.r.t base B, are eigenvalues:<br><ul><li>Consider&nbsp;\(\lambda \in F\)</li><li>To test weather it is an eigenvalue we can test whether \(T&nbsp; - \lambda I\) is not injective</li><li>Put in matrix form,&nbsp;\ test \(M_B(T - \lambda I)\) is&nbsp; not invertible</li><li>Since \(M_B(T)\) is upper-triangular, \(M_B(T - \lambda I)\) has the same shape with&nbsp;\(\lambda\) subtracted from the diagonal<br></li><li>Since \(M_B(T - \lambda I)\) is not invertible iff one of the elements is 0, then the elements of the diagonal of&nbsp; \(M_B(T - \lambda I)\) must be eigenvalues since&nbsp;\(\lambda_k - \lambda = 0\) iff&nbsp;\(\lambda = lambda_k\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1696499724493 (Block 84) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       A diagonal matrix has nonzero elements only on the diagonal, however, it can still have zeros on the diagonal
+
+============================================================
----------------------------

=== Note ID: 1696501568253 (Block 85) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       We can use the idea of an evaluate map between dual vectors and normal vectors:<br><img src="paste-b21f2ee395dd92d1c84acbacfe432b9b730010fe.jpg"><br>to diagrammatically define the dual dual as follows:<br><img src="paste-77c82a1ec0e5648ed15767befa65c99caa70538f.jpg"><br>Which is why the dual dual is naturally isomorphic to V
+
+============================================================
----------------------------

=== Note ID: 1696511620822 (Block 86) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Direct sum projection definition:<br><br><ul><li>Suppose V =&nbsp;\(U \oplus W\)</li><li>This means that&nbsp;\(\forall v \in V, \exists u \in U, w \in W \) such that&nbsp;\(v = u+w\)</li><li>We can then define a projection:</li><ul><li>\(P_{u,w}(v)\)&nbsp; = \( u \) which is a function beacuse u is unique and can be thought of as a map from V onto U or onto V</li></ul><li>And thus the direct sum decomposition can be rewritten:</li><li>V =&nbsp;\(P_{u,w}(v) \) +&nbsp;\(P_{w,u}(v)\)</li></ul><br>
+
+============================================================
----------------------------

=== Note ID: 1696511707034 (Block 87) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       A projection onto a subspace is surjective::surj/inj? as a map on that subspace
+
+============================================================
----------------------------

=== Note ID: 1696514323722 (Block 88) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition</b> dot product<br><br>For \(x, y \in \mathbf{R}^{n}\), the dot product of \(x\) and \(y\), denoted \(x \cdot y\), is defined by<br><br><ul><li>\(x \cdot y\) = \(x_{1} y_{1}+\cdots+x_{n} y_{n},\)</li></ul><br>where \(x=\left(x_{1}, \ldots, x_{n}\right)\) and \(y=\left(y_{1}, \ldots, y_{n}\right)\).<br>
+
+============================================================
----------------------------

=== Note ID: 1696514525565 (Block 89) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The dot product on \(\mathbf{R}^{n}\) has the following properties:<br><br><ul><li>- \(x \cdot x \geq 0\) for all \(x \in \mathbf{R}^{n}\);</li><li>- \(x \cdot x=0\) if and only if \(x=0\);</li><li>- for \(y \in \mathbf{R}^{n}\) fixed, the map from \(\mathbf{R}^{n}\) to \(\mathbf{R}\) that sends \(x \in \mathbf{R}^{n}\) to \(x \cdot y\) is linear;</li><li>- \(x \cdot y\) = \(y \cdot x\) for all \(x, y \in \mathbf{R}^{n}\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1696518025193 (Block 90) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition</b> inner product space<br><br>An inner product space is a vector space \(V\) along with an inner product on \(V\).
+
+============================================================
----------------------------

=== Note ID: 1696522867339 (Block 91) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Basic properties of an inner product</b><br><br><ul><li>(a) For each fixed \(u \in V\)&nbsp;the function that takes \(v\) to \(\langle v, u\rangle\)  is a linear map from \(V\) to \(\mathbf{F}\).</li><li>(b) \(\langle 0, u\rangle\) = \(0\) for every \(u \in V\).</li><li>(c) \(\langle u, 0\rangle\) = \(0\) for every \(u \in V\).</li><li>(d) \(\langle u, v+w\rangle\) = \(\langle u, v\rangle+\langle u, w\rangle\) for all \(u, v, w \in V\).</li><li>(e) \(\langle u, \lambda v\rangle\) = \(\bar{\lambda}\langle u, v\rangle\) for all \(\lambda \in \mathbf{F}\) and \(u, v \in V\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1696523273833 (Block 92) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition norm, \(\|v\|\)<br><br>For \(v \in V\), the norm of \(v\), denoted \(\|v\|\), is defined by<br><br><ul><li>\(\|v\|\) = \(\sqrt{\langle v, v\rangle}\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1696575250539 (Block 93) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition</b> orthogonal<br><br>Two vectors \(u, v \in V\) are called orthogonal if \(\langle u, v\rangle\) = \(0\).
+
+============================================================
----------------------------

=== Note ID: 1696575541031 (Block 94) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Orthogonality and 0<br><br><ul><li>(a) 0 is orthogonal to every vector in \(V\).</li><li>(b) 0 is also the only vector in \(V\) that is orthogonal to itself.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1696576676043 (Block 95) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>The</b> Cauchy-Schwarz Inequality<br><br>Suppose \(u, v \in V\). Then<br><br><ul><li>\(|\langle u, v\rangle|\)\(\leq\)\(\|u\|\|v\| .\)</li></ul><br>This inequality is an equality if and only if one of \(u, v\) is a scalar multiple of the other.<br>
+
+============================================================
----------------------------

=== Note ID: 1696579055196 (Block 96) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Triangle Inequality<br><br>Suppose \(u, v \in V\). Then<br><br><ul><li>\(\|u+v\|\)&nbsp;\(\leq\) \(\|u\|+\|v\| .\)</li></ul><br>This inequality is an equality if and only if one of \(u, v\) is a nonnegative multiple of the other.<br>
+
+============================================================
----------------------------

=== Note ID: 1696579547355 (Block 97) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-d1816ecb0211e3b519d82b39b5c6bb028b1276cd.jpg"><br><br><ul><li>\(\|u+v\|^2+\|u-v\|^2\)=<br></li><ul><li>= \(\langle u+v, u+v\rangle\) +\(\langle u-v, u-v\rangle\)</li><li>=&nbsp;\(\|u\|^2\) + \(\|v\|^2\) + \(\langle u, v\rangle+\langle v, u\rangle\) + \(\|u\|^2\) + \(\|v\|^2\) - \(\langle u, v\rangle\) - \(\langle v, u\rangle\)</li><li>=&nbsp;\(2\left(\|u\|^2+\|v\|^2\right)\)</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1696580053681 (Block 98) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       8 Suppose \(u, v \in V\) and \(\|u\|\) =\(\|v\| \) = \(1\) and \(\langle u, v\rangle\) = \(1\). Prove that \(u\) = \(v\).
+
+============================================================
----------------------------

=== Note ID: 1696580229834 (Block 99) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       9 Suppose \(u, v \in V\) and \(\|u\|\)&nbsp;<br>\(\leq\) \(1\) and \(\|v\|\) \(\leq\) \(1\). Prove that<br><br><ul><li>\(\sqrt{1-\|u\|^{2} }\)\(\sqrt{1-\|v\|^{2} }\) \( \leq\) \( 1\)-\(|\langle u, v\rangle|\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1696581855455 (Block 100) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       21 A norm on a vector space \(U\) is a function \|\|:&nbsp; \(U\)&nbsp; \(\rightarrow\) \([0, \infty)\) such that:<br><ul><li>&nbsp;\(\|u\|\) = \(0\) if and only if \(u\) = \(0\)</li><li>&nbsp;\(\|\alpha u\|\) = \(|\alpha|\|u\|\)&nbsp;</li><li>&nbsp;\(\|u+v\|\)&nbsp; \(\leq\) \(\|u\|+\|v\|\)&nbsp;</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1696582089663 (Block 101) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       24 Suppose \(S \in \mathcal{L}(V)\) is an injective::inj/surj? operator on \(V\). Define \(\langle\cdot, \cdot\rangle_{1}\) by<br><br><ul><li>\(\langle u, v\rangle_{1}\)&nbsp;= \(\langle S u, S v\rangle\)</li></ul><br>for \(u, v \in V\). Show that \(\langle\cdot, \cdot\rangle_{1}\) is an inner product on \(V\).<br>
+
+============================================================
----------------------------

=== Note ID: 1696600499455 (Block 102) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       A basis is a sufficient choice to determine an isomorphism between V and its dual V' and sufficient to provide a definition for the norm
+
+============================================================
----------------------------

=== Note ID: 1696600778748 (Block 103) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Ideally we would like an inner product to be bilinear rather than sequilinear( linear only in the first coordinate), but this is only possible on real vector spaceses
+
+============================================================
----------------------------

=== Note ID: 1696600871921 (Block 104) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       While a basis can help define an inner product via composing vectors with their duals, an inner product can exist independent of a basis. This is why we can talk about the length of vectors in R^3 without specifying a basis
+
+============================================================
----------------------------

=== Note ID: 1696602772145 (Block 105) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Given an inner product, we can define a norm such that:<br><ul><li>\(\vert\vert v \vert\vert\) =&nbsp;\(\sqrt{\langle v,v\rangle}\)</li><li>Note that the axioms of the inner product make it clear that&nbsp;\(\vert\vert v \vert\vert\) =&nbsp; \(0 \) requires that&nbsp;\(v=0\)<br><br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1696603035112 (Block 106) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The norm of a scalar times a vector equals the norm of the scalar times the norm of the vector
+
+============================================================
----------------------------

=== Note ID: 1696605366984 (Block 107) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Two vectors being orthogonal implies that all vectors in their span are orthogonal
+
+============================================================
----------------------------

=== Note ID: 1696846574485 (Block 108) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition</b>&nbsp;orthonormal<br><br>- A list of vectors is called orthonormal if each vector in the list has norm 1 and is orthogonal to all the other vectors in the list.<br><br>- In other words, a list \(e_{1}, \ldots, e_{m}\) of vectors in \(V\) is orthonormal if<br><br><ul><li>\(\left\langle e_{j}, e_{k}\right\rangle\)&nbsp;= \( \begin{cases}1 &amp; \text { if } j =k, \\ 0 &amp; \text { if } j \neq k\end{cases}\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1696846717144 (Block 109) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The norm of an orthonormal linear combination<br><br>If \(e_{1}, \ldots, e_{m}\) is an orthonormal list of vectors in \(V\), then<br><br><ul><li>\(\left\|a_{1} e_{1}+\cdots+a_{m} e_{m}\right\|^{2}\) =\(\left|a_{1}\right|^{2}+\cdots+\left|a_{m}\right|^{2}\)</li></ul><br>for all \(a_{1}, \ldots, a_{m} \in \mathbf{F}\).<br>
+
+============================================================
----------------------------

=== Note ID: 1696846797658 (Block 110) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       An orthonormal list is linearly independent<br><br>Every orthonormal list of vectors is linearly independent.
+
+============================================================
----------------------------

=== Note ID: 1696847053177 (Block 111) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       An orthonormal list of the right length is an orthonormal basis<br><br>Every orthonormal list of vectors in \(V\) with length \(\operatorname{dim} V\) is an orthonormal basis of \(V\).
+
+============================================================
----------------------------

=== Note ID: 1696848990759 (Block 112) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       \subsection{Gram-Schmidt Procedure}<br><br>Suppose \(v_{1}, \ldots, v_{m}\) is a linearly independent list of vectors in \(V\). Let \(e_{1}=v_{1} /\left\|v_{1}\right\|\). For \(j=2, \ldots, m\), define \(e_{j}\) inductively by<br><br><ul><li>\(e_{j}\) =&nbsp;</li><ul><li>\((v_{j}-\left\langle v_{j}, e_{1}\right\rangle e_{1}\) \(-\)&nbsp;</li><li>\(\cdots\)&nbsp;</li><li>\(-\) \(\left\langle v_{j}, e_{j-1}\right\rangle e_{j-1}) \)&nbsp;&nbsp;</li><li>\(/\)&nbsp;&nbsp;</li><ul><li>\(\|v_{j}-\left\langle v_{j}, e_{1}\right\rangle e_{1} \) \(-\)&nbsp;</li><li>\(\cdots\)&nbsp;</li><li>\(-\) \(\left\langle v_{j}, e_{j-1}\right\rangle e_{j-1}\| .\)</li></ul></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1696856359647 (Block 113) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Compact version of gram-schmidt process for orthonormal basis creation:<br><ul><li>\(e_{1}\) = \(v_{1} /\left\|v_{1}\right\|\).<br></li><li>\(e_j\) =&nbsp;\(v_j\) \(-\)&nbsp;\((\sum_{i=1}^{j-1} \left\langle v_j, e_i\right\rangle e_i) \)&nbsp;\(/\)&nbsp;\(\) \(\|v_j\) \(-\)&nbsp; \((\sum_{i=1}^{j-1} \left\langle v_j, e_i\right\rangle e_i)&nbsp;\|\)<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1696856955192 (Block 114) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Existence of orthonormal basis<br><br>Every finite-dimensional inner product space has an orthonormal basis.
+
+============================================================
----------------------------

=== Note ID: 1696857076470 (Block 115) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Orthonormal list extends to orthonormal basis<br><br>Suppose \(V\) is finite-dimensional. Then every orthonormal list of vectors in \(V\) can be extended to an orthonormal basis of \(V\).
+
+============================================================
----------------------------

=== Note ID: 1696857253285 (Block 116) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Upper-triangular matrix with respect to orthonormal basis<br><br>Suppose \(T \in \mathcal{L}(V)\). If \(T\) has an upper-triangular matrix with respect to some basis of \(V\), then \(T\) has an upper-triangular matrix with respect to some orthonormal basis of \(V\).
+
+============================================================
----------------------------

=== Note ID: 1696857718252 (Block 117) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition linear functional<br><br>A linear functional on \(V\) is a linear map from \(V\) to \(\mathbf{F}\). In other words, a linear functional is an element of \(\mathcal{L}(V, \mathbf{F})\).
+
+============================================================
----------------------------

=== Note ID: 1696858443346 (Block 118) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-cfcd6e590022102504c02a0eedc7c40ebaec55ef.jpg"><br><br>Proof First we show there exists a vector \(u \in V\) such that \(\varphi(v)=\langle v, u\rangle\) for every \(v \in V\). Let \(e_{1}, \ldots, e_{n}\) be an orthonormal basis of \(V\). Then<br><br>\[<br>\begin{aligned}<br>\varphi(v) &amp; =\varphi\left(\left\langle v, e_{1}\right\rangle e_{1}+\cdots+\left\langle v, e_{n}\right\rangle e_{n}\right) \\<br>&amp; =\left\langle v, e_{1}\right\rangle \varphi\left(e_{1}\right)+\cdots+\left\langle v, e_{n}\right\rangle \varphi\left(e_{n}\right) \\<br>&amp; =\left\langle v, \overline{\varphi\left(e_{1}\right)} e_{1}+\cdots+\overline{\varphi\left(e_{n}\right)} e_{n}\right\rangle<br>\end{aligned}<br>\]<br><br>for every \(v \in V\), where the first equality comes from 6.30. Thus setting<br><br>\[<br>u=\overline{\varphi\left(e_{1}\right)} e_{1}+\cdots+\overline{\varphi\left(e_{n}\right)} e_{n},<br>\]<br><br>we have \(\varphi(v)=\langle v, u\rangle\) for every \(v \in V\), as desired.<br><br>Now we prove that only one vector \(u \in V\) has the desired behavior. Suppose \(u_{1}, u_{2} \in V\) are such that<br><br>\[<br>\varphi(v)=\left\langle v, u_{1}\right\rangle=\left\langle v, u_{2}\right\rangle<br>\]<br><br>for every \(v \in V\). Then<br><br><ul><li>\(0\)&nbsp;= \(\left\langle v, u_{1}\right\rangle-\left\langle v, u_{2}\right\rangle\) = \(\left\langle v, u_{1}-u_{2}\right\rangle\)</li></ul><br>for every \(v \in V\). Taking \(v\) = \(u_{1}-u_{2}\) shows that \(u_{1}-u_{2}\) = \(0\). In other words, \(u_{1}=u_{2}\), completing the proof of the uniqueness part of the result.<br>
+
+============================================================
----------------------------

=== Note ID: 1696858540291 (Block 119) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose \(V\) is finite-dimensional and \(\varphi\) a linear functional on \(V\). Then 6.43 gives a formula for the vector \(u\) that satisfies \(\varphi(v)=\langle v, u\rangle\) for all \(v \in V\). Specifically, we have<br><br><ul><li>\(u\) = \(\overline{\varphi\left(e_{1}\right)} e_{1}\) \(+\)\(\cdots\)\(+\) \(\overline{\varphi\left(e_{n}\right)} e_{n} .\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1696858602337 (Block 120) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       1 (a) Suppose \(\theta \in \mathbf{R}\). Show that \((\cos \theta, \sin \theta)\) ,\((-\sin \theta, \cos \theta)\) and \((\cos \theta, \sin \theta)\),\((\sin \theta,-\cos \theta)\) are orthonormal bases of \(\mathbf{R}^{2}\).
+
+============================================================
----------------------------

=== Note ID: 1696859577811 (Block 121) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       11 Suppose \(\langle\cdot, \cdot\rangle_{1}\) and \(\langle\cdot, \cdot\rangle_{2}\) are inner products on \(V\) such that:<br><ul><li>&nbsp;\(\langle v, w\rangle_{1}\) = \(0\) if and only if \(\langle v, w\rangle_{2}\) = \(0\).&nbsp;</li><li>Prove that:</li><ul><li>&nbsp;There is a positive number \(c\) such that&nbsp;</li><li>\(\langle v, w\rangle_{1}\) =\(c\langle v, w\rangle_{2}\) for every \(v, w \in V\).</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1696951947700 (Block 122) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition</b>: The orthogonal complement<br><br>The orthogonal complement of a subspace U of an inner product space V, denoted <img src="paste-e44b8da29e4394a8ff7a653eb7b4ad1189fcd710.jpg">,&nbsp;&nbsp;is the set of vectors in V which are orthogonal to every vector in U
+
+============================================================
----------------------------

=== Note ID: 1696952791531 (Block 123) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If U is a subspace of an inner product space V then V can be decomposed into a direct sum of U and its orthogonal complement
+
+============================================================
----------------------------

=== Note ID: 1697008645430 (Block 124) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition</b> orthogonal complement, \(U^{\perp}\)<br><br>If \(U\) is a subset of \(V\), then the orthogonal complement of \(U\), denoted \(U^{\perp}\), is the set of all vectors in \(V\) that are orthogonal to every vector in \(U\) :<br><br><ul><li>\(U^{\perp}\) =\(\{v \in V:\langle v, u\rangle=0 \text { for every } u \in U\} \)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1697008807681 (Block 125) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Basic properties of orthogonal complement</b><br><br><ul><li>(a) If \(U\)&nbsp;is a subset of \(V\) then \(U^{\perp}\) is a subspace of \(V\).</li><li>(b) \(\{0\}^{\perp}\) =\(V\).</li><li>(c) \(V^{\perp}\) = \(\{0\}\).</li><li>(d) If \(U\) is a subset of \(V\), then \(U \cap U^{\perp} \subset\{0\}\).</li><li>(e) If \(U\) and \(W\) are subsets of \(V\) and \(U \subset W\), then \(W^{\perp} \subset U^{\perp}\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1697009058697 (Block 126) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Basic properties of orthogonal complement</b><br><br><br><br><ul><li>(d) If \(U\) is a subset of \(V\), then \(U\) \(\cap\) \( U^{\perp}\)&nbsp; \(\subset\) \(\{0\}\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1697009530228 (Block 127) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       \subsection{Basic properties of orthogonal complement}<br><br><br>(b) \(\{0\}^{\perp}=V\).<br><br><br>Proof:<br><ul><li>(b) Suppose \(v \in V\). Then \(\langle v, 0\rangle\) = \(0\), which implies that \(v\) \(\in\) \(\{0\}^{\perp}\). Thus \(\{0\}^{\perp}\) = \(V\).<br></li></ul><br>
+
+============================================================
----------------------------

=== Note ID: 1697009581415 (Block 128) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       \subsection{Basic properties of orthogonal complement}<br><br><br>(c) \(V^{\perp}=\{0\}\).<br><br>Proof:<br><ul><li>(c) Suppose \(v \in V^{\perp}\). Then \(\langle v, v\rangle\) = \(0\), which implies that \(v\) = \(0\). Thus \(V^{\perp}\) = \(\{0\}\).<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1697009650525 (Block 129) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       \subsection{Basic properties of orthogonal complement}<br><br>(d) If \(U\) is a subset of \(V\), then \(U \cap U^{\perp} \subset\{0\}\).<br><br>Proof:<br><ul><li>(d) Suppose \(U\) is a subset of \(V\) and \(v\)&nbsp; \(\in\) \(U \cap U^{\perp}\). Then \(\langle v, v\rangle\) = \(0\), which implies that \(v\) = \(0\). Thus \(U \cap U^{\perp} \subset\{0\}\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1697009756689 (Block 130) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       \subsection{Basic properties of orthogonal complement}<br><br>(e) If \(U\) and \(W\) are subsets of \(V\) and \(U \subset W\), then \(W^{\perp} \subset U^{\perp}\).<br><br>Proof:<br><ul><li>(e) Suppose \(U\) and \(W\) are subsets of \(V\) and \(U \subset W\). Suppose \(v\) \(\in\) \(W^{\perp}\). Then \(\langle v, u\rangle\) = \(0\) for every \(u \in W\), which implies that \(\langle v, u\rangle\) = \(0\) for every \(u \in U\). Hence \(v \in U^{\perp}\). Thus \(W^{\perp} \subset U^{\perp}\).<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1697010860121 (Block 131) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-63bed7b30dc6453d921aa714f2982932d33a701a.jpg"><br>Proof:<br><ul><li>Proof First we will show that</li><ul><li>6.48:&nbsp;</li><li>\(V\) = \(U+U^{\perp} .\)</li></ul><li>To do this, suppose \(v \in V\). Let \(e_{1}, \ldots, e_{m}\) be an orthonormal basis of \(U\). Obviously</li><ul><li>6.49:&nbsp;</li><li>\(&nbsp;v\) = \(\underbrace{\left\langle v, e_{1}\right\rangle e_{1}+\cdots+\left\langle v, e_{m}\right\rangle e_{m} }_{u}\)  + \(\underbrace{v-\left\langle v, e_{1}\right\rangle e_{1}-\cdots-\left\langle v, e_{m}\right\rangle e_{m} }_{w} .\)</li><li>where u is in U&nbsp;</li></ul><li>Because \(e_{1}, \ldots, e_{m}\) is an orthonormal basis, for each j up to m we have:</li><ul><li>\(\left\langle w, e_j\right\rangle\)&nbsp;</li><li>= \(\left\langle v, e_j\right\rangle-\left\langle v, e_j\right\rangle\)</li><li>= \(0\)</li></ul><li>Thus \(w\) is orthogonal to every vector in \(\operatorname{span}\left(e_{1}, \ldots, e_{m}\right)\). In other words,&nbsp;\(w \in U^{\perp}\).&nbsp;<br></li><li>This completes the proof that&nbsp;</li><ul><li>\(V\) = \(U+U^{\perp} .\)</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1697011100153 (Block 132) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The orthogonal complement of the orthogonal complement<br><br>Suppose \(U\) is a finite-dimensional subspace of \(V\). Then<br><br><ul><li>\(U\)=\(\left(U^{\perp}\right)^{\perp}\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1697011834281 (Block 133) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Properties of the orthogonal projection \(P_{U}\)</b><br><br>Suppose \(U\) is a finite-dimensional subspace of \(V\) and \(v \in V\). Then<ul><li>(b) \(P_{U}\) \(u\) = \(u\) for every \(u \in U\);</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1697012174244 (Block 134) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       \subsection{Properties of the orthogonal projection \(P_{U}\)}<br><br>Suppose \(U\) is a finite-dimensional subspace of \(V\) and \(v \in V\). Then<br><ul><li>(g) \(P_{U}\)\(^{2}\) = \(P_{U}\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1697012280546 (Block 135) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       \subsection{Properties of the orthogonal projection \(P_{U}\)}<br><br>Suppose \(U\) is a finite-dimensional subspace of \(V\) and \(v \in V\). Then<br><br><ul><li>(i) for every orthonormal basis \(e_{1}, \ldots, e_{m}\) of \(U\),<br></li><li>\(P_{U} v\)= \(\left\langle v, e_{1}\right\rangle e_{1}\)\(+\)\(\cdots\)\(+\)\(\left\langle v, e_{m}\right\rangle e_{m} .\)<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1697013273244 (Block 136) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose \(U\) is a finite-dimensional subspace of \(V, v \in V\), and \(u \in U\). Then<br><ul><li>\(\left\|v-P_{U} v\right\|\)\(\leq\)\(\|v-u\|\)</li><li>Furthermore, the inequality above is an equality if and only if \(u\) = \(P_{U} v\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1697013519392 (Block 137) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-81d56a8739ee61a20b8ed7889ba69c4b1a1ab3be.jpg"><br>Proof:<br><ul><li>\(\left\|v-P_U v\right\|^2\)&nbsp;</li><li>\(\leq\) \(\left\|v-P_U v\right\|^2\) +\(\left\|P_U v-u\right\|^2\)<br></li><li>=&nbsp;\(\|\) \(\left(v-P_U v\right)\) +\(\left(P_U v-u\right)\)\(\|\)\(^2\)</li><li>=&nbsp;\(\|v-u\|\) \(^2\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1697014050845 (Block 138) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       6 Suppose \(U\) and \(W\) are finite-dimensional subspaces of \(V\). <br><ul><li>Prove that \(P_{U}\)\(P_{W}\) = \(0\) if and only if \(\langle u, w\rangle\)=\(0\) for all \(u \in U\) and all \(w \in W\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1697014134548 (Block 139) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       7 <br><br><ul><li>Suppose \(V\) is finite-dimensional&nbsp;</li><li>And \(P \in \mathcal{L}(V)\) is such that \(P^{2}\) = \(P\)&nbsp;</li><li>And every vector in null \(P\) is orthogonal to every vector in range \(P\).&nbsp;</li><li>Prove that there exists a subspace \(U\) of \(V\) such that \(P\)=\(P_{U}\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1697014221624 (Block 140) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       8 Suppose \(V\) is finite-dimensional and \(P \in \mathcal{L}(V)\) is such that:<br><ul><li>&nbsp;\(P^{2}\)=\(P\) and</li><li>\(\|P v\|\) \(\leq\)\(\|v\|\) for every \(v \in V\).&nbsp;</li><li>Prove that there exists a subspace \(U\) of \(V\) such that \(P\)=\(P_{U}\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1697016044715 (Block 141) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If \(U\) is a subset of \(V\), then \(U \cap U^{\perp} \subset\{0\}\).<br><br>Why does this only require containment and not equality?<br><ul><li>Because we only require U to be a subset and not a subspace, there is no guarantee the origin is in U</li><li>If we were require it to be a subset then we could use equiality</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1697016987800 (Block 142) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-f0ee3eb59c1cdc0d47dfd909a0162907a48105fa.jpg"><br>Why is the property useful?<br><ul><li>Because&nbsp;\(P_U v\) is an element of U</li><li>This means that the element of U which minimizes the norm/distance must be&nbsp;\(P_U v\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1697118024126 (Block 143) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Proof that vector spaces can be decomposed as a direct sum of a subspace U and its orthogonal complement \(U^\perp\):<br><ul><li>Choose an orthonormal basis for U</li><li>Then, it can be extended to a basis of V</li><li>As such, any vector in V can be written as the sum of&nbsp;\(v_u\), which equals&nbsp;\(\vec{c} \cdot \vec{u}\), and&nbsp;\(v-v_u\)</li><li>In this case,&nbsp;\(v-v_u\) must be orthogonal to every vector in U because we have removed the components of v which correspond to&nbsp;\(\vec{u}\)</li><li>Since the intersection of U and its orthogonal complement is 0, this is a direct sum</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1697119638422 (Block 144) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       We can think of a projection&nbsp;\(P_u\) in two ways<br><ul><li>\(P_u\)&nbsp;\(\in\)&nbsp;\(L(V)\)<br></li><li>\(P_u\)&nbsp;\(\in\)&nbsp;\(L(V,U)\)<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1697120754541 (Block 145) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       How can we think of using orthogonal projections as a means of solving minimization problems conceptually?<br><img src="paste-9a3bc3fb59f7c39d492add8e0057da0f3f0b3e29.jpg"><br><ul><li>Logically: if U is a model of reality and v is a noisy observation, what is the closest point in reality to the noisy v. With U being lower-dimensional.</li><ul><li>This requires that the phenomena actually be linear</li></ul><li>Statistically: if U is a staistical model based on a&nbsp; lower-dimensional choice of parameters and v is a phenomena we want the point in the model that best explains v</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1697782460196 (Block 146) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition</b> adjoint, \(T^{*}\)<br><br>Suppose \(T \in \mathcal{L}(V, W)\). The adjoint of \(T\) is the function \(T^{*}\) : \(W\)&nbsp; \(\rightarrow\) \(V\) such that<br><br><ul><li>\(\langle T v, w\rangle\) = \(\left\langle v, T^{*} w\right\rangle\)</li></ul><br>for every \(v \in V\) and every \(w \in W\).<br>
+
+============================================================
----------------------------

=== Note ID: 1697784369263 (Block 147) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Properties of the adjoint<br><br><ul><li>(a) \(\quad(S+T)^{*}\) = \(S^{*}+T^{*}\) for all \(S, T \in \mathcal{L}(V, W)\);</li><li><br></li><li>(b) \((\lambda T)^{*}\) = \(\bar{\lambda} T^{*}\) for all \(\lambda \in \mathbf{F}\) and \(T \in \mathcal{L}(V, W)\);</li><li><br></li><li>(c) \(\left(T^{*}\right)^{*}\) = \(T\) for all \(T \in \mathcal{L}(V, W)\);</li><li><br></li><li>(d) \(I^{*}\) = \(I\), where \(I\) is the identity operator on \(V\);</li><li><br></li><li>(e) \((S T)^{*}\) = \(T^{*} S^{*}\) for all \(T \in \mathcal{L}(V, W)\) and \(S \in \mathcal{L}(W, U)\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1697784536542 (Block 148) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       \subsection{Properties of the adjoint}<br><br>(d) \(I^{*}\) = \(I\), where \(I\) is the identity operator on \(V\);
+
+============================================================
----------------------------

=== Note ID: 1697785805313 (Block 149) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-4679dd4a3c60e3380cda6e152f581258b49de687.jpg"><br><br>Proof We begin by proving (a). Let \(w \in W\). Then<br><br><ul><li>\(w \in \operatorname{null} T^{*} \)&nbsp;&nbsp;</li><li>\(\Longleftrightarrow\) \( T^{*} w\)::by def::by def::by def::by def = \(0 \)::by def::by def::by def::by def</li><li>\(\Longleftrightarrow\) \(\left\langle v, T^{*} w\right\rangle\)::expand def::expand def::expand def::expand def::expand def = \(0\) \(\text { for all } v \in V \)</li><li>\(\Longleftrightarrow\) \(\langle T v, w\rangle\)::further expand def::further expand def::further expand def::further expand def::further expand def = \(0\) \(\text { for all } v \in V \)</li><li>\(\Longleftrightarrow\) \(w\) \(\in\) \((\text { range } T)^{\perp} .\)</li></ul><br>Thus null \(T^{*}=(\operatorname{range} T)^{\perp}\), proving (a).<br><br>
+
+============================================================
----------------------------

=== Note ID: 1697785949680 (Block 150) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-3a17ead3b362bcf2c581832d899b7b869839ee01.jpg"><br>Proof We begin by proving (a). Let \(w \in W\). Then<br><br>\[<br>\begin{aligned}<br>w \in \operatorname{null} T^{*} &amp; \Longleftrightarrow T^{*} w=0 \\<br>&amp; \Longleftrightarrow\left\langle v, T^{*} w\right\rangle=0 \text { for all } v \in V \\<br>&amp; \Longleftrightarrow\langle T v, w\rangle=0 \text { for all } v \in V \\<br>&amp; \Longleftrightarrow w \in(\text { range } T)^{\perp} .<br>\end{aligned}<br>\]<br><br>Thus null \(T^{*}=(\operatorname{range} T)^{\perp}\), proving (a).<br><br>THen:<br><br><ul><li>If we take the orthogonal complement of both sides of (a), we get (d),&nbsp;</li><li>Replacing \(T\) with \(T^{*}\) in (a) gives (c)</li><li>Finally, replacing \(T\) with \(T^{*}\) in (d) gives (b).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1697786059177 (Block 151) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition</b> conjugate transpose<br><br>The conjugate transpose of an \(m\)-by- \(n\)::size::size::size::size::size matrix is the \(n\)-by- \(m\)::size::size::size::size::size matrix obtained by interchanging the rows and columns and then taking the complex conjugate of each entry.
+
+============================================================
----------------------------

=== Note ID: 1698179738196 (Block 152) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition</b> conjugate transpose<br><br>The conjugate transpose of an \(m\)-by- \(n\)::size?::size?::size?::size?::size? matrix is the \(n\)-by- \(m\)::size?::size?::size?::size?::size? matrix obtained by interchanging the rows and columns::step 1::step 1::step 1::step 1::step 1 and then taking the complex conjugate of each entry.::step 2::step 2::step 2
+
+============================================================
----------------------------

=== Note ID: 1698179922292 (Block 153) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The matrix of \(T^{*}\)<br><br>Let \(T \in \mathcal{L}(V, W)\). Suppose \(e_{1}, \ldots, e_{n}\) is an orthonormal basis of \(V\) and \(f_{1}, \ldots, f_{m}\) is an orthonormal basis of \(W\). Then<br><br><ul><li>\(\mathcal{M}\left(T^{*},\left(f_{1}, \ldots, f_{m}\right),\left(e_{1}, \ldots, e_{n}\right)\right)\)</li></ul><br>is the conjugate transpose of<br><br><ul><li>\(\mathcal{M}\left(T,\left(e_{1}, \ldots, e_{n}\right),\left(f_{1}, \ldots, f_{m}\right)\right) .\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1698180120123 (Block 154) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition</b> self-adjoint<br><br>An operator \(T \in \mathcal{L}(V)\) is called self-adjoint if \(T\) = \(T^{*}\). In other words, \(T \in \mathcal{L}(V)\) is self-adjoint if and only if<br><br><ul><li>\(\langle T v, w\rangle\) = \(\langle v, T w\rangle\)</li></ul><br>for all \(v, w \in V\).<br>
+
+============================================================
----------------------------

=== Note ID: 1698180204868 (Block 155) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       You should verify that the sum of two self-adjoint operators is self-adjoint and that the product of a real scalar and a self-adjoint operator is self-adjoint.
+
+============================================================
----------------------------

=== Note ID: 1698180341439 (Block 156) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Eigenvalues of self-adjoint operators are real<br><br>Every eigenvalue of a self-adjoint operator is real.
+
+============================================================
----------------------------

=== Note ID: 1698181134195 (Block 157) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If \(T\) = \(T^{*}\) and \(\langle T v, v\rangle\) = \(0\) for all \(v\), then \(T\) = \(0\)
+
+============================================================
----------------------------

=== Note ID: 1698181355163 (Block 158) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition normal<br><br>- An operator on an inner product space is called normal if it commutes with its adjoint.<br><br>- In other words, \(T \in \mathcal{L}(V)\) is normal if<br><br><ul><li>\(T T^{*}\) = \(T^{*} T \text {. }\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1698181535908 (Block 159) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Obviously every self-adjoint operator is normal, because if \(T\) is self-adjoint then \(T^{*}\) = \(T\).
+
+============================================================
----------------------------

=== Note ID: 1698182150227 (Block 160) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Orthogonal eigenvectors for normal operators<br><br>Suppose \(T \in \mathcal{L}(V)\) is normal. Then eigenvectors of \(T\) corresponding to distinct eigenvalues are orthogonal.
+
+============================================================
----------------------------

=== Note ID: 1698182354081 (Block 161) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       4 Suppose \(T \in \mathcal{L}(V, W)\). Prove that<br><br>(a) \(T\) is injective if and only if \(T^{*}\) is surjective;<br><br>(b) \(T\) is surjective if and only if \(T^{*}\) is injective.
+
+============================================================
----------------------------

=== Note ID: 1698182654404 (Block 162) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       5 Prove that<br><br><ul><li>dim range\(T^*\) = dim rangeT</li></ul><br>
+
+============================================================
----------------------------

=== Note ID: 1698182702101 (Block 163) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       7 Suppose \(S, T \in \mathcal{L}(V)\) are self-adjoint. Prove that \(S T\) is self-adjoint if and only if \(S T\) = \(T S\).
+
+============================================================
----------------------------

=== Note ID: 1698182878959 (Block 164) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose \(P \in \mathcal{L}(V)\) is such that \(P^{2}\) = \(P\). Prove that there is a subspace \(U\) of \(V\) such that \(P\) = \(P_{U}\) if and only if \(P\) is self-adjoint.
+
+============================================================
----------------------------

=== Note ID: 1698183596900 (Block 165) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       17 Suppose \(T \in \mathcal{L}(V)\) is normal. Prove that<br><br><ul><li>\(\text { null }\) \(T^{k}\) = \(\operatorname{null}\) \(T\)&nbsp;</li><li>\( \operatorname{range }\)\(T^{k}\)= \(\operatorname{range}\)\(T\)</li></ul><br>for every positive integer \(k\).<br>
+
+============================================================
----------------------------

=== Note ID: 1698269581351 (Block 166) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       5. Define \(T \in \mathcal{L}\left(\mathbf{F}^{2}\right)\) by<br><br>\[<br>T(w, z)=(z, w)<br>\]<br><br>Find all eigenvalues and eigenvectors of \(T\).<br><br><ul><li>\(w\)&nbsp; =&nbsp; \( \lambda z\)<br></li><li>\(z\) =&nbsp;\(\lambda w\)<br></li><li>Thus:</li><li>\(\lambda^2 w \) = \(w\)<br></li><li>\(w (\lambda -1) (\lambda + 1)\) = 0<br></li><li>This implies that&nbsp;\(\lambda\) = 1 or&nbsp;\(\lambda\) = -1</li><li>For&nbsp;\(\lambda \) = 1</li><ul><li>The eigenvector is (w,w)</li></ul><li>For&nbsp;\(\lambda\) = -1</li><ul><li>The eigenvector is (w,-w)</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1698270134037 (Block 167) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       7. Suppose \(n\) is a positive integer and \(T \in \mathcal{L}\left(\mathrm{F}^{n}\right)\) is defined by<br><br>\[<br>T\left(x_{1}, \ldots, x_{n}\right)=\left(x_{1}+\cdots+x_{n}, \ldots, x_{1}+\cdots+x_{n}\right) ;<br>\]<br><br>in other words, \(T\) is the operator whose matrix (with respect to the standard basis) consists of all 1's. Find all eigenvalues and eigenvectors of \(T\).<br><br>Solution:<br>Solution: Suppose \(\lambda\) is an eigenvalue of \(T\). For this particular operator, the eigenvalue-eigenvector equation \(T x=\lambda x\) becomes the system of equations<br><br><ul><li>\(x_{1}+\cdots+x_{n}\) = \(\lambda x_{1} \)</li><li>\(\vdots \)</li><li>\(x_{1}+\cdots+x_{n}\) = \(\lambda x_{n} .\)</li></ul><br>Thus<br><br><ul><li>\(\lambda x_{1}\) = \(\cdots\) = \(\lambda x_{n} .\)</li></ul><div>Then either:</div><div><br></div><div><ul><li>\(\lambda\) = 0<br></li><ul><li>In which case the corresponding set of eigenvectors is&nbsp;</li><li>\[<br>\left\{\left(x_{1}, \ldots, x_{n}\right) \in \mathbf{F}^{n}: x_{1}+\cdots+x_{n}=0\right\} .<br>\]<br></li></ul><li>\(\lambda\) = n<br></li><ul><li>In which cas the corresponding set of eigenvectors equals:</li><li>\[<br>\left\{\left(x_{1}, \ldots, x_{n}\right) \in \mathbf{F}^{n}: x_{1}=\cdots=x_{n}\right\} .<br>\]<br></li></ul></ul></div>
+
+============================================================
----------------------------

=== Note ID: 1698270631386 (Block 168) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       11. Suppose \(S, T \in \mathcal{L}(V)\). Prove that \(S T\) and \(T S\) have the same eigenvalues.<br><br>Solution: Suppose that \(\lambda \in \mathbf{F}\) is an eigenvalue of \(S T\). We want to prove that \(\lambda\) is an eigenvalue of \(T S\). Because \(\lambda\) is an eigenvalue of \(S T\), there exists a nonzero vector \(v \in V\) such that<br><br><ul><li>\((ST)v\) =&nbsp;\( \lambda v\)</li><li>If&nbsp;</li><li>\((TS)v\) =&nbsp;\( \alpha v\)</li><li>Then</li><li>\(T(ST)v\) =&nbsp;\( T \lambda v\)</li><li>\(T(ST)v\) =&nbsp;\(&nbsp; \lambda Tv\)</li><li>\(TST v\) =&nbsp;\(&nbsp; \lambda Tv\)</li><li>Which implies that&nbsp;\(Tv\) is an eigvenvector of&nbsp;\(TS\) and&nbsp;\(\lambda\) is also an eigenvalue of T</li><li>Mutatis mulandis</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1698270919652 (Block 169) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       15. Suppose \(\mathbf{F}=\mathbf{C}, T \in \mathcal{L}(V), p \in \mathcal{P}(\mathbf{C})\), and \(a \in \mathbf{C}\). Prove that \(a\) is an eigenvalue of \(p(T)\) if and only if \(a=p(\lambda)\) for some eigenvalue \(\lambda\) of \(T\).<br><br>Solution:<br><br><br>SOLUTION: First suppose that \(a\) is an eigenvalue of \(p(T)\). Thus \(p(T)-a I\) is not injective. Write the polynomial \(p(z)-a\) in factored form:<br><br>\[<br>p(z)-a=c\left(z-\lambda_{1}\right) \ldots\left(z-\lambda_{m}\right),<br>\]<br><br>where \(c, \lambda_{1}, \ldots, \lambda_{m} \in \mathrm{C}\). We can assume that \(c \neq 0\) (otherwise \(p\) is a constant polynomial, in which case the desired result clearly holds). The equation above implies that<br><br><ul><li>\(p(T)-a I\) = \(c\left(T-\lambda_{1} I\right) \ldots\left(T-\lambda_{m} I\right) .\)</li><li>Because \(p(T)-a I\) is not injective</li><ul><li>This implies that \(T-\lambda_{j} I\) is not injective for some \(j\).&nbsp;</li><li>In other words, some \(\lambda_{j}\) is an eigenvalue of \(T\).&nbsp;</li><li>The formula above for \(p(z)-a\) shows that \(p\left(\lambda_{j}\right)-a\) = \(0\).&nbsp;</li><li>Hence \(a=p\left(\lambda_{j}\right)\), as desired.</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1698774906462 (Block 170) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 0.3. Suppose \(\left(f_{1}, \ldots, f_{n}\right)\) is an orthogonal basis of \(V\). Then<br><br><ul><li>\(v\) = \(\frac{\left\langle v, f_{1}\right\rangle}{\left\langle f_{1}, f_{1}\right\rangle} f_{1}\) +\(\cdots\)+\(\frac{\left\langle v, f_{n}\right\rangle}{\left\langle f_{n}, f_{n}\right\rangle} f_{n}\)</li></ul><br>and<br><br><ul><li>\(\|v\|^{2}\) = \(\frac{\left|\left\langle v, f_{1}\right\rangle\right|^{2} }{\left\langle f_{1}, f_{1}\right\rangle}\)+\(\cdots\)+\(\frac{\left|\left\langle v, f_{n}\right\rangle\right|^{2} }{\left\langle f_{n}, f_{n}\right\rangle}\)</li></ul><br>for every \(v \in V\).<br>
+
+============================================================
----------------------------

=== Note ID: 1698791362067 (Block 171) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       5. (4 points) This problem is about the inner product space \(C[0,1]\) of real-valued continuous functions on the interval \([0,1]\), with inner product<br><br>\[<br>\langle p, q\rangle=\int_{0}^{1} p(x) q(x) d x<br>\]<br><br>It's like the example in the text about finding a good polynomial approximation to \(\sin (x)\).<br><br>b) Define \(f(x)=\sqrt{x}\), regarded as a function in \(C[0,1]\). Calculate the orthogonal projection \(p_{U}(f)\) of \(f\) on the subspace \(U\). (Your answer should be a cubic polynomial in \(x\) with rational numbers as coefficients.)<br><br><br>According to the formula from the notes,<br><br><ul><li>\(P_{U}(f)\)&nbsp;</li><li>= \(\frac{\left\langle f, f_{1}\right\rangle}{\left\langle f_{1}, f_{1}\right\rangle} f_{1}\) + \(\frac{\left\langle f, f_{2}\right\rangle}{\left\langle f_{2}, f_{2}\right\rangle} f_{2}\) + \(\frac{\left\langle f, f_{3}\right\rangle}{\left\langle f_{3}, f_{3}\right.} f_{3}\) + \(\frac{\left\langle f, f_{4}\right\rangle}{\left\langle f_{4}, f_{f}\right\rangle} f_{4}\)</li></ul><div><br></div><div>This projection is equivalent to finding the best approximation of&nbsp;\(\sqrt{x}\) using polynomials of dgree 3,2,1,0</div>
+
+============================================================
----------------------------

=== Note ID: 1698949200186 (Block 172) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       1. (25 points) Let<br><br>\[<br>A=\left(\begin{array}{lll}<br>1 &amp; 1 &amp; 1 \\<br>0 &amp; 2 &amp; 2 \\<br>0 &amp; 0 &amp; 2<br>\end{array}\right)<br>\]<br><br>a matrix of real numbers.<br><br>b) For each eigenvalue (1,2), find all the corresponding eigenvectors of \(A\).<br><br>Solution:<br><ul><li>For&nbsp;\(\lambda = 1\), we have&nbsp;\(Av \) =&nbsp;\( [x+y+z, 2(y+z), 2z]\) =&nbsp;\([x,y,z]\) which automatically implies z and y must be 0. Thus&nbsp;\(E(1, A) \)=&nbsp; \(\mathrm{span}(1,0,0)\)</li><li>For&nbsp;\(\lambda = 2\), we have&nbsp;\(Av \) =&nbsp;\( [x+y+z, 2(y+z), 2z]\) =&nbsp;\([2x,2y,2z]\) which automatically implies that&nbsp;\(2y+2z \) = 0 which implies that&nbsp;\(z\) = \(0\) and x,y can be any value.&nbsp; Thus&nbsp;\(E(2,A)\) =&nbsp;\(\mathrm{span}(1,1,0)\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1698956231228 (Block 173) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       2. (30 points) Define a sequence of real numbers by \(a_{0}=0, a_{1}=1\), and<br><br>\[<br>a_{n+1}=\left(a_{n}+a_{n-1}\right) / 2 \quad(n \geq 1)<br>\]<br><br>that is, each term is the average of the two preceding terms. The first few terms of the sequence are<br><br>\[<br>0,1,1 / 2,3 / 4,5 / 8,11 / 16,21 / 32,43 / 64,85 / 128, \ldots<br>\]<br><br>a) Find a \(2 \times 2\) real matrix \(A\) such that the following is true<br><br>\[<br>\left(\begin{array}{c}<br>a_{n+1} \\<br>a_{n}<br>\end{array}\right)=A\left(\begin{array}{c}<br>a_{n} \\<br>a_{n-1}<br>\end{array}\right)<br>\]<br><br>\[<br>A=\left(\begin{array}{cc}<br>1 / 2 &amp; 1 / 2 \\<br>1 &amp; 0<br>\end{array}\right)<br>\]<br><br>b) Find all the eigenvalues and eigenvectors of your matrix \(A\).<br><br>We want to find \(\lambda\) such that<br><br>\[<br>A-\lambda I=\left(\begin{array}{cc}<br>1 / 2-\lambda &amp; 1 / 2 \\<br>1 &amp; -\lambda<br>\end{array}\right)<br>\]<br><br>is not invertible. Using Gauss elimination (or just be speculating), we found this is the same as saying \(\lambda^{2}-\frac{1}{2} \lambda-\frac{1}{2}=0\). So \(\lambda\) has two solutions \(1,-\frac{1}{2}\).<br><br>The eigenvectors for 1 are the solutions of \((A-1 \cdot I)\left(\begin{array}{l}x_{1} \\ x_{2}\end{array}\right)=0\), or<br><br><ul><li>\(-(1 / 2) x_{1}+(1 / 2) x_{2}\) = \(0\)</li><li>\(x_{1}-x_{2}\)&nbsp;= \(0\)<br></li><li>Thus the solutions are&nbsp;\(\left(\begin{array}{l}x \\ x\end{array}\right)\) with a basis&nbsp;\(u\)= \(\left(\begin{array}{l}1 \\ 1\end{array}\right)\)</li><li>Mutatis mulandis for -1/2</li></ul><div><br></div>
+
+============================================================
----------------------------

=== Note ID: 1698999646952 (Block 174) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-a8ca47d946f7f00d5c54658f32e74eca4f4cc5b8.jpg"><br><br>Lets prove (a), let w be in W Then<br><ul><li>\(w \in \operatorname{null} T^*\)&nbsp;</li><li>\(\Longleftrightarrow\) \(T^* w=0\)</li><li>\(\Longleftrightarrow\) \(\left\langle v, T^* w\right\rangle\) =\(0\) for all \(v \in V\)<br></li><li>\(\Longleftrightarrow\) \(\langle T v, w\rangle\) = \(0\) for all \(v \in V\)<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1699003127975 (Block 175) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-cfb62edf1a09ee4b7ca829651b4d80b40ebd0f4e.jpg"><br><br>Proof, let v be a vector in V, then:<br><ul><li>\(\langle T v, v\rangle\) - \(\overline{\langle T v, v\rangle}\)&nbsp;</li><li>= \(\langle T v, v\rangle\) - \(\langle v, T v\rangle\)<br></li><li>=&nbsp;\(\langle T v, v\rangle\) - \(\left\langle T^* v, v\right\rangle\)</li><li>= \(\left\langle\left(T-T^*\right) v, v\right\rangle\).<br></li></ul><div><br></div><div><ul><li>If \(\langle T v, v\rangle \in \mathbf{R}\) for every \(v \in V\), then the left side of the first equation above equals 0 ,&nbsp;</li><li>So \(\left\langle\left(T-T^*\right) v, v\right\rangle\) = \(0\) for every \(v \in V\).&nbsp;</li><li>This implies that \(T-T^*\) = \(0\). Hence \(T\) is self-adjoint.</li></ul></div>
+
+============================================================
----------------------------

=== Note ID: 1699037262586 (Block 176) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-a24bfef9e14eef153c19c7edcdde6d4f0f3b680e.jpg"><br><br>Proof<br><ul><li>T is normal</li><li>\(\Longleftrightarrow\)&nbsp; \(T^* T-T T^*\) = \(0\)<br></li><li>\(<br>\Longleftrightarrow\)&nbsp; \(\left\langle\left(T^* T-T T^*\right) v, v\right\rangle\)::inner prod::inner prod::inner prod::inner prod = \(0\)&nbsp;for all \(v \in V\)<br></li><li>\(\Longleftrightarrow\) \(\left\langle T^* T v, v\right\rangle\)::inner prod::inner prod::inner prod = \(\left\langle T T^* v, v\right\rangle\)::inner prod::inner prod::inner prod for all v in V<br></li><li>\(\Longleftrightarrow\) \(\|T v\|^2\)::norm::norm = \(\left\|T^* v\right\|^2\)::norm::norm for all v in V<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1701881706874 (Block 177) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>Smaller models can use mixed precision fp32 parameters and bf16 activations</li><li>While larger models use bf16 for both</li><li>bf16 parameters are updated using stochastic rounding to maintain stability</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1701881882199 (Block 178) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <span style="font-style:italic">The&nbsp;</span>continuation<span style="font-style:italic">&nbsp;</span>toxicity<span style="font-style:italic">&nbsp;of larger models is more consistent with&nbsp;</span>prompt toxicity<span style="font-style:italic">&nbsp;than for smaller models</span>
+
+============================================================
----------------------------

=== Note ID: 1701882251288 (Block 179) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Distillation from large to small or small to large seems to bring little benefit to LLMs.
+
+============================================================
----------------------------

=== Note ID: 1701882675893 (Block 180) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>Unlike the model parameters and activations, it is best to maintain optimzier states in fp32</li><li>The fp32 parameters are used for the update and then cast to bf16 during the fowrad pass</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1701882726063 (Block 181) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Keeping only the optimiser state in fp32 matches the performance of fp32 training and only slightly increases memory compared to bf16
+
+============================================================
----------------------------

=== Note ID: 1701947741469 (Block 182) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -4,3 +4,5 @@
 <li>Specifically, they found a&nbsp;<strong>temporary&nbsp;improvement&nbsp;in test&nbsp;error</strong> and a&nbsp;<strong>temporary&nbsp;increase&nbsp;in train&nbsp;error.</strong></li>
 </ol>
 </li></ol>
+
+============================================================
----------------------------

=== Note ID: 1701947774412 (Block 183) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       While both local SGD and minibatch SGD greatly improve with intra-node parallelisation, local SGD is the only one to scale well to multi-node settings (approaches linear::what kind of speedup?::what kind of speedup? speedup).
+
+============================================================
----------------------------

=== Note ID: 1701948371327 (Block 184) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <span style="color: inherit; font-style: italic;">Performing local SGD comes at a price:&nbsp;</span>lower<span style="color: inherit; font-style: italic;">&nbsp;</span>communication costs<span style="color: inherit; font-style: italic;">&nbsp;(and thereby&nbsp;</span>faster<span style="color: inherit; font-style: italic;">&nbsp;training) are accompanied by&nbsp;</span>lower<span style="color: inherit; font-style: italic;">&nbsp;</span>accuracy<span style="color: inherit; font-style: italic;">.</span>
+
+============================================================
----------------------------

=== Note ID: 1702021813378 (Block 185) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1,2 +1,4 @@
       <ol>
 <li>Each working node requires k times less gpus where k is the total number of nodes.<br></li></ol>
+
+============================================================
----------------------------

=== Note ID: 1702115746627 (Block 186) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>From a probabilistic perspective, translation is equivalent to finding a target sentence \(\mathbf{y}\) that maximizes the conditional probability of \(\mathbf{y}\) given a source sentence \(\mathbf{x}\), i.e., :<br><br></li><ul><li>\(\arg \max _{\mathbf{y} }\) p(&nbsp;\(\mathbf{y} \mid \mathbf{x}\) ).&nbsp;</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1702122699247 (Block 187) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-4f405f4122a2f4b25a46858f63800308295a137f.jpg"><br>In the attention-based RNN architecture:<br><ul><li>We define each conditional probability in Eq. (2) as:</li><ul><li>\(p\) \( (y_i \mid y_1, \ldots, y_{i-1}, \mathbf{x})\) = \(g\)\((y_{i-1}, s_i, c_i)\)</li></ul><li>where \(s_i\) is an RNN hidden state for time \(i\), computed by</li><ul><li>\(s_i\) = \(f\) \(\left(s_{i-1}, y_{i-1}, c_i\right) \)</li></ul><li>It should be noted that unlike the existing encoder-decoder approach (see Eq. (2)), here the probability is conditioned on a distinct context vector \(c_i\) for each target word \(y_i\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702125852562 (Block 188) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       A Bidirectional RNN:<br><ul><li>Consists of forward and backward RNN's.&nbsp;</li><li>The forward RNN \(\vec{f}\) reads the input sequence as it is ordered (from \(x_1\) to \(x_{T_x}\) ) and calculates a sequence of forward hidden states \(\left(\vec{h}_1, \cdots, \vec{h}_{T_x}\right)\).&nbsp;</li><li>The backward RNN \(\overleftarrow{f}\) reads the sequence in the reverse order (from \(x_{T_x}\) to \(x_1\) ), resulting in a sequence of backward hidden states \(\left(\overleftarrow{h}_1, \cdots, \overleftarrow{h}_{T_x}\right)\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702131761215 (Block 189) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li><img src="paste-d7fe315a3abdaa5d8ff22fb35d4eae8fd319a56c.jpg"><br></li><li>The strength of the soft-alignment, opposed to a hard-alignment, is evident, for instance.</li><li>Consider the source phrase [the man] which was translated into [l' homme].&nbsp;</li><li>Any hard alignment will map [the] to [l'] and [man] to [homme].&nbsp;</li><li>This is not helpful for translation, as one must consider the word following [the] to determine whether it should be translated into [le], [la], [les] or [l'].&nbsp;</li><li>Our soft-alignment solves this issue naturally by letting the model look at both [the] and [man].&nbsp;</li><li>An additional benefit of the soft alignment is that it naturally deals with source and target phrases of different lengths, without requiring a counter-intuitive way of mapping some words to or from nowhere ([NULL])&nbsp;</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702132482922 (Block 190) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The model takes a source sentence of 1 -of-K coded word vectors as input<br>\[<br>\mathbf{x}=\left(x_1, \ldots, x_{T_x}\right), x_i \in \mathbb{R}^{K_x}<br>\]<br>and outputs a translated sentence of 1 -of-K coded word vectors<br>\[<br>\mathbf{y}=\left(y_1, \ldots, y_{T_y}\right), y_i \in \mathbb{R}^{K_y}<br>\]<br>where \(K_x\) and \(K_y\) are the vocabulary sizes of source and target languages, respectively. \(T_x\) and \(T_y\) respectively denote the lengths of source and target sentences.
+
+============================================================
----------------------------

=== Note ID: 1702201627835 (Block 191) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       For complext inner product spaces<br><ul><li>\(\langle Tv, v \rangle\) = 0<br></li><li>Implies T = 0</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702201961636 (Block 192) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Proof that \(\langle Tv,v \rangle \in R\) for all v implies that T is self-adjoint:<br><ul><li>\(\langle\)&nbsp; \(Tv,v\) \(\rangle\) - \(\langle\) \(\overline{Tv,v}\) \(\rangle\)</li><li>= \(\langle\)&nbsp; \(Tv,v\) \(\rangle\) - \(\langle\)&nbsp; \(v,Tv\) \(\rangle\)&nbsp;<br></li><li>= \(\langle\)&nbsp; \(Tv,v\) \(\rangle\) -&nbsp;\(\langle\)&nbsp; \(T^*v,v\) \(\rangle\)&nbsp;</li><li>= \(\langle\)&nbsp; \( (T-T*)v,v\) \(\rangle\) = 0</li><li>Which implies&nbsp;\(T^*\) = \(T\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702203378249 (Block 193) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>The nicest operators on \(V\) are those for which there is an orthonormal basis of \(V\) with respect to which the operator has a diagonal matrix.</li><li>&nbsp;These are precisely the operators \(T \in \mathcal{L}(V)\) such that there is an orthonormal basis of \(V\) consisting of eigenvectors of \(T\).&nbsp;</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702203840360 (Block 194) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-530a5e94b1b46c19656893fea1bedce3bd55f09a.jpg"><br>Proof:<br><ul><li>Proof First suppose (c) holds.&nbsp;</li><li>So \(T\) has a diagonal matrix with respect to some orthonormal basis of \(V\).&nbsp;</li><li>The matrix of \(T^{*}\) (with respect to the same basis) is obtained by taking the conjugate transpose of the matrix of \(T\)&nbsp;</li><li>Hence \(T^{*}\) also has a diagonal matrix.&nbsp;</li><li>Any two diagonal matrices commute; thus \(T\) commutes with \(T^{*}\), which means that \(T\) is normal.&nbsp;</li><li>In other words, (a) holds.<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702204465110 (Block 195) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-b177df9f882ef3397e176a2b7100706a58dab475.jpg"><br>Now suppose (a) holds, so \(T\) is normal. By Schur's Theorem (6.38), there is an orthonormal basis \(e_{1}, \ldots, e_{n}\) of \(V\) with respect to which \(T\) has an upper-triangular matrix. Thus we can write<br><ul><li>\(\mathcal{M}\left(T,\left(e_1, \ldots, e_n\right)\right)\) =&nbsp;\(\left(\begin{array}{ccc}a_{1,1} &amp; \cdots &amp; a_{1, n} \\ &amp; \ddots &amp; \vdots \\ 0 &amp; &amp; a_{n, n}\end{array}\right)\).<br></li><li>We will show that this matrix is actually a diagonal matrix.<br></li><li>We see from the matrix above that&nbsp;</li><ul><li>\(\|\) \(T e_{1}\) \(\|^{2}\) = \(|\) \(a_{1,1}\) \(|^{2}\)</li><li>and</li><li>\(\| \) \(T^{*} e_{1}\) \(\|^{2}\) = \(\left|a_{1,1}\right|^{2}+\left|a_{1,2}\right|^{2}+\cdots+\left|a_{1, n}\right|^{2} .\)</li></ul><li>Because \(T\) is normal:</li><ul><li>&nbsp;\(\|\)&nbsp; \(T e_{1}\)&nbsp; \( \|\) = \(\| \)&nbsp; \(T^{*} e_{1}\) \(\|\).&nbsp;</li></ul><li>Thus the two equation above implity that all entries in the first row of the matrix, except possibly the first entry, equal 0</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702204668168 (Block 196) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-b177df9f882ef3397e176a2b7100706a58dab475.jpg"><br>Now suppose (a) holds, so \(T\) is normal. By Schur's Theorem (6.38), there is an orthonormal basis \(e_{1}, \ldots, e_{n}\) of \(V\) with respect to which \(T\) has an upper-triangular matrix. Thus we can write<br><ul><li>\(\mathcal{M}\left(T,\left(e_1, \ldots, e_n\right)\right)\) =&nbsp;\(\left(\begin{array}{ccc}a_{1,1} &amp; \cdots &amp; a_{1, n} \\ &amp; \ddots &amp; \vdots \\ 0 &amp; &amp; a_{n, n}\end{array}\right)\).<br></li><li>We will show that this matrix is actually a diagonal matrix.<br></li><li>We see from the matrix above that&nbsp;</li><ul><li>\(\left\|T e_{1}\right\|^{2}\) = \(\left|a_{1,1}\right|^{2}\)</li><li>and</li><li>\(\left\|T^{*} e_{1}\right\|^{2}\) = \(\left|a_{1,1}\right|^{2}+\left|a_{1,2}\right|^{2}+\cdots+\left|a_{1, n}\right|^{2} .\)</li></ul><li>Because \(T\) is normal:</li><ul><li>&nbsp;\(\left\|T e_{1}\right\|\) = \(\left\|T^{*} e_{1}\right\|\) (see 7.20).&nbsp;</li></ul><li>Thus the two equations above imply that all entries in the first row of the matrix in 7.25 , except possibly the first entry \(a_{1,1}\), equal 0 .</li><li>Now from the matrix we see that</li><ul><li>\(\|\) \(T e_{2}\) \(\|^{2}\) = \(|\) \(a_{2,2}\) \( |^{2}\)</li><li>(because \(a_{1,2}=0\), as we showed in the paragraph above) and</li><li>\(\|\) \(T^{*} e_{2}\) \(|^{2}\) =\(\left|a_{2,2}\right|^{2}+\left|a_{2,3}\right|^{2}+\cdots+\left|a_{2, n}\right|^{2} .\)</li></ul><li>Because \(T\) is normal, \(\| \) \(T e_{2}\) \(\|\) = \(\|\) \(T^{*} e_{2}\) \(\|\).&nbsp;</li><li>Thus the two equations above imply that all entries in the second row of the matrix in 7.25, except possibly the diagonal entry \(a_{2,2}\), equal 0 .</li><li>Continuing in this fashion, we see that all the nondiagonal entries in the matrix 7.25 equal 0 . Thus (c) holds.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702205631760 (Block 197) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-83b3831e34c826764b0093760f2f884050344989.jpg"><br>What are the high-level points of the complex spectral theorem proof?<br><ul><li>We can create an upper-triangular matrix w.r.t some orthonormal basis for any vectors over C</li><li>For the first element \(\left\|T e_1\right\|^2\) = \(\left|a_{1,1}\right|^2\) since the column is zero<br></li><li>Because T is normal, the same applies to the adjoint/complex transpose and thus the row must also be zero:</li><li><img src="paste-caa70de9bb00431a9366f94d7841a6d2253f01e0.jpg"><br></li><ul><li>I.e for normal operators the norms of columns and their matching rows must match</li></ul><li>Since we know this row is zero, repeat for all other elements by induction</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702206379921 (Block 198) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Self-adjoint operators and invariant subspaces<br><br>Suppose \(T \in \mathcal{L}(V)\) is self-adjoint and \(U\) is a subspace of \(V\) that is invariant under \(T\). Then<br><br>(a) \(U^{\perp}\) is invariant under \(T\);<br><br>(b) \(\left.T\right|_{U} \in \mathcal{L}(U)\) is self-adjoint;<br><br>(c) \(\left.T\right|_{U^{\perp} }  \in \mathcal{L}\left(U^{\perp}\right)\) is self-adjoint.
+
+============================================================
----------------------------

=== Note ID: 1702207086330 (Block 199) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-347f8df15eca2c89ed5a9727d6b582e0021fda62.jpg"><br>What are the broad steps of the proof ?<br><ul><li>Make non-lin-indp list via powers of Tv for v not 0</li><li>Decompose into degree-1 and degree-2 polynomials&nbsp;</li><ul><li>The degree-2 polynomials must by-definition have b^2 &lt; 4ac since otherwise they would have been broken down into degree 1 polynomials</li></ul></ul><br>
+
+============================================================
----------------------------

=== Note ID: 1702209131622 (Block 200) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       9 Suppose \(V\) is a complex inner product space. Prove that every normal operator on \(V\) has a square root.<br><ul><li>&nbsp;(An operator \(S \in \mathcal{L}(V)\) is called a square root of \(T \in \mathcal{L}(V)\) if \(S^{2}\) = \(T\).)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702209220210 (Block 201) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       12 Suppose \(T \in \mathcal{L}(V)\) is self-adjoint, \(\lambda \in \mathbf{F}\), and \(\epsilon&gt;0\). Suppose there exists \(v \in V\) such that \(\|v\|=1\) and<br><br><ul><li>\(\|T v-\lambda v\|\)&lt; \(\epsilon .\)</li></ul><br>Prove that \(T\) has an eigenvalue \(\lambda^{\prime}\) such that:<br><ul><li>&nbsp;\(\left|\lambda-\lambda^{\prime}\right|\) &lt; \(\epsilon\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702209350114 (Block 202) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <br>Definition positive operator<br><br>An operator \(T \in \mathcal{L}(V)\) is called positive if \(T\) is self-adjoint and<br><br><ul><li>\(\langle T v, v\rangle\)&nbsp; \(\geq\) \(0\)</li><li>for all \(v \in V\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702209470978 (Block 203) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <br>Definition square root<br><br>An operator \(R\) is called a square root of an operator \(T\) if \(R^{2}\) = \(T\).
+
+============================================================
----------------------------

=== Note ID: 1702209525930 (Block 204) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The positive operators correspond to the numbers \([0, \infty)\), so better terminology would use the term nonnegative instead of positive. However, operator theorists consistently call these the positive operators, so we will follow that custom.
+
+============================================================
----------------------------

=== Note ID: 1702209741686 (Block 205) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Characterization of positive operators<br><br>Let \(T \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(c) \(T\) has a positive square root;<br></li><li><br></li><li>(d) \(T\) has a self-adjoint square root;</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702210314559 (Block 206) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Each nonnegative number has a unique nonnegative square root.
+
+============================================================
----------------------------

=== Note ID: 1702210345950 (Block 207) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Each positive operator has only one positive square root<br><br>Every positive operator on \(V\) has a unique positive square root.<br>
+
+============================================================
----------------------------

=== Note ID: 1702210371566 (Block 208) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <br>Some mathematicians also use the term positive semidefinite operator, which means the same as positive operator.
+
+============================================================
----------------------------

=== Note ID: 1702210860335 (Block 209) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition isometry<br><br>- An operator \(S \in \mathcal{L}(V)\) is called an isometry if<br><br><ul><li>\(\|S v\|\) = \(\|v\|\)</li><li>for all \(v \in V\).</li></ul><br>In other words, an operator is an isometry if it preserves norms.<br>
+
+============================================================
----------------------------

=== Note ID: 1702211338609 (Block 210) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Characterization of isometries<br><br>Suppose \(S \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(S\) is an isometry;</li><li>(d) there exists an orthonormal basis \(e_{1}, \ldots, e_{n}\) of \(V\) such that \(S e_{1}, \ldots, S e_{n}\) is orthonormal;</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702211356480 (Block 211) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Characterization of isometries<br><br>Suppose \(S \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(S\) is an isometry;</li><li>(e) \(S^{*} S\) = \(I\);</li><li>(f) \(S S^{*}\) = \(I\);</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702211390938 (Block 212) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Characterization of isometries<br><br>Suppose \(S \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(S\) is an isometry;</li><li>(g) \(S^{*}\) is an isometry;</li><li>(h) \(S\) is invertible and \(S^{-1}\) = \(S^{*}\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702211878776 (Block 213) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>Every isometry is normal<br></li><li>Thus characterization of normal operators can be used to give descriptions of isometries</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702212316004 (Block 214) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose \(T \in \mathcal{L}(V, W)\). Prove that \(T^{*}\) \(\circ\) \(T\) is a positive operator on \(V\) and \(T\) \(\circ\) \(T^{*}\) is a positive operator on \(W\).
+
+============================================================
----------------------------

=== Note ID: 1702212326595 (Block 215) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       5 Prove that the sum of two positive operators on \(V\) is positive.
+
+============================================================
----------------------------

=== Note ID: 1702212362887 (Block 216) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose \(T \in \mathcal{L}(V)\) is positive. <br>Prove that \(T^{k}\) is positive for every positive integer \(k\).
+
+============================================================
----------------------------

=== Note ID: 1702212593662 (Block 217) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       10 Suppose \(S \in \mathcal{L}(V)\). Prove that the following are equivalent:<br><br><ul><li>(a) \(S\) is an isometry;</li><li>(c) \(S^{*} e_{1}, \ldots, S^{*} e_{m}\) is an orthonormal list for every orthonormal list of vectors \(e_{1}, \ldots, e_{m}\) in \(V\);</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702213237454 (Block 218) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-c7360bede1694146732269f8d646b388bb42064d.jpg"><br>Proof First suppose (c) holds.<br>The matrix of \(T^*\) is obtained by taking the conjugate transpose of the matrix of \(T\); hence \(T^*\) also has a diagonal matrix.<br><br>Any two diagonal matrices commute; thus \(T\) commutes with \(T^*\).<br><br>Thus \(T\) is normal. In other words, (a) holds.
+
+============================================================
----------------------------

=== Note ID: 1702215912788 (Block 219) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       - If \(\mathcal{M}(T)\) is a diagonal matrix with nonnegative entries on the diagonal, then \(T\) is a positive operator.
+
+============================================================
----------------------------

=== Note ID: 1702220190504 (Block 220) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-0629f4e38b9bf7aa8b7c49773aaa24db716252dc.jpg"><br>Proof Suppose (a) holds, so \(S\) is an isometry. By the Complex Spectral Theorem, there is an orthonormal basis \(e_1, \ldots, e_n\) of \(V\) consisting of eigenvectors of \(S\). For \(j \in\{1, \ldots, n\}\), let \(\lambda_j\) be the eigenvalue corresponding to \(e_j\). Then<br><ul><li>\(\left|\lambda_j\right|\)</li><li>=&nbsp;\(\left\|\lambda_j e_j\right\|\) because the norm of orthonormal vectors is 1</li><li>= \(\left\|S e_j\right\|\) because eigenvector</li><li>=&nbsp;\(\left\|e_j\right\|\) because isometry</li><li>= 1 because norm of orthonormal vectors is 1</li></ul><div>Thus each eigenvalue of S has absolute<br></div>value 1. Hence (b) holds.
+
+============================================================
----------------------------

=== Note ID: 1702394082186 (Block 221) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Self-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.
+
+============================================================
----------------------------

=== Note ID: 1702395142696 (Block 222) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -8,3 +8,5 @@
 <li>x = FeedForward(x)</li>
 <li>x = prev + x</li>
 <li>x = LayerNorm(x)</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1702398536671 (Block 223) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       An original encoder block does the following operations:<br><ol><li>x = positional_encoding(input)</li><li>prev =x</li><li>x = Multi-headed Attention (MHA)(x)</li><li>x = prev + x</li><li>x = LayerNorm(x)</li><li>prev = x</li><li>x = FeedForward(x)</li><li>x = prev + x</li><li>x = LayerNorm(x)</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1702398919559 (Block 224) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The transformer's decoder can be summarised as follows:<br><ol><li></li><li>x = positional_encoding(right_shifted_input)<br></li><li>prev = x<br></li><li>x = MHA(x)<br></li><li>x = prev+x<br></li><li>x = LayerNorm(x)<br></li><li>x = MHA( encoder(input), encoder(input), x)<br></li><li>x = prev + x<br></li><li>x = LayerNorm(x)<br></li><li>prev = x<br></li><li>x = FeedForward(x)<br></li><li>x = prev + x<br></li><li>x = LayerNorm(x)</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1702398951276 (Block 225) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The transformer's decoder can be summarised as follows:<br><ol><li></li><li>x = positional_encoding(right_shifted_input)<br></li><li>prev = x<br></li><li>x = MHA(x)<br></li><li>x = prev+x<br></li><li>x = LayerNorm(x)<br></li><li>x = MHA( encoder(input), encoder(input), x)<br></li><li>x = prev + x<br></li><li>x = LayerNorm(x)<br></li><li>prev = x<br></li><li>x = FeedForward(x)<br></li><li>x = prev + x<br></li><li>x = LayerNorm(x)</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1702399018840 (Block 226) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The self-attention sub-layer in the decoder stack is modified compared to the encoder to prevent positions from attending to subsequent positions via masking
+
+============================================================
----------------------------

=== Note ID: 1702545421830 (Block 227) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       An operator T is normal iff :<br><ul><li>\(T T^*\) =&nbsp;\(T^* T\)<br></li><li>so every self-adjoint operator is normal by default</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702545541381 (Block 228) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       An operator is also normal, besides the full definition, iff:<br><ul><li>\(&nbsp; \vert \vert\)&nbsp; \(Tv\)&nbsp;\(&nbsp; \vert \vert\)&nbsp; = \(&nbsp; \vert \vert\)&nbsp; \(T^*v\)&nbsp;\(&nbsp; \vert \vert\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702545958895 (Block 229) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       For an arbitrary operator T and&nbsp;\(\lambda \in F\):<br><ul><li>&nbsp;\(\lambda\) is an eigenvalue of T iff&nbsp;\(\bar{\lambda}\) is an eigenvalue of&nbsp;\(T^*\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702547032646 (Block 230) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Proof that for normal operators eigenvectors corresponding to different eigenvalues are orthogonal:<br><ul><li>Take two pairs of eigenvectors-eigenvalues for T and its adjoint</li><li>&nbsp;\(T v\) = \(\alpha v\)</li><li>\(T^* u\) =&nbsp;\(\beta u\)<br></li><li>(\(\alpha - \beta\)) \(\langle\) \(u,v\) \(\rangle\)<br></li><ul><li>= \(\langle\) \(\alpha u,v\) \(\rangle\) -&nbsp;&nbsp;\(\langle\) \(u, \bar{\beta} v\) \(\rangle\) due to distributivity and conjugate symmetry</li><li>= \(\langle\) \(Tu,v\) \(\rangle\) -&nbsp;&nbsp;\(\langle\) \(u, T^* v\) \(\rangle\) due to eigenvectors</li><li>= \(\langle\) \(Tu,v\) \(\rangle\) -&nbsp;&nbsp;\(\langle\) \(Tu, v\) \(\rangle\) due to adjoint definition</li><li>&nbsp;= 0</li></ul><li>Thus the vectors must have been orthogonal since the eigenvalues were distinct</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702547445165 (Block 231) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       For an operator T with eigenvalues \(\lambda_i\)&nbsp;the following are equivalent:<br><ul><li>T has a diagonalizing basis</li><li>V =&nbsp;\(\oplus_{i=1}^n\)&nbsp;\(U_i\) each of which is is invariant under T</li><li>V =&nbsp;\(\oplus_{i=1}^n\)&nbsp;\(\mathrm{null}(T-\lambda_i I)\)&nbsp;</li><li>dim V =&nbsp;\(\sum_{i=1}^n\) \(\mathrm{dim} \,\mathrm{null} (T-\lambda_i I)\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702556807253 (Block 232) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <br>To prove the real spectral theorem we use an induction on V<br><ul><li>Use an inductive technique, shwoing that it is self-evident for dim V = 1</li><li>For other ones decompose the vector space into an eigenvector, which exists because of self-adjointness,&nbsp; and its orthogonal complement</li><li>Show that the orthogonal complement has an orthonormal basis formed of eigenvectors by showing that it is self-adjoint and using the I.H</li><li>Since the extra vector is orthogonal to all vectors in the orthogonal complement, just concatenate it to get a new orthonormal eigenbasis</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702557494110 (Block 233) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose V is a 2-dimensional inner product space over R and T is an operator. Then the following are equivalent:<br><ul><li>T is normal but not self-adjoint</li><li>The matirix of T is of the form &nbsp;\(\left[\begin{array}{cc}a &amp; -b \\ b &amp; a\end{array}\right]\) for any orthonormal basis with b != 0</li><li>There exists an orthonormal basis such that the matrix is of the previous form with b != 0</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702564689508 (Block 234) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Self-attention<span style="font-weight:600">, sometimes called&nbsp;</span>intra-attention<span style="font-weight:600">&nbsp;is an&nbsp;</span>attention<span style="font-weight:600">&nbsp;mechanism&nbsp;</span>relating different positions<span style="font-weight:600">&nbsp;of&nbsp;</span>a single sequence<span style="font-weight:600">&nbsp;in order to compute&nbsp;</span>a representation of the sequence.
+
+============================================================
----------------------------

=== Note ID: 1702565340841 (Block 235) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.
+
+============================================================
----------------------------

=== Note ID: 1702570136805 (Block 236) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -4,3 +4,5 @@
 <li>\(W_i^K\) \(\in\) \(\mathbb{R}^{d_{\text {model } }  \times d_k}\)</li>
 <li>\(W_i^V\) \(\in\) \(\mathbb{R}^{d_{\text {model } } \times d_v}\)</li>
 <li>\(W^O\) \(\in\) \(\mathbb{R}^{h d_v \times d_{\text {model } } }\)</li></ol></li></ol>
+
+============================================================
----------------------------

=== Note ID: 1702570176326 (Block 237) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ol><li>\( \operatorname{MultiHeadAttn}\) ( \(Q, K, V\))&nbsp; = Concat ( \(\text {head }1, \ldots, \text { head }{\mathrm{h}} \) )&nbsp; \(W^O \)</li><li>head \(_i\) = \(\operatorname{Attention}\) ( \(Q W_i^Q, K W_i^K, V W_i^V\))&nbsp;</li><li>The projections are simple parameter matrices<ol><li>\(W_i^Q\) \(\in\) \( \mathbb{R}^{d_{\text {model } } \times d_k}\)</li><li>\(W_i^K\) \(\in\) \(\mathbb{R}^{d_{\text {model } }  \times d_k}\)</li><li>\(W_i^V\) \(\in\) \(\mathbb{R}^{d_{\text {model } }  \times d_v}\)</li><li>\(W^O\) \(\in\) \(\mathbb{R}^{h d_v \times d_{\text {model &nbsp;} } }\)</li></ol></li></ol>
+
+============================================================
----------------------------

=== Note ID: 1702571374309 (Block 238) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The connections between the encoder and decoder passes through an attention layer which is not<b>&nbsp;</b>a<b>&nbsp;</b>self-attention<b>&nbsp;</b>layer. Rather it is an attention layer between different sequences.
+
+============================================================
----------------------------

=== Note ID: 1702571923436 (Block 239) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -4,3 +4,5 @@
 </ol>
 </li>
 <li>Use softmax to turn outputs into a probability distribution</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702629143323 (Block 240) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">EncoderDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="sd">"""</span><br><span class="sd">    A standard Encoder-Decoder architecture. Base for this and many</span><br><span class="sd">    other models.</span><br><span class="sd">    """</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">src_embed</span><span class="p">,</span> <span class="n">tgt_embed</span><span class="p">,</span> <span class="n">generator</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">src_embed</span> <span class="o">=</span> <span class="n">src_embed</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">tgt_embed</span> <span class="o">=</span> <span class="n">tgt_embed</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">generator</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span><br>        <span class="s2">"Take in and process masked src and target sequences."</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">),</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">):</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">src_embed</span><span class="p">(</span><span class="n">src</span><span class="p">)</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tgt_embed</span><span class="p">(</span><span class="n">tgt</span><span class="p">),</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
+
+============================================================
----------------------------

=== Note ID: 1702629245485 (Block 241) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">EncoderDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="sd">"""</span><br><span class="sd">    A standard Encoder-Decoder architecture. Base for this and many</span><br><span class="sd">    other models.</span><br><span class="sd">    """</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">src_embed</span><span class="p">,</span> <span class="n">tgt_embed</span><span class="p">,</span> <span class="n">generator</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">src_embed</span> <span class="o">=</span> <span class="n">src_embed</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">tgt_embed</span> <span class="o">=</span> <span class="n">tgt_embed</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">generator</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span><br>        <span class="s2">"Take in and process masked src and target sequences."</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">),</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">):</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">src_embed</span><span class="p">(</span><span class="n">src</span><span class="p">),</span> <span class="n">src_mask</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tgt_embed</span><span class="p">(</span><span class="n">tgt</span><span class="p">),</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
+
+============================================================
----------------------------

=== Note ID: 1702629797899 (Block 242) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Encoder is made up of self-attn and feed forward (defined below)"</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">feed_forward</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span><br>        <span class="s2">"Follow Figure 1 (left) for connections."</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">))</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
+
+============================================================
----------------------------

=== Note ID: 1702629840290 (Block 243) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Encoder is made up of self-attn and feed forward (defined below)"</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">feed_forward</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span><br>        <span class="s2">"Follow Figure 1 (left) for connections."</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">))</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
+
+============================================================
----------------------------

=== Note ID: 1702630130628 (Block 244) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">src_attn</span><span class="p">,</span> <span class="n">feed_forward</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span> <span class="o">=</span> <span class="n">src_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">),</span> <span class="mi">3</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span><br>        <span class="s2">"Follow Figure 1 (right) for connections."</span><br>        <span class="n">m</span> <span class="o">=</span> <span class="n">memory</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">))</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">))</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
+
+============================================================
----------------------------

=== Note ID: 1702630145721 (Block 245) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">src_attn</span><span class="p">,</span> <span class="n">feed_forward</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span> <span class="o">=</span> <span class="n">src_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">),</span> <span class="mi">3</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span><br>        <span class="s2">"Follow Figure 1 (right) for connections."</span><br>        <span class="n">m</span> <span class="o">=</span> <span class="n">memory</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">))</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">))</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
+
+============================================================
----------------------------

=== Note ID: 1702630171744 (Block 246) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">src_attn</span><span class="p">,</span> <span class="n">feed_forward</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span> <span class="o">=</span> <span class="n">src_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">),</span> <span class="mi">3</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span><br>        <span class="s2">"Follow Figure 1 (right) for connections."</span><br>        <span class="n">m</span> <span class="o">=</span> <span class="n">memory</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">))</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">))</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
+
+============================================================
----------------------------

=== Note ID: 1702630193637 (Block 247) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">src_attn</span><span class="p">,</span> <span class="n">feed_forward</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span> <span class="o">=</span> <span class="n">src_attn</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">),</span> <span class="mi">3</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span><br>        <span class="s2">"Follow Figure 1 (right) for connections."</span><br>        <span class="n">m</span> <span class="o">=</span> <span class="n">memory</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">))</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">))</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
+
+============================================================
----------------------------

=== Note ID: 1702630477937 (Block 248) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span><br>    <span class="s2">"Compute 'Scaled Dot Product Attention'"</span><br>    <br></code></pre></div></td></tr></tbody></table><br>
+
+============================================================
----------------------------

=== Note ID: 1702630650293 (Block 249) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span><br>    <span class="s2">"Compute 'Scaled Dot Product Attention'"</span><br>    <span class="n">d_k</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><br>    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span><br>    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span><br>        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span><br>    <span class="n">p_attn</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><br>    <span class="k">if</span> <span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span><br>        <span class="n">p_attn</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">p_attn</span><span class="p">)</span><br>    <span class="k">return</span> torch.<span class="n">matmul</span><span class="p">(</span><span class="n">p_attn</span><span class="p">,</span> <span class="n">value</span><span class="p">),</span> <span class="n">p_attn</span><br></code></pre></div></td></tr></tbody></table><br>
+
+============================================================
----------------------------

=== Note ID: 1702630764250 (Block 250) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">MultiHeadedAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span><br>        <span class="s2">"Take in model size and number of heads."</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadedAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">h</span> <span class="o">==</span> <span class="mi">0</span><br>        <span class="c1"># We assume d_v always equals d_k</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">h</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">h</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">linears</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span> <span class="mi">{{c2</span></code>::4}}<span class="p" style="font-family: Arial;">)</span></pre><pre><code>        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="kc">None</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span><br>        <span class="s2">"Implements Figure 2"</span><br>        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span><br>            <span class="c1"># Same mask applied to all h heads.</span><br>            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><br>        <span class="n">nbatches</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><br><br>        <span class="c1"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k</span><br>        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="p">[</span><br>            <span class="n">lin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">nbatches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><br>            <span class="k">for</span> <span class="n">lin</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">,</span> <span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">))</span><br>        <span class="p">]</span><br><br>        <span class="c1"># 2) Apply attention on all the projected vectors in batch.</span><br>        <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><br>            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><br>        <span class="p">)</span><br><br>        <span class="c1"># 3) "Concat" using a view and apply a final linear.</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><br>            <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><br>            <span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><br>            <span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">nbatches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><br>        <span class="p">)</span><br>        <span class="k">del</span> <span class="n">query</span><br>        <span class="k">del</span> <span class="n">key</span><br>        <span class="k">del</span> <span class="n">value</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
+
+============================================================
----------------------------

=== Note ID: 1702633787686 (Block 251) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">MultiHeadedAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span><br>        <span class="s2">"Take in model size and number of heads."</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadedAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">h</span> <span class="o">==</span> <span class="mi">0</span><br>        <span class="c1"># We assume d_v always equals d_k</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">h</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">h</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">linears</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span> <span class="mi">4</span><span class="p">)</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="kc">None</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span><br>        <span class="s2">"Implements Figure 2"</span><br>        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span><br>            <span class="c1"># Same mask applied to all h heads.</span><br>            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><br>        <span class="n">nbatches</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><br><br>        <span class="c1"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k</span><br>        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="p">[</span><br>            <span class="n">lin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">nbatches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><br>            <span class="k">for</span> <span class="n">lin</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">,</span> <span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">))</span><br>        <span class="p">]</span><br><br>        <span class="c1"># 2) Apply attention on all the projected vectors in batch.</span><br>        <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><br>            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><br>        <span class="p">)</span><br><br>        <span class="c1"># 3) "Concat" using a view and apply a final linear.</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><br>            <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><br>            <span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><br>            <span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">nbatches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><br>        <span class="p">)</span><br>        <span class="k">del</span> <span class="n">query</span><br>        <span class="k">del</span> <span class="n">key</span><br>        <span class="k">del</span> <span class="n">value</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
+
+============================================================
----------------------------

=== Note ID: 1702634062132 (Block 252) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">Embeddings</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">vocab</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">Embeddings</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">lut</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span><br><br></code></pre></div></td></tr></tbody></table><br>
+
+============================================================
----------------------------

=== Note ID: 1702634163797 (Block 253) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Implement the PE function."</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span><br><br>        <span class="c1"># Compute the positional encodings once in log space.</span><br>        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span><br>        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><br>        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><br>            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span><br>        <span class="p">)</span><br>        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span><br>        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span><br>        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">"pe"</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
+
+============================================================
----------------------------

=== Note ID: 1702634237360 (Block 254) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Implement the PE function."</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span><br><br>        <span class="c1"># Compute the positional encodings once in log space.</span><br>        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span><br>        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><br>        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><br>            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span><br>        <span class="p">)</span><br>        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span><br>        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span><br>        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">"pe"</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
+
+============================================================
----------------------------

=== Note ID: 1702634285670 (Block 255) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Implement the PE function."</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span><br><br>        <span class="c1"># Compute the positional encodings once in log space.</span><br>        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span><br>        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><br>        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><br>            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span><br>        <span class="p">)</span><br>        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span><br>        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span><br>        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">"pe"</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
+
+============================================================
----------------------------

=== Note ID: 1702634332321 (Block 256) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span><br>    <span class="s2">"Implement the PE function."</span><br><br>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span><br>        <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span><br><br>        <span class="c1"># Compute the positional encodings once in log space.</span><br>        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span><br>        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><br>        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><br>            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span><br>        <span class="p">)</span><br>        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span><br>        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span><br>        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><br>        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">"pe"</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span><br><br>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span><br>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="p">]</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span><br>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table><br>
+
+============================================================
----------------------------

=== Note ID: 1702635173135 (Block 257) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="k">def</span> <span class="nf">make_model</span><span class="p">(</span><br>    <span class="n">src_vocab</span><span class="p">,</span> <span class="n">tgt_vocab</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><br><span class="p">):</span><br>    <span class="s2">"Helper: Construct a model from hyperparameters."</span><br>    <span class="n">c</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><br>    <span class="n">attn</span> <span class="o">=</span> <span class="n">MultiHeadedAttention</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span><br>    <span class="n">ff</span> <span class="o">=</span> <span class="n">PositionwiseFeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span><br>    <span class="n">position</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span><br>    <span class="n">model</span> <span class="o">=</span> <span class="n">EncoderDecoder</span><span class="p">(</span><br>        <span class="n">Encoder</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">c</span><span class="p">(</span><span class="n">attn</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">ff</span><span class="p">),</span> <span class="n">dropout</span><span class="p">),</span> <span class="n">N</span><span class="p">),</span><br>        <span class="n">Decoder</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">c</span><span class="p">(</span><span class="n">attn</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">attn</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">ff</span><span class="p">),</span> <span class="n">dropout</span><span class="p">),</span> <span class="n">N</span><span class="p">),</span><br>        <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">Embeddings</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">src_vocab</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">position</span><span class="p">)),</span><br>        <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">Embeddings</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tgt_vocab</span><span class="p">),</span> <span class="n">c</span><span class="p">(</span><span class="n">position</span><span class="p">)),</span><br>        <span class="n">Generator</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tgt_vocab</span><span class="p">),</span><br>    <span class="p">)</span><br><br>    <span class="c1"># This was important from their code.</span><br>    <span class="c1"># Initialize parameters with Glorot / fan_avg.</span><br>    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span><br>        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span><br>            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><br>    <span class="k">return</span> <span class="n">model</span><br></code></pre></div></td></tr></tbody></table><br>
+
+============================================================
----------------------------

=== Note ID: 1702638382651 (Block 258) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div>Second, an attention&nbsp;<span style="font-style: inherit; font-weight: inherit; color: rgb(179, 106, 226);">decoder</span>&nbsp;does an extra step before producing its output. In order to focus on the parts of the input that are relevant to this decoding time step, the&nbsp;<span style="font-style: inherit; font-weight: inherit; color: rgb(179, 106, 226);">decoder</span>&nbsp;does the following:</div><ol><li>Look at the set of encoder&nbsp;<span style="font-style: inherit; font-weight: inherit; color: rgb(243, 144, 25);">hidden states</span>&nbsp;it received – each&nbsp;encoder hidden state&nbsp;is most associated with a certain word in the input sentence</li><li>Give each&nbsp;hidden state&nbsp;a score</li><li>Multiply each&nbsp;hidden state&nbsp;by its softmaxed score, thus amplifying&nbsp;hidden states&nbsp;with high scores, and drowning out&nbsp;hidden states&nbsp;with low scores</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1702645185923 (Block 259) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it&nbsp;</span>creates its Queries matrix<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;</span>from the layer below it<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">, and takes the&nbsp;</span>Keys<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;and&nbsp;</span>Values<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;</span>matrix<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;from&nbsp;</span>the output of the encoder stack.
+
+============================================================
----------------------------

=== Note ID: 1702645233698 (Block 260) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div><ul><li>The decoder stack outputs a vector of floats. How do we turn that into a word?&nbsp;</li><li>That’s the job of the final Linear layer which is followed by a Softmax Layer.</li><li>The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.</li></ul></div>
+
+============================================================
----------------------------

=== Note ID: 1702651158497 (Block 261) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Continuing with our analogy, note that each complex number \(z\) except 0 can be written in the form<br><br><ul><li>\(z\)</li><li>=\((\frac{z}{|z|})\) \(|z|\)</li><li>=\((\frac{z}{|z|})\)&nbsp; \(\sqrt{\bar{z} z}\)</li></ul><br>where the first factor, namely, \(z /|z|\), is an element of the unit circle.<br>
+
+============================================================
----------------------------

=== Note ID: 1702651309182 (Block 262) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Notation \(\sqrt{T}\)<br><br>If \(T\) is a positive operator, then \(\sqrt{T}\) denotes the unique positive square root of \(T\).
+
+============================================================
----------------------------

=== Note ID: 1702651882859 (Block 263) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Consider the case \(\mathbf{F}=\mathbf{C}\), and suppose \(T=S \sqrt{T^{*} T}\) is a Polar Decomposition of an operator \(T \in \mathcal{L}(V)\), where \(S\) is an isometry.<br><ul><li>Then there is an orthonormal basis of \(V\) with respect to which \(S\) has a diagonal matrix, and there is an orthonormal basis of \(V\) with respect to which \(\sqrt{T^{*} T}\) has a diagonal matrix.&nbsp;</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702652061664 (Block 264) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition singular values<br><br>Suppose \(T \in \mathcal{L}(V)\). The singular values of \(T\) are the eigenvalues of \(\sqrt{T^{*} T}\), with each eigenvalue \(\lambda\) repeated \(\operatorname{dim} E\left(\lambda, \sqrt{T^{*} T}\right)\) times.
+
+============================================================
----------------------------

=== Note ID: 1702652445767 (Block 265) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       \subsection{Example Define \(T \in \mathcal{L}\left(\mathbf{F}^{4}\right)\) by}<br><br>\[<br>T\left(z_{1}, z_{2}, z_{3}, z_{4}\right)=\left(0,3 z_{1}, 2 z_{2},-3 z_{4}\right) .<br>\]<br><br>Find the singular values of \(T\).<br><br>Solution A calculation shows \(T^{*} T\left(z_{1}, z_{2}, z_{3}, z_{4}\right)=\left(9 z_{1}, 4 z_{2}, 0,9 z_{4}\right)\), as you should verify. Thus<br><br>\[<br>\sqrt{T^{*} T}\left(z_{1}, z_{2}, z_{3}, z_{4}\right)=\left(3 z_{1}, 2 z_{2}, 0,3 z_{4}\right),<br>\]<br><br>and we see that the eigenvalues of \(\sqrt{T^{*} T}\) are 3,2,0 and<br><br><ul><li>\(\operatorname{dim} E\) \(\left(3, \sqrt{T^{*} T}\right)\) = 2</li><li>\(\operatorname{dim} E\) \(\left(2, \sqrt{T^{*} T}\right)\)</li><li>=1</li><li>\(\operatorname{dim} E\) \(\left(0, \sqrt{T^{*} T}\right)\) =1.</li></ul><br>Hence the singular values of \(T\) are 3,3,2,0.<br><br>
+
+============================================================
----------------------------

=== Note ID: 1702654648908 (Block 266) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       4 Suppose \(T \in \mathcal{L}(V)\) and \(s\) is a singular value of \(T\). Prove that there exists:<br><ul><li>A vector \(v \in V\) such that:</li><ul><li>&nbsp;\(\|\) \(v\) \(\|\) = \(1\)&nbsp;</li><li>\(\|\) \(T\) \(v\|\) = \(s\).</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1702654746209 (Block 267) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       8 Suppose \(T \in \mathcal{L}(V), S \in \mathcal{L}(V)\) is an isometry, and \(R \in \mathcal{L}(V)\) is a positive operator such that \(T=S R\). Prove that \(R=\sqrt{T^{*} T}\).<br><br>[The exercise above shows that if we write \(T\) as the product of an isometry and a positive operator (as in the Polar Decomposition, then the positive operator equals \(\sqrt{T^{*} T}\).]
+
+============================================================
----------------------------

=== Note ID: 1702655602837 (Block 268) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       18 Suppose \(T \in \mathcal{L}(V)\). Let \(\hat{s}\) denote the smallest singular value of \(T\), and let \(s\) denote the largest singular value of \(T\).<br><ul><li>(b) Suppose \(\lambda\) is an eigenvalue of \(T\). Prove that \(\hat{s}\) \(\leq\) \(|\lambda|\) \(\leq\) \(s\).<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702838148958 (Block 269) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -8,3 +8,5 @@
 </div>
 </div>
 </div>
+
+============================================================
----------------------------

=== Note ID: 1702889665625 (Block 270) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Nesterov Accelerated Gradient formula:<br><ul><li>\(v_{t+1}\) = \(\mu v_t\) \(-\) \(\eta \nabla f\) \((\)\(\theta_t+\mu v_t\)\()\)<br></li><li>\(\theta_{t+1}\) = \(\theta_t\) \(+\) \(v_{t+1}\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702896614681 (Block 271) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>While local convergence is all that matters in terms of asymptotic convergence rates &nbsp;in practice, the "transient phase" of convergence seems to matter a lot more for optimizing deep neural networks.</li><li>In this transient phase of learning, directions of reduction in the objective tend to persist across many successive gradient estimates and are not completely swamped by noise.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702897146780 (Block 272) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div><ul><li>The convergence rate of stochastic gradient descent on smooth convex functions is given by \(O\) \((\)\(L / T+\sigma / \sqrt{T}\)\()\), where \(\sigma\) is the variance in the gradient estimate and \(L\) is the Lipshits coefficient of \(\nabla f\).&nbsp;</li><li>The convergence rate of an accelerated gradient method of Lan (2010) (which is related to but different from NAG, in that it combines Nesterov style momentum with dual averaging) is \(O\) \((\)\(L / T^2\)\(+\)\(\sigma / \sqrt{T}\)\()\).&nbsp;</li><li>Thus, for convex objectives, momentum-based methods will outperform SGD in the early/transient stages of the optimization where \(L / T\) is the dominant term.&nbsp;</li><li>However, the two methods will be equally effective during the final stages&nbsp;of the optimization where \(\sigma / \sqrt{T}\) is the dominant term (i.e., when the optimization problem resembles an estimation one).</li></ul></div>
+
+============================================================
----------------------------

=== Note ID: 1702899184266 (Block 273) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>The authors found it beneficial to reduce \(\mu\) during the final few parameter updates of the optimization without reducing the learning rate&nbsp;</li><li>They think that reducing the momentum coefficient allows for finer convergence to take place whereas otherwise the overly aggressive nature of Classical Momentum \(\mathrm{CM}\) or Nesterov Accelerated Gradient would prevent this</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702976691118 (Block 274) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ol><li>FedMOM Nesterov Accelerated Gradient:<ol><li>\(v_{t+1}\) = \(w_t\) - \(\eta\) \(\sum_{k=1}^K\) \(\frac{n_k}{n}\)\((\underbrace{w_t-w_{t+1}^k}_{\Delta \omega})\),</li><li>\(w_{t+1}\) = \(v_{t+1}\) + \(\beta\left(v_{t+1}-v_t\right)\)</li></ol></li></ol>
+
+============================================================
----------------------------

=== Note ID: 1702978099854 (Block 275) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -6,3 +6,5 @@
 <li>g is, therefore, an appropriate if biased direction towards the target point.</li>
 </ol>
 </li></ol>
+
+============================================================
----------------------------

=== Note ID: 1702978210220 (Block 276) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>The inner product of FedAvg with the optimal direction is generally larger than FedSGD.&nbsp;</li><li>FedAvg converges faster on training loss and testing loss and testing accuracy.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1702982734871 (Block 277) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -11,3 +11,5 @@
 <li>The parameter server averages gradients</li>
 </ol>
 </li></ol>
+
+============================================================
----------------------------

=== Note ID: 1702983014930 (Block 278) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -3,3 +3,5 @@
 <li>During this communication each worker sends and receives a chunk of the data buffer.</li>
 <li>In the first \(N-1\) iterations workers add received values to buffers</li>
 <li>In the last \(N-1\) iterations workers completely replace received values to buffers</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1702984883630 (Block 279) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -5,3 +5,5 @@
 <li>Execute the allreduce operation on the fusion buffer.</li>
 <li>Copy data from the fusion buffer into the output tensors.</li>
 <li>Repeat until there are no more tensors to reduce in the cycle.</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1703279061247 (Block 280) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Using a single GPU to aggregate gradients in data-parallel training requires O(N) communication where N is the number of GPUs since:<br><ul><li>Every worker has to send its gradients to the parameter server&nbsp;&nbsp;</li><li>Then the parameter server must send back the averaged gradient to all workers.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1703279251860 (Block 281) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -4,3 +4,5 @@
 <li>Each GPU should have a left neighbor and a right neighbor; it will only ever send data to its right neighbor, and receive data from its left neighbor.</li>
 </ol>
 </li></ol>
+
+============================================================
----------------------------

=== Note ID: 1703345171973 (Block 282) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Scatter-Reduce Phase:<ol><li>The GPUs will do N-1 iterations of the scatter-reduce; in every iteration:<ol><li>A GPU will send one of its chunks to its right neighbor,</li><li>It will then receive a chunk from its left neighbor and accumulate into that chunk.</li></ol></li><li>The chunk being sent and received by every GPU is different every iteration:<ol><li>The n-th GPU starts by sending chunk n and receiving chunk n - 1</li><li>It then proceeds backwards from there, each iteration sending the chunk it received in the previous iteration.</li></ol></li></ol>
+
+============================================================
----------------------------

=== Note ID: 1703345627892 (Block 283) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1,2 +1,4 @@
       <ol>
 </ol>The Allgather phase:<br><ol><li>After the scatter-reduce step is complete, every GPU has an array of values, and some of those values (one chunk per GPU) are the final values which include contributions from all the GPUs.</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1703345758533 (Block 284) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The Allgather phase:<br><ol><li>The nth GPU starts by sending the n+1th chunk and receiving the nth chunk, and then in future iterations always sends the chunk it has just received.</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1703433958415 (Block 285) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div><ul><li>At the start of the training phase, we create two matrices – an&nbsp;<code>Embedding</code>&nbsp;matrix and a&nbsp;<code>Context</code>&nbsp;matrix.&nbsp;</li><li>These two matrices have an embedding for each word in our vocabulary (So&nbsp;<code>vocab_size</code>&nbsp;is one of their dimensions). The second dimension is how long we want each embedding to be.</li></ul></div><br>
+
+============================================================
----------------------------

=== Note ID: 1703500765527 (Block 286) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -9,3 +9,5 @@
 </li>
 </ol>
 </li></ol>
+
+============================================================
----------------------------

=== Note ID: 1703501588911 (Block 287) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1,2 +1,4 @@
       Nexte sentence prediction involves:<br><ul><li>A simple binarized choice of weather :sentence B follows sentence A in the original text</li><ul><li>50% of the time B is the actual next sentence</li>
 <li>50% of the time B is a random sentence</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1703503047188 (Block 288) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The “next sentence prediction” task involves a binarized choice: determining if sentence B is the actual next sentence after A in the corpus. There's a 50% chance that B is the actual next sentence and a 50% chance it is a random sentence.
+
+============================================================
----------------------------

=== Note ID: 1703504726322 (Block 289) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -5,3 +5,5 @@
 <li>BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross-attention between two sentences.</li>
 </ul>
 </li></ul>
+
+============================================================
----------------------------

=== Note ID: 1703526117582 (Block 290) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -5,3 +5,5 @@
 <li>There is no consensus on the most effective way to transfer these learned representations to the target task.</li>
 </ol>
 </li></ol>
+
+============================================================
----------------------------

=== Note ID: 1703526137499 (Block 291) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The approach taken by GPT is semi-supervised, being composed of unsupervised pre-training via language modelling and and supervised fine-tuning.
+
+============================================================
----------------------------

=== Note ID: 1703526911103 (Block 292) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Transformer structure:<br><ul><li>\(h_0\) = \(U W_e\) \(+\) \(W_p\)<br></li><li>\(h_l\) = transformer_block(\(h_{l-1}\) )\(\forall i \in [1,n]\)<br></li><li>\(P(u)\) = softmax ( \( h_n, \mathbf{W}_e^t \) )<br></li><li>where \( U\) is the context vector of tokens, n is the number of layers</li><li>&nbsp;\(\mathbf{W}_e\) is the token embedding matrix</li><li>\(W_p\) is the position embedding matrix</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1703620610472 (Block 293) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -5,3 +5,5 @@
 <li>It effectively interpolates between word-level inputs for frequent symbol sequences and character-level inputs for infrequent symbol sequences.</li>
 </ol>
 </li></ol>
+
+============================================================
----------------------------

=== Note ID: 1703623911161 (Block 294) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li><span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">The simplest way to run a trained GPT-2 is to allow it to&nbsp;</span>generate unconditional samples</li><li><span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">Alternatively, we can give it a prompt&nbsp;to&nbsp;</span><span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">generate&nbsp;</span><em>interactive conditional samples</em></li><li><span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">In the generate unconditional samples&nbsp;case, we can simply hand it the start token and have it start generating words&nbsp;</span></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1703624004159 (Block 295) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">GPT-2 has a parameter called&nbsp;</span>top-k<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;that we can use to have the model&nbsp;</span>consider sampling words<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;</span>other than the top word<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;(which is the case when</span> top-k<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;=&nbsp;</span>1<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">).</span>
+
+============================================================
----------------------------

=== Note ID: 1703629974113 (Block 296) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>Query: The query is a representation of the current word used to score against all the other words (using their keys). We only care about the query of the token we’re currently processing.</li><li>Key: Key vectors are like labels for all the words in the segment. They’re what we match against in our search for relevant words.</li><li>Value: Value vectors are actual word representations, once we’ve scored how relevant each word is, these are the values we add up to represent the current word.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1703668003522 (Block 297) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       After applyinh self attention to one query&nbsp;<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">we end up with&nbsp;</span>a vector<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;representing&nbsp;</span>each token<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;containing&nbsp;</span>the appropriate context<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">&nbsp;of that&nbsp;</span>token<span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">.</span>
+
+============================================================
----------------------------

=== Note ID: 1703694592195 (Block 298) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       How GPT-2 optimizes attention for the inference step<br><ul><li>Avoid re-computing self-attention for previous tokens in light of new tokens</li><li>Storing the the query,key,and value of previous tokens at every single attention layer</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1703696597787 (Block 299) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li><span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">Each trnasformer block has</span> its own set of these weights.&nbsp;</li><li><span style="color: rgb(34, 34, 34); background-color: rgb(255, 255, 255);">On the other hand, the model has only one&nbsp;token embedding matrix&nbsp;and one&nbsp;</span>positional encoding matrix:</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1703768562592 (Block 300) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -5,3 +5,5 @@
 <li>If hooks of all gradients in the same buckets have fired, the last hook will trigger an asynchronous AllReduce on that bucket.</li>
 </ol>
 </li></ol>
+
+============================================================
----------------------------

=== Note ID: 1703768879258 (Block 301) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -4,3 +4,5 @@
 <li>Within the backward pass, the communication step takes more than half of the total delay and this is exacerbated with the increase of model size.</li>
 </ol>
 </li></ol>
+
+============================================================
----------------------------

=== Note ID: 1703860996939 (Block 302) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <span style="font-style:italic">ZeRO</span>-DP has three main optimization stages, which correspond to partitioning optimizer states, gradients and params:<br><ul><li><span style="font-weight:600">Optimizer State&nbsp;</span>Partitioning<br></li><li><span style="font-weight:600">Optimizer State&nbsp;+ Gradient&nbsp;Partitioning<br></span></li><li><span style="font-weight:600">Optimizer State&nbsp;+ Gradient&nbsp;+ Parameter&nbsp;Partitioning<br></span></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1703862505041 (Block 303) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -5,3 +5,5 @@
 <li>Similarly, in the backward computation, the activation gradients are short-lived, while the parameter gradients are long-lived.</li>
 </ol>
 </li></ol>
+
+============================================================
----------------------------

=== Note ID: 1703862710391 (Block 304) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       MP splits the model vertically, partitioning the computation and parameters in each layer across multiple devices, requiring significant communication between each layer
+
+============================================================
----------------------------

=== Note ID: 1704202466826 (Block 305) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -5,3 +5,5 @@
 <li>The authors observe that large models do not necessarily generalize better out-of-distribution</li>
 </ol>
 </li></ol>
+
+============================================================
----------------------------

=== Note ID: 1704202781951 (Block 306) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Few-shot can significantly reduce the need for task-specific data and reduce the potential to learn an overly narrow distribution from a large but narrow fine-tuning dataset.
+
+============================================================
----------------------------

=== Note ID: 1704203089694 (Block 307) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -4,3 +4,5 @@
 <li>The authors use the gradient noise scale during training and use it to guide the choice of batch size</li>
 </ol>
 </li></ol>
+
+============================================================
----------------------------

=== Note ID: 1704203106702 (Block 308) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       To train the larger models without running out of memory, GPT-3 uses a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network.
+
+============================================================
----------------------------

=== Note ID: 1704205571176 (Block 309) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -15,3 +15,5 @@
 </li>
 </ol>
 </li></ol>
+
+============================================================
----------------------------

=== Note ID: 1704205797750 (Block 310) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -4,3 +4,5 @@
 <li>One might worry that improvements in cross-entropy loss come only from modeling spurious details of the training corpus.</li><li>However, improvements in cross-entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks.<br></li>
 </ol>
 </li></ol>
+
+============================================================
----------------------------

=== Note ID: 1704210823537 (Block 311) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Human abilities to detect model-generated text decrease as model size increases: there appears to be a trend towards chance accuracy with model size, and human detection of GPT-3 is close to chance. This is true even though participants spend more time on each output as the model size increases
+
+============================================================
----------------------------

=== Note ID: 1704219020264 (Block 312) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Byte-pair encoding allows for the representation of an open vocabulary through a fixed-size vocabulary of variable-length character sequences.
+
+============================================================
----------------------------

=== Note ID: 1704277966602 (Block 313) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -4,3 +4,5 @@
 <li>They also show that subword systems are capable of transliteration.</li>
 </ol>
 </li></ol>
+
+============================================================
----------------------------

=== Note ID: 1704318805759 (Block 314) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>Offloading optimizer computation requires CPU to perform <span style="font-style:italic">O</span>(M) computation compared to <span style="font-style:italic">O</span>(MB) on GPU&nbsp;</li><li>where M&nbsp;and <span style="font-style:italic">B</span> are the model size and batch size.&nbsp;</li><li>In most cases, the batch size is large, and CPU computation is not a bottleneck, but for small batch sizes, the CPU compute can be a bottleneck.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1704319429751 (Block 315) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       2. Limiting CPU Computation:<br><br>The compute complexity of DL training per iteration is generally given by \(O(MB)\), where \(M\) is the model size and \(B\) is the effective batch size.<br><br>To prevent CPU computation from becoming a bottleneck, only those computations that have a compute complexity lower than \(O(MB)\) should be offloaded to the CPU.<br><br>Therefore, ZeRO-Offload does forward and backward propagation on the GPU and remaining computations that have a complexity of \(O(M)\) on the CPU.
+
+============================================================
----------------------------

=== Note ID: 1704319462595 (Block 316) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       7. Multi-GPU Scheduling:<br><br>Using ZeRO DP allows each DP process to update a subset of parameters, thus decreasing the CPU compute load linearly with the number of compute nodes.<br><br>ZeRO-Offload partitions gradients and optimizer states among different GPUs, and each GPU offloads the partition it owns to the CPU memory and keeps it there for the entire training.<br><br>During the backward propagation, gradients are computed and averaged using reduce-scatter on the GPU, and each GPU only offloads the averaged gradients belonging to its partition to the CPU memory.<br><br>Once the gradients are available on the CPU, optimizer state partitions are updated in parallel by each data parallel process directly on the CPU.
+
+============================================================
----------------------------

=== Note ID: 1704319467351 (Block 317) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       8. CPU Tricks:<br><br>To make CPU compute more efficient, ZeRO-Offload provides a highly optimized CPU-only ADAM.<br><br>This achieves a performance comparable to the PyTorch GPU ADAM.<br><br>Additionally, it adds a one-step delayed parameter update to overlap CPU and GPU compute more for small-batch settings where the CPU may still be a bottleneck.<br><br>They show this has little impact on accuracy.
+
+============================================================
----------------------------

=== Note ID: 1704319639505 (Block 318) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       4. The original ZeRO DP approach has three stages, ZeRO-1, ZeRO-2, and ZeRO-3, corresponding to the partitioning of the three different model states, optimizer states, gradients, and parameters, respectively. ZeRO-1 partitions the optimizer states only, while ZeRO-2 partitions gradients in addition to optimizer states, and ZeRO-3 partitions all model states.<br><br>ZeRO-Offload works symbiotically with ZeRO-2.
+
+============================================================
----------------------------

=== Note ID: 1704319810097 (Block 319) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       10. Notice that any partitioning strategy that does not co-locate the fp32 model states with its producer and consumer nodes cannot achieve the minimum communication volume of 4*M. Such a partition must cut at least one edge with a weight of 4*M, and the other with at least 2*M, resulting in a communication volume of at least 6*M.
+
+============================================================
----------------------------

=== Note ID: 1704319868500 (Block 320) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       13. **Delayed-Parameter Updates training schedule**<br><br>1) The first *N-1* steps are trained without DPU to avoid destabilizing the training during the early stages where gradients change rapidly. 2) On step *N*, we obtain the gradients from the GPU, but we skip the CPU optimizer step, and do not update the fp16 parameters on the GPU either. 3) At step *N+1*, we compute the parameter updates on the CPU using gradients from step *N*, while computing the forward and backward pass on the GPU in parallel using parameters updated at step *N-1*. From this step onwards, the model at \((i+1)^{th}\) step will be trained using the parameters updated with gradients from \((i-1)^{th}\) step instead of parameters updated at \(i^{th}\) step, overlapping CPU compute with GPU compute.
+
+============================================================
----------------------------

=== Note ID: 1704320011332 (Block 321) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -6,3 +6,5 @@
 <li>Thus, ZeRO-Offload does forward and backward prop on the GPU and remaining computations that have a complexity of O(M) on the CPU.</li>
 </ol>
 </li></ol>
+
+============================================================
----------------------------

=== Note ID: 1704322057075 (Block 322) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -14,3 +14,5 @@
 </li>
 </ol>
 </li></ol>
+
+============================================================
----------------------------

=== Note ID: 1704322305869 (Block 323) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -6,3 +6,5 @@
 <li>During the backward propagation, gradients are computed and averaged using reduce-scatter on the GPU, and each GPU only offloads the averaged gradients belonging to its partition to the CPU memory.</li>
 </ol>
 </li></ol>
+
+============================================================
----------------------------

=== Note ID: 1704453331549 (Block 324) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Pipeline parallelism partitions a model instance into stages and distributes stages across multiple devices, where activations and gradients are communicated across stage boundaries.
+
+============================================================
----------------------------

=== Note ID: 1704454535294 (Block 325) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       During forward and backward computation, FSDP only materializes unsharded parameters and gradients of one unit at a time, and otherwise, it keeps parameters and gradients sharded together with the optimizer states.
+
+============================================================
----------------------------

=== Note ID: 1704468497677 (Block 326) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li><span style="color: rgb(53, 55, 64);">Card 5: Full Sharding's {c1::NCCL efficiency}} is impacted by factors like input size, with larger&nbsp;inputs and even-sized inputs across ranks yielding higher efficiency measures in collective communications such as all-gather and reduce-scatter, as shown in experiments detailed in a study.</span></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1704473266154 (Block 327) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>For \(\Psi\) number of parameter elements , \(K_{\text {low }}\) bytes per low precision element, and \(K_{\text {full }}\) bytes per full precision element, this approach to mixed precision normally increases the memory overhead from \(K_{\text {full }} \Psi\) to ( \(\left.K_{\text {low }}+K_{\text {full }}\right) \Psi\) due to maintaining both precision copies.<br></li><li>For \(N\) FlatParameters with numels given by \(\psi_1, \ldots, \psi_N\), the parameter peak memory contribution for FSDP actually decreases from:</li><ul><li>&nbsp;\(\frac{K_{\text {full } } }{F} \sum_{i=1}^N \psi_i\) &nbsp;+ \(K_{\text {full } } \max {i=1}^N \psi_i \) &nbsp;</li></ul><li>to<br></li><ul><li>\(\frac{K{\text {full } } }{F} \sum_{i=1}^N \psi_i\) &nbsp;+ \(K_{\text {low } }&nbsp; \max {i=1}^N \psi_i\) &nbsp;bytes.&nbsp;</li></ul><li>In other words, FSDP directly reduces the second \(K{\text {full } } \max {i=1}^N \psi_i\) term to \(K{\text {low } } \max _{i=1}^N \psi_i\).<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1704474558237 (Block 328) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -12,3 +12,5 @@
 overhead.</li></ol></li><ol>
 </ol>
 </ol>
+
+============================================================
----------------------------

=== Note ID: 1704478354394 (Block 329) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ol><li><div><strong>Estimation of Current Data Stocks:</strong>&nbsp;The paper presents aggregated models to estimate the current total stock of language and image data. The range of growth rates is quite wide, indicating uncertainty in the long-term availability of such data.</div></li></ol>
+
+============================================================
----------------------------

=== Note ID: 1704709798900 (Block 330) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ol><li><div>Distributed Mini-batch SGD is shown to suffer from generalization issues compared to local SGD or single-machine SGD even when using the same local batch sizes as local SGD.</div></li></ol>
+
+============================================================
----------------------------

=== Note ID: 1704709830560 (Block 331) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ol><li><div>In direct comparison, post-local SGD is more communication-efficient than mini-batch SGD (while less than local SGD) and achieves better generalization performance than both.</div></li></ol>
+
+============================================================
----------------------------

=== Note ID: 1704710496774 (Block 332) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <span style="color: rgb(53, 55, 64);">For post-local SGD, local SGD is only started in the&nbsp;</span>second phase<span style="color: rgb(53, 55, 64);">&nbsp;of training, after (t) initial steps with&nbsp;</span>standard mini-batch SGD.
+
+============================================================
----------------------------

=== Note ID: 1704710539589 (Block 333) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <span style="color: rgb(53, 55, 64);">Tthe&nbsp;</span>effective batch size<span style="color: rgb(53, 55, 64);">&nbsp;of Post-local SGD scales from (B) = (B_{loc}) in the&nbsp;</span>initial<span style="color: rgb(53, 55, 64);">&nbsp;phase to (B) = (H B_{loc})</span>
+
+============================================================
----------------------------

=== Note ID: 1704710678046 (Block 334) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ol><li><div>In direct comparison, post-local SGD is more communication-efficient than mini-batch SGD (while less than local SGD) and achieves better generalization performance than both.</div></li></ol>
+
+============================================================
----------------------------

=== Note ID: 1704710760876 (Block 335) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ol><li><div>For post-local SGD, local SGD is only started in the second phase of training, after t initial steps with standard mini-batch SGD.</div><ol><li>Thus the effective batch size scales from \(B\) = \(B_{loc}\) in the initial phase to \(B\) = \(H B_{loc}\).</li><li>It is crucial to use local SGD in the second phase, as e.g. just resorting to large batch training does achieve worse performance.</li></ol></li></ol>
+
+============================================================
----------------------------

=== Note ID: 1704722555507 (Block 336) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       {c1::Tensor}} parallelism is typically used within a single multi-GPU server or closely interconnected TPU cores.
+
+============================================================
----------------------------

=== Note ID: 1704722725271 (Block 337) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -5,3 +5,5 @@
 <li>Fine-tuning requires updating either all of the model's parameters or a small set of trainable weights (e.g., adapters or soft prompts).</li>
 </ol>
 </li></ol>
+
+============================================================
----------------------------

=== Note ID: 1704725643705 (Block 338) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Both re-starting on every network and node failure and recomputing attention caches on every node struggle to scale to long sequences.
+
+============================================================
----------------------------

=== Note ID: 1704730054470 (Block 339) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -5,3 +5,5 @@
 <li>Client-side cache&nbsp;holds past inputs sent to a given pipeline stage.</li>
 </ol>
 </li></ol>
+
+============================================================
----------------------------

=== Note ID: 1704730260544 (Block 340) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -5,3 +5,5 @@
 <li>These queues are maintained through the lifetime of a client.</li>
 </ol>
 </li></ol>
+
+============================================================
----------------------------

=== Note ID: 1704735048148 (Block 341) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If a server detects that reblancing would improve throughput, they switch layers until the throughput becomes near-optimal. In particular, if all peers serving certain blocks suddenly leave the system, this procedure quickly redistributes the remaining resources to close the gaps that have emerged.
+
+============================================================
----------------------------

=== Note ID: 1704736441336 (Block 342) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Before joining for the first time, each server measures its Internet connection throughput (in tokens/second, using one of public web APIs for doing that) and GPU throughput (in tokens/second, using a small benchmark running several forward passes). The minimum of these values becomes the overall server throughput, which is then cached for future runs.
+
+============================================================
----------------------------

=== Note ID: 1706082635686 (Block 343) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       4. Suppose \(P \in \mathcal{L}(V)\) is such that \(P^{2}=P\). Prove that \(P\) is an orthogonal projection if and only if \(P\) is self-adjoint.<br><br><ul><li>Suppose that P is an orthogonal projection</li><li>Thus we can make an orthogonal decomposition and</li><ul><li>\(v_1\) =&nbsp;\(u_1 + w_1\)</li><li>\(v_2\) =&nbsp;\(u_2 + w_2\)</li></ul><li>Then \(\langle\) \(P v_1,v_2\) \(\rangle\) =&nbsp;</li><ul><li>&nbsp;\(\langle\)&nbsp;\(u_1, u_2 + w_2\) \(\rangle\) by orthogonal projection</li><li>&nbsp;\(\langle\)&nbsp;\(u_1, u_2\) \(\rangle\) +&nbsp; \(\langle\)&nbsp;\(u_1, w_2\) \(\rangle\) by linearity&nbsp;</li><li>&nbsp;\(\langle\)&nbsp; \(u_1,u_2\) \(\rangle\) by orthogonal complement</li><li>&nbsp;\(\langle\)&nbsp; \(u_1,u_2\) \(\rangle\) +&nbsp;&nbsp;\(\langle\)&nbsp; \(w_1,u_2\) \(\rangle\) by orthogonal complement in the other direction<br></li><li>&nbsp;\(\langle\)&nbsp; \(u_1 + w_1,u_2\) \(\rangle\) by linearity<br></li><li>&nbsp;\(\langle\)&nbsp; \(v_1 P v_2\) \(\rangle\) by orthognal projection<br></li></ul><li>Thus&nbsp;\(P\) =&nbsp;\(P*\) and hence P is self-adjoint</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1706084748658 (Block 344) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       8 Suppose \(T \in \mathcal{L}(V), S \in \mathcal{L}(V)\) is an isometry, and \(R \in \mathcal{L}(V)\) is a positive operator such that \(T=S R\). Prove that \(R=\sqrt{T^* T}\).<br>[The exercise above shows that if we write \(T\) as the product of an isometry and a positive operator (as in the Polar Decomposition 7.45), then the positive operator equals \(\sqrt{T^* T}\).]<br><br>Proof:<br><ul><li>\(T = SR\) implies&nbsp;\(R\) =&nbsp;\(S^* T\) since isometries are their own inverses<br></li><li>||&nbsp;\(Rv\) ||\(^2\) =&nbsp; ||&nbsp;\(S^* Tv\) ||\(^2\) =&nbsp;</li><ul><li>\(\langle\)&nbsp;\(S^* Tv, S^* T\) \(\rangle\) by definition of squared norm<br></li><li>\(\langle\)&nbsp;\(Tv, Tv\) \(\rangle\) since isometries and their adjoints preserver inner products</li><li>\(\langle\)&nbsp;\(T^* Tv, v\) \(\rangle\) by definition of adjoint</li><li>\(\langle\)&nbsp;&nbsp;\(\sqrt{T^* Tv} \sqrt{T^* Tv}, v\)&nbsp;\(\rangle\) by definition of square root</li><li>|| \(\sqrt{T^* Tv}&nbsp;\) ||\(^2\)</li></ul><li>Which implies that&nbsp;\(R\) = \(\sqrt{T^* Tv}&nbsp;\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1706167648563 (Block 345) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       1. (6 points) Suppose that \(V\) is a complex inner product space with orthogonal basis \(\left(f_{1}, \ldots, f_{n}\right)\), and \(T \in \mathcal{L}(V)\).<br><br>a) Prove that any vector \(v \in V\) can be written<br><br>\[<br>v=\sum_{i=1}^{n} \frac{\left\langle v, f_{i}\right\rangle}{\left\langle f_{i}, f_{i}\right\rangle} f_{i}<br>\]<br><br>Proof:<br><ul><li>First, take inner product with&nbsp;\(f_j\)&nbsp;and use the fact that \(\langle\) \(f_i, f_j\) \(\rangle\) = \(0\) for \(i \neq j\).</li><ul><li>\(\langle\) \(v, f_j\) \(\rangle\) =&nbsp;\(a_j\) \(\langle\)&nbsp;\(f_j, f_j\) \(\rangle\)<br></li></ul><li>Now, because \(\langle\)&nbsp;\(f_j, f_j\) \(\rangle\) is guaranteed to be nonzero due to basis vector we can rearrange the equation into</li><ul><li>\(a_j\) =&nbsp;\(\frac{\left\langle v, f_j\right\rangle}{\left\langle f_j, f_j\right\rangle}\),<br></li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1706169059472 (Block 346) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       19 Suppose \(V\) is a real inner product space. Prove that<br>\[<br>\langle u, v\rangle=\frac{\|u+v\|^2-\|u-v\|^2}{4}<br>\]<br>for all \(u, v \in V\).<br><br>What is the trick?<br><ul><li>Expand until you get (\( 2\) \(\langle\) u,v \(\rangle\) + \(2\) \(\langle\) v, u \(\rangle\) )&nbsp;\(/ 4\)&nbsp;</li><li>And then use the fact that the vector space is real to conclude that&nbsp;\(\langle\)&nbsp;\(v,u\) \(\rangle\)&nbsp; = \(\langle\)&nbsp;\(u,v\) \(\rangle\)&nbsp;</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1706281373927 (Block 347) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       30. Suppose \(T \in \mathcal{L}(V, W)\). Prove that<br><br>(a) \(T\) is injective if and only if \(T^{*}\) is surjective;<br><br><ul><li>\(T\) is injective \(\Longleftrightarrow\) null \(T\)=\(\{0\}\)<br></li><li>\(\Longleftrightarrow\)&nbsp;\(\left(\text { range } T^*\right)^{\perp}\) = \(\{0\}\)<br></li><li>\(\Longleftrightarrow\)&nbsp;\(\operatorname{range} T^*\) = \(W\)<br></li><li>\(\Longleftrightarrow\)&nbsp;\(T^*\) is surjective<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1706287270553 (Block 348) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-ddc1d44cb2a70885dac2caf27b8c6caa23806516.jpg"><br>Why do we understand both operators in the polar decomposition pretty well?<br><ul><li>The second operator&nbsp;\(\sqrt{T^* T}\). is a positive opeartor thus it is self-adjoint and normal meaning that it is also diagonalizable i.e a basis of orthonormal eigenvectors exists for it</li><li>The first is an isometry, so over C since all isometries are normal then S is also diagonalizable by the spectral theorem</li><li>Warning: the orthonormal eigenbasse of the two operators are&nbsp; not guaranteed to be the same.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1706685486391 (Block 349) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-6594a741e8f9def1ba601b6671b48342832c0e5e.jpg"><br>Proof:<br><ul><li>By the Spectral Theorem, there is an orthonormal basis \(e_1, \ldots, e_n\) of \(V\) such that:</li><ul><li>&nbsp;\(\sqrt{T^* T} e_j\) = \(s_j e_j\) for \(j=1, \ldots, n\).</li><li>the associated eigenvalues of this are precisely the singular values by definitiojn</li></ul><li>Write&nbsp;\(v\) =&nbsp;\(\sum\)&nbsp;\(\langle\)\(v,e_i\)&nbsp;\(\rangle\) \(e_i\)</li><li>Apply \(\sqrt{T^* T}\) to both sides of this equation, getting<br></li><ul><li>\(\sqrt{T^* T}v\) = \(\sum\)&nbsp;\(\langle\)\(v,e_i\)&nbsp;\(\rangle\) \(\sqrt{T^* T}\)\(e_i\) by linearity<br></li><li>= \(\sum\)&nbsp;\(s_i\)\(\langle\)\(v,e_i\)&nbsp;\(\rangle\)&nbsp;\(e_i\) by definition</li></ul><li>By the polar decomposition there exists an isometry S such that&nbsp;\(T\) =&nbsp;\(S \sqrt{T^* T}\).</li><li>Apply S to both sides of the equation above to get</li><ul><li>\(T\) = \(\sum\)&nbsp;\(s_i\)\(\langle\)\(v,e_i\)&nbsp;\(\rangle\)&nbsp;\(S\)\(e_i\)&nbsp;<br></li></ul><li>For each j let&nbsp;\(f_i\) =&nbsp;\(S e_i\), this is an orthonormal basis since S is an isometry</li><li>The equation now becomes</li><ul><li>\(T\) = \(\sum\)&nbsp;\(s_i\)\(\langle\)\(v,e_i\)&nbsp;\(\rangle\)&nbsp;\(f_i\)</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1706685649536 (Block 350) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-6594a741e8f9def1ba601b6671b48342832c0e5e.jpg"><br>Proof:<br><ul><li>By the Spectral Theorem, there is an orthonormal basis \(e_1, \ldots, e_n\) of \(V\) such that:</li><ul><li>&nbsp;\(\sqrt{T^* T} e_j\) = \(s_j e_j\) for \(j=1, \ldots, n\).</li><li>the associated eigenvalues of this are precisely the singular values by definitiojn</li></ul><li>Write&nbsp;\(v\) =&nbsp;\(\sum\)&nbsp;\(\langle\)\(v,e_i\)&nbsp;\(\rangle\) \(e_i\)</li><li>Apply \(\sqrt{T^* T}\) to both sides of this equation, getting<br></li><ul><li>\(\sqrt{T^* T}v\) = \(\sum\)&nbsp;\(\langle\)\(v,e_i\)&nbsp;\(\rangle\) \(\sqrt{T^* T}\)\(e_i\) by linearity<br></li><li>= \(\sum\)&nbsp;\(s_i\)\(\langle\)\(v,e_i\)&nbsp;\(\rangle\)&nbsp;\(e_i\) by definition</li></ul><li>By the polar decomposition there exists an isometry S such that&nbsp;\(T\) =&nbsp;\(S \sqrt{T^* T}\).</li><li>Apply S to both sides of the equation above to get</li><ul><li>\(T\) = \(\sum\)&nbsp;\(s_i\)\(\langle\)\(v,e_i\)&nbsp;\(\rangle\)&nbsp;\(S\)\(e_i\)&nbsp;<br></li></ul><li>For each j let&nbsp;\(f_i\) =&nbsp;\(S e_i\), this is an orthonormal basis since S is an isometry</li><li>The equation now becomes</li><ul><li>\(T\) = \(\sum\)&nbsp;\(s_i\)\(\langle\)\(v,e_i\)&nbsp;\(\rangle\)&nbsp;\(f_i\)</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1706687074577 (Block 351) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Sequence of increasing null spaces<br><br>Suppose \(T \in \mathcal{L}(V)\). Then<br><br><ul><li>\(\{0\}\) = \(\operatorname{null} T^{0}\) \(\subset\) \(\text { null } T^{1}\) \(\subset\) \(\cdots\) \(\subset\) \(\text { null } T^{k}\) \(\subset\) \(\text { null } T^{k+1}\) \(\subset\) \(\cdots .\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1706687244135 (Block 352) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-b8c98635f068d0821371de190b579e04dea816cb.jpg"><br><span style="color: rgb(0, 0, 0);">Proof:</span><br><ul><li>Proof Suppose \(k\) is a nonnegative integer and \(v \in \operatorname{null} T^{k}\).&nbsp;</li><li>Then \(T^{k} v\) = \(0\), and hence&nbsp;</li><ul><li>\(T^{k+1} v\) = \(T\left(T^{k} v\right) \)= \(T(0)\)= \(0\).&nbsp;</li></ul><li>Thus \(v \in\) \(\operatorname{null} T^{k+1}\).&nbsp;</li><li>Hence null \(T^{k} \subset\) null \(T^{k+1}\), as desired.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1706687384414 (Block 353) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Equality in the sequence of null spaces<br><br>Suppose \(T \in \mathcal{L}(V)\). Suppose \(m\) is a nonnegative integer such that null \(T^{m}\) = \(\operatorname{null} T^{m+1}\). Then<br><br><ul><li>\(\operatorname{null} T^{m}\) = \(\operatorname{null} T^{m+1}\) = \(\operatorname{null} T^{m+2}\) = \(\operatorname{null} T^{m+3}\) = \(\cdots .\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1706687590895 (Block 354) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Null spaces stop growing<br><br>Suppose \(T \in \mathcal{L}(V)\). Let \(n\) = \(\operatorname{dim} V\). Then<br><br><ul><li>\(\operatorname{null} T^{n}\) = \(\operatorname{null} T^{n+1}\) = \(\operatorname{null} T^{n+2}\) = \(\cdots .\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1706687730510 (Block 355) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Why is this true?<br><img src="paste-40474fac57ce81a59681cd605a5adf81d42817d1.jpg"><br>Because:<br><ul><li>Null spaces of increasing powers of an operator are contained within the null space of the larger power</li><li>Null spaces are subspaces&nbsp;</li><li>A subspaces cannot have dimension larger than the vector space so the growth must stop</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1706687909107 (Block 356) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       \(V\) is the direct sum of null \(T^{\operatorname{dim} V}\) and \(\operatorname{range} T^{\operatorname{dim} V}\)<br><br>Suppose \(T \in \mathcal{L}(V)\). Let \(n\) = \(\operatorname{dim} V\). Then<br><br><ul><li>\(V\) = \(\operatorname{null} T^{n}\) \(\oplus\) \(\text { range } T^{n} .\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1706688232781 (Block 357) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition generalized eigenvector<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(\lambda\) is an eigenvalue of \(T\). A vector \(v \in V\) is called a generalized eigenvector of \(T\) corresponding to \(\lambda\) if \(v\) \(\neq\) \(0\)&nbsp;and<br><br><ul><li>\((T-\lambda I)^{j}\) \(v\) = \(0\)</li></ul><br>for some positive integer \(j\).<br>
+
+============================================================
----------------------------

=== Note ID: 1706688302939 (Block 358) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>We do not define the concept of a generalized eigenvalue, because this would not lead to anything new.&nbsp;</li><li>Reason: if \((T-\lambda I)^j\) is not injective for some positive integer \(j\), then \(T-\lambda I\) is not injective, and hence \(\lambda\) is an eigenvalue of \(T\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1706688406713 (Block 359) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition generalized eigenspace, \(G(\lambda, T)\)<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(\lambda \in \mathbf{F}\). The generalized eigenspace of \(T\) corresponding to \(\lambda\), denoted \(G(\lambda, T)\), is defined to be the set of all generalized eigenvectors of \(T\) corresponding to \(\lambda\), along with the 0 vector.
+
+============================================================
----------------------------

=== Note ID: 1706688911201 (Block 360) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Linearly independent generalized eigenvectors<br><br>Let \(T \in \mathcal{L}(V)\). Suppose \(\lambda_{1}, \ldots, \lambda_{m}\) are distinct eigenvalues of \(T\) and \(v_{1}, \ldots, v_{m}\) are corresponding generalized eigenvectors. Then \(v_{1}, \ldots, v_{m}\) is linearly independent.
+
+============================================================
----------------------------

=== Note ID: 1706770183244 (Block 361) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-e9899024693e7c56ae0e87d2ec9f8b6081e05ae0.jpg"><br>Proof Suppose \(a_{1}, \ldots, a_{m}\) are complex numbers such that<br><br>8.14<br><br>\[<br>0=a_{1} v_{1}+\cdots+a_{m} v_{m} .<br>\]<br><br><ul><li>Let \(k\) be the largest nonnegative integer such that:</li><ul><li>&nbsp;\(\left(T-\lambda_{1} I\right)^{k}\) \(v_{1}\)&nbsp; \(\neq\) \(0\).&nbsp;</li></ul><li>Let \(w\) = \(\left(T-\lambda_{1} I\right)^{k}\) \(v_{1} .\)</li></ul><br>Thus<br><br><ul><li>\(\left(T-\lambda_{1} I\right)\) \(w\) = \(\left(T-\lambda_{1} I\right)^{k+1}\) \(w\) = \(0,\)</li></ul><br>and hence \(T w\) = \(\lambda_{1} w\). Thus \((T-\lambda I) w=\left(\lambda_{1}-\lambda\right) w\) for every \(\lambda \in \mathbf{F}\) and hence<br><br><ul><li>\((T-\lambda I)^{n} w=\left(\lambda_{1}-\lambda\right)^{n} w\)</li></ul><br>for every \(\lambda \in \mathbf{F}\), where \(n=\operatorname{dim} V\).<br><br>Apply the operator<br><br>\(\left(T-\lambda_{1} I\right)^{k}\) \(\left(T-\lambda_{2} I\right)^{n}\) \(\cdots\) \(\left(T-\lambda_{m} I\right)^{n}\)<br><br>to both sides of 8.14 , getting<br><br><ul><li>0 = \(a_1\left(T-\lambda_1 I\right)^k\left(T-\lambda_2 I\right)^n \cdots\left(T-\lambda_m I\right)^n v_1\)</li><li>=&nbsp;\(a_1\left(T-\lambda_2 I\right)^n \cdots\left(T-\lambda_m I\right)^n w\)</li><li>&nbsp;=&nbsp;\(a_1\left(\lambda_1-\lambda_2\right)^n \cdots\left(\lambda_1-\lambda_m\right)^n w\),</li></ul>where we have used 8.11 to get the first equation above and 8.15 to get the last equation above.<br><br>The equation above implies that \(a_{1}=0\). In a similar fashion, \(a_{j}=0\) for each \(j\), which implies that \(v_{1}, \ldots, v_{m}\) is linearly independent.<br>
+
+============================================================
----------------------------

=== Note ID: 1706770722152 (Block 362) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-e9899024693e7c56ae0e87d2ec9f8b6081e05ae0.jpg"><br>Proof Suppose \(a_{1}, \ldots, a_{m}\) are complex numbers such that<br><br>8.14<br><br>\[<br>0=a_{1} v_{1}+\cdots+a_{m} v_{m} .<br>\]<br><br><ul><li>Let \(k\) be the largest nonnegative integer such that:</li><ul><li>&nbsp;\(\left(T-\lambda_{1} I\right)^{k} v_{1}\)&nbsp; \(\neq\) \(0\).&nbsp;</li></ul><li>Let \(w\) = \(\left(T-\lambda_{1} I\right)^{k} v_{1} .\)</li></ul><br>Thus<br><br><ul><li>\(\left(T-\lambda_{1} I\right) w\) = \(\left(T-\lambda_{1} I\right)^{k+1} w\) = \(0,\)</li></ul><br>Hence:<br><ul><li>\(T w\) = \(\lambda_{1} w\).&nbsp;</li><li>Thus:</li><ul><li>&nbsp;\((T-\lambda I)\) \(w\) = \(\left(\lambda_{1}-\lambda\right) w\) for every \(\lambda \in \mathbf{F}\)&nbsp;</li></ul><li>&nbsp;hence</li><ul><li>\((T-\lambda I)^{n} w=\left(\lambda_{1}-\lambda\right)^{n} w\)</li><li>for every \(\lambda \in \mathbf{F}\), where \(n=\operatorname{dim} V\).</li></ul></ul><br>Apply the operator<br><ul><li>\(\left(T-\lambda_{1} I\right)^{k}\) \(\left(T-\lambda_{2} I\right)^{n}\) \(\cdots\) \(\left(T-\lambda_{m} I\right)^{n}\)</li></ul><br>to both sides of 8.14 , getting<br><br><ul><li>0 = \(a_1\)\(\left(T-\lambda_1 I\right)^k\left(T-\lambda_2 I\right)^n \cdots\left(T-\lambda_m I\right)^n\) \(v_1\)</li><li>=&nbsp;\(a_1\) \(\left(T-\lambda_2 I\right)^n \cdots\left(T-\lambda_m I\right)^n\) \(w\)</li><li>&nbsp;=&nbsp;\(a_1\) \(\left(\lambda_1-\lambda_2\right)^n \cdots\left(\lambda_1-\lambda_m\right)^n\) \(w\),</li></ul>where we have used 8.11 to get the first equation above and 8.15 to get the last equation above.<br><br>The equation above implies that \(a_{1}\) = \(0\). In a similar fashion, \(a_{j}\) = \(0\) for each \(j\), which implies that \(v_{1}, \ldots, v_{m}\) is linearly independent.<br>
+
+============================================================
----------------------------

=== Note ID: 1706771367335 (Block 363) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Matrix of a nilpotent operator<br><br>Suppose \(N\) is a nilpotent operator on \(V\). Then there is a basis of \(V\) with respect to which the matrix of \(N\) has the form<br><br>\[<br>\left(\begin{array}{ccc}<br>0 &amp; &amp; * \\<br>&amp; \ddots &amp; \\<br>0 &amp; &amp; 0<br>\end{array}\right) \text {; }<br>\]<br><br>here all entries on and below the diagonal are 0's.
+
+============================================================
----------------------------

=== Note ID: 1706771853459 (Block 364) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-4c50ecf5917708940cd50b108f4f45d3b1fc9680.jpg"><br><br>Proof:<br><ul><li>First choose a basis of null \(N\).&nbsp;</li><li>Then extend this to a basis of null \(N^{2}\), then null \(N^{3}\), eventually getting a basis of \(V\)&nbsp;</li><li>Because 8.18 states that null \(N^{\operatorname{dim} V}\) = \(V\) .</li></ul><br>Now let's think about the matrix of \(N\) with respect to this basis. :<br><ul><li>The first column, and perhaps additional columns at the beginning, consists of all 0 's, because the corresponding basis vectors are in null \(N\).&nbsp;</li><li>The next set of columns comes from basis vectors in null \(N^{2}\).&nbsp;</li><li>Applying \(N\) to any such vector, we get a vector in null \(N\); in other words, we get a vector that is a linear combination of the previous basis vectors.&nbsp;</li><li>Thus all nonzero entries in these columns lie above the diagonal.&nbsp;</li><li>Repeat like this&nbsp;</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1706771976297 (Block 365) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       3 Suppose \(T \in \mathcal{L}(V)\) is invertible. Prove that \(G\)( \(\lambda, T)\) ) = \(G\) ( \(\frac{1}{\lambda}, T^{-1}\) ) for every \(\lambda \in \mathbf{F}\) with \(\lambda \neq 0\).
+
+============================================================
----------------------------

=== Note ID: 1706772110894 (Block 366) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       4 Suppose \(T \in \mathcal{L}(V)\) and \(\alpha, \beta\)&nbsp; \(\in\) \(\mathbf{F}\) with \(\alpha \neq \beta\). Prove that<br><br><ul><li>G( \(\alpha, T\) ) \(\cap\) G( \(\beta, T\) ) = \(\{0\} .\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1706772311428 (Block 367) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       5 Suppose \(T \in \mathcal{L}(V), m\) is a positive integer, and \(v \in V\) is such that \(T^{m-1}\) \(v\) \(\neq\) \(0\) but \(T^{m}\) \(v\)= \(0\). Prove that<br><br><ul><li>\(v\), \(T v\), \(T^{2} v\), \(\ldots\), \(T^{m-1} v\)</li></ul><br>is linearly independent.<br>
+
+============================================================
----------------------------

=== Note ID: 1706772369902 (Block 368) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       9 Suppose \(S, T \in \mathcal{L}(V)\) and \(S T\) is nilpotent. Prove that \(T S\) is nilpotent.
+
+============================================================
----------------------------

=== Note ID: 1706772660862 (Block 369) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       14 Suppose \(V\) is an inner product space and \(N \in \mathcal{L}(V)\) is nilpotent. Prove that there exists an orthonormal basis of \(V\) with respect to which \(N\) has an upper-triangular matrix.<br><br>[If \(F\) = \(\mathbf{C}\), then the result above follows from Schur's Theorem (6.38) without the hypothesis that \(N\) is nilpotent. Thus the exercise above needs to be proved only when \(\mathbf{F}\) = \(\mathbf{R}\).]
+
+============================================================
----------------------------

=== Note ID: 1706772890415 (Block 370) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       15 Suppose \(N \in \mathcal{L}(V)\) is such that:<br><ul><li>&nbsp;null \(N^{\operatorname{dim} V-1}\) \(\neq\) \(\operatorname{null}\) \(N^{\operatorname{dim} V}\).&nbsp;</li></ul>Prove that \(N\) is nilpotent and that<br><ul><li>\(\operatorname{dim}\) \(\operatorname{null} N^{j}\) = \(j\)</li></ul><br>for every integer \(j\) with \(0\) \(\leq\) \(j\) \(\leq\) \(\operatorname{dim} V\).<br>
+
+============================================================
----------------------------

=== Note ID: 1706773019395 (Block 371) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       16 Suppose \(T \in \mathcal{L}(V)\). Show that<br><br><ul><li>V = \(\operatorname{range} T^{0}\) \(\supset\) \(\text { range } T^{1}\) \(\supset\) \(\cdots\) \(\supset\) \(\text { range } T^{k}\) \(\supset\) \(\text { range } T^{k+1}\) \(\supset\) \(\cdots\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1706773127832 (Block 372) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       17 Suppose \(T \in \mathcal{L}(V)\) and \(m\) is a nonnegative integer such that<br><br><ul><li>\(\text { range }\) \(T^{m}\) = \(\operatorname{range}\) \(T^{m+1} \text {. }\)</li></ul><br>Prove that range \(T^{k}\) = \(\operatorname{range}\) \(T^{m}\) for all \(k&gt;m\).<br>
+
+============================================================
----------------------------

=== Note ID: 1706773225413 (Block 373) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       18 Suppose \(T \in \mathcal{L}(V)\). Let \(n\) = \(\operatorname{dim} V\). Prove that<br><br><ul><li>\(\text { range }\) \(T^{n}\) = \(\operatorname{range}\) \(T^{n+1}\) = \(\operatorname{range}\) \(T^{n+2}\) = \(\cdots .\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1706773343817 (Block 374) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       19 Suppose \(T \in \mathcal{L}(V)\) and \(m\) is a nonnegative integer. <br>Prove that:<br><ul><li>\(\operatorname{null}\) \(T^{m}\) = \(\operatorname{null}\) \(T^{m+1}\)&nbsp;</li><li>if and only if:</li><ul><li>\(\operatorname{range}\) \(T^{m}\) = \(\operatorname{range}\) \( T^{m+1}\).</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1706857930056 (Block 375) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-8c6fc87474d7fbea330a1f7f6f6bd1dc03017126.jpg"><br>Proof:<br><ul><li>Let k be a positive integer, we want to prove that</li><ul><li>null&nbsp;\(T^{m+k}\) = null&nbsp;\(T^{m+k+1}\)</li></ul><li>From this:</li><ul><li><img src="paste-c5fa00abe1b56df3a213f4abf509f5b1d86d056f.jpg"></li></ul><li>We already know that null&nbsp;\(T^{m+k}\) = null&nbsp;\(T^{m+k+1}\)</li><li>To prove the inclusion in the other direction suppose&nbsp;\(v\)&nbsp;\(\in\) null \(T^{m+k+1}\) then:</li><ul><li>\(T^{m+1} T^k\)&nbsp;\(v\) = \(T^{m+k+1}\)&nbsp;\(v\) = 0<br></li></ul><li>hence&nbsp;&nbsp;\(T^k\) \(v\) \(\in\) \(\operatorname{null}\) \(T^{m+1}\) = null&nbsp;\(T^m\)</li><li>Thus</li><li>\(T^{m+k} \)&nbsp;\(v\) =&nbsp;\(T^m T^k\)&nbsp;\(v\) = 0<br></li><li>which means that&nbsp;\(v\)&nbsp;\(\in\)&nbsp;\(null\) \(T^{m+k}\) and thus that&nbsp;\(\operatorname{null}\) \(T^{m+k+1}\) \(\subset\) \(\operatorname{null}\) \(T^{m+k}\)</li></ul><br><br><br>
+
+============================================================
----------------------------

=== Note ID: 1706858774806 (Block 376) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-5b23b582d8d40ab6b0b3b40dd0a5b85fe4075b15.jpg"><br>Proof:<br><ul><li>First we need to show that</li><ul><li>\(\left(\right.\) null \(\left.T^n\right)\) \(\cap\) \(\left(\operatorname{range} T^n\right)\) = \(\{0\}\).<br></li></ul><li>Suppose&nbsp;\(v\)&nbsp;\(\in\) \(\left(\right.\) null \(\left.T^n\right) \cap\left(\operatorname{range} T^n\right)\).</li><li>Then:</li><ul><li>\(T^{n}\) \(v\) = 0 because it is in the null<br></li></ul><li>and there exists&nbsp;\(u\)&nbsp;\(\in\)&nbsp;\(V\) such that</li><ul><li>\(T^{n}\) \(u\) = \(v\) because v is in the range<br></li></ul><li>we can then transform the last equation:</li><ul><li>\(T^{2 n}\) \(u\) = \(T^n\) \(v\) = 0<br></li></ul><li>Since the null space of&nbsp;\(T^n\) is the same as that of&nbsp;\(T^{2n}\) given that n is the dimension of V, then&nbsp;\(T^{n}\) u = 0&nbsp;</li><li>Finally this shows that v is 0 meaning the intersection is empty</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1706861197311 (Block 377) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The null space and range of \(p(T)\) are invariant under \(T\)<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(p \in \mathcal{P}(\mathbf{F})\). Then null \(p(T)\) and range \(p(T)\) are invariant under \(T\).
+
+============================================================
----------------------------

=== Note ID: 1706946923158 (Block 378) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       A basis of generalized eigenvectors<br><br>Suppose \(V\) is a complex vector space and \(T \in \mathcal{L}(V)\). Then there is a basis of \(V\) consisting of generalized eigenvectors of \(T\).
+
+============================================================
----------------------------

=== Note ID: 1706947106102 (Block 379) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition multiplicity<br><br><ul><li>Suppose \(T \in \mathcal{L}(V)\). The multiplicity of an eigenvalue \(\lambda\) of \(T\) is defined to be the dimension of the corresponding generalized eigenspace \(G(\lambda, T)\).</li><li>In other words, the multiplicity of an eigenvalue \(\lambda\) of \(T\) equals \(\operatorname{dim} \operatorname{null}(T-\lambda I)^{\operatorname{dim} V}\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1706947221054 (Block 380) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Sum of the multiplicities equals \(\operatorname{dim} V\)<br><br>Suppose \(V\) is a complex vector space and \(T \in \mathcal{L}(V)\). Then the sum of the multiplicities of all the eigenvalues of \(T\) equals \(\operatorname{dim} V\).
+
+============================================================
----------------------------

=== Note ID: 1706947523983 (Block 381) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If \(T \in \mathcal{L}(V)\) and \(\lambda\) is an eigenvalue of \(T\), then<br><br><ul><li>algebraic multiplicity of \(\lambda\) = \(\operatorname{dim} \) \(\operatorname{null}\) \((T-\lambda I)^{\operatorname{dim} V} \) = \(\operatorname{dim}\) \(G(\lambda, T)\)</li><li>geometric multiplicity of \(\lambda\)= \(\operatorname{dim}\) \(\operatorname{null}\) \((T-\lambda I)\)=\(\operatorname{dim}\) \(E(\lambda, T)\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1706947751796 (Block 382) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition block diagonal matrix<br><br>A block diagonal matrix is a square matrix of the form<br><br>\[<br>\left(\begin{array}{ccc}<br>A_{1} &amp; &amp; 0 \\<br>&amp; \ddots &amp; \\<br>0 &amp; &amp; A_{m}<br>\end{array}\right),<br>\]<br><br>where \(A_{1}, \ldots, A_{m}\) are square matrices lying along the diagonal and all the other entries of the matrix equal 0 .
+
+============================================================
----------------------------

=== Note ID: 1706948685291 (Block 383) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Block diagonal matrix with upper-triangular blocks<br><br>Suppose \(V\) is a complex vector space and \(T \in \mathcal{L}(V)\). Let \(\lambda_{1}, \ldots, \lambda_{m}\) be the distinct eigenvalues of \(T\), with multiplicities \(d_{1}, \ldots, d_{m}\). Then there is a basis of \(V\) with respect to which \(T\) has a block diagonal matrix of the form<br><br>\[<br>\left(\begin{array}{ccc}<br>A_{1} &amp; &amp; 0 \\<br>&amp; \ddots &amp; \\<br>0 &amp; &amp; A_{m}<br>\end{array}\right)<br>\]<br><br>where each \(A_{j}\) is a \(d_{j}\)-by- \(d_{j}\) upper-triangular matrix of the form<br><br>\[<br>A_{j}=\left(\begin{array}{ccc}<br>\lambda_{j} &amp; &amp; * \\<br>&amp; \ddots &amp; \\<br>0 &amp; &amp; \lambda_{j}<br>\end{array}\right)<br>\]<br>
+
+============================================================
----------------------------

=== Note ID: 1706949204170 (Block 384) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Identity plus nilpotent has a square root<br><br>Suppose \(N \in \mathcal{L}(V)\) is nilpotent. Then \(I\) + \(N\) has a square root.
+
+============================================================
----------------------------

=== Note ID: 1706949407550 (Block 385) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Over \(\mathbf{C}\), invertible operators have square roots<br><br>Suppose \(V\) is a complex vector space and \(T \in \mathcal{L}(V)\) is invertible. Then \(T\) has a square root.
+
+============================================================
----------------------------

=== Note ID: 1706949850028 (Block 386) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-2796e54f31fed3b00816f8632242366536386ab3.jpg"><br><br>Proof Let \(\lambda_{1}, \ldots, \lambda_{m}\) be the distinct eigenvalues of \(T\). For each \(j\), there exists a nilpotent operator \(N_{j} \in \mathcal{L}\left(G\left(\lambda_{j}, T\right)\right)\) such that \(\left.T\right|_{G\left(\lambda_{j}, T\right)}=\lambda_{j} I+N_{j}\) [see 8.21(c)]. Because \(T\) is invertible, none of the \(\lambda_{j}\) 's equals 0 , so we can write<br><br>\[<br>\left.T\right|_{G\left(\lambda_{j}, T\right)}=\lambda_{j}\left(I+\frac{N_{j}}{\lambda_{j}}\right)<br>\]<br><br>for each \(j\). Clearly \(N_{j} / \lambda_{j}\) is nilpotent, and so \(I+N_{j} / \lambda_{j}\) has a square root (by 8.31). <br><br>Multiplying a square root of the complex number \(\lambda_{j}\) by a square root of \(I+N_{j} / \lambda_{j}\), we obtain a square root \(R_{j}\) of \(\left.T\right|_{G\left(\lambda_{j}, T\right)}\).<br><br>A typical vector \(v \in V\) can be written uniquely in the form<br><br>\[<br>v=u_{1}+\cdots+u_{m}<br>\]<br><br>where each \(u_{j}\) is in \(G\left(\lambda_{j}, T\right)\) (see 8.21). Using this decomposition, define an operator \(R \in \mathcal{L}(V)\) by<br><ul><li>\(R\) v= \(R_{1} u_{1}+\cdots+R_{m} u_{m} .\)<br></li></ul><br>You should verify that this operator \(R\) is a square root of \(T\), completing the proof.<br>
+
+============================================================
----------------------------

=== Note ID: 1706949963245 (Block 387) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       3 Suppose \(T \in \mathcal{L}(V)\). Suppose \(S \in \mathcal{L}(V)\) is invertible. Prove that \(T\) and \(S^{-1} T S\) have the same eigenvalues with the same multiplicities.
+
+============================================================
----------------------------

=== Note ID: 1707118059081 (Block 388) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Define \(T \in \mathcal{L}\left(\mathbf{C}^3\right)\) by<br>\[<br>\begin{aligned}<br>T\left(z_1, z_2, z_3\right) &amp; =\left(3 z_1+4 z_2, 3 z_2, 8 z_3\right) . \\<br>\mathcal{M}(T) &amp; =\left(\begin{array}{lll}<br>3 &amp; 4 &amp; 0 \\<br>0 &amp; 3 &amp; 0 \\<br>0 &amp; 0 &amp; 8<br>\end{array}\right)<br>\end{aligned}<br>\]<br><br>The eigenvalues of \(T\) are 3 and 8 .<br>The eigenspaces of \(T\) are<br>\[<br>\begin{aligned}<br>&amp; E(3, T)=\left\{\left(z_1, 0,0\right): z_1 \in \mathbf{C}\right\}, \\<br>&amp; E(8, T)=\left\{\left(0,0, z_3\right): z_3 \in \mathbf{C}\right\} .<br>\end{aligned}<br>\]<br><br>Thus the eigenvalue 3 has geometric multiplicity 1 and the eigenvalue 8 has geometric multiplicity 1 .<br><br>The generalized eigenspaces of \(T\) are<br>\[<br>\begin{aligned}<br>&amp; G(3, T)=\left\{\left(z_1, z_2, 0\right): z_1, z_2 \in \mathbf{C}\right\}, \\<br>&amp; G(8, T)=\left\{\left(0,0, z_3\right): z_3 \in \mathbf{C}\right\} .<br>\end{aligned}<br>\]<br><br>Thus the eigenvalue 3 has algebraic multiplicity 2 and the eigenvalue 8 has algebraic multiplicity 1 .<br><br>We have<br>\[<br>\mathbf{C}^3=G(3, T) \oplus G(8, T),<br>\]<br>as expected by the Decomposition Theorem.
+
+============================================================
----------------------------

=== Note ID: 1707118234366 (Block 389) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       A block diagonal matrix looks just like a diagonal matrix execpt for maybe having matrices rather than numbers on the daigonal
+
+============================================================
----------------------------

=== Note ID: 1707120139677 (Block 390) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Over C, invertible operators have square roots<br><br>Suppose \(V\) is a complex vector space and \(T \in \mathcal{L}(V)\) is invertible. Then \(T\) has a square root.
+
+============================================================
----------------------------

=== Note ID: 1707120600097 (Block 391) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-5423a84c5fb4f78ff3c0be8eebc6d33e230a9f71.jpg"><br>Proof:<br><ul><li>Let \(\lambda_1, \ldots, \lambda_m\) be the distinct eigenvalues of \(T\).&nbsp;</li><li>For each \(j\), there exists a nilpotent operator \(N_j \in \mathcal{L}\left(G\left(\lambda_j, T\right)\right)\) such that:</li><ul><li>&nbsp;\(\left.T\right|_{G\left(\lambda_j, T\right)}\) = \(\lambda_j I+N_j\).&nbsp;</li></ul><li>Because \(T\) is invertible, none of the \(\lambda_j\) 's equals 0 , so we can write</li><ul><li>\(\left.T\right|_{G\left(\lambda_j, T\right)}\) = \(\lambda_j\left(I+\frac{N_j}{\lambda_j}\right) .\)</li></ul><li>Because a nillpotent operator divided by a scalar remains nillpotent then:</li><li>Clearly \(N_j / \lambda_j\) is nilpotent, and so \(I+N_j / \lambda_j\) has a square root because all such sums have square roots.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1707120709736 (Block 392) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-5423a84c5fb4f78ff3c0be8eebc6d33e230a9f71.jpg"><br>Proof:<br><ul><li>Let \(\lambda_1, \ldots, \lambda_m\) be the distinct eigenvalues of \(T\).&nbsp;</li><li>For each \(j\), there exists a nilpotent operator \(N_j \in \mathcal{L}\left(G\left(\lambda_j, T\right)\right)\) such that:</li><ul><li>&nbsp;\(\left.T\right|_{G\left(\lambda_j, T\right)}=\lambda_j I+N_j\).&nbsp;</li></ul><li>Because \(T\) is invertible, none of the \(\lambda_j\) 's equals 0 , so we can write</li><ul><li>\(\left.T\right|_{G\left(\lambda_j, T\right)}\) = \(\lambda_j\left(I+\frac{N_j}{\lambda_j}\right) .\)</li></ul><li>Because a nillpotent operator divided by a scalar remains nillpotent then:</li><li>Clearly \(N_j / \lambda_j\) is nilpotent, and so \(I+N_j / \lambda_j\) has a square root because all such sums have suare roots.<br></li><li>Then because every complex number has a complex square root:</li><li>Multiplying a square root of the number \(\lambda_j\) by a square root of \(I+N_j / \lambda_j\) gives a square root \(R_j\) of \(\left.T\right|_{G\left(\lambda_j, T\right)}\).<br></li><li>Then we can define an operator R by applying each&nbsp;\(R_j\) to the corresponding eigenvector from the generalized eigensapce decomposition</li><li>This operator is a square root of T.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1707201890607 (Block 393) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition characteristic polynomial<br><br>Suppose \(V\) is a complex vector space and \(T \in \mathcal{L}(V)\). Let \(\lambda_{1}, \ldots, \lambda_{m}\) denote the distinct eigenvalues of \(T\), with multiplicities \(d_{1}, \ldots, d_{m}\). The polynomial<br><br><ul><li>\(\prod_{i=1}^m\)\((z-\lambda_i)\)\(^{d_i}\)</li></ul><br>is called the characteristic polynomial of \(T\).<br>
+
+============================================================
----------------------------

=== Note ID: 1707203295964 (Block 394) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-e1fc3bcece69a82f3536472bbc5ebb238a8984e0.jpg"><br><span style="color: rgb(0, 0, 0);">Proof:</span><br><ol><li>Let \(\lambda_{1}, \ldots, \lambda_{m}\) be the distinct eigenvalues of the operator \(T\).</li><li>Let \(d_{1}, \ldots, d_{m}\) be the dimensions of the corresponding generalized eigenspaces \(G\left(\lambda_{1}, T\right), \ldots, G\left(\lambda_{m}, T\right)\).&nbsp;<br></li><li>For each j,&nbsp;\(\left.\left(T-\lambda_j I\right)\right|_{G\left(\lambda_j, T\right)}\) is nilpotent</li><li>Thus we have:</li><ol><li>&nbsp;\(\left.\left(T-\lambda_{j} I\right)^{d_{j}}\right|_{G\left(\lambda_{j}, T\right)}\) = \(0\)&nbsp;</li><li>Because nill potent operators to the power of the dimension are 0</li></ol><li>Every vector v can be written as the sum of vectors in generalized eigenspaces, thus to prove that&nbsp;\(q(T)\) = 0 we need to show that&nbsp;\(\left.q(T)\right|_{G\left(\lambda_j, T\right)}\) = \(0\) for each j</li><li>Since the characteristic polynomial is defined as&nbsp;</li><ol><li>\(q(T)\) = \(\left(T-\lambda_{1} I\right)^{d_{1} } \cdots\left(T-\lambda_{m} I\right)^{d_{m} } .\)<br></li></ol><li>We can match the component corresponding to&nbsp;\(\lambda_j\) to vectors from&nbsp;\(G(\lambda_j,T)\).</li><li>Because \(\left.\left(T-\lambda_{j} I\right)^{d_{j} }\right|_{G\left(\lambda_{j}, T\right) }\) = \(0\), we conclude that \(\left.q(T)\right|_{G\left(\lambda_{j}, T\right)}\) = \(0\), as desired.</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1707203387841 (Block 395) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition monic polynomial<br><br>A monic polynomial is a polynomial whose highest-degree coefficient equals 1 .
+
+============================================================
----------------------------

=== Note ID: 1707204151415 (Block 396) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-622c8f0ee70dfe028246bca066e500d732fed2c0.jpg"><br>Proof broad steps:<br><ol><li>Construct a list in L(V,V), since dim L(V,V) = \(n^2\) we can construct a list which is linearly dependent as \(<br>I, T, T^{2}, \ldots, T^{n^{2} } <br>\)&nbsp;because the list is longer than the dimension the of the space</li><li>Let m be the smallest positive integer such that the list&nbsp;\(I, T, T^2, \ldots, T^m\) is linearly dependent</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1707204330801 (Block 397) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-622c8f0ee70dfe028246bca066e500d732fed2c0.jpg"><br>Proof broad steps:<br><ol><li>Construct a list in L(V,V), since dim L(V,V) = \(n^2\) we can construct a list which is linearly dependent as \(<br>I, T, T^{2}, \ldots, T^{n^{2}}<br>\) because the list is longer than the dimension the of the space</li><li>Let m be the smallest positive integer such that the list&nbsp;\(I, T, T^2, \ldots, T^m\) is linearly dependent</li><li>Thus one of the powers of T in the list is a linear combination of the other ones by the linear dependence lema</li><li>Because m was chosen as the smallest positive integer such that the list is lin dep, we can conclude&nbsp;\(T^m\) is a linear combination of the rest of the list</li><li>Thus there exist scalars such tthat</li><ol><li>\(\sum_{i=0}^{m-1}\) \(a_i T^{i}\) +&nbsp;\(T^m\) = 0<br></li></ol><li>Define a monic polynomial p by:</li><ol><li>\(p(z)\) = \(\sum_{i=0}^{m-1}\)&nbsp;\(a_i z^{i}\) +&nbsp;\(z^m\)<br></li><li>p(T) = 0 by the above</li></ol><li>Since m is the smallest degree, the polynomial is unique</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1707204808942 (Block 398) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition minimal polynomial<br><br>Suppose \(T \in \mathcal{L}(V)\). Then the minimal polynomial of \(T\) is the unique monic polynomial \(p\) of smallest degree such that \(p(T)\) = \(0\).
+
+============================================================
----------------------------

=== Note ID: 1707205284442 (Block 399) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       \(q(T)\) = \(0\) implies \(q\) is a multiple of the minimal polynomial<br><br>Suppose \(T \in \mathcal{L}(V)\) and \(q \in \mathcal{P}(\mathbf{F})\). Then \(q(T)\) = \(0\) if and only if \(q\) is a polynomial multiple of the minimal polynomial of \(T\).
+
+============================================================
----------------------------

=== Note ID: 1707205718959 (Block 400) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-87a16b8c0a0a94ae4ff1a161bd2d53bfb9533191.jpg"><br>Proof:<br><ol><li>First direction is easy, if q is a multiple then q(T) = p(T) s(T) = 0 s(T) = 0</li><li>The other direction:</li><ol><li>Decompose</li><ol><li>\(q\) = \(ps\) + \(r\)&nbsp;</li><li>and \(deg r\) &lt; \(deg p\) by division algorithm</li></ol><li>\(0\) = \(q(T)\) = \(p(T) s(T)\)+\(r(T)\)=\(r(T) .\)<br></li><li>Since dividing r by its highest-degree coefficient would produce a smaller minimal polynomial, r must be 0</li><li>Thus \(q\) = \(ps\) and q is a polynomial multiple of p</li></ol></ol>
+
+============================================================
----------------------------

=== Note ID: 1707205801502 (Block 401) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Characteristic polynomial is a multiple of minimal polynomial<br><br>Suppose \(\mathbf{F}=\mathbf{C}\) and \(T \in \mathcal{L}(V)\). Then the characteristic polynomial of \(T\) is a polynomial multiple of the minimal polynomial of \(T\).
+
+============================================================
----------------------------

=== Note ID: 1707205900035 (Block 402) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Eigenvalues are the zeros of the minimal polynomial<br><br>Let \(T \in \mathcal{L}(V)\). Then the zeros of the minimal polynomial of \(T\) are precisely the eigenvalues of \(T\).
+
+============================================================
----------------------------

=== Note ID: 1707206593240 (Block 403) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-ffefb39e579ba840ec32457ebf2aa829f125a9a8.jpg"><br>Proof:<br><ul><li>Let p be the minimal polynomial of T</li><li>First suppose \(\lambda \in \mathbf{F}\) is a zero of \(p\). Then \(p\) can be written in the form</li><ul><li>\(p(z)\) = \((z-\lambda)\) \(q(z)\)</li><li>where \(q\) is a monic polynomial with coefficients in \(\mathbf{F}\) (see 4.11).&nbsp;</li></ul><li>Because \(p(T)=0\), we have</li><ul><li>\(0\) =\((T-\lambda I)\) \((q(T) v)\) for al v</li></ul><li>Because the degree of q is less than the minimal polynomial, there must exist a vector such that&nbsp;\(q(T)\)&nbsp;\(v\)&nbsp;\(\neq\)&nbsp;\(0\)&nbsp;</li><ul><li>This implies that&nbsp;\(\lambda \) is an eigenvalue of T</li></ul><li>Conversely, if&nbsp;\(\lambda\) is an eigenvalue of T with&nbsp;\(v\)&nbsp; \(\neq 0\) then&nbsp;\(T^{j}\)&nbsp;\(v\) =&nbsp;\(\lambda^j\)&nbsp;\(v\)</li><ul><li>Thus:</li><ul><li>0 =&nbsp;</li><li>= \(p(T)\)&nbsp;\(v\)&nbsp;&nbsp;</li><li>= (\(\sum_{i=0}^{m-1}\)&nbsp;\(a_i T^{i}\) +&nbsp;\(T^m\))&nbsp;\(v\)&nbsp;</li><li>= (\(\sum_{i=0}^{m-1}\)&nbsp;\(a_i \lambda^{i}\) +&nbsp;\(\lambda^m\))&nbsp;\(v\)</li><li>=&nbsp;\(p(\lambda)\) \(v\).</li></ul><li>Because \(v \neq 0\), the equation above implies that \(p(\lambda)\) = \(0\), as desired.</li></ul><li></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1707376475826 (Block 404) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       16 Suppose \(V\) is an inner product space and \(T \in \mathcal{L}(V)\). Suppose<br><br><ul><li>\(\sum_{i=0}^{m-1}\)&nbsp;\(a_i\) \(z^i\) +&nbsp;\(z^m\)<br></li><li>is the minimal polynomial of \(T\).&nbsp;</li></ul>Prove that<br><br><ul><li>\(\sum_{i=0}^{m-1}\)&nbsp;\(\overline{a_i}\) \(z^i\) +&nbsp;\(z^m\)</li><li>is the minimal polynomial of \(T^{*}\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1707376571432 (Block 405) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose \(\mathbf{F}\) = \(\mathbf{C}\) and \(T \in \mathcal{L}(V)\). <br><ul><li>Suppose the minimal polynomial of \(T\) has degree \(\operatorname{dim} V\).&nbsp;</li><li>Prove that the characteristic polynomial of \(T\) equals the minimal polynomial of \(T\).</li></ul><br>
+
+============================================================
----------------------------

=== Note ID: 1707376750961 (Block 406) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       20 <br><br><ul><li>Suppose \(V\) is a complex vector space and \(V_{1}, \ldots, V_{m}\) are nonzero subspaces of \(V\) such that:</li><ul><li>&nbsp;\(V\)= \(V_{1} \oplus \cdots \oplus V_{m}\).&nbsp;</li></ul><li>Suppose \(T \in \mathcal{L}(V)\) and each \(V_{j}\) is invariant under \(T\).&nbsp;</li><li>For each \(j\), let \(p_{j}\) denote the characteristic polynomial of \(\left.T\right|_{V_{j} }\).&nbsp;</li><li>Prove that the characteristic polynomial of \(T\) equals \(p_{1} \cdots p_{m}\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1707377976263 (Block 407) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-cf0cefa25f25833e1845a70954b0a91d7fded35f.jpg"><br><br>Proof:<br><ul><li>Let&nbsp;\(\lambda_i\) be the distinct eigenvalues of T</li><li>Let&nbsp;\(d_i\) be the dimensions of their corresponding generalized eigenspaces&nbsp;\(G(\lambda_i, T)\)</li><li>Each \((T-\lambda_j I)\)&nbsp;\(|_{G\left(\lambda_j, T\right)}\) is nilpotent and:<br></li><ul><li>\( (T-\lambda_j I)\) \(|_{G\left(\lambda_j, T\right)}\) = 0<br></li></ul><li>V =&nbsp;\(\bigoplus\) \(G(\lambda_i,T)\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1707378172730 (Block 408) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-cf0cefa25f25833e1845a70954b0a91d7fded35f.jpg" style="width: 793.987px;"><br><br>Proof:<br><ul><li>Let&nbsp;\(\lambda_i\) be the distinct eigenvalues of T</li><li>Let&nbsp;\(d_i\) be the dimensions of their corresponding generalized eigenspaces&nbsp;\(G(\lambda_i, T)\)</li><li>Each \(\left.\left(T-\lambda_j I\right)\right|_{G\left(\lambda_j, T\right)}\) is nilpotent and:<br></li><ul><li>\(\left.\left(T-\lambda_j I\right)\right|_{G\left(\lambda_j, T\right)}^{d_j}\) = 0<br></li></ul><li>V =&nbsp;\(\bigoplus G(\lambda_i,T)\)</li><li>Thus we only need to show that&nbsp;\(q(T)\)\(|_{G\left(\lambda_j, T\right)}\) = \(0\) for each \(j\).</li><li>We have</li><ul><li>\(q(T)\) =&nbsp;\(\prod_i\) \((T-\lambda_i)^{d_i}\)<br></li></ul><li>If we fix a j, we can comute the equation above so&nbsp;\(\left(T-\lambda_j I\right)^{d_j}\) is the last term on the RHS</li><li>Because :</li><ul><li>\((T-\lambda_j I)^{d_j}\) \(|_{G\left(\lambda_j, T\right)}\) = \(0\)</li></ul><li>We conclude that&nbsp;</li><ul><li>\(q(T)\) \(|_{G\left(\lambda_j, T\right)}\) = \(0\)</li></ul><li>As desired.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1707378611272 (Block 409) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-64cacea41c038d23efa347a13437b5661ba6ef19.jpg"><br>Proof <br>Let \(n=\operatorname{dim} V\). The list<br>\[<br>I, T, T^2, \ldots, T^{n^2}<br>\]<br>is not linearly independent in \(\mathcal{L}(V)\), because \(\mathcal{L}(V)\) has dimension \(n^2\) and the list has length \(n^2+1\). Let \(m\) be the smallest positive integer such that<br>\[<br>I, T, T^2, \ldots, T^m<br>\]<br>is linearly dependent.<br><ul><li>The Linear Dependence Lemma implies that \(T^m\) is a linear combination of \(I, T, T^2, \ldots, T^{m-1}\).&nbsp;<br></li><li>Thus there exist scalars \(a_0, a_1, a_2, \ldots, a_{m-1} \in \mathbf{F}\) such that \(a_0 I+a_1 T+a_2 T^2+\cdots+a_{m-1} T^{m-1}+T^m\) = \(0\).&nbsp;</li><li>Define a monic polynomial \(p \in \mathcal{P}(\mathbf{F})\) by:</li><ul><li>&nbsp;\(p(z)\) = \(a_0+a_1 z+a_2 z^2+\cdots+a_{m-1} z^{m-1}+z^m\).&nbsp;</li></ul><li>Then \(p(T)\) = \(0\).&nbsp;<br></li><li>No monic polynomial \(q \in \mathcal{P}(\mathbf{F})\) with degree smaller than \(m\) can satisfy \(q(T)\) = \(0\).&nbsp; because we choose m to be the smallest degree</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1707379133416 (Block 410) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-0d7e8c1725ccdad8afdeb991e0554f8aa428adfd.jpg"><br>To prove the other direction, now suppose \(q(T)\) = \(0\). By the Division Algorithm for Polynomials, there exist polynomials \(s, r \in \mathcal{P}(\mathbf{F})\) such that<br><ul><li>\(q\) = \(p s\) +\(r\)</li></ul>and \(\operatorname{deg} r&lt;\operatorname{deg} p\). We have<br><br><ul><li>0 =&nbsp;\(q(T)\)</li><li>=&nbsp;\(p(T) s(T)\) +&nbsp;\(r(T)\)</li><li>= \(r(T)\)</li></ul><br>The equation above implies that \(r\) = \(0\).<br>
+
+============================================================
----------------------------

=== Note ID: 1707379958890 (Block 411) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>If you choose an operator at random it is extremely likely the characteristic polynomial and minimal polynomial are the same</li><li>Thus the easiest way to find the characteristic polynomial is usually to find the minimal polynomial</li><li>If the minimal polynomial has degree dim V then we are done, otherwise search by other means</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1707380316327 (Block 412) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-f9fa7e823504027c64a83fb58982269824641558.jpg"><br>Proof:<br><ul><li>Let p be the minimal polynomial of T<br></li><li>First: suppose&nbsp;\(\lambda \in F\) is a zero of p</li><ul><li>Then we can write p in the form</li><ul><li>p(z) =&nbsp;\((z-\lambda)\)&nbsp;\(q(z)\)</li><li>where q is a monic polynomial</li></ul><li>For every v</li><ul><li>0 = p(T) v</li><li>=&nbsp;\((T-\lambda I)\) \(q(T)\) \(v\)</li></ul><li>Because the degree of \(q\) is less than the degree of the minimal polynomial \(p\), there exists at least one vector \(v \in V\) such that \(q(T) v\) \(\neq\) \(0\) but </li><li>&nbsp;\(T-\lambda I\) applied to v is 0</li><li>Thus&nbsp;\(\lambda\) is an eigenvalue of T</li><li>Thus every 0 of p is an eigenvalue of T</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1707380851714 (Block 413) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-779c6b5fbf7a19d21c5ba2a62669e4885ece5984.jpg"><br>Proof:<br><ul><li>Let p be the minimal polynomial</li><li>Suppose&nbsp;\(\lambda \in F\) is an eigenvalue of T</li><li>Thus there exists&nbsp;\(v\in V\) with&nbsp;\(v\) \(\neq 0\) such that</li><ul><li>\(T\)&nbsp;\(v\) =&nbsp;\(\lambda\)&nbsp;\(v\)<br></li></ul><li>Repeated applications of T to both sides show that</li><ul><li>\(T^j\)&nbsp;\(v\) =&nbsp;\(\lambda^j\)&nbsp;\(v\)</li><li>for every nonnegative integer j</li></ul><li>Since p(T) is just taking linear combinations of that we can conclude that</li><ul><li>\(p(T)\)&nbsp;\(v\) =&nbsp;\(p(\lambda)\)&nbsp;\(v\)<br></li></ul><li>Then:</li><ul><li>\(0\) =&nbsp;\(p(T)\)&nbsp;\(v\) because p is the minimal polynomial<br></li><li>=&nbsp;\(p(\lambda)\)&nbsp;\(v\)</li></ul><li>Because&nbsp;\(v\)&nbsp;\(\neq\)&nbsp;\(0\), the equation above implies that&nbsp;\(p(\lambda)\) =&nbsp;\(0\)</li><li>Thus we have shown that every eigenvalue of T is a zero of p</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1707463982269 (Block 414) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Basis corresponding to a nilpotent operator<br><br>Suppose \(N \in \mathcal{L}(V)\) is nilpotent. Then there exist vectors \(v_{1}, \ldots, v_{n} \in V\) and nonnegative integers \(m_{1}, \ldots, m_{n}\) such that<br><br><ul><li>(a) \(N^{m_{1} } v_{1}, \ldots, N v_{1}, v_{1}, \ldots, N^{m_{n} } v_{n}, \ldots, N v_{n}, v_{n} \) is a basis of \(V\);</li><li>(b) \(N^{m_{1}+1} v_{1}\) = \(\cdots=N^{m_{n}+1} v_{n}\) = \(0\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1707665323978 (Block 415) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-e6c465f8f4dddf39cd587c1c082ae6c53e083170.jpg"><br>Proof by induction:<br><ul><li>Obviously true for dim V = 1</li><li>Inductive case:</li><li>Because N is nilpotent , N is not injective and thus not surjective, thus range N is a strict subspace of V</li><li>Thus we can apply I.H to the restriction operatotr&nbsp;\(\left.N\right|_{\text {range } N} \in \mathcal{L}(\) range \(N)\).</li><li>Thus the following is a basis of range N:</li><ul><li>\(N^{m_{1}} v_{1}, \ldots,\) \(N v_{1}, v_{1}, \ldots, \) \( N^{m_{n}} v_{n}, \ldots,\) \(N v_{n}, v_{n}\)<br></li></ul><li>And&nbsp;</li><ul><li>\(N^{m_{1}+1} v_{1}\) = \(\cdots\) =\(N^{m_{n}+1} v_{n}\) = \(0 .\)<br></li></ul><li>Because each&nbsp;\(v_j\) is in range N, there exists&nbsp;\(u_j\) such that&nbsp;\(v_j\)&nbsp; =&nbsp;&nbsp;\(N\)&nbsp;\(u_j\) which means we can claim the following basis to be linearly independent in V:</li><ul><li>\(N^{m_{1}+1} u_{1}, \ldots,\) \(N u_{1}, u_{1}, \ldots,\)\( N^{m_{n}+1} u_{n}, \ldots,\)\( N u_{n}, u_{n}\)</li></ul><li>Which can be proven by contradiction, since if it were to equal 0 and we applied N to it we would get the original list which we know is linearly independent</li><ul><li>Thus the only vectors which could have non-zero coefficients are:</li><ul><li>\[<br>N^{m_{1}+1} u_{1}, \ldots, N^{m_{n}+1} u_{n},<br>\]<br></li></ul><li>Which equal:</li><ul><li>\[<br>N^{m_{1} } v_{1}, \ldots, N^{m_{n} } v_{n}<br>\]<br></li></ul><li>Again, since the original list was linearly indepedent, these must have coefficients 0</li></ul><li>We can extend the u-list to a basis by concatenating basis vectors</li><ul><li>\(N^{m_{1}+1} u_{1}, \ldots,\) \(N u_{1}, u_{1}, \ldots,\)\( N^{m_{n}+1} u_{n}, \ldots,\) \(N u_{n}, u_{n}, \)\(w_{1}, \ldots, w_{p}\)</li></ul><li>Applying N to any of the new vectors would result in a vector in range N and thus in the span of the riginal list.&nbsp;</li><li>Since each vector in the original list can be written based on u's, there exists&nbsp;\(x_j\) in the span of the u-list such that&nbsp;\(N w_j\) =&nbsp;\(N x_j\).</li><li>Let: \(u_{n+j}\) = \(w_{j}-x_{j} .\)</li><li>Then&nbsp;\(N u_{n+j}\) = \(0\), furthermore the list:</li><ul><li>\[<br>N^{m_{1}+1} u_{1}, \ldots, N u_{1}, u_{1}, \ldots, N^{m_{n}+1} u_{n}, \ldots, N u_{n}, u_{n}, u_{n+1}, \ldots, u_{n+p}<br>\]&nbsp;</li><li>spans V becuase its span contains each&nbsp;\(x_j\), and each&nbsp;\(u_{n+j}\) and hence each&nbsp;\(w_j\)</li></ul><li>This is the basis of the entire space we were looking for</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1707665514594 (Block 416) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition Jordan basis<br><br>Suppose \(T \in \mathcal{L}(V)\). A basis of \(V\) is called a Jordan basis for \(T\) if with respect to this basis \(T\) has a block diagonal matrix<br><br>\[<br>\left(\begin{array}{ccc}<br>A_{1} &amp; &amp; 0 \\<br>&amp; \ddots &amp; \\<br>0 &amp; &amp; A_{p}<br>\end{array}\right),<br>\]<br><br>where each \(A_{j}\) is an upper-triangular matrix of the form<br><br>\[<br>A_{j}=\left(\begin{array}{cccc}<br>\lambda_{j} &amp; 1 &amp; &amp; 0 \\<br>&amp; \ddots &amp; \ddots &amp; \\<br>&amp; &amp; \ddots &amp; 1 \\<br>0 &amp; &amp; &amp; \lambda_{j}<br>\end{array}\right) .<br>\]
+
+============================================================
----------------------------

=== Note ID: 1707665569086 (Block 417) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Jordan Form<br><br>Suppose \(V\) is a complex vector space. If \(T \in \mathcal{L}(V)\), then there is a basis of \(V\) that is a Jordan basis for \(T\).
+
+============================================================
----------------------------

=== Note ID: 1707665929604 (Block 418) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-38f854533fe9c300346a9fd950b3594d316f8b04.jpg"><br>ProofL<br><br><ul><li>First consider a nilpotent operator \(N \in \mathcal{L}(V)\) and the vectors \(v_{1}, \ldots, v_{n} \in V\) given by 8.55 .&nbsp;</li><ul><li>8.55: <img src="paste-469e4737e5f98e9ac1a08cabf2103e7ef4c2a7e2.jpg"></li></ul><li>For each \(j\), note that \(N\) sends the first vector in the list \(N^{m_{j}} v_{j}, \ldots, N v_{j}, v_{j}\) to 0 and that \(N\) sends each vector in this list other than the first vector to the previous vector.&nbsp;</li><li>In other words, 8.55 gives a basis of \(V\) with respect to which \(N\) has a block diagonal matrix, where each matrix on the diagonal has the form</li><ul><li>\[\left(\begin{array}{cccc}0 &amp; 1 &amp; &amp; 0 \\&amp; \ddots &amp; \ddots &amp; \\&amp;&amp; \ddots &amp; 1 \\0 &amp; &amp; &amp; 0\end{array}\right) .\]</li></ul></ul>Thus the desired result holds for nilpotent operators.
+
+============================================================
----------------------------

=== Note ID: 1707672965891 (Block 419) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-16ac95ba2f11b60a610de9c7d53062babf1be5a6.jpg"><br><img src="paste-f102893410e4bd563bc3aff300b2318a2a446864.jpg"><br>Proof:<br><ul><li>Let&nbsp;\(\lambda_i\) be the eigenvalues of T</li><li>Decompose V into generalized eigenspaces&nbsp;</li><ul><li>\(V\) =&nbsp;\(\bigoplus\)&nbsp;\(G(\lambda_i,T)\) because the vector space is complex<br></li></ul><li>Where each&nbsp;\((T-\lambda_j I)\) \(|_{G(\lambda_j, T)}\) is nilpotent</li><li>Thus some basis of each generalized eigenspace is a Jordan basis for&nbsp;&nbsp;\((T-\lambda_j I)\) \(|_{G(\lambda_j, T)}\).</li><li>Thus &nbsp;\((T-\lambda_j I)\) \(|_{G(\lambda_j, T)}\).. has the form <img src="paste-501e90f5a0cb38845eb8352dfe16597aeb9c46da.jpg"></li><li>Put the bases together to get a jordan basis for T</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1707675426248 (Block 420) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>As we will soon see, a real vector space \(V\) can be embedded, in a natural way, in a complex vector space called the complexification of \(V\).&nbsp;</li><li>Each operator on \(V\) can be extended to an operator on the complexification of \(V\).&nbsp;</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1707675651640 (Block 421) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition complexification of \(V\), \(V_{\mathbf{C} }\)<br><br>Suppose \(V\) is a real vector space.<br><br><ul><li>The complexification of \(V\), denoted \(V_{\mathbf{C} }\), equals \(V \times V\). An element of \(V_{\mathbf{C}}\) is an ordered pair \((u, v)\), where \(u, v \in V\), but we will write this as \(u+i v\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1707676205426 (Block 422) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       We think of \(V\) as a subset of \(V_{\mathbf{C} }\) by identifying \(u \in V\) with \(u+i 0\). The construction of \(V_{\mathbf{C} }\) from \(V\) can then be thought of as generalizing the construction of \(\mathbf{C}^{n}\) from \(\mathbf{R}^{n}\).
+
+============================================================
----------------------------

=== Note ID: 1707676315095 (Block 423) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       &nbsp;\(V_{\mathbf{C} }\) is a complex vector space.<br><br>Suppose \(V\) is a real vector space. Then with the definitions of addition and scalar multiplication as above, \(V_{\mathbf{C} }\) is a complex vector space.
+
+============================================================
----------------------------

=== Note ID: 1707678155529 (Block 424) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Basis of \(V\) is basis of \(V_{\mathbf{C} }\)<br><br>Suppose \(V\) is a real vector space.<br><ul><li>(a) If \(v_{1}, \ldots, v_{n}\) is a basis of \(V\) (as a real vector space), then \(v_{1}, \ldots, v_{n}\) is a basis of \(V_{\mathbf{C} }\) (as a complex vector space).<br></li><li>(b) The dimension of \(V_{\mathbf{C} }\) (as a complex vector space) equals the dimension of \(V\) (as a real vector space).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1707721377883 (Block 425) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-2d07474ae11ae00257e7560eef98d3a972a46a78.jpg"><br>Proof:<br><ul><li>To show that \(v_{1}, \ldots, v_{n}\) is linearly independent in the complex vector space \(V_{\mathbf{C}}\), suppose \(\lambda_{1}, \ldots, \lambda_{n} \in \mathbf{C}\) and<br></li><ul><li>\(\sum\) \(\lambda_i\) \(v_i \) = \( 0 \)</li></ul><li>Then the equation above and our definitions imply that:</li><ul><li>&nbsp;\(\sum\) \((\operatorname{Re}\lambda_i)\)&nbsp;\(v_i\) = \(0\)&nbsp;</li><li>and&nbsp;</li><li>\(\sum\) \((\operatorname{Im}\lambda_i)&nbsp;\)&nbsp;\(v_i\)&nbsp;= \(0\).</li></ul><li>Because \(v_{1}, \ldots, v_{n}\) is linearly independent in \(V\), the equations above imply:</li><ul><li>&nbsp;\(\operatorname{Re}\lambda_i\)&nbsp;=&nbsp;\(0\) for all i</li><li>and&nbsp;</li><li>\(\operatorname{Im}\lambda_i\) =&nbsp;\(0\) for all i</li></ul><li>Thus we have \(\lambda_{1}=\cdots=\lambda_{n}\) = \(0\).&nbsp;</li><li>Hence \(v_{1}, \ldots, v_{n}\) is linearly independent in \(V_{\mathbf{C}}\), completing the proof of (a).<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1707721587946 (Block 426) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition complexification of \(T\), \(T_{\mathbf{C} }\)<br><br>Suppose \(V\) is a real vector space and \(T \in \mathcal{L}(V)\). <br><br>The complexification of \(T\), denoted \(T_{\mathbf{C} }\), is the operator \(T_{\mathbf{C} }\) \(\in\) \(\mathcal{L}\left(V_{\mathbf{C} }\right)\) defined by<br><br><ul><li>\(T_{\mathbf{C} }\) \((\) \(u+i v\) \()\)= \(T u+i T v\)</li><li>for \(u, v \in V\).<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1707722272420 (Block 427) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Every operator has an invariant subspace of dimension 1 or 2<br><br>Every operator on a nonzero finite-dimensional vector space has an invariant subspace of dimension 1 or 2 .
+
+============================================================
----------------------------

=== Note ID: 1707722737703 (Block 428) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-9eb0c523b8c723672c35b0444259780a6274209c.jpg"><br>Proof:<br>For real vector spaces (since we know the complex case is guaranteed to have an eigenvalue and a 1-dim invariant subspace):<br><ul><li>Hence assume \(V\) is a real vector space and \(T \in \mathcal{L}(V)\).&nbsp;</li><li>The complexification \(T_{\mathbf{C}}\) has an eigenvalue \(a+b i\) (by 5.21), where \(a, b \in \mathbf{R}\).&nbsp;</li><li>Thus there exist \(u, v \in V\), not both 0 , such that:</li><ul><li>&nbsp;\(T_{\mathbf{C}}\) ( \(u+i v\) )= \((a+b i)\) \((u+i v)\)</li></ul><li>Using the definition of \(T_{\mathbf{C}}\), the last equation can be rewritten as</li><ul><li>\(T u\) + \(i T v\)=\((a u-b v)\) + \((a v+b u) i \)</li></ul><li>Thus</li><ul><li>\(T u\) = \(a u-b v\)&nbsp;</li><li>and&nbsp;</li><li>\(T v\) = \(a v+b u .\)</li></ul><li>Let \(U\) equal the span in \(V\) of the list \(u, v\).&nbsp;</li><li>Then \(U\) is a subspace of \(V\) with dimension 1 or 2 .&nbsp;</li><li>The equations above show that \(U\) is invariant under \(T\), completing the proof.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1707723430293 (Block 429) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Minimal polynomial of \(T_{\mathbf{C} }\) equals minimal polynomial of \(T\)<br><br>Suppose \(V\) is a real vector space and \(T \in \mathcal{L}(V)\). Then the minimal polynomial of \(T_{\mathbf{C} }\) equals the minimal polynomial of \(T\).
+
+============================================================
----------------------------

=== Note ID: 1707723806458 (Block 430) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-db663e2e48fddd04cd496ff7918e29dd3828c303.jpg"><br>Proof:<br>Let \(p \in \mathcal{P}(\mathbf{R})\) denote the minimal polynomial of \(T\). From 9.9 it is easy to see that:<br><ul><ul><li>&nbsp;\(p\left(T_{\mathbf{C} }\right)\) = \((p(T))_{\mathbf{C} }\),&nbsp;</li><li>and thus \(p\left(T_{\mathbf{C} }\right)\) = \(0\).</li></ul></ul><ul><li>Suppose \(q \in \mathcal{P}(\mathbf{C})\) is a monic polynomial such that \(q\left(T_{\mathbf{C}}\right)\) = \(0\). </li><li>Then \(\left(q\left(T_{\mathbf{C}}\right)\right)(u)\) = \(0\) for every \(u \in V\).&nbsp;</li><li>Letting \(r\) denote the polynomial whose \(j^{\text {th}&nbsp; &nbsp;}\) coefficient is the real part of the \(j^{\text {th } }\) coefficient of \(q\), we see that \(r\) is a monic polynomial and \(r(T)\) = \(0\).&nbsp;</li><li>Thus \(\operatorname{deg} q\) = \(\operatorname{deg} r\) \(\geq\) \(\operatorname{deg} p\).</li><li>The conclusions of the two previous paragraphs imply that \(p\) is the minimal polynomial of \(T_{\mathbf{C}}\), as desired.<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1707723918734 (Block 431) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Real eigenvalues of \(T_{\mathbf{C} }\)<br><br>Suppose \(V\) is a real vector space, \(T \in \mathcal{L}(V)\), and \(\lambda\) \(\in\) \(\mathbf{R}\). Then \(\lambda\) is an eigenvalue of \(T_{\mathbf{C} }\) if and only if \(\lambda\) is an eigenvalue of \(T\).
+
+============================================================
----------------------------

=== Note ID: 1707724095904 (Block 432) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-2ed8a0710d360c78a55de02592b68673ce530e6b.jpg"><br>Proof 1 First suppose \(\lambda\) is an eigenvalue of \(T\). Then there exists \(v \in V\) with \(v \neq 0\) such that \(T v\) = \(\lambda v\). Thus \(T_{\mathbf{C} } v\) = \(\lambda v\), which shows that \(\lambda\) is an eigenvalue of \(T_{\mathbf{C}}\), completing one direction of the proof.
+
+============================================================
----------------------------

=== Note ID: 1707724212571 (Block 433) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-e111a18693d7de267dd56c7e3d0d8d6eabe3a3a2.jpg"><br>To prove the other direction, suppose now that \(\lambda\) is an eigenvalue of \(T_{\mathbf{C} }\). Then there exist \(u, v \in V\) with \(u+i v \neq 0\) such that<br><br><ul><li>\(T_{\mathbf{C} }\) \((\) \(u+i v\) \()\)=\( \lambda(u+i v) .\)</li></ul><br>The equation above implies that:<br><ul><li>&nbsp;\(T u\) = \(\lambda u\)&nbsp;</li><li>and&nbsp;</li><li>\(T v\) = \(\lambda v\).&nbsp;</li></ul>Because \(u \neq 0\) or \(v \neq 0\), this implies that \(\lambda\) is an eigenvalue of \(T\), completing the proof.<br>
+
+============================================================
----------------------------

=== Note ID: 1707724335663 (Block 434) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-ae66688317a74162cc0c0752164d11d60ca8e611.jpg"><br>Proof 2 <br><ul><li>The (real) eigenvalues of \(T\) are the (real) zeros of the minimal polynomial of \(T\)&nbsp;</li><li>The real eigenvalues of \(T_{\mathbf{C}}\) are the real zeros of the minimal polynomial of \(T_{\mathbf{C} }\) .&nbsp;</li><li>These two minimal polynomials are the same.&nbsp;</li><li>Thus the eigenvalues of \(T\) are precisely the real eigenvalues of \(T_{\mathbf{C} }\), as desired.</li></ul><br>
+
+============================================================
----------------------------

=== Note ID: 1707724633729 (Block 435) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose \(V\) is a real vector space, \(T \in \mathcal{L}(V)\), and \(\lambda\)&nbsp; \(\in \mathbf{C}\). Then \(\lambda\) is an eigenvalue of \(T_{\mathbf{C} }\) if and only if \(\bar{\lambda}\) is an eigenvalue of \(T_{\mathbf{C} }\).
+
+============================================================
----------------------------

=== Note ID: 1707724895413 (Block 436) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose \(V\) is a real vector space, \(T \in \mathcal{L}(V)\), and \(\lambda \in \mathbf{C}\) is an eigenvalue of \(T_{\mathbf{C}}\). Then the multiplicity of \(\lambda\) as an eigenvalue of \(T_{\mathbf{C}}\) equals the multiplicity of \(\bar{\lambda}\) as an eigenvalue of \(T_{\mathbf{C} }\).
+
+============================================================
----------------------------

=== Note ID: 1707809966802 (Block 437) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose \(V\) is a real vector space and \(T \in \mathcal{L}(V)\). Then the coefficients of the characteristic polynomial of \(T_{\mathbf{C} }\) are all real.
+
+============================================================
----------------------------

=== Note ID: 1707810168652 (Block 438) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-29f0dab45ab6209f206d88b81643b2c70028a006.jpg"><br>Proof Suppose \(\lambda\) is a nonreal eigenvalue of \(T_{\mathbf{C}}\) with multiplicity \(m\). Then \(\bar{\lambda}\) is also an eigenvalue of \(T_{\mathbf{C} }\) with multiplicity \(m\) (by 9.17). Thus the characteristic polynomial of \(T_{\mathbf{C}}\) includes factors of \((z-\lambda)^{m}\) and \((z-\bar{\lambda})^{m}\). Multiplying together these two factors, we have<br><br><ul><li>\( (z-\lambda)^{m}\) \((z-\bar{\lambda})^{m}\)= \( \left(z^{2}-2(\operatorname{Re} \lambda) z+|\lambda|^{2}\right)^{m} \)</li></ul><br>The polynomial above on the right has real coefficients.<br>
+
+============================================================
----------------------------

=== Note ID: 1707810585374 (Block 439) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose \(V\) is a real vector space and \(T \in \mathcal{L}(V)\). Then<br><br><ul><li>(a) the coefficients of the characteristic polynomial of \(T\) are all real;</li><li>(b) the characteristic polynomial of \(T\) has degree \(\operatorname{dim} V\);</li><li>(c) the eigenvalues of \(T\) are precisely the real zeros of the characteristic polynomial of \(T\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1707810829830 (Block 440) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose \(T\) \(\in\) \(\mathcal{L}(V)\). Then<br><br><ul><li>(a) the degree of the minimal polynomial of \(T\) is at most \(\operatorname{dim} V\);</li><li>(b) the characteristic polynomial of \(T\) is a polynomial multiple of the minimal polynomial of \(T\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1707810971294 (Block 441) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       3 Suppose \(V\) is a real vector space and \(v_{1}, \ldots, v_{m} \in V\). Prove that \(v_{1}, \ldots, v_{m}\) is linearly independent in \(V_{\mathbf{C} }\) if and only if \(v_{1}, \ldots, v_{m}\) is linearly independent in \(V\).
+
+============================================================
----------------------------

=== Note ID: 1707811020056 (Block 442) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       4 Suppose \(V\) is a real vector space and \(v_{1}, \ldots, v_{m} \in V\). Prove that \(v_{1}, \ldots, v_{m}\) spans \(V_{\mathbf{C} }\) if and only if \(v_{1}, \ldots, v_{m}\) spans \(V\).
+
+============================================================
----------------------------

=== Note ID: 1707811137962 (Block 443) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       6 Suppose \(V\) is a real vector space and \(T \in \mathcal{L}(V)\). Prove that \(T_{\mathbf{C} }\) is invertible if and only if \(T\) is invertible.
+
+============================================================
----------------------------

=== Note ID: 1707811157462 (Block 444) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       7 Suppose \(V\) is a real vector space and \(N \in \mathcal{L}(V)\). Prove that \(N_{\mathbf{C} }\) is nilpotent if and only if \(N\) is nilpotent.
+
+============================================================
----------------------------

=== Note ID: 1707811927900 (Block 445) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       17 Suppose \(V\) is a real vector space and \(T \in \mathcal{L}(V)\) satisfies \(T^{2}\) = \(-I\). Define complex scalar multiplication on \(V\) as follows: if \(a, b \in \mathbf{R}\), then<br><ul><ul><li>\((a+b i)\) \(v\) = \(a v+b T v \)</li></ul><li>(a) Show that the complex scalar multiplication on \(V\) defined above and the addition on \(V\) makes \(V\) into a complex vector space.</li><li>(b) Show that the dimension of \(V\) as a complex vector space is half the dimension of \(V\) as a real vector space.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1707812009806 (Block 446) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       18 Suppose \(V\) is a real vector space and \(T \in \mathcal{L}(V)\). Prove that the following are equivalent:<br><br><ul><li>(a) All the eigenvalues of \(T_{\mathbf{C} }\) are real.</li><li>(b) There exists a basis of \(V\) with respect to which \(T\) has an uppertriangular matrix.</li><li>(c) There exists a basis of \(V\) consisting of generalized eigenvectors of \(T\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1707812256160 (Block 447) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       19 Suppose \(V\) is a real vector space with \(\operatorname{dim} V\) = \(n\) and \(T \in \mathcal{L}(V)\) is such that null \(T^{n-2}\) \(\neq\) \(\operatorname{null} T^{n-1}\). Prove that \(T\) has at most two distinct eigenvalues and that \(T_{\mathbf{C}}\) has no nonreal eigenvalues.
+
+============================================================
----------------------------

=== Note ID: 1708414868804 (Block 448) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose \(V\) is a 2-dimensional real inner product space and \(T \in \mathcal{L}(V)\). Then the following are equivalent:<br><br><ul><li>(a) \(T\) is normal but not self-adjoint.</li><li>(b) The matrix of \(T\) with respect to every orthonormal basis of \(V\) has the form</li><ul><li>\[\left(\begin{array}{cc}a &amp; -b \\b &amp; a\end{array}\right)\]</li><li>with \(b\) \(\neq\) \(0\).</li></ul><li>(c) The matrix of \(T\) with respect to some orthonormal basis of \(V\) has the form</li><ul><li>\[\left(\begin{array}{cc}a &amp; -b \\b &amp; a\end{array}\right)\]</li><li>with \(b\) \(&gt;\) \(0\).</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1708414996353 (Block 449) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       A normal operator restricted to an invariant subspace is normal
+
+============================================================
----------------------------

=== Note ID: 1708415208144 (Block 450) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose \(V\) is an inner product space, \(T \in \mathcal{L}(V)\) is normal, and \(U\) is a subspace of \(V\) that is invariant under \(T\). Then<br><br><ul><li>(a) \(U^{\perp}\) is invariant under \(T\);</li><li>(b) \(U\) is invariant under \(T^{*}\);</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1708499217609 (Block 451) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       3 Suppose \(V\) is a real inner product space. Show that<br><br><ul><li>\(\langle\) \(u+i v, x+i y\)&nbsp;\(\rangle\) = \(\langle u, x\rangle\) + \(\langle v, y\rangle\) + \((\langle v, x\rangle-\langle u, y\rangle)\) \(i\)</li></ul><ul><li>for \(u, v, x, y \in V\) defines a complex inner product on \(V_{\mathbf{C}}\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1708499270351 (Block 452) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       4 Suppose \(V\) is a real inner product space and \(T \in \mathcal{L}(V)\) is self-adjoint. Show that \(T_{\mathbf{C}}\) is a self-adjoint operator on the inner product space \(V_{\mathbf{C}}\) defined by the previous exercise.<br><br>\[<br>\langle u+i v, x+i y\rangle=\langle u, x\rangle+\langle v, y\rangle+(\langle v, x\rangle-\langle u, y\rangle) i<br>\]
+
+============================================================
----------------------------

=== Note ID: 1708499305931 (Block 453) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       6 Give an example of an operator \(T\) on an inner product space such that \(T\) has an invariant subspace whose orthogonal complement is not invariant under \(T\).<br><br>[The exercise above shows that 9.30 can fail without the hypothesis that \(T\) is normal.]
+
+============================================================
----------------------------

=== Note ID: 1708502861097 (Block 454) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Every isometry is normal<br><ul><li>Suppose \(S \in \mathcal{L}(V)\). Then the following are equivalent:</li><li>- \(S\) is an isometry.</li><li>- \(S^* S\) = \(I\).</li><li>- \(S S^*\) = \(I\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1708541394466 (Block 455) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-b640ed7d09b8eea0b2f067bceb71c9a39c266f85.jpg"><br>Proof:<br><ul><li>Suppose a holds</li><li>Since S is a normal operator there is an orthonormal basis such that S has a block diagonal matrix such that each block is a 1-by-1 matrix or a 2-by-2 matrix of the form<br>\[<br>\left(\begin{array}{cc}<br>a &amp; -b \\<br>b &amp; a<br>\end{array}\right),<br>\]<br>with \(b&gt;0\).</li><li>If&nbsp;\(\lambda\) is an entry in a 1-by-1 matrix, because S is an isometry&nbsp;\(\lambda\) must have absolute value 1 and thus be -1 or 1</li><li>For 2-by-2 matrices, since S is an isometry it must map every orthonormal vector to a vector with norm 1 as well, thus&nbsp;\(a^2+b^2\) =&nbsp; 1&nbsp; as they are the coefficients of an orthonormal vector and must thus have norm equal 1</li><li>Since&nbsp;\(b&gt;0\) we can by definition pick&nbsp;\(\theta \in (0,\pi)\) such that&nbsp;\(a\)&nbsp;\(= \)&nbsp;\(\cos \theta\) and&nbsp;\(b\) =&nbsp;\(\sin \theta\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1708586672970 (Block 456) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition invertible, inverse, \(A^{-1}\)<br><br>A square matrix \(A\) is called invertible if there is a square matrix \(B\) of the same size such that \(A B\) = \(B A\) = \(I\); we call \(B\) the inverse of \(A\) and denote it by \(A^{-1}\).
+
+============================================================
----------------------------

=== Note ID: 1708586691758 (Block 457) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Some mathematicians use the terms nonsingular, which means the same as invertible, and singular, which means the same as noninvertible.
+
+============================================================
----------------------------

=== Note ID: 1708587591550 (Block 458) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition trace of an operator<br><br>Suppose \(T \in \mathcal{L}(V)\).<br><br>- If \(\mathbf{F}=\mathbf{C}\), then the trace of \(T\) is the sum of the eigenvalues of \(T\), with each eigenvalue repeated according to its multiplicity.<br><br>- If \(\mathbf{F}=\mathbf{R}\), then the trace of \(T\) is the sum of the eigenvalues of \(T_{\mathbf{C} }\), with each eigenvalue repeated according to its multiplicity.<br><br>The trace of \(T\) is denoted by trace \(T\).
+
+============================================================
----------------------------

=== Note ID: 1708591187968 (Block 459) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition trace of a matrix<br><br>The trace of a square matrix \(A\), denoted trace \(A\), is defined to be the sum of the diagonal entries of \(A\).
+
+============================================================
----------------------------

=== Note ID: 1708595896096 (Block 460) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Let \(T \in \mathcal{L}(V)\). Suppose \(u_{1}, \ldots, u_{n}\) and \(v_{1}, \ldots, v_{n}\) are bases of \(V\). Then<br><br><ul><li>\(\operatorname{trace}\) \(\mathcal{M}\left(T,\left(u_{1}, \ldots, u_{n}\right)\right)\) = \(\operatorname{trace}\) \( \mathcal{M}\left(T,\left(v_{1}, \ldots, v_{n}\right)\right) \text {. }\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1708596402629 (Block 461) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The identity is not the difference of \(S T\) and \(T S\)<br><br>There do not exist operators \(S, T \in \mathcal{L}(V)\) such that \(S T\) - \(T S\) = \(I\).
+
+============================================================
----------------------------

=== Note ID: 1708596475022 (Block 462) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose \(T \in \mathcal{L}(V)\) and \(v_{1}, \ldots, v_{n}\) is a basis of \(V\). Prove that the matrix \(\mathcal{M}\left(T,\left(v_{1}, \ldots, v_{n}\right)\right)\) is invertible if and only if \(T\) is invertible.
+
+============================================================
----------------------------

=== Note ID: 1708596697054 (Block 463) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       5 Suppose \(B\) is a square matrix with complex entries. Prove that there exists an invertible square matrix \(A\) with complex entries such that \(A^{-1}\) \(B\)&nbsp;\(A\) is an upper-triangular matrix.
+
+============================================================
----------------------------

=== Note ID: 1708597522029 (Block 464) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       9 Suppose \(P \in \mathcal{L}(V)\) satisfies \(P^{2}\) = \(P\). Prove that<br><br><ul><li>\(\text { trace }\) \(P\) = \(\text { dim range } P \text {. }\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1708598002532 (Block 465) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       15 Suppose \(S, T \in \mathcal{L}(V)\). Prove that \(\operatorname{trace}\) \((S T)\) = \(\operatorname{trace}\) \((T S)\).
+
+============================================================
----------------------------

=== Note ID: 1708598329821 (Block 466) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       18 Suppose \(V\) is an inner product space with orthonormal basis \(e_{1}, \ldots, e_{n}\) and \(T \in \mathcal{L}(V)\). Prove that<br><br><ul><li>\(\operatorname{trace}\) \(\left(T^{*} T\right)\) = \(\left\|T e_{1}\right\|^{2}\) + \(\cdots\) + \(\left\|T e_{n}\right\|^{2} .\)</li></ul><br>Conclude that the right side of the equation above is independent of which orthonormal basis \(e_{1}, \ldots, e_{n}\) is chosen for \(V\).<br>
+
+============================================================
----------------------------

=== Note ID: 1708935664270 (Block 467) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       - Suppose \(P \in \mathcal{L}(V)\) satisfies \(P^2\) = \(P\). <br>Prove that trace \(P\) = \(\operatorname{dim}\) range \(P\).<br><br>This implies that if \(P^2\) = \(P\) then the trace is a nonnegative integer
+
+============================================================
----------------------------

=== Note ID: 1708986194191 (Block 468) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition determinant of an operator<br><br>Suppose \(T \in \mathcal{L}(V)\).<br><br>- If \(\mathbf{F}=\mathbf{C}\), then the determinant of \(T\) is the product of the eigenvalues of \(T\), with each eigenvalue&nbsp;&nbsp;repeated according to its multiplicity.<br><br>- If \(\mathbf{F}=\mathbf{R}\), then the determinant of \(T\) is the product of the eigenvalues of \(T_{\mathbf{C} }\), with each eigenvalue repeated according to its multiplicity.<br>
+
+============================================================
----------------------------

=== Note ID: 1708987132541 (Block 469) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose \(T \in \mathcal{L}(V)\). Then the characteristic polynomial of \(T\) can be written as<br><br><ul><li>\(z^{n}\) - \((\operatorname{trace} T)\) \(z^{n-1}\) +\(\cdots\)+\((-1)^{n}\) \((\operatorname{det} T)\)&nbsp;</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709022341039 (Block 470) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Invertible is equivalent to nonzero determinant<br><br>An operator on \(V\) is invertible if and only if its determinant is nonzero.
+
+============================================================
----------------------------

=== Note ID: 1709022540268 (Block 471) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Characteristic polynomial of \(T\) equals \(\operatorname{det}\) \((z I-T)\)<br><br>Suppose \(T \in \mathcal{L}(V)\). Then the characteristic polynomial of \(T\) equals \(\operatorname{det}\) \((z I-T)\).
+
+============================================================
----------------------------

=== Note ID: 1709023136690 (Block 472) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       10.26 Example Suppose \(a_{1}, \ldots, a_{n} \in \mathbf{F}\). Let<br><br>\[<br>A=\left(\begin{array}{ccccc}<br>0 &amp; &amp; &amp; &amp; a_{n} \\<br>a_{1} &amp; 0 &amp; &amp; &amp; \\<br>&amp; a_{2} &amp; 0 &amp; &amp; \\<br>&amp; &amp; \ddots &amp; \ddots &amp; \\<br>&amp; &amp; &amp; a_{n-1} &amp; 0<br>\end{array}\right) ;<br>\]<br><br>here all entries of the matrix are 0 except for the upper-right corner and along the line just below the diagonal. Suppose \(v_{1}, \ldots, v_{n}\) is a basis of \(V\) and \(T \in \mathcal{L}(V)\) is such that \(\mathcal{M}\left(T,\left(v_{1}, \ldots, v_{n}\right)\right)=A\). Find the determinant of \(T\).<br><br>Solution First assume \(a_{j} \neq 0\) for each \(j=1, \ldots, n-1\). Note that the list \(v_{1}, T v_{1}, T^{2} v_{1}, \ldots, T^{n-1} v_{1}\) equals \(v_{1}, a_{1} v_{2}, a_{1} a_{2} v_{3}, \ldots, a_{1} \cdots a_{n-1} v_{n}\).<br><br>Thus \(v_{1}, T v_{1}, \ldots, T^{n-1} v_{1}\) is linearly independent (because the \(a\) 's are all nonzero). Hence if \(p\) is a monic polynomial with degree at most \(n-1\), then \(p(T) v_{1}\) \(\neq 0\). Thus the minimal polynomial of \(T\) cannot have degree less than \(n\).<br><br>As you should verify, \(T^{n} v_{j}\) = \(a_{1} \cdots a_{n} v_{j}\) for each \(j\). Thus we have \(T^{n}\) = \(a_{1} \cdots a_{n} I\). Hence \(z^{n}-a_{1} \cdots a_{n}\) is the minimal polynomial of \(T\). Because \(n=\operatorname{dim} V\) and the characteristic polynomial is a polynomial multiple of the minimal polynomial (9.26), this implies that \(z^{n}-a_{1} \cdots a_{n}\) is also the characteristic polynomial of \(T\).<br><br>Thus 10.22 implies that<br><br><ul><li>\(\operatorname{det} T\) = \((-1)^{n-1} a_{1} \cdots a_{n} .\)</li></ul><br>If some \(a_{j}\) equals 0 , then \(T v_{j}=0\) for some \(j\), which implies that 0 is an eigenvalue of \(T\) and hence \(\operatorname{det} T=0\). In other words, the formula above also holds if some \(a_{j}\) equals 0 .<br><br>Thus in order to have \(\operatorname{det} T=\operatorname{det} \mathcal{M}(T)\), we will have to make the determinant of the matrix in Example 10.26 equal to \((-1)^{n-1} a_{1} \cdots a_{n}\). However, we do not yet have enough evidence to make a reasonable guess about the proper definition of the determinant of an arbitrary square matrix.<br>
+
+============================================================
----------------------------

=== Note ID: 1709023260623 (Block 473) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition permutation, perm \(n\)<br><br>- A permutation of \((1, \ldots, n)\) is a list \(\left(m_{1}, \ldots, m_{n}\right)\) that contains each of the numbers \(1, \ldots, n\) exactly once.<br><br>- The set of all permutations of \((1, \ldots, n)\) is denoted perm \(n\).
+
+============================================================
----------------------------

=== Note ID: 1709068654662 (Block 474) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       - The sign of a permutation \(\left(m_{1}, \ldots, m_{n}\right)\) is defined to be 1 if the number of pairs of integers \((j, k)\) with \(1 \leq j&lt;k \leq n\) such that \(j\) appears after \(k\) in the list \(\left(m_{1}, \ldots, m_{n}\right)\) is even and -1 if the number of such pairs is odd.<br><br>- In other words, the sign of a permutation equals 1 if the natural order has been changed an even number of times and equals -1 if the natural order has been changed an odd number of times.
+
+============================================================
----------------------------

=== Note ID: 1709106730218 (Block 475) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Determinant of an operator equals determinant of its matrix<br><br>Suppose \(T \in \mathcal{L}(V)\). Then \(\operatorname{det}\) \(T\) = \(\operatorname{det}\) \(\mathcal{M}(T)\).
+
+============================================================
----------------------------

=== Note ID: 1709107314665 (Block 476) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Isometries have determinant with absolute value 1<br><br>Suppose \(V\) is an inner product space and \(S \in \mathcal{L}(V)\) is an isometry. Then \(|\) \(\operatorname{det} S\) \(|\) = \(1\).
+
+============================================================
----------------------------

=== Note ID: 1709192865635 (Block 477) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Recall that if \(V\) is an inner product space and \(T \in \mathcal{L}(V)\), then \(T^{*} T\) is a positive operator and hence has a unique positive square root, denoted \(\sqrt{T^{*} T}\) . Because \(\sqrt{T^{*} T}\) is positive, all its eigenvalues are nonnegative, and hence det \(\sqrt{T^{*} T}\) \(\geq\) \( 0\).
+
+============================================================
----------------------------

=== Note ID: 1709242452341 (Block 478) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       10.46 Example Suppose \(V\) is a real inner product space and \(T \in \mathcal{L}(V)\) is invertible (and thus det \(T\) is either positive or negative). Attach a geometric meaning to the sign of \(\operatorname{det} T\).<br><br>Solution First we consider an isometry \(S \in \mathcal{L}(V)\). By 10.45, the determinant of \(S\) equals 1 or -1 . Note that<br><br>\[<br>\{v \in V: S v=-v\}<br>\]<br><br>is the eigenspace \(E(-1, S)\). Thinking geometrically, we could say that this is the subspace on which \(S\) reverses direction. An examination of proof 2 of 10.45 shows that \(\operatorname{det} S\) = \(1\) if this subspace has even dimension and \(\operatorname{det} S\) = \(-1\) if this subspace has odd dimension.<br><br>Returning to our arbitrary invertible operator \(T \in \mathcal{L}(V)\), by the Polar Decomposition (7.45) there is an isometry \(S \in \mathcal{L}(V)\) such that<br><br>\[<br>T=S \sqrt{T^{*} T}<br>\]<br><br>Now 10.44 tells us that<br><br>\[<br>\operatorname{det} T=(\operatorname{det} S)\left(\operatorname{det} \sqrt{T^{*} T}\right) .<br>\]<br><br>The remarks just before this example pointed out that det \(\sqrt{T^{*} T} \geq 0\). Thus whether \(\operatorname{det} T\) is positive or negative depends on whether \(\operatorname{det} S\) is positive or negative. As we saw in the paragraph above, this depends on whether the subspace on which \(S\) reverses direction has even or odd dimension.<br><br>Because \(T\) is the product of \(S\) and an operator that never reverses direction (namely, \(\sqrt{T^{*} T}\) ), we can reasonably say that whether \(\operatorname{det} T\) is positive or negative depends on whether \(T\) reverses vectors an even or an odd number of times.
+
+============================================================
----------------------------

=== Note ID: 1709242528682 (Block 479) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose \(V\) is an inner product space and \(T \in \mathcal{L}(V)\). Then<br><br><ul><li>\(|\) \(\operatorname{det} T\) \(|\) = \(\operatorname{det} \sqrt{T^{*} T} .\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709242703281 (Block 480) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       A box in \(\mathbf{R}^{n}\) is a set of the form<br><br>\[<br>\left\{\left(y_{1}, \ldots, y_{n}\right) \in \mathbf{R}^{n}: x_{j}&lt;y_{j}&lt;x_{j}+r_{j} \text { for } j=1, \ldots, n\right\},<br>\]<br><br>where \(r_{1}, \ldots, r_{n}\) are positive numbers and \(\left(x_{1}, \ldots, x_{n}\right) \in \mathbf{R}^{n}\). The numbers \(r_{1}, \ldots, r_{n}\) are called the side lengths of the box.
+
+============================================================
----------------------------

=== Note ID: 1709242786058 (Block 481) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition volume of a box<br><br>The volume of a box \(B\) in \(\mathbf{R}^{n}\) with side lengths \(r_{1}, \ldots, r_{n}\) is defined to be \(r_{1} \cdots r_{n}\) and is denoted by volume \(B\).
+
+============================================================
----------------------------

=== Note ID: 1709243005829 (Block 482) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition volume<br><br>Suppose \(\Omega \subset \mathbf{R}^{n}\). Then the volume of \(\Omega\), denoted volume \(\Omega\), is defined to be the infimum of<br><br><ul><li>\(\text { volume } B_{1}+\text { volume } B_{2}+\cdots \text {, }\)</li></ul><br>where the infimum is taken over all sequences \(B_{1}, B_{2}, \ldots\) of boxes in \(\mathbf{R}^{n}\) whose union contains \(\Omega\).<br>
+
+============================================================
----------------------------

=== Note ID: 1709245083939 (Block 483) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose \(S \in \mathcal{L}\left(\mathbf{R}^{n}\right)\) is an isometry and \(\Omega \subset \mathbf{R}^{n}\). Then<br><br><ul><li>\(\text { volume }\)\(S(\Omega)\) = \(\text { volume }\)\( \Omega \text {. }\)</li></ul><br>
+
+============================================================
----------------------------

=== Note ID: 1709245486942 (Block 484) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose \(T \in \mathcal{L}\left(\mathbf{R}^{n}\right)\) and \(\Omega \subset \mathbf{R}^{n}\). Then<br><ul><li>\(\text { volume }\) \(T(\Omega)\) = \(|\) \(\operatorname{det} T\) \(|\) \((\text { volume } \Omega) .\)<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709276287009 (Block 485) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose \(\Omega\) is an open subset of \(\mathbf{R}^{n}\) and \(\sigma\) is a function from \(\Omega\) to \(\mathbf{R}^{n}\). We can write<br><br><ul><li>\(\sigma(x)\) = \(\left(\sigma_{1}(x), \ldots, \sigma_{n}(x)\right),\)</li></ul><br>where each \(\sigma_{j}\) is a function from \(\Omega\) to \(\mathbf{R}\). The partial derivative of \(\sigma_{j}\) with respect to the \(k^{\text {th } } \) coordinate is denoted \(D_{k} \sigma_{j}\). Evaluating this partial derivative at a point \(x \in \Omega\) gives \(D_{k} \sigma_{j}(x)\). If \(\sigma\) is differentiable at \(x\), then the matrix of \(\sigma^{\prime}(x)\) with respect to the standard basis of \(\mathbf{R}^{n}\) contains \(D_{k} \sigma_{j}(x)\) in row \(j\), column \(k\) (this is left as an exercise). In other words,<br><br>\(\mathcal{M}\left(\sigma^{\prime}(x)\right)\) = \(\left(\begin{array}{ccc}D_{1} \sigma_{1}(x) &amp; \ldots &amp; D_{n} \sigma_{1}(x) \\ \vdots &amp; &amp; \vdots \\ D_{1} \sigma_{n}(x) &amp; \ldots &amp; D_{n} \sigma_{n}(x)\end{array}\right)\).<br>
+
+============================================================
----------------------------

=== Note ID: 1709276550489 (Block 486) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Change of variables in an integral</b><br><br>Suppose \(\Omega\) is an open subset of \(\mathbf{R}^{n}\) and \(\sigma: \Omega \rightarrow \mathbf{R}^{n}\) is differentiable at every point of \(\Omega\). If \(f\) is a real-valued function defined on \(\sigma(\Omega)\), then<br><ul><li>\(\int_{\sigma(\Omega)} f(y)\) \(d y\) = \(\int_{\Omega}\) \(f(\sigma(x))\) \(|\) \(\operatorname{det}\) \(\sigma^{\prime}(x)\) \(|\) \(d x\)<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709277181425 (Block 487) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       7 Suppose \(A\) is an \(n\)-by- \(n\) matrix with real entries. Let \(S \in \mathcal{L}\left(\mathbf{C}^{n}\right)\) denote the operator on \(\mathbf{C}^{n}\) whose matrix equals \(A\), and let \(T \in \mathcal{L}\left(\mathbf{R}^{n}\right)\) denote the operator on \(\mathbf{R}^{n}\) whose matrix equals \(A\). <br><br>Prove that:<br><ul><li>&nbsp;\(\operatorname{trace}\) \(S\) = \(\operatorname{trace}\) \(T\)</li><li>\(\operatorname{det}\) \(S\)= \(\operatorname{det}\) \(T\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709369198956 (Block 488) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       For an integral to make sense:<br>Actually, \(\Omega\) in the definition needs to be a reasonable set (for example, open or measurable) and \(f\) needs to be a reasonable function (for example, continuous or measurable).
+
+============================================================
----------------------------

=== Note ID: 1709377028987 (Block 489) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 1.1.1. There is no rational number whose square is 2.<br><br>Proof:<br><br>And so assume, for contradiction, that there exist integers \(p\) and \(q\) satisfying<br><br>\(\left(\frac{p}{q}\right)^2\) = 2<br><br>We may also assume that \(p\) and \(q\) have no common factor, because, if they had one, we could simply cancel it out and rewrite the fraction in lowest terms. Now, equation (1) implies<br><br>\(p^2\) =&nbsp;\(2 q^2\)<br><br>From this, we can see that the integer \(p^{2}\) is an even number (it is divisible by 2 ), and hence \(p\) must be even as well because the square of an odd number is odd. This allows us to write \(p\) = \(2 r\), where \(r\) is also an integer. If we substitute \(2 r\) for \(p\) in equation (2), then a little algebra yields the relationship<br><br>\(2 r^{2}\) = \(q^{2}\)<br>But now the absurdity is at hand. This last equation implies that \(q^{2}\) is even, and hence \(q\) must also be even. Thus, we have shown that \(p\) and \(q\) are both even&nbsp; when they were originally assumed to have no common factor.&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1709377429997 (Block 490) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The next issue after addtion (what the integers get us) is multiplication and division. The number 1 acts as the multiplicative identity, but in order to define division we need to have multiplicative inverses. Thus, we extend our system again to the rational numbers<br><br><ul><li>\(\mathbf{Q}\) = <ul><li>\(\left\{\text { all fractions } \frac{p}{q} \text { where } p \text { and } q \text { are integers with } q \neq 0\right\}\)</li></ul></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709377526864 (Block 491) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Taken together, the properties of Q:<br><ul><li>Closed under addtion</li><li>Addivite identiy</li><li>Addivitve inverse</li><li>Closed under multiplication</li><li>Multiplicative identity</li><li>Multiplicative inverse</li></ul><div>make Q into a field</div>
+
+============================================================
----------------------------

=== Note ID: 1709378592558 (Block 492) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Intuitively speaking, a set is any collection of objects. These objects are referred to as the elements of the set. For our purposes, the sets in question will most often be sets of real numbers, although we will also encounter sets of functions and, on a few occasions, sets whose elements are other sets.
+
+============================================================
----------------------------

=== Note ID: 1709378809948 (Block 493) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The set \(\emptyset\) is called the empty set and is understood to be the set that contains no elements. An equivalent statement would be to say that two sets whose intersection is&nbsp; \(\emptyset\)&nbsp; are disjoint.
+
+============================================================
----------------------------

=== Note ID: 1709378929963 (Block 494) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>The inclusion relationship \(A \subseteq B\) or \(B \supseteq A\) is used to indicate that every element of \(A\) is also an element of \(B\).&nbsp;<br></li><li>In this case, we say \(A\) is a subset of \(B\), or \(B\) contains \(A\). To assert that \(A=B\) means that \(A \subseteq B\) and \(B \subseteq A\).&nbsp;</li><li>Put another way, \(A\) and \(B\) have exactly the same elements.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709379261842 (Block 495) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Given \(A \subseteq \mathbf{R}\), the complement of \(A\), written \(A^{c}\), refers to the set of all elements of \(\mathbf{R}\) not in \(A\). Thus, for \(A \subseteq \mathbf{R}\),<br><br><ul><li>\(A^{c}\) = \(\{x \in \mathbf{R}: x \notin A\} .\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709379813335 (Block 496) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       &nbsp;The absolute value function is so important that it merits the special notation \(|x|\) in place of the usual \(f(x)\) or \(g(x)\). It is defined for every real number via the piecewise definition<br><br><ul><li>\(|x|\) =&nbsp; \(\begin{cases}x &amp; \text { if } x \geq 0 \\ -x &amp; \text { if } x&lt;0\end{cases}\)</li></ul><br>With respect to multiplication and division, the absolute value function satisfies<br><br><ul><li>(i) \(|a b|\) = \(|a||b|\) and</li><li>(ii) \(|a+b|\)&nbsp; \(\leq\) \(|a|+|b|\)</li></ul><br>for all choices of \(a\) and \(b\).&nbsp;<br>
+
+============================================================
----------------------------

=== Note ID: 1709380980907 (Block 497) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Theorem 1.2.6. </b>Two real numbers \(a\) and \(b\) are equal if and only if for every real number \(\epsilon&gt;0\) it follows that \(|a-b|&lt;\epsilon\).<br><br><ul><li>For the second statement, we give a proof by contradiction. The conclusion of the proposition in this direction states that \(a=b\), so we assume that \(a \neq b\).&nbsp;</li><li>Heading off in search of a contradiction brings us to a consideration of the phrase "for every \(\epsilon&gt;0\)." Some equivalent ways to state the hypothesis would be to say that "for all possible choices of \(\epsilon&gt;0\) " or "no matter how \(\epsilon&gt;0\) is selected, it is always the case that \(|a-b|&lt;\epsilon\)."&nbsp;</li><li>But assuming \(a \neq b\) (as we are doing at the moment), the choice of</li><ul><li>\(\epsilon_{0}\) = \(|a-b|&gt;0\)</li><li>poses a serious problem.&nbsp;</li></ul><li>We are assuming that \(|a-b|&lt;\epsilon\) is true for every \(\epsilon&gt;0\), so this must certainly be true of the particular \(\epsilon_{0}\) just defined.&nbsp;</li><li>However, the statements:</li><ul><li>\(|a-b|\) \(&lt;\) \(\epsilon_{0}\)&nbsp;</li><li>\(|a-b|\) = \(\epsilon_{0}\)</li></ul><li>cannot both be true.&nbsp;</li><li>This contradiction means that our initial assumption that \(a \neq b\) is unacceptable.&nbsp;</li><li>Therefore, \(a=b\), and the indirect proof is complete.<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709381303587 (Block 498) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 1.2.5 (De Morgan's Laws). Let \(A\) and \(B\) be subsets of \(\mathbf{R}\).<br><br>(a) If \(x\)&nbsp; \(\in\) \((A \cap B)^{c}\), explain why \(x\) \(\in\) \(A^{c} \cup B^{c}\). This shows that \((A \cap B)^{c}\) \(\subseteq\) \(A^{c} \cup B^{c}\).
+
+============================================================
----------------------------

=== Note ID: 1709381359204 (Block 499) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <br>(b) Prove the reverse inclusion \((A \cap B)^{c}\) \(\supseteq\) \(A^{c} \cup B^{c}\), and conclude that \((A \cap B)^{c}\) = \(A^{c} \cup B^{c}\).
+
+============================================================
----------------------------

=== Note ID: 1709381535816 (Block 500) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 1.2.6. (a) Verify the triangle inequality in the special case where \(a\) and \(b\) have the same sign.<br><br><ul><li>(b) Find an efficient proof for all the cases at once by first demonstrating \((a+b)^{2}\) \( \leq\) \((|a|+|b|)^{2}\).</li><li>(c) Prove \(|a-b|\)&nbsp; \(\leq\) \(|a-c|\)+\(|c-d|\) + \(|d-b|\) for all \(a, b\), \(c\), and \(d\).</li><li>(d) Prove ||\(a|-| b||\) \(\leq\) \(|a-b|\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709382149715 (Block 501) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 1.2.9. Given a function \(f: D \rightarrow \mathbf{R}\) and a subset \(B \subseteq \mathbf{R}\):<br><ul><li>Let \(f^{-1}(B)\) be the set of all points from the domain \(D\) that get mapped into \(B\)</li><li>That is, \(f^{-1}(B)\) = \(\{x \in D: f(x) \in B\}\). This set is called the preimage of \(B\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709384732387 (Block 502) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       (a) Show how induction can be used to conclude that<br><br><ul><li>\((\) \(A_{1} \cup A_{2} \cup \cdots \cup A_{n}\) \()^{c}\) = \(A_{1}^{c}\) \(\cap\) \(A_{2}^{c}\) \(\cap\) \(\cdots\) \(\cap A_{n}^{c}\)</li></ul><br>for any finite \(n \in \mathbf{N}\).<br>
+
+============================================================
----------------------------

=== Note ID: 1709385325335 (Block 503) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>An Initial Definition for \(\mathrm{R}\)<br></b><br><ul><li>First, \(\mathbf{R}\) is a set containing \(\mathbf{Q}\).&nbsp;</li><li>The operations of addition and multiplication on \(\mathbf{Q}\) extend to all of \(\mathbf{R}\) in such a way that every element of \(\mathbf{R}\) has an additive inverse and every nonzero element of \(\mathbf{R}\) has a multiplicative inverse. Echoing the discussion in Section 1.1, we assume \(\mathbf{R}\) is a field, meaning that addition and multiplication of real numbers are commutative, associative, and the distributive property holds.&nbsp;</li><li>We also assume that the familiar properties of the ordering on \(\mathbf{Q}\) extend to all of \(\mathbf{R}\).&nbsp;</li><li>To summarize the situation in the official terminology&nbsp;of the subject, we assume that \(\mathbf{R}\) is an ordered field, which contains \(\mathbf{Q}\) as a subfield</li><li>This brings us to the final, and most distinctive, assumption about the real number system. We must find some way to clearly articulate what we mean by insisting that \(\mathbf{R}\) does not contain the gaps that permeate \(\mathbf{Q}\). Because this is the defining difference between the rational numbers and the real numbers, we will be excessively precise about how we phrase this assumption, hereafter referred to as the <b>Axiom of&nbsp;</b>Completeness.</li><ul><li>Every nonempty set of real numbers that is bounded above has a least upper bound.</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1709385650858 (Block 504) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition 1.3.1.</b>&nbsp;<br><ul><li>A set \(A \subseteq \mathbf{R}\) is bounded above if there exists a number \(b \in \mathbf{R}\) such that \(a\) \(\leq\) \(b\) for all \(a \in A\). The number \(b\) is called an upper bound for \(A\).</li><li>Similarly, the set \(A\) is bounded below if there exists a lower bound \(l \in \mathbf{R}\) satisfying \(l\) \(\leq\) \(a\) for every \(a \in A\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709385727199 (Block 505) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 1.3.1. A set \(A \subseteq \mathbf{R}\) is bounded above if there exists a number \(b\) \(\in\) \(\mathbf{R}\) such that \(a\)&nbsp;&nbsp;\(\leq\) \(b\) for all \(a \in A\). The number \(b\) is called an upper bound for \(A\).
+
+============================================================
----------------------------

=== Note ID: 1709386890930 (Block 506) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition 1.3.2.<br></b><br><ul><li>A real number \(s\) is the least upper bound for a set \(A \subseteq \mathbf{R}\) if it meets the following two criteria:</li><ul><li>(i) \(s\) is an upper bound for \(A\);</li><li>(ii) if \(b\) is any upper bound for \(A\), then \(s\) \(\leq\) \(b\).</li></ul><li>The least upper bound is also frequently called the supremum of the set \(A\).&nbsp;</li><ul><li>Although the notation \(s\) = \(\operatorname{lub} A\) is sometimes used, we will always write \(s=\) \(\sup A\) for the least upper bound.</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1709387099580 (Block 507) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>Definition 1.3.2. A real number \(s\) is the least upper bound for a set \(A \subseteq \mathbf{R}\) if it meets the following two criteria:</li><li>(i) \(s\) is an upper bound for \(A\);</li><li>(ii) if \(b\) is any upper bound for \(A\), then \(s \leq b\).</li></ul><br>Although a set can have a host of upper bounds, it can have only one least upper bound. If \(s_{1}\) and \(s_{2}\) are both least upper bounds for a set \(A\), then by property (ii) in Definition 1.3 .2 we can assert \(s_{1}\) \(\leq\) \(s_{2}\) and \(s_{2}\) \(\leq\) \(s_{1}\). The conclusion is that \(s_{1}=s_{2}\) and least upper bounds are unique.<br>
+
+============================================================
----------------------------

=== Note ID: 1709388407297 (Block 508) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition 1.3.4.</b><br><ul><li>&nbsp;A real number \(a_{0}\) is a maximum of the set \(A\) if \(a_{0}\) is an element of \(A\) and \(a_{0}\) \(\geq\) \(a\) for all \(a \in A\).&nbsp;</li><li>Similarly, a number \(a_{1}\) is a minimum of \(A\) if \(a_{1}\) \(\in\) \(A\) and \(a_{1}\) \(\leq\) \(a\) for every \(a \in A\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709388499675 (Block 509) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Example 1.3.5. To belabor the point, consider the open interval<br><br>\[<br>(0,2)=\{x \in \mathbf{R}: 0&lt;x&lt;2\},<br>\]<br><br>and the closed interval<br><br>\[<br>[0,2]=\{x \in \mathbf{R}: 0 \leq x \leq 2\} .<br>\]<br><br>Both sets are bounded above (and below), and both have the same least upper bound, namely 2 . It is not the case, however, that both sets have a maximum. A maximum is a specific type of upper bound that is required to be an element of the set in question, and the open interval \((0,2)\) does not possess such an element. Thus, the supremum can exist and not be a maximum, but when a maximum exists, then it is also the supremum.
+
+============================================================
----------------------------

=== Note ID: 1709388868517 (Block 510) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Example 1.3.6. Consider again the set<br><br>\[<br>S=\left\{r \in \mathbf{Q}: r^{2}&lt;2\right\},<br>\]<br><br><ul><li>and pretend for the moment that our world consists only of rational numbers.&nbsp;</li><li>The set \(S\) is certainly bounded above. Taking \(b=2\) works, as does \(b=3 / 2\). But notice what happens as we go in search of the least upper bound. (It may be useful here to know that the decimal expansion for \(\sqrt{2}\) begins \(1.4142 \ldots\). .)&nbsp;</li><li>We might try \(b=142 / 100\), which is indeed an upper bound, but then we discover that \(b=1415 / 1000\) is an upper bound that is smaller still. Is there a smallest one?</li><li>In the rational numbers, there is not. In the real numbers, there is.&nbsp;</li><li>Back in \(\mathbf{R}\), the Axiom of Completeness states that we may set \(\alpha=\sup S\) and be confident that such a number exists.&nbsp;</li><li>In the next section, we will prove that \(\alpha^{2}=2\). But according to Theorem 1.1.1, this implies \(\alpha\) is not a rational number.&nbsp;</li><li>If we are restricting our attention to only rational numbers, then \(\alpha\) is not an allowable option for \(\sup S\), and the search for a least upper bound goes on indefinitely.&nbsp;</li><li>Whatever rational upper bound is discovered, it is always possible to find one smaller.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709388987297 (Block 511) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Example 1.3.7. Let \(A \subseteq \mathbf{R}\) be nonempty and bounded above, and let \(c \in \mathbf{R}\). Define the set \(c+A\) by<br><br><ul><li>\(c+A\) = \(\{c+a: a \in A\} .\)</li><li>Then \(\sup\) \((c+A)\) = \(c+\sup A\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709389479039 (Block 512) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       It is certainly the case that all of our conclusions to this point about least upper bounds have analogous versions for greatest lower bounds. The Axiom of Completeness does not explicitly assert that a nonempty set bounded below has an infimum, but this is because we do not need to assume this fact as part of the axiom. Using the Axiom of Completeness, there are several ways to prove that greatest lower bounds exist for nonempty bounded sets.&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1709389717900 (Block 513) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition 1.3.2</b>. A real number \(i\) is the greatest lower bound or infimum for a set \(A\) \(\subseteq\) \( \mathbf{R}\) if it meets the following two criteria:<ul><li>(i) \(i\) is a lower bound for \(A\);</li><li>(ii) if \(b\) is any lower bound for \(A\), then \(i\) \(\geq\) \(b\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709393724882 (Block 514) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 1.3.9. <br><ul><li>(a) If \(\sup A\) &lt; \(\sup B\), show that there exists an element \(b \in B\) that is an upper bound for \(A\).<br></li><li>(b) Give an example to show that this is not always the case if we only assume \(\sup A\) \(\leq\) \(\sup B\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709394068033 (Block 515) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 1.3.10<br><br>The Cut Property of the real numbers is the following:<ul><li>If \(A\) and \(B\) are nonempty, disjoint sets with \(A \cup B\) = \(\mathbf{R}\) and \(a\) &lt; \(b\) for all \(a \in A\) and \(b \in B\), then there exists \(c\)&nbsp; \(\in\) \(\mathbf{R}\) such that \(x\) \(\leq\) \(c\) whenever \(x\) \(\in\) \(A\) and \(x\) \(\geq\) \(c\) whenever \(x\) \(\in\) \(B\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709394153090 (Block 516) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 1.3.10 (Cut Property). The Cut Property of the real numbers is the following:<br><br>If \(A\) and \(B\) are nonempty, disjoint sets with \(A \cup B=\mathbf{R}\) and \(a&lt;b\) for all \(a \in A\) and \(b \in B\), then there exists \(c \in \mathbf{R}\) such that \(x \leq c\) whenever \(x \in A\) and \(x \geq c\) whenever \(x \in B\).<br><br><ul><li>(a) Use the Axiom of Completeness to prove the Cut Property.</li><li>(b) Show that the implication goes the other way; that is, assume \(\mathbf{R}\) possesses the Cut Property and let \(E\) be a nonempty set that is bounded above. Prove \(\sup E\) exists.</li><li>(c) The punchline of parts (a) and (b) is that the Cut Property could be used in place of the Axiom of Completeness as the fundamental axiom that distinguishes the real numbers from the rational numbers. To drive this point home, give a concrete example showing that the Cut Property is not a valid statement when \(\mathbf{R}\) is replaced by \(\mathbf{Q}\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709394528937 (Block 517) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 1.4.1 (Nested Interval<b>&nbsp;Property</b>). <br><br><ul><li>For each \(n \in \mathbf{N}\), assume we are given a closed interval \(I_{n}\) = \(\left[a_{n}, b_{n}\right] \) = \(\left\{x \in \mathbf{R}: a_{n} \leq x \leq b_{n}\right\}\).&nbsp;</li><li>Assume also that each \(I_{n}\) contains \(I_{n+1}\).&nbsp;</li><li>Then, the resulting nested sequence of closed intervals</li><ul><li>\(I_{1}\) \(\supseteq\) \(I_{2}\) \(\supseteq\) \(I_{3}\) \(\supseteq\) \(I_{4}\) \(\supseteq\) \(\cdots\)</li></ul><li>has a nonempty intersection; that is, \(\bigcap_{n=1}^{\infty}\) \(I_{n}\)&nbsp; \(\neq\) \(\emptyset\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709394909450 (Block 518) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-82b591d2788564fcb184d3545fa186cb89d3a466.jpg"><br>Proof. In order to show that \(\bigcap_{n=1}^{\infty} I_{n}\) is not empty, we are going to use the Axiom of Completeness (AoC) to produce a single real number \(x\) satisfying \(x \in I_{n}\) for every \(n \in \mathbf{N}\). Now, AoC is a statement about bounded sets, and the one we want to consider is the set<br><br><ul><li>\(A\) = \(\left\{a_{n}: n \in \mathbf{N}\right\}\)</li></ul><br>of left-hand endpoints of the intervals. as seen in the figure:<br><img src="paste-8125e3d4b152b61d72dd0cc79b9e550fcab212ce.jpg"><br><br>Because the intervals are nested, we see that every \(b_{n}\) serves as an uper bound for \(A\). Thus, we are justified in setting<br><ul><li>\(x\) = \(\sup A \text {. }\)<br></li></ul><br>Now, consider a particular \(I_{n}=\left[a_{n}, b_{n}\right]\). Because \(x\) is an upper bound for \(A\), we have \(a_{n}\) \(\leq\) \(x\). The fact that each \(b_{n}\) is an upper bound for \(A\) and that \(x\) is the least upper bound implies \(x\) \(\leq\) \(b_{n}\).<br><br>Altogether then, we have \(a_{n} \leq x \leq b_{n}\), which means \(x \in I_{n}\) for every choice of \(n \in \mathbf{N}\). Hence, \(x \in \bigcap_{n=1}^{\infty} I_{n}\), and the intersection is not empty.<br>
+
+============================================================
----------------------------

=== Note ID: 1709395050204 (Block 519) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 1.4.2 (Archimedean<b>&nbsp;Property</b>). <br><br><ul><li>(i) Given any number \(x\)&nbsp; \(\in\) \(\mathbf{R}\), there exists an \(n\) \(\in\) \(\mathbf{N}\) satisfying \(n\) \(&gt;\) \(x\).</li><li>(ii) Given any real number \(y\) \(&gt;0\), there exists an \(n\) \(\in\) \(\mathbf{N}\) satisfying \(1 / n\) \(&lt;\) \(y\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709395313481 (Block 520) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 1.4.2 (Archimedean Property). (i) Given any number \(x \in \mathbf{R}\), there exists an \(n \in \mathbf{N}\) satisfying \(n&gt;x\).<br><br>(ii) Given any real number \(y&gt;0\), there exists an \(n \in \mathbf{N}\) satisfying \(1 / n&lt;y\).<br><br><b>Proof:</b><br><ul><li>Part I: And so to the proof. Assume, for contradiction, that \(\mathbf{N}\) is bounded above. By the Axiom of Completeness (AoC), \(\mathbf{N}\) should then have a least upper bound, and we can set \(\alpha\) = \(\sup \mathbf{N}\). If we consider \(\alpha-1\), then we no longer have an upper bound, and therefore there exists an \(n \in \mathbf{N}\) satisfying \(\alpha-1&lt;n\). But this is equivalent to \(\alpha\) &lt; \(n+1\). Because \(n+1\)&nbsp; \(\in \mathbf{N}\), we have a contradiction to the fact that \(\alpha\) is supposed to be an upper bound for \(\mathbf{N}\). (Notice that the contradiction here depends only on AoC and the fact that \(\mathbf{N}\) is closed under addition.)</li><li>Part (ii) follows from (i) by letting \(x\) = \(1 / y\).<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709395425250 (Block 521) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 1.4.3 (Density&nbsp;of \(\mathbf{Q}\) in \(\mathbf{R}\) ). <br><br>For every two real numbers \(a\) and \(b\) with \(a\) &lt; \(b\), there exists a rational&nbsp;number \(r\) satisfying \(a\) &lt; r &lt; \(b\).
+
+============================================================
----------------------------

=== Note ID: 1709395588018 (Block 522) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Corollary</b> 1.4.4. Given any two real numbers \(a\) \(&lt;\) \(b\), there exists an irrational number \(t\) satisfying \(a\) &lt; \(t\) &lt; \(b\).
+
+============================================================
----------------------------

=== Note ID: 1709396201315 (Block 523) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 1.4.5. There exists a real number \(\alpha \in \mathbf{R}\) satisfying \(\alpha^{2}=2\).<br><br>Proof. After reviewing Example 1.3.6, consider the set<br><br>\[<br>T=\left\{t \in \mathbf{R}: t^{2}&lt;2\right\}<br>\]<br>and set \(\alpha=\sup T\).&nbsp;<br><br><br>Let's first see what happens if we assume \(\alpha^{2}&lt;2\). In search of an element of \(T\) that is larger than \(\alpha\), write<br><br><ul><li>\(\left(\alpha+\frac{1}{n}\right)^2\) =&nbsp;\(\alpha^2+\frac{2 \alpha}{n}+\frac{1}{n^2}\)</li><li>&lt;&nbsp;\(\alpha^2+\frac{2 \alpha}{n}+\frac{1}{n}\)</li><li>=&nbsp;\(\alpha^2+\frac{2 \alpha+1}{n}\).</li></ul><div><br></div><div>But now assuming \(\alpha^{2}&lt;2\) gives us a little space in which to fit the \((2 \alpha+1) / n\) term and keep the total less than 2 . Specifically, choose \(n_{0} \in \mathbf{N}\) large enough so that<br><br><ul><li>\(\frac{1}{n_{0}}\) \(&lt;\) \(\frac{2-\alpha^{2} }{2 \alpha+1}\)</li></ul><br>This implies \((2 \alpha+1) / n_{0}\) &lt; \(2-\alpha^{2}\), and consequently that</div><div><br><ul><li>\(\left(\alpha+\frac{1}{n_{0}}\right)^{2}\) &lt; \(\alpha^{2}+\left(2-\alpha^{2}\right)\) = \(2 .\)</li></ul><div><br></div><div>Now, what about the case \(\alpha^{2}&gt;2\) ? This time, write<br></div></div><div><ul><li>\(\left(\alpha+\frac{1}{n}\right)^2\) =&nbsp;\(\alpha^2+\frac{2 \alpha}{n}+\frac{1}{n^2}\)</li><li>&gt;&nbsp;\(\alpha^2-\frac{2 \alpha}{n}\).</li></ul><div><br></div></div>
+
+============================================================
----------------------------

=== Note ID: 1709397193027 (Block 524) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 1.4.1. Recall that I stands for the set of irrational numbers.<br><ul><li>(a) Show that if \(a, b \in \mathbf{Q}\), then \(a b\) and \(a+b\) are elements of \(\mathbf{Q}\) as well<br></li><li>(b) Show that if \(a \in \mathbf{Q}\) and \(t \in \mathbf{I}\), then \(a+t\) \(\in\) \(\mathbf{I}\) and \(a t\) \(\in\) \(\mathbf{I}\) as long as \(a\) \(\neq\) \(0\).</li><li>(c) Part (a) can be summarized by saying that \(\mathbf{Q}\) is closed under addition and multiplication. Is I closed under addition and multiplication?&nbsp;</li><ul><li>Answer: no</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1709399857200 (Block 525) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition 1.5.1</b>. <br><br><ul><li>A function \(f: A \rightarrow B\) is one-to-one (1-1) if \(a_{1}\) \(\neq\) \(a_{2}\) in \(A\) implies that \(f\left(a_{1}\right)\) \(\neq\) \(f\left(a_{2}\right)\) in \(B\).&nbsp;</li><li>The function \(f\) is onto if, given any \(b \in B\), it is possible to find an element \(a \in A\) for which \(f(a)\)= \(b\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709399913310 (Block 526) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       A function \(f: A \rightarrow B\) that is both 1-1 and onto provides us with exactly what we mean by a \(1-1\) correspondence between two sets. The property of being 1-1 means that no two elements of \(A\) correspond to the same element of \(B\) , and the property of being onto ensures that every element of \(B\) corresponds to something in \(A\)&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1709403075504 (Block 527) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition 1.5.5</b>. A set \(A\) is countable if \(\mathbf{N} \sim A\). An infinite set that is not countable is called an uncountable set.
+
+============================================================
----------------------------

=== Note ID: 1709403971804 (Block 528) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       &nbsp;It is an important exercise to show that any subset of a countable set must be either countable or finite.
+
+============================================================
----------------------------

=== Note ID: 1709403993743 (Block 529) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       This means that countable sets are the smallest type of infinite set. Anything smaller is either still countable or finite.
+
+============================================================
----------------------------

=== Note ID: 1709404121335 (Block 530) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>&nbsp;The set \(\mathbf{Q}\), on the other hand, is countable.&nbsp;</li><li>As far as infinite sets are concerned, this is as small as it gets.&nbsp;</li><li>What does this imply about the set I of irrational numbers?&nbsp;</li><li>By imitating the demonstration that \(\mathbf{N} \sim \mathbf{Z}\), we can prove that the union of two countable sets must be countable.&nbsp;</li><li>Because \(\mathbf{R}\) = \(\mathbf{Q} \cup \mathbf{I}\), it follows that \(\mathbf{I}\) cannot be countable because otherwise \(\mathbf{R}\) would be.&nbsp;</li><li>The inescapable conclusion is that, the irrational numbers form a far greater subset of \(\mathbf{R}\) than \(\mathbf{Q}\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709404152106 (Block 531) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Theorem</b> 1.5.7. If \(A\) \(\subseteq\) \(B\) and \(B\) is countable, then \(A\) is either countable or finite.
+
+============================================================
----------------------------

=== Note ID: 1709410042987 (Block 532) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 1.5.8. <br><ul><li><br></li><li>(i) If \(A_{1}, A_{2}, \ldots A_{m}\) are each countable sets, then the union \(A_{1} \cup A_{2} \cup \cdots \cup A_{m}\) is countable.</li><li>(ii) If \(A_{n}\) is a countable set for each \(n \in \mathbf{N}\), then \(\bigcup_{n=1}^{\infty}\) \(A_{n}\) is countable.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709410126365 (Block 533) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Assume \(B\) is a countable set. Thus, there exists \(f: \mathbf{N} \rightarrow B\), which is \(1-1\) and onto. Let \(A \subseteq B\) be an infinite subset of \(B\). We must show that \(A\) is countable.
+
+============================================================
----------------------------

=== Note ID: 1709410411951 (Block 534) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       (c) Using open intervals makes it more convenient to produce the required \(1-1\), onto functions, but it is not really necessary. Show that \([0,1)\) \(\sim\) \((0,1)\) by exhibiting a 1-1 onto function between the two sets.
+
+============================================================
----------------------------

=== Note ID: 1709410441376 (Block 535) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 1.5.5. (a) Why is \(A\) \(\sim\) \(A\) for every set \(A\) ?
+
+============================================================
----------------------------

=== Note ID: 1709410486498 (Block 536) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       (b) Given sets \(A\) and \(B\), explain why \(A\) \(\sim\) \(B\) is equivalent to asserting \(B\) \(\sim\) \(A\).
+
+============================================================
----------------------------

=== Note ID: 1709410555711 (Block 537) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       (c) For three sets \(A, B\), and \(C\), show that \(A\) \(\sim\) \(B\) and \(B\) \(\sim\) \(C\) implies \(A\) \(\sim\) \(C\).
+
+============================================================
----------------------------

=== Note ID: 1709410805033 (Block 538) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 1.5.9. A real number \(x \in \mathbf{R}\) is called algebraic if there exist integers \(a_{0}, a_{1}, a_{2}, \ldots, a_{n} \in \mathbf{Z}\), not all zero, such that<br><br><ul><li>\(\sum_{i=0}^{n}\) \(a_i x^i\)&nbsp;= \(0 .\)</li></ul><br>Said another way, a real number is algebraic if it is the root of a polynomial with integer coefficients. Real numbers that are not algebraic are called transcendental numbers. Reread the last paragraph of Section 1.1. The final question posed here is closely related to the question of whether or not transcendental numbers exist.<br>
+
+============================================================
----------------------------

=== Note ID: 1709410995011 (Block 539) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 1.5.9. A real number \(x \in \mathbf{R}\) is called algebraic if there exist integers \(a_{0}, a_{1}, a_{2}, \ldots, a_{n} \in \mathbf{Z}\), not all zero, such that<br><br>\[<br>a_{n} x^{n}+a_{n-1} x^{n-1}+\cdots+a_{1} x+a_{0}=0 .<br>\]<br><ul><li>(a) Show that \(\sqrt{2}, \sqrt[3]{2}\), and \(\sqrt{3}+\sqrt{2}\) are::are/not::are/not::are/not::are/not::are/not algebraic.</li><li>(b) Fix \(n \in \mathbf{N}\), and let \(A_{n}\) be the algebraic numbers obtained as roots of polynomials with integer coefficients that have degree \(n\). Using the fact that every polynomial has a finite number of roots, show that \(A_{n}\) is countable.</li><li>(c) Now, argue that the set of all algebraic numbers is countable.&nbsp;</li><ul><li>Thus we may conclude transcendental numebers are uncountable</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1709411083805 (Block 540) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 1.5.10. (a) Let \(C \subseteq\) \([0,1]\) be uncountable. Show that there exists \(a\) \(\in\) \((0,1)\) such that \(C \) \(\cap\) \([a, 1]\) is uncountable.
+
+============================================================
----------------------------

=== Note ID: 1709411194326 (Block 541) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 1.5.11 (Schröder-Bernstein Theorem). Assume there exists a 1-1 function \(f: X \rightarrow Y\) and another 1-1 function \(g: Y \rightarrow X\). Follow the steps to show that there exists a 1-1, onto function \(h:\) \(X \rightarrow Y\) and hence \(X\)&nbsp; \(\sim\) \(Y\).
+
+============================================================
----------------------------

=== Note ID: 1709411294840 (Block 542) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 1.5.11 (Schröder-Bernstein Theorem). Assume there exists a 1-1 function \(f: X \rightarrow Y\) and another 1-1 function \(g: Y \rightarrow X\). Follow the steps to show that there exists a 1-1, onto function \(h: X \rightarrow Y\) and hence \(X \sim Y\).<br><br>The strategy is to partition \(X\) and \(Y\) into components<br><br><ul><li>\(X\) = \(A \cup A^{\prime} \)</li><li>\(Y\) = \(B \cup B^{\prime}\)</li></ul><br>with \(A \cap A^{\prime}\) = \(\emptyset\) and \(B \cap B^{\prime}\) = \(\emptyset\), in such a way that \(f\) maps \(A\) onto \(B\), and \(g\) maps \(B^{\prime}\) onto \(A^{\prime}\).<br>
+
+============================================================
----------------------------

=== Note ID: 1709411607147 (Block 543) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 1.6.1. Show that \((0,1)\) is uncountable if and only if \(\mathbf{R}\) is uncountable.&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1709460323398 (Block 544) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Given a set \(A\), the power set \(P(A)\) refers to the collection of all subsets of \(A\). It is important to understand that \(P(A)\) is itself considered a set whose elements are the different possible subsets of \(A\).
+
+============================================================
----------------------------

=== Note ID: 1709461189125 (Block 545) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Theorem 1.6.2 (Cantor's Theorem). Given any set \(A\), there does not exist a function \(f: A \rightarrow P(A)\) that is onto.</b><br><br>Proof. This proof, like the others of its kind, is indirect. Thus, assume, for contradiction, that \(f: A \rightarrow P(A)\) is onto. Unlike the usual situation in which we have sets of numbers for the domain and range, \(f\) is a correspondence between a set and its power set. For each element \(a \in A, f(a)\) is a particular subset of \(A\).<br><br><br>Construct \(B\) using the following rule:<br><ul><li>\(B\) = \(\{a \in A: a \notin f(a)\} .\)<br></li></ul><div>We now focus on the general argument. Because we have assumed that our function \(f: A \rightarrow P(A)\) is onto, it must be that \(B\) = \(f\left(a^{\prime}\right)\) for some \(a^{\prime} \in A\). The contradiction arises when we consider whether or not \(a^{\prime}\) is an element of \(B\).</div><div><br></div><div>Exercise 1.6.8. (a) First, show that the case \(a^{\prime}\)&nbsp; \(\in\)&nbsp; \(B\) leads to a contradiction.<br><br>(b) Now, finish the argument by showing that the case \(a^{\prime}\)&nbsp; \(\notin\) \(B\) is equally unacceptable.<br></div>
+
+============================================================
----------------------------

=== Note ID: 1709461481795 (Block 546) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Two sets appear in the same group, or equivalence class, if and only if they have the same cardinality. Thus, \(\mathbf{N}, \mathbf{Z}\), and \(\mathbf{Q}\) are grouped together in one class with all of the&nbsp;other countable sets, whereas \(\mathbf{R}\) is in another class that includes the intervals \((a, b)\) as well as \(P(\mathbf{N})\).&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1709461539740 (Block 547) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       &nbsp;One implication of Cantor's Theorem is that \(P(\mathbf{R})\) - the set of all subsets of \(\mathbf{R}\) - is in a different class from \(\mathbf{R}\), and there is no reason to stop here. The set of subsets of \(P(\mathbf{R})\) - namely \(P(P(\mathbf{R}))\) - is in yet another class, and this process continues indefinitely.
+
+============================================================
----------------------------

=== Note ID: 1709461870515 (Block 548) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       A more downto-earth problem in need of attention is demonstrating that our definition of " \(\leq\) " between cardinal numbers really is an ordering. This involves showing that cardinal numbers possess a property analogous to real numbers, which states that if \(\operatorname{card} X \leq \operatorname{card} Y\) and \(\operatorname{card} Y \leq \operatorname{card} X\), then \(\operatorname{card} X=\operatorname{card} Y\). In the end, this boils down to proving that if there exists \(f: X \rightarrow Y\) that is \(1-1\), and if there exists \(g: Y \rightarrow X\) that is \(1-1\), then it is possible to find a function \(h: X \rightarrow Y\) that is both 1-1 and onto.&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1709534610935 (Block 549) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Consider the infinite series<br><ul><li>\(\sum_{n=1}^{\infty}\) \(\frac{(-1)^{n+1} }{n}\)&nbsp;= \(1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\frac{1}{5}-\frac{1}{6}+\frac{1}{7}-\frac{1}{8}+\cdots\)</li></ul><br>If we naively begin adding from the left-hand side, we get a sequence of what are called partial sums. In other words, let \(s_{n}\) equal the sum of the first \(n\) terms of the series, so that \(s_{1}=1, s_{2}=1 / 2, s_{3}=5 / 6, s_{4}=7 / 12\), and so on. One immediate observation is that the successive sums oscillate in a progressively narrower space. The odd sums decrease \(\left(s_{1}&gt;s_{3}&gt;s_{5}&gt;\ldots\right)\) while the even sums increase \(\left(s_{2}&lt;s_{4}&lt;s_{6}&lt;\ldots\right.\) ). As shown in the following figure<br><br><img src="paste-79643f453b463079d965aae31704eba344f8b31e.jpg"><br><br>It seems reasonable - and we will soon prove - that the sequence \(\left(s_{n}\right)\) eventually hones in on a value, call it \(S\), where the odd and even partial sums "meet."<br>
+
+============================================================
----------------------------

=== Note ID: 1709536784899 (Block 550) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 2.2.3 (Convergence&nbsp;of a Sequence):<br><ul><li>A sequence \(\left(a_{n}\right)\) converges to a real number \(a\) if, for every positive number \(\epsilon\), there exists an \(N \in \mathbf{N}\) such that whenever \(n\) \(\geq\) \(N\) it follows that:</li><ul><li>&nbsp;\(|\) \(a_{n}-a\) \(|\) &lt; \(\epsilon\).</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1709537103479 (Block 551) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition</b> 2.2.4. Given a real number \(a \in \mathbf{R}\) and a positive number \(\epsilon\) \(&gt;\) \(0\), the set<br><br><ul><li>\(V_{\epsilon}(a)\) = \(\{\) \(x \in \mathbf{R}\) : \(|x-a|\) \(&lt;\) \(\epsilon\) \(\}\)</li></ul><br>is called the \(\epsilon\)-neighborhood of \(a\).<br>
+
+============================================================
----------------------------

=== Note ID: 1709537244871 (Block 552) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Notice that \(V_{\epsilon}(a)\) consists of all of those points whose distance from \(a\) is less than \(\epsilon\). Said another way, \(V_{\epsilon}(a)\) is an interval, centered at \(a\), with radius \(\epsilon\) as shown in the following figure<br><br><img src="2bJsgJdaSiTgiXeKSRq_TdOQ1Ln2IU5lkv_OapXjnDw.original.fullsize.png"><br><br>Recasting the definition of convergence in terms of \(\epsilon\)-neighborhoods gives a more geometric impression of what is being described.
+
+============================================================
----------------------------

=== Note ID: 1709537445781 (Block 553) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 2.2.3B (Convergence of a Sequence: Topological Version):<br><ul><li>&nbsp;A sequence \(\left(a_{n}\right)\) converges to \(a\) if, given any \(\epsilon\)-neighborhood \(V_{\epsilon}(a)\) of \(a\), there exists a point in the sequence after which all of the terms are in \(V_{\epsilon}(a)\).&nbsp;</li><li>In other words, every \(\epsilon\)-neighborhood contains all but a finite number of the terms of \(\left(a_{n}\right)\).</li></ul><br><br>As shown in the following figure:<br><img src="paste-17aeb92339aa6b54792faef3af6093ad37bac36f.jpg"><br>
+
+============================================================
----------------------------

=== Note ID: 1709537508915 (Block 554) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The natural number \(N\) in the original version of the convergence definition is the point where the sequence \(\left(a_{n}\right)\) enters \(V_{\epsilon}(a)\), never to leave. It should be apparent that the value of \(N\) depends on the choice of \(\epsilon\). The smaller the \(\epsilon\)-neighborhood, the larger \(N\) may have to be.
+
+============================================================
----------------------------

=== Note ID: 1709707878449 (Block 555) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       TEMplate for A PROOF THAT \(\left(x_{n}\right) \rightarrow x\) :<br><ul><li>- "Let \(\epsilon\) \(&gt;\) \(0\) be arbitrary."</li><li>- Demonstrate a choice for \(N\)&nbsp; \(\in \mathbf{N}\).&nbsp;</li><li>- Now, show that \(N\) actually works.</li><li>- "Assume \(n\) \(\geq\) \(N\)."</li><li>- With \(N\) well chosen, it should be possible to derive the inequality \(\left|x_{n}-x\right|\) \(&lt;\) \(\epsilon\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709709299812 (Block 556) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 2.2.7. Here are two useful definitions:<br><br><ul><li>(i) A sequence \(\left(a_{n}\right)\) is eventually in a set \(A \subseteq \mathbf{R}\) if there exists an \(N \in \mathbf{N}\) such that \(a_{n}\) \(\in A\) for all \(n \geq N\).</li><li>(ii) A sequence \(\left(a_{n}\right)\) is frequently in a set \(A \subseteq \mathbf{R}\) if, for every \(N \in \mathbf{N}\), there exists an \(n \geq N\) such that \(a_{n}\) \(\in A\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709709652708 (Block 557) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 2.3.1. A sequence \(\left(x_{n}\right)\) is bounded if there exists a number \(M\) &gt; \(0\) such that \(\left|x_{n}\right|\) \(\leq\) \(M\) for all \(n \in \mathbf{N}\).<br>
+
+============================================================
----------------------------

=== Note ID: 1709710317865 (Block 558) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 2.3.2. Every convergent sequence is bounded.<br><br>Proof. Assume \(\left(x_{n}\right)\) converges to a limit \(l\). This means that given a particular value of \(\epsilon\), say \(\epsilon=1\), we know there must exist an \(N \in \mathbf{N}\) such that if \(n \geq N\), then \(x_{n}\) is in the interval \((l-1, l+1)\). Not knowing whether \(l\) is positive or negative, we can certainly conclude that<br><ul><li>\(\left|x_{n}\right|\) &lt; \(|l|+1\)<br></li></ul><br>for all \(n \geq N\) as shown in the figure<br><br><img src="paste-515c094bd5b761e5a75a79ed5446007b6f07cc0f.jpg"><br>We still need to worry (slightly) about the terms in the sequence that come before the \(N\) th term. Because there are only a finite number of these, we let<br><br><ul><li>\(M\) = \(\max \left\{\left|x_{1}\right|,\left|x_{2}\right|,\left|x_{3}\right|, \ldots,\left|x_{N-1}\right|,|l|+1\right\} .\)</li></ul><br>It follows that \(\left|x_{n}\right|\)&nbsp; \(\leq M\) for all \(n \in \mathbf{N}\), as desired.<br><br>
+
+============================================================
----------------------------

=== Note ID: 1709710815979 (Block 559) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 2.3.3 (Algebraic Limit Theorem). Let \(\lim a_{n}=a\), and \(\lim b_{n}=\) b. Then,<br><ol><li>(i) \(\lim \left(c a_{n}\right)=c a\), for all \(c \in \mathbf{R}\);</li></ol>Proof. (i) Consider the case where \(c \neq 0\). We want to show that the sequence \(\left(c a_{n}\right)\) converges to \(c a\), so the structure of the proof follows the template we described in Section 2.2. First, we let \(\epsilon\) be some arbitrary positive number. Our goal is to find some point in the sequence \(\left(c a_{n}\right)\) after which we have<br><br><ul><li>\(\left|c a_{n}-c a\right|\) &lt; \(\epsilon\)</li></ul><br>Now,<br><br><ul><li>\(\left|c a_{n}-c a\right|\) = \(|c|\left|a_{n}-a\right| .\)</li></ul><br>We are given that \(\left(a_{n}\right) \rightarrow a\), so we know we can make \(\left|a_{n}-a\right|\) as small as we like. In particular, we can choose an \(N\) such that<br><br><ul><li>\(\left|a_{n}-a\right|\) \(&lt;\) \(\frac{\epsilon}{|c|}\)</li></ul><br>whenever \(n \geq N\). To see that this \(N\) indeed works, observe that, for all \(n \geq N\),<br><br><ul><li>\(\left|c a_{n}-c a\right|\) = \(|c|\left|a_{n}-a\right|\) &lt; \(|c| \frac{\epsilon}{|c|}\) = \(\epsilon\)</li></ul><br>The case \(c=0\) reduces to showing that the constant sequence \((0,0,0, \ldots)\) converges to 0 , which is easily verified.<br>
+
+============================================================
----------------------------

=== Note ID: 1709713087895 (Block 560) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       &nbsp;Limits can be computed from the individual component sequences provided that each component limit exists.
+
+============================================================
----------------------------

=== Note ID: 1709760036708 (Block 561) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 2.3.4 (Order Limit Theorem). Assume \(\lim\) \(a_{n}\) = \(a\) and \(\lim b_{n}\) = \(b\).<br><ul><li>(i) If \(a_{n}\)&nbsp; \(\geq\) \(0\) for all \(n\) \(\in\) \(\mathbf{N}\), then \(a\) \(\geq\) \(0\).</li><li>(ii) If \(a_{n}\) \(\leq\) \(b_{n}\) for all \(n\) \(\in\) \(\mathbf{N}\), then \(a\) \(\leq\) \(b\).</li><li>(iii) If there exists \(c\) \(\in\) \(\mathbf{R}\) for which \(c\) \(\leq\) \(b_{n}\) for all \(n \in \mathbf{N}\), then \(c\) \(\leq\) \(b\).&nbsp;</li><ul><li>Similarly, if \(a_{n}\) \(\leq\) \(c\) for all \(n\) \(\in\) \(\mathbf{N}\), then \(a\) \(\leq\) \(c\).</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1709760218730 (Block 562) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 2.3.4 (Order Limit Theorem). Assume \(\lim a_{n}=a\) and \(\lim b_{n}=b\).<br><br>(i) If \(a_{n} \geq 0\) for all \(n \in \mathbf{N}\), then \(a \geq 0\).<br><br><br>Proof. (i) We will prove this by contradiction; thus, let's assume \(a&lt;0\). The idea is to produce a term in the sequence \(\left(a_{n}\right)\) that is also less than zero. To do this, we consider the particular value \(\epsilon\) = \(|a|\). The definition of convergence guarantees that we can find an \(N\) such that \(\left|a_{n}-a\right|\) &lt; \(|a|\) for all \(n \geq N\). In particular, this would mean that \(\left|a_{N}-a\right|\) \(&lt;\) \(|a|\), which implies \(a_{N}\) &lt; \(0\). This contradicts our hypothesis that \(a_{N} \geq 0\). We therefore conclude that \(a \geq 0\).<br><br>Ass seen in the figure:<br><img src="paste-fb03c153b4593cc1e8ec890a70d3c024cb470ec3.jpg">
+
+============================================================
----------------------------

=== Note ID: 1709760447743 (Block 563) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       In the language of analysis, when a property (such as non-negativity) is not necessarily possessed by some finite number of initial terms but is possessed&nbsp; by all terms in the sequence after some point \(N\), we say that the sequence eventually has this property. (See Exercise 2.2.7.) Theorem 2.3.4, part (i), could be restated, "Convergent sequences that are eventually nonnegative converge to nonnegative limits."
+
+============================================================
----------------------------

=== Note ID: 1709760689022 (Block 564) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 2.3.3 (Squeeze Theorem):<br><ul><li>&nbsp;Show that if \(x_{n}\) \(\leq\) \(y_{n}\) \(\leq\) \(z_{n}\) for all \(n \in \mathbf{N}\)</li><li>And if \(\lim x_{n}\) = \(\lim z_{n}\) = \(l\)</li><li>Then \(\lim y_{n}\) = \(l\)&nbsp;</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709761070348 (Block 565) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 2.3.11 (Cesaro Means). <br><br><ul><li>(a) Show that if \(\left(x_{n}\right)\) is a convergent sequence, then the sequence given by the averages</li><ul><li>\(y_{n}\) = \(\frac{x_{1}+x_{2}+\cdots+x_{n} }{n}\)</li><li>converges to the same limit.</li></ul><li>(b) Give an example to show that it is possible for the sequence \(\left(y_{n}\right)\) of averages to converge even if \(\left(x_{n}\right)\) does not.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709761668787 (Block 566) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 2.3.13 (Iterated Limits). Given a doubly indexed array \(a_{m n}\) where \(m, n \in \mathbf{N}\), what should \(\lim _{m, n \rightarrow \infty} a_{m n}\) represent?<br><br><ul><li>(a) Let \(a_{m n}=m /(m+n)\) and compute the iterated limits</li><ul><ul><li>\(\lim _{n \rightarrow \infty}\left(\lim _{m \rightarrow \infty} a_{m n}\right)\)&nbsp;&nbsp;</li><li>\( \lim _{m \rightarrow \infty}\left(\lim _{n \rightarrow \infty} a_{m n}\right)\)</li></ul><li>Define \(\lim _{m, n \rightarrow \infty} a_{m n}\)=\(a\) to mean that for all \(\epsilon\) &gt; \(0\) there exists an \(N \in \mathbf{N}\) such that if both \(m, n \geq N\), then \(\left|a_{m n}-a\right|\) &lt; \(\epsilon\).</li></ul><li>(c) Let \(a_{m n}=1 /(m+n)\), Produce an example where \(\lim _{m, n \rightarrow \infty} a_{m n}\) exists but where neither iterated limit can be computed.</li><li>(d) Assume \(\lim _{m, n \rightarrow \infty} a_{m n}=a\), and assume that for each fixed \(m \in \mathbf{N}\), \(\lim _{n \rightarrow \infty}\left(a_{m n}\right) \rightarrow b_{m}\). Show \(\lim _{m \rightarrow \infty} b_{m}\) = \(a\).</li><li>(e) Prove that if \(\lim _{m, n \rightarrow \infty} a_{m n}\) exists and the iterated limits both exist, then all three limits must be equal.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709761826977 (Block 567) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 2.4.1. <br><ul><li>A sequence \(\left(a_{n}\right)\) is increasing if \(a_{n}\) \(\leq a_{n+1}\) for all \(n \in \mathbf{N}\) and decreasing if \(a_{n}\)&nbsp; \(\geq a_{n+1}\) for all \(n \in \mathbf{N}\).&nbsp;</li><li>A sequence is monotone if it is either increasing or decreasing.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709762152626 (Block 568) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 2.4.2 (Monotone Convergence Theorem). If a sequence is monotone and bounded, then it converges.<br><br>Proof. Let \(\left(a_{n}\right)\) be monotone and bounded. To prove \(\left(a_{n}\right)\) converges using the definition of convergence, we are going to need a candidate for the limit. <br><ul><li>Let's assume the sequence is increasing (the decreasing case is handled similarly), and consider the set of points \(\left\{a_{n}: n \in \mathbf{N}\right\}\). By assumption, this set is bounded, so we can let</li></ul><ul><ul><li>\(s\) = \(\sup \left\{a_{n}: n \in \mathbf{N}\right\}\)</li></ul></ul><br>It seems reasonable to claim that \(\lim a_{n}\) = \(s\) as shown in the figure:<br><img src="paste-d9580e49717308ae667f3f6a572fa5e73f93c8c7.jpg"><br><br>To prove this, let \(\epsilon&gt;0\). Because \(s\) is the least upper bound for \(\left\{a_{n}: n \in \mathbf{N}\right\}\), \(s-\epsilon\) is not an upper bound, so there exists a point in the sequence \(a_{N}\) such that \(s-\epsilon\) &lt; \(a_{N}\). Now, the fact that \(\left(a_{n}\right)\) is increasing implies that if \(n \geq N\), then \(a_{N}\)&nbsp; \(\leq a_{n}\). Hence,<br><br><ul><li>\(s-\epsilon\) &lt; \(a_{N}\) \(\leq\) \(a_{n}\) \(\leq\) \(s\)&lt; \(s+\epsilon\)</li></ul><br>which implies \(\left|a_{n}-s\right|\) &lt; \(\epsilon\), as desired.<br>
+
+============================================================
----------------------------

=== Note ID: 1709762376279 (Block 569) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition</b> 2.4.3 (Convergence of a Series). Let \(\left(b_{n}\right)\) be a sequence. An infinite series is a formal expression of the form<br><br><ul><li>\(\sum_{n=1}^{\infty}\) \( b_{n}\) = \(b_{1}+b_{2}+b_{3}+b_{4}+b_{5}+\cdots\)</li></ul><br>We define the corresponding sequence of partial sums \(\left(s_{m}\right)\) by<br><br><ul><li>\(s_{m}\) = \(b_{1}+b_{2}+b_{3}+\cdots+b_{m},\)</li></ul><br>and say that the series \(\sum_{n=1}^{\infty}\) \(b_{n}\) converges to \(B\) if the sequence \(\left(s_{m}\right)\) converges to \(B\). In this case, we write \(\sum_{n=1}^{\infty}\) \(b_{n}\) = \(B\).<br>
+
+============================================================
----------------------------

=== Note ID: 1709762435681 (Block 570) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 2.4.3 (Convergence of a Series). Let \(\left(b_{n}\right)\) be a sequence. An infinite series is a formal expression of the form<br><br>\[<br>\sum_{n=1}^{\infty} b_{n}=b_{1}+b_{2}+b_{3}+b_{4}+b_{5}+\cdots<br>\]<br><br>We define the corresponding sequence of partial sums \(\left(s_{m}\right)\) by<br><br>\[<br>s_{m}=b_{1}+b_{2}+b_{3}+\cdots+b_{m},<br>\]<br><br>and say that the series \(\sum_{n=1}^{\infty} b_{n}\) converges to \(B\) if the sequence \(\left(s_{m}\right)\) converges to \(B\). In this case, we write \(\sum_{n=1}^{\infty} b_{n}\) = \(B\).
+
+============================================================
----------------------------

=== Note ID: 1709794331551 (Block 571) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 2.4.4. (a) In Section 1.4 we used the Axiom of Completeness (AoC) to prove the Archimedean Property of \(\mathbf{R}\) (Theorem 1.4.2). Show that the Monotone Convergence Theorem can also be used to prove the Archimedean Property without making any use of AoC.
+
+============================================================
----------------------------

=== Note ID: 1709795042914 (Block 572) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 2.4.7 (Limit Superior). Let \(\left(a_{n}\right)\) be a bounded sequence.<br><br><ul><li>(a) Prove that the sequence defined by \(y_{n}=\sup \left\{a_{k}: k \geq n\right\}\) converges.</li><li>(b) The limit superior of \(\left(a_{n}\right)\), or \(\lim \sup a_{n}\), is defined by</li><ul><ul><li>\[\limsup a_{n}=\lim y_{n},\]</li></ul><li>where \(y_{n}\) is the sequence from part (a) of this exercise. Provide a reasonable definition for \(\lim \inf a_{n}\) and briefly explain why it always exists for any bounded sequence.</li></ul><li>(c) Prove that \(\liminf a_{n}\) \(\leq\) \(\lim \sup a_{n}\) for every bounded sequence.</li><li>(d) Show that \(\lim \inf a_{n}\) = \(\lim \sup a_{n}\) if and only if \(\lim a_{n}\) exists. In this case, all three share the same value.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709795255396 (Block 573) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Consider the special class of infinite products of the form<br><br><ul><li>\(\prod_{n=1}^{\infty}\) \(\left(1+a_{n}\right)\)= \(\left(1+a_{1}\right)\left(1+a_{2}\right)\left(1+a_{3}\right) \cdots\)</li><li>\(\text { where } a_{n} \geq 0\)</li></ul><br>(b) Show, in general, that the sequence of partial products converges if and only if \(\sum_{n=1}^{\infty} a_{n}\) converges. (The inequality \(1+x\) \(\leq\) \(3^{x}\) for positive \(x\) will be useful in one direction.)<br>
+
+============================================================
----------------------------

=== Note ID: 1709796216831 (Block 574) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Example 2.5.3. Let \(0&lt;b&lt;1\). Because<br><br>\[<br>b&gt;b^{2}&gt;b^{3}&gt;b^{4}&gt;\cdots&gt;0<br>\]<br><br>the sequence \(\left(b^{n}\right)\) is decreasing and bounded below. <br><br><ul><li>The Monotone Convergence Theorem allows us to conclude that \(\left(b^{n}\right)\) converges to some \(l\) satisfying \(b&gt;l \geq 0\).&nbsp;</li><li>To compute \(l\), notice that \(\left(b^{2 n}\right)\) is a subsequence, so \(\left(b^{2 n}\right) \rightarrow\) \(l\) by Theorem 2.5.2.&nbsp;</li><li>But \(b^{2 n}\)= \(b^{n} \cdot b^{n}\), so by the Algebraic Limit Theorem, \(\left(b^{2 n}\right) \rightarrow l \cdot l\) = \(l^{2}\). Because limits are unique (Theorem 2.2.7), \(l^{2}\) = \(l\), and thus \(l\) = \(0\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709796859581 (Block 575) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 2.5.3. (a) Prove that if an infinite series converges, then the associative property holds. Assume \(a_{1}+a_{2}+a_{3}+a_{4}+a_{5}+\cdots\) converges to a limit \(L\) . Show that any regrouping of the terms:<br><br><ul><li>\(\left(a_{1}+a_{2}+\cdots+a_{n_{1}&nbsp; }\right)+\left(a_{n_{1}+1}+\cdots+a_{n_{2} }\right)+\left(a_{n_{2}+1}+\cdots+a_{n_{3} }\right)+\cdots\)</li></ul><br>leads to a series that also converges to \(L\).<br>
+
+============================================================
----------------------------

=== Note ID: 1709797092655 (Block 576) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 2.5.8. Another way to prove the Bolzano-Weierstrass Theorem is to show that every sequence contains a monotone subsequence. A useful device in this endeavor is the notion of a peak term. Given a sequence \(\left(x_{n}\right)\), a particular term \(x_{m}\) is a peak term if no later term in the sequence exceeds it; i.e., if \(x_{m} \) \(\geq x_{n}\) for all \(n \geq m\).
+
+============================================================
----------------------------

=== Note ID: 1709797392230 (Block 577) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       To spoil the surprise, we will argue in this section that in fact these two definitions are equivalent: Convergent sequences are Cauchy sequences, and Cauchy sequences converge. The significance of the definition of a Cauchy sequence is that there is no mention of a limit.&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1709797591616 (Block 578) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Lemma 2.6.3. Cauchy sequences are bounded.<br><br>Proof. Given \(\epsilon\) = \(1\), there exists an \(N\) such that \(\left|x_{m}-x_{n}\right|\) &lt; \(1\) for all \(m, n \geq N\). Thus, we must have \(\left|x_{n}\right|\) \(&lt;\) \(\left|x_{N}\right|+1\) for all \(n \geq N\). It follows that<br><ul><li>\(M\) = \(\max \left\{\left|x_{1}\right|,\left|x_{2}\right|,\left|x_{3}\right|, \ldots,\left|x_{N-1}\right|,\left|x_{N}\right|+1\right\}\)</li></ul><br>is a bound for the sequence \(\left(x_{n}\right)\).<br>
+
+============================================================
----------------------------

=== Note ID: 1709797647785 (Block 579) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 2.6.4 (Cauchy Criterion). A sequence converges if and only if it is a Cauchy sequence.
+
+============================================================
----------------------------

=== Note ID: 1709798381005 (Block 580) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 2.7.1 (Algebraic Limit Theorem for Series). If \(\sum_{k=1}^{\infty} a_{k}\) = \(A\) and \(\sum_{k=1}^{\infty} b_{k}\) = \(B\), then<br><br><ul><li>(i) \(\sum_{k=1}^{\infty}\) \(c a_{k}\) = \(c A\) for all \(c \in \mathbf{R}\) and</li><li>(ii) \(\sum_{k=1}^{\infty}\) \(\left(a_{k}+b_{k}\right)\) = \(A+B\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709798626220 (Block 581) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 2.7.2 (Cauchy Criterion for Series). The series \(\sum_{k=1}^{\infty} a_{k}\) converges if and only if, given \(\epsilon\) \(&gt;0\), there exists an \(N \in \mathbf{N}\) such that whenever \(n&gt;m\) \(\geq N\) it follows that<br><br><ul><li>\(\left|a_{m+1}+a_{m+2}+\cdots+a_{n}\right|\) &lt; \(\epsilon\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709817670288 (Block 582) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       &nbsp;Theorem 2.7.4 (Comparison Test). Assume \(\left(a_{k}\right)\) and \(\left(b_{k}\right)\) are sequences satisfying \(0\) \(\leq\) \(a_{k}\) \(\leq\) \(b_{k}\) for all \(k \in \mathbf{N}\).<br><br><ul><li>(i) If \(\sum_{k=1}^{\infty}\) \(b_{k}\) converges, then \(\sum_{k=1}^{\infty}\) \(a_{k}\) converges.</li><li>(ii) If \(\sum_{k=1}^{\infty}\) \(a_{k}\) diverges, then \(\sum_{k=1}^{\infty}\) \(b_{k}\) diverges.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1709818085570 (Block 583) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 2.7.6 (Absolute Convergence Test). If the series \(\sum_{n=1}^{\infty}\)\(\left|a_{n}\right|\) converges, then \(\sum_{n=1}^{\infty}\) \(a_{n}\) converges as well.
+
+============================================================
----------------------------

=== Note ID: 1710095993312 (Block 584) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition</b> 2.7.8. If \(\sum_{n=1}^{\infty}\) \(\left|a_{n}\right|\) converges, then we say that the original series \(\sum_{n=1}^{\infty}\) \(a_{n}\) converges absolutely. If, on the other hand, the series \(\sum_{n=1}^{\infty}\) \( a_{n}\) converges but the series \(\sum_{n=1}^{\infty}\) \(\left|a_{n}\right|\) does not converge, then we say that the original series \(\sum_{n=1}^{\infty}\) \(a_{n}\) converges conditionally.
+
+============================================================
----------------------------

=== Note ID: 1710096372026 (Block 585) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition</b> 2.7.9. Let \(\sum_{k=1}^{\infty}\) \(a_{k}\) be a series. A series \(\sum_{k=1}^{\infty}\) \(b_{k}\) is called a rearrangement of \(\sum_{k=1}^{\infty}\) \(a_{k}\) if there exists a one-to-one, onto function \(f:\) \(\mathbf{N}\) \(\rightarrow \) \(\mathbf{N}\) such that \(b_{f(k)}\) = \(a_{k}\) for all \(k \in \mathbf{N}\).
+
+============================================================
----------------------------

=== Note ID: 1710099139055 (Block 586) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 2.7.10. If a series converges absolutely, then any rearrangement of this series converges to the same limit.<br><br>Proof. Assume \(\sum_{k=1}^{\infty} a_{k}\) converges absolutely to \(A\), and let \(\sum_{k=1}^{\infty} b_{k}\) be a rearrangement of \(\sum_{k=1}^{\infty} a_{k}\). Let's use<br><br><ul><li>\(s_{n}\) = \(\sum_{k=1}^{n} a_{k}\) = \(a_{1}+a_{2}+\cdots+a_{n}\)</li></ul><br>for the partial sums of the original series and use<br><br><ul><li>\(t_{m}\) = \(\sum_{k=1}^{m} b_{k}\) = \(b_{1}+b_{2}+\cdots+b_{m}\)</li></ul><br>for the partial sums of the rearranged series. Thus we want to show that \(\left(t_{m}\right) \rightarrow A\).<br><br>Let \(\epsilon&gt;0\). By hypothesis, \(\left(s_{n}\right) \rightarrow A\), so choose \(N_{1}\) such that<br><br><ul><li>\(\left|s_{n}-A\right|\) &lt; \(\frac{\epsilon}{2}\)</li></ul>for all \(n \geq N_{1}\). <br><br>Because the convergence is absolute, we can choose \(N_{2}\) so that<br><br><ul><li>\(\sum_{k=m+1}^{n}\) \(\left|a_{k}\right|\) &lt; \(\frac{\epsilon}{2}\)</li></ul><br>for all \(n&gt;m \geq N_{2}\). Now, take \(N\) = \(\max \left\{N_{1}, N_{2}\right\}\). We know that the finite set of terms \(\left\{a_{1}, a_{2}, a_{3}, \ldots, a_{N}\right\}\) must all appear in the rearranged series, and we want to move far enough out in the series \(\sum_{n=1}^{\infty} b_{n}\) so that we have included all of these terms. Thus, choose<br><br><ul><li>\(M\) = \(\max \{f(k): 1 \leq k \leq N\}\)</li></ul><br>It should now be evident that if \(m \geq M\), then \(\left(t_{m}-s_{N}\right)\) consists of a finite set of terms, the absolute values of which appear in the tail \(\sum_{k=N+1}^{\infty}\left|a_{k}\right|\). Our choice of \(N_{2}\) earlier then guarantees \(\left|t_{m}-s_{N}\right|\) &lt; \(\epsilon / 2\), and so<br><br><ul><li>\(\left|t_{m}-A\right| \) =&nbsp;</li><li>= \(\left|t_{m}-s_{N}+s_{N}-A\right| \)</li><li>\( \leq\) \(\left|t_{m}-s_{N}\right|+\left|s_{N}-A\right| \)</li><li>\( &lt;\) \(\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon\)</li></ul><br>whenever \(m \geq M\).<br>
+
+============================================================
----------------------------

=== Note ID: 1710100093282 (Block 587) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>Exercise 2.7.7.&nbsp;</li><li>(a) Show that if \(a_{n}\) \(&gt;0\) and \(\lim\) \(\left(n a_{n}\right)\) = \(l\) with \(l\) \(\neq 0\), then the series \(\sum\) \(a_{n}\) diverges.</li><li>(b) Assume \(a_{n}\) \(&gt;0\) and \(\lim\) \(\left(n^{2} a_{n}\right)\) exists. Show that \(\sum\) \(a_{n}\) converges.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1710100256259 (Block 588) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 2.7.9 (Ratio Test). Given a series \(\sum_{n=1}^{\infty} a_{n}\) with \(a_{n}\) \(\neq 0\), the Ratio Test states that if \(\left(a_{n}\right)\) satisfies<br><br><ul><li>\(\lim\) \(\left|\frac{a_{n+1} }{a_{n} }\right|\) = \(r\) \(&lt;\) \(1\)</li></ul><br>then the series converges absolutely.<br>
+
+============================================================
----------------------------

=== Note ID: 1710101735370 (Block 589) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 2.7.12 (Summation-by-parts). Let \(\left(x_{n}\right)\) and \(\left(y_{n}\right)\) be sequences, let \(s_{n}\) = \(x_{1}+x_{2}+\cdots+x_{n}\) and set \(s_{0}\) = \(0\). Use the observation that \(x_{j}\) = \(s_{j}-s_{j-1}\) to verify the formula<br><ul><li>\(\sum_{j=m}^{n}\)&nbsp; \(x_{j} y_{j}\) = \(s_{n} y_{n+1}\) - \(s_{m-1} y_{m}\) + \(\sum_{j=m}^{n}\) \(s_{j}\) (\(y_{j}-y_{j+1}\))<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1710101872457 (Block 590) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Exercise</b> 2.7.13 (Abel's Test). Abel's Test for convergence states that if the series \(\sum_{k=1}^{\infty}\) \(x_{k}\) converges, and if \(\left(y_{k}\right)\) is a sequence satisfying<br><ul><li>\(y_{1}\) \(\geq\) \(y_{2}\) \(\geq\) \(y_{3}\) \(\geq\) \(\cdots\) \(\geq\) \(0\)<br></li></ul><br>then the series \(\sum_{k=1}^{\infty}\) \(x_{k} y_{k}\) converges.<br>
+
+============================================================
----------------------------

=== Note ID: 1710102009664 (Block 591) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 2.7.13 (Abel's Test). Abel's Test for convergence states that if the series \(\sum_{k=1}^{\infty} x_{k}\) converges, and if \(\left(y_{k}\right)\) is a sequence satisfying<br><br>\[<br>y_{1} \geq y_{2} \geq y_{3} \geq \cdots \geq 0<br>\]<br><br>then the series \(\sum_{k=1}^{\infty} x_{k} y_{k}\) converges.<br><br><ul><li>(a) Use Exercise 2.7.12 to show that</li><ul><li>\(\sum_{k=1}^{n}\) \(x_{k} y_{k}\) = \(s_{n} y_{n+1}\) + \(\sum_{k=1}^{n}\)&nbsp; \(s_{k}\) ( \(y_{k}-y_{k+1}\) )</li><li>where \(s_{n}\) = \(\sum_{i=1}^n x_i\).</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1710102304544 (Block 592) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 2.7.14 (Dirichlet's Test). Dirichlet's Test for convergence states that:<br><ul><li>If the partial sums of \(\sum_{k=1}^{\infty}\) \(x_{k}\) are bounded (but not necessarily convergent),&nbsp;</li><li>And if ( \(\left.y_{k}\right)\) is a sequence satisfying \(y_{1}\) \(\geq\) \(y_{2}\) \(\geq\) \(y_{3}\) \(\geq\) \(\cdots\) \(\geq\) \(0\) with \(\lim y_{k}\) = \(0\)</li><li>Then the series \(\sum_{k=1}^{\infty}\) \(x_{k} y_{k}\) converges.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1710102693834 (Block 593) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       There are still other ways to reasonably define \(\sum_{i, j=1}^{\infty} a_{i j}\). One natural idea is to calculate a kind of partial sum by adding together finite numbers of terms in larger and larger "rectangles" in the array; that is, for \(m, n \in \mathbf{N}\), set<br><br>\[<br>\begin{equation*}<br>s_{m n}=\sum_{i=1}^{m} \sum_{j=1}^{n} a_{i j} \tag{1}<br>\end{equation*}<br>\]<br><br>The order of the sum here is irrelevant because the sum is finite. Of particular interest to our discussion are the sums \(s_{n n}\) (sums over "squares"), which form a legitimate sequence indexed by \(n\) and thus can be subjected to our arsenal of theorems and definitions. If the sequence \(\left(s_{n n}\right)\) converges, for instance, we might wish to define<br><br><ul><li>\(\sum_{i, j=1}^{\infty} a_{i j}\) = \(\lim _{n \rightarrow \infty}\) \(s_{n n}\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1710102813119 (Block 594) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       There is a deep similarity between the issue of how to define a double summation and the topic of rearrangements. Both relate to the commutativity of addition in an infinite setting. For rearrangements, the resolution came with the added hypothesis of absolute convergence, and it is not surprising that the same remedy applies for double summations.
+
+============================================================
----------------------------

=== Note ID: 1710102886436 (Block 595) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Under the assumption of absolute convergence taking the limit over rows then column, columns then rows or over squares gives the same result for double summation
+
+============================================================
----------------------------

=== Note ID: 1710103305623 (Block 596) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 2.8.1. Let \(\left\{a_{i j}: i, j \in \mathbf{N}\right\}\) be a doubly indexed array of real numbers. If<br><br><ul><li>\(\sum_{i=1}^{\infty}\) \(\sum_{j=1}^{\infty}\) \(\left|a_{i j}\right|\)</li></ul><br>converges, then both \(\sum_{i=1}^{\infty}\) \(\sum_{j=1}^{\infty}\) \(a_{i j}\) and \(\sum_{j=1}^{\infty}\) \(\sum_{i=1}^{\infty}\) \(a_{i j}\) converge to the same value. Moreover,<br><br><ul><li>\(\lim _{n \rightarrow \infty}\) \(s_{n n}\)&nbsp; =</li><li>= \(\sum_{i=1}^{\infty} \sum_{j=1}^{\infty}\) \(a_{i j}\)&nbsp;</li><li>= \(\sum_{j=1}^{\infty} \sum_{i=1}^{\infty}\) \(a_{i j}\)</li></ul><br>where \(s_{n n}\) = \(\sum_{i=1}^{n} \sum_{j=1}^{n} a_{i j}\).<br>
+
+============================================================
----------------------------

=== Note ID: 1710227413350 (Block 597) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       One final common way of computing a double summation is to sum along diagonals where \(i+j\) equals a constant. Given a doubly indexed array \(\left\{a_{i j}\right.\) : \(i, j \in \mathbf{N}\}\), let<br><br><ul><li>\(d_{k}\) = \(a_{1, k-1}\) + \(a_{2, k-2}\) +\(\cdots\)+ \(a_{k-1,1} .\)</li></ul><br>Then, \(\sum_{k=2}^{\infty}\) \(d_{k}\) represents another reasonable way of summing over every \(a_{i j}\) in the array.<br>
+
+============================================================
----------------------------

=== Note ID: 1710228396841 (Block 598) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 2.8.7. Assume that \(\sum_{i=1}^{\infty} a_{i}\) converges absolutely to \(A\), and \(\sum_{j=1}^{\infty} b_{j}\) converges absolutely to \(B\).<br><br><br>(a) Show that the iterated sum \(\sum_{i=1}^{\infty} \sum_{j=1}^{\infty}\) \(\left|a_{i} b_{j}\right|\) converges so that we may apply Theorem 2.8.1.<br><br>(b) Let \(s_{n n}\) =\(\sum_{i=1}^{n}\) \(\sum_{j=1}^{n}\) \(a_{i} b_{j}\), and prove that \(\lim _{n \rightarrow \infty} s_{n n}\) = \(A B\). Conclude that<br><br><ul><li>\(\sum_{i=1}^{\infty} \sum_{j=1}^{\infty}\) \(a_{i} b_{j}\)&nbsp;</li><li>= \(\sum_{j=1}^{\infty} \sum_{i=1}^{\infty}\) \(a_{i} b_{j} \)</li><li>= \(\sum_{k=2}^{\infty}\) \(d_{k}\)&nbsp;</li><li>= \(A B\)</li></ul><br>where, as before, \(d_{k}\) = \(a_{1} b_{k-1}+a_{2} b_{k-2}+\cdots+a_{k-1} b_{1}\).<br>
+
+============================================================
----------------------------

=== Note ID: 1710228584504 (Block 599) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       In the case of rearrangements, not only are conditionally convergent series no longer guaranteed to converge to the same limit, but in fact if \(\sum_{n=1}^{\infty} a_{n}\) converges conditionally, then for any \(r \in \mathbf{R}\) there exists a rearrangement of \(\sum_{n=1}^{\infty} a_{n}\) that converges to \(r\).&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1710228871987 (Block 600) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Perhaps the best way to summarize the situation is to say that the hypothesis of absolute convergence essentially allows us to treat infinite sums as though they were finite sums. This assessment extends to double sums as well
+
+============================================================
----------------------------

=== Note ID: 1710228980406 (Block 601) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       We showed in Exercise 2.8.7 that the Cauchy product of two absolutely convergent infinite series converges to the product of the two factors, but in fact the same conclusion follows if we only have absolute convergence in one of the two original series. In the notation of Exercise 2.8.7, if \(\sum a_{n}\) converges absolutely to \(A\), and if \(\sum b_{n}\) converges&nbsp; to \(B\), then the Cauchy product \(\sum d_{k}\) = \(A B\).
+
+============================================================
----------------------------

=== Note ID: 1710229072640 (Block 602) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Of course, it is also possible to find \(\sum a_{n}\) = \(A\) conditionally and \(\sum b_{n}\) = \(B\) conditionally whose Cauchy product \(\sum d_{k}\) converges. If this is the case, then the convergence is to the right value, namely \(\sum d_{k}\) = \(A B\).
+
+============================================================
----------------------------

=== Note ID: 1710229470854 (Block 603) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Let \(C_{0}\) be the closed interval \([0,1]\), and define \(C_{1}\) to be the set that results when the open middle third is removed; that is,<br><br><ul><li>\(C_{1}\) =</li><li>= \(C_{0} \backslash\left(\frac{1}{3}, \frac{2}{3}\right)\)&nbsp;</li><li>= \(\left[0, \frac{1}{3}\right] \cup\left[\frac{2}{3}, 1\right]\)</li></ul><br>Now, construct \(C_{2}\) in a similar way by removing the open middle third of each of the two components of \(C_{1}\) :<br><br><ul><li>\(C_{2}\) =&nbsp;</li><li>\(\left(\left[0, \frac{1}{9}\right] \cup\left[\frac{2}{9}, \frac{1}{3}\right]\right) \cup\left(\left[\frac{2}{3}, \frac{7}{9}\right] \cup\left[\frac{8}{9}, 1\right]\right) .\)</li></ul><br>If we continue this process inductively, then for each \(n=0,1,2, \ldots\) we get a set \(C_{n}\) consisting of \(2^{n}\)::how many?::how many?::how many? closed intervals each having length \(1 / 3^{n}\). Finally, we define the Cantor set \(C\) (Fig. 3.1) to be the intersection<br><ul><li>\(C\) = \(\bigcap_{n=0}^{\infty}\) \(C_{n}\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1710229660533 (Block 604) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       It may be useful to understand the cantor set \(C\) as the remainder of the interval \([0,1]\) after the iterative process of removing open middle thirds is taken to infinity:<br><br><ul><li>\(C\) = \([0,1]\) \(\backslash \) [\(\left(\frac{1}{3}, \frac{2}{3}\right)\)&nbsp; \(\cup\) \(\left(\frac{1}{9}, \frac{2}{9}\right)\)&nbsp; \(\cup\) \(\left(\frac{7}{9}, \frac{8}{9}\right)\) \(\cup\) \(\cdots\) ]</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1710229944618 (Block 605) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-5b106ae2d07c386ea9fa88da651c450b6caba5f6.jpg"><br>Figure 3.1: Defining the Cantor set; \(C=\bigcap_{n=0}^{\infty} C_{n}\).<br><br>It may be useful to understand \(C\) as the remainder of the interval \([0,1]\) after the iterative process of removing open middle thirds is taken to infinity:<br><br>\[<br>C=[0,1] \backslash\left[\left(\frac{1}{3}, \frac{2}{3}\right) \cup\left(\frac{1}{9}, \frac{2}{9}\right) \cup\left(\frac{7}{9}, \frac{8}{9}\right) \cup \cdots\right] .<br>\]<br><br><ul><li>There is some initial doubt whether anything remains at all,&nbsp;</li><li>But notice that because we are always removing open middle thirds, then for every \(n \in \mathbf{N}\), \(0\) \(\in C_{n}\) and hence \(0\) \(\in C\).&nbsp;</li><li>The same argument shows \(1\) \(\in C\).&nbsp;</li><li>In fact, if \(y\) is the endpoint of some closed interval of some particular set \(C_{n}\), then it is also an endpoint of one of the intervals of \(C_{n+1}\).&nbsp;</li><li>Because, at each stage, endpoints are never removed, it follows that \(y\) \(\in C_{n}\) for all \(n\). Thus, \(C\) at least contains the endpoints of all of the intervals that make up each of the sets \(C_{n}\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1710230413421 (Block 606) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       It may be useful to understand \(C\) as the remainder of the interval \([0,1]\) after the iterative process of removing open middle thirds is taken to infinity:<br><br>\[<br>C=[0,1] \backslash\left[\left(\frac{1}{3}, \frac{2}{3}\right) \cup\left(\frac{1}{9}, \frac{2}{9}\right) \cup\left(\frac{7}{9}, \frac{8}{9}\right) \cup \cdots\right] .<br>\]<br><br>There is some strong evidence that not much is left in \(C\) if we consider the total length of the intervals removed. To form \(C_{1}\), an open interval of length \(1 / 3\) was taken out. In the second step, we removed two intervals of length \(1 / 9\), and to construct \(C_{n}\) we removed \(2^{n-1}\) middle thirds of length \(1 / 3^{n}\). There is some logic, then, to defining the "length" of \(C\) to be 1 minus the total<br><br><ul><li>\(1\) - \(\sum_{I=1}^{n}\) \(2^{i-1} \frac{1}{3^i}\) = \(1\) - \(\frac{\frac{1}{3} }{1-\frac{2}{3} }\) = \(0\)</li></ul><br>The Cantor set has zero length.<br>
+
+============================================================
----------------------------

=== Note ID: 1710230523929 (Block 607) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The cantor set:<br>\[<br>C=[0,1] \backslash\left[\left(\frac{1}{3}, \frac{2}{3}\right) \cup\left(\frac{1}{9}, \frac{2}{9}\right) \cup\left(\frac{7}{9}, \frac{8}{9}\right) \cup \cdots\right] .<br>\]<br><br>To this point, the information we have collected suggests a mental picture of \(C\) as a relatively small, thin set. For these reasons, the set \(C\) is often referred to as Cantor "dust." But there are some strong counterarguments that imply a very different picture. However, \(C\) is actually uncountable, with cardinality equal to the cardinality of \(\mathbf{R}\).
+
+============================================================
----------------------------

=== Note ID: 1710235274428 (Block 608) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       &nbsp;First, \(C\) is actually uncountable, with cardinality equal to the cardinality of \(\mathbf{R}\). One slightly intuitive but convincing way to see this is to create a 1-1 correspondence between \(C\) and sequences of the form \(\left(a_{n}\right)_{n=1}^{\infty}\), where \(a_{n}\) = (\(0\) or 1) . For each \(c \in C\), set \(a_{1}\) = \(0\) if \(c\) falls in the left-hand component of \(C_{1}\) and set \(a_{1}\) = \(1\) if \(c\) falls in the right-hand component. Having established where in \(C_{1}\) the point \(c\) is located, there are now two possible components of \(C_{2}\) that might contain \(c\). This time, we set \(a_{2}\) = \(0\) or 1 depending on whether \(c\) falls in the left or right half of these two components of \(C_{2}\). Continuing in this way, we come to see that every element \(c \in C\) yields a sequence \(\left(a_{1}, a_{2}, a_{3}, \ldots\right)\) of zeros and ones that acts as a set of directions for how to locate \(c\) within \(C\). Likewise, every such sequence corresponds to a point in the Cantor set. Because the set of sequences of zeros and ones is uncountable (Exercise 1.6.4), we must conclude that \(C\) is uncountable as well.
+
+============================================================
----------------------------

=== Note ID: 1710279502488 (Block 609) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Given \(a \in \mathbf{R}\) and \(\epsilon&gt;0\), recall that the \(\epsilon\)-neighborhood of \(a\) is the set<br><br><ul><li>\(V_{\epsilon}(a)\) = \(\{x \in \mathbf{R}:|x-a|&lt;\epsilon\} .\)</li></ul><br>In other words, \(V_{\epsilon}(a)\) is the open interval \((a-\epsilon, a+\epsilon)\), centered at \(a\) with radius \(\epsilon\).<br>
+
+============================================================
----------------------------

=== Note ID: 1710279643226 (Block 610) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Example 3.2.2. (i) Perhaps the simplest example of an open set is \(\mathbf{R}\) itself. Given an arbitrary element \(a \in \mathbf{R}\), we are free to pick any \(\epsilon\)-neighborhood we like and it will always be true that \(V_{\epsilon}(a)\)&nbsp; \(\subseteq \mathbf{R}\). It is also the case that the logical structure of Definition 3.2.1 requires us to classify the empty set \(\emptyset\) as an open subset of the real line.
+
+============================================================
----------------------------

=== Note ID: 1710279861044 (Block 611) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Theorem</b> 3.2.3. <br><ul><li>(i) The union of an arbitrary collection of open sets is open.</li><li>(ii) The intersection of a finite collection of open sets is open.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1710281096680 (Block 612) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 3.2.5. \(A\) point \(x\) is a limit point of a set \(A\) if and only if \(x=\lim a_{n}\) for some sequence \(\left(a_{n}\right)\) contained in \(A\) satisfying \(a_{n} \neq x\) for all \(n \in \mathbf{N}\).<br><br>The restriction that \(a_{n} \neq x\) in Theorem 3.2.5 deserves a comment. Given a point \(a \in A\), it is always the case that \(a\) is the limit of a sequence in \(A\) if we are allowed to consider the constant sequence \((a, a, a, \ldots)\). There will be occasions where we will want to avoid this somewhat uninteresting situation, so it is important to have a vocabulary that can distinguish limit points of a set from isolated points.
+
+============================================================
----------------------------

=== Note ID: 1710323391531 (Block 613) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Example 3.2.9. (i) Consider<br><br>\[<br>A=\left\{\frac{1}{n}: n \in \mathbf{N}\right\}<br>\]<br><br>Let's show that each point of \(A\) is isolated. Given \(1 / n \in A\), choose \(\epsilon\) =\(1 / n-1 /(n+1)\). Then,<br><br><ul><li>\(V_{\epsilon}(1 / n) \cap A\) = \(\left\{\frac{1}{n}\right\}\)</li></ul><br>It follows from Definition 3.2.4 that \(1 / n\) is not a limit point and so is isolated. Although all of the points of \(A\) are isolated, the set does have&nbsp;<br>one limit point, namely 0 . This is because every neighborhood centered at zero, no matter how small, is going to contain points of \(A\). Because \(0\) \(\notin A, A\) is not closed. The set \(F\) = \(A \cup\{0\}\) is an example of a closed set and is called the closure of \(A\)<br><br>
+
+============================================================
----------------------------

=== Note ID: 1710323659229 (Block 614) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       (ii) Let's prove that a closed interval<br><br>\[<br>[c, d]=\{x \in \mathbf{R}: c \leq x \leq d\}<br>\]<br><br>is a closed set using Definition 3.2.7. If \(x\) is a limit point of \([c, d]\), then by Theorem 3.2.5 there exists \(\left(x_{n}\right)\) \(\subseteq[c, d]\) with \(\left(x_{n}\right) \rightarrow x\). We need to prove that \(x \in[c, d]\).<br><br>The key to this argument is contained in the Order Limit Theorem (Theorem 2.3.4), which summarizes the relationship between inequalities and the limiting process. Because \(c\) \(\leq\) \(x_{n}\) \(\leq\) \(d\), it follows from Theorem 2.3.4 (iii) that \(c\) \(\leq\) \(x\) \(\leq\) \(d\).<div>Thus, \([c, d]\) is closed.</div>
+
+============================================================
----------------------------

=== Note ID: 1710371071554 (Block 615) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Theorem</b> 3.2.12. For any \(A \subseteq \mathbf{R}\), the closure::name::name::name::name \(\bar{A}\)::notation::notation::notation::notation is a closed set and is the smallest closed set containing \(A\).
+
+============================================================
----------------------------

=== Note ID: 1710371185580 (Block 616) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 3.2.12. For any \(A \subseteq \mathbf{R}\), the closure \(\bar{A}\) is a closed set and is the smallest closed set containing \(A\).<br><br>Proof. If \(L\) is the set of limit points of \(A\), then it is immediately clear that \(\bar{A}\) contains the limit points of \(A\). There is still something more to prove, however, because taking the union of \(L\) with \(A\) could potentially produce some new limit points of \(\bar{A}\). In Exercise 3.2.7, we outline the argument that this does not happen.<br><br>Now, any closed set containing \(A\) must contain \(L\) as well. This shows that \(\bar{A}\) = \(A \cup L\) is the smallest closed set containing \(A\).
+
+============================================================
----------------------------

=== Note ID: 1710371287138 (Block 617) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If a set is not open, that does not imply it must be closed. Many sets such as the half-open interval \((c, d]\) = \(\{x \in \mathbf{R}: c&lt;x \leq d \} \) are neither open nor closed.&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1710371399514 (Block 618) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Theorem</b> 3.2.13. A set \(O\) is open if and only if \(O^{c}\) is closed. Likewise, a set \(F\) is closed if and only if \(F^{c}\) is open.
+
+============================================================
----------------------------

=== Note ID: 1710371495262 (Block 619) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 3.2.13. A set \(O\) is open if and only if \(O^{c}\) is closed. Likewise, a set \(F\) is closed if and only if \(F^{c}\) is open.<br><br>Proof. Given an open set \(O \subseteq \mathbf{R}\), let's first prove that \(O^{c}\) is a closed set. To prove \(O^{c}\) is closed, we need to show that it contains all of its limit points. If \(x\) is a limit point of \(O^{c}\), then every neighborhood of \(x\) contains some point of \(O^{c}\). But that is enough to conclude that \(x\) cannot be in the open set \(O\) because \(x \in O\) would imply that there exists a neighborhood \(V_{\epsilon}(x)\) \(\subseteq O\). Thus, \(x \in O^{c}\), as desired.
+
+============================================================
----------------------------

=== Note ID: 1710371722458 (Block 620) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li><b>Theorem</b> 3.2.14. (i) The union of a finite collection of closed sets is closed.</li><li>(ii) The intersection of an arbitrary collection of closed sets is closed.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1710372066227 (Block 621) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 3.2.7. Given \(A \subseteq \mathbf{R}\), let \(L\) be the set of all limit points of \(A\).<br><br><ul><li>(a) Show that the set \(L\) is closed.</li><li><br></li><li>(b) Argue that if \(x\) is a limit point of \(A \cup L\), then \(x\) is a limit point of \(A\).&nbsp;</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1710372254516 (Block 622) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 3.2.12. Let \(A\) be an uncountable set and let \(B\) be the set of real numbers that divides \(A\) into two uncountable sets; that is, \(s \in B\) if both \(\{x\) : \(x \in A\) and \(x&lt;s\}\) and \(\{x: x \in A\) and \(x&gt;s\}\) are uncountable. Show \(B\) is nonempty and open.
+
+============================================================
----------------------------

=== Note ID: 1710372339406 (Block 623) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 3.2.14. A dual notion to the closure of a set is the interior of a set. The interior of \(E\) is denoted \(E^{\circ}\) and is defined as<br><br><ul><li>\(E^{\circ}\) = \(\left\{x \in E: \text { there exists } V_{\epsilon}(x) \subseteq E\right\}\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1710372484907 (Block 624) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 3.2.15. A set \(A\) is called an \(F_{\sigma}\) set if it can be written as the countable union of closed sets. A set \(B\) is called a \(G_{\delta}\) set if it can be written as the countable intersection of open sets.
+
+============================================================
----------------------------

=== Note ID: 1710455150587 (Block 625) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Example 3.3.2. The most basic example of a compact set is a closed interval. To see this, notice that if \(\left(a_{n}\right)\) is contained in an interval \([c, d]\), then the Bolzano Weierstrass Theorem guarantees that we can find a convergent subsequence \(\left(a_{n_{k} }\right)\). Because a closed interval is a closed set (Example 3.2.9, (ii)), we know that the limit of this subsequence is also in \([c, d]\).
+
+============================================================
----------------------------

=== Note ID: 1710455700401 (Block 626) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 3.3.4 (Characterization of Compactness in \(\mathbf{R}\) ). \(A\) set \(K \subseteq \mathbf{R}\) is compact if and only if it is closed and bounded.<br><br>Proof. Let \(K\) be compact. We will first prove that \(K\) must be bounded, so assume, for contradiction, that \(K\) is not a bounded set. The idea is to produce a sequence in \(K\) that marches off to infinity in such a way that it cannot have a convergent subsequence as the definition of compact requires. To do this, notice that because \(K\) is not bounded there must exist an element \(x_{1} \in K\) satisfying \(\left|x_{1}\right|\) \(&gt;1\). Likewise, there must exist \(x_{2} \in K\) with \(\left|x_{2}\right|\) \(&gt;2\), and in general, given any \(n \in \mathbf{N}\), we can produce \(x_{n} \in K\) such that \(\left|x_{n}\right|\) &gt; \(n\).<br><br>Now, because \(K\) is assumed to be compact, \(\left(x_{n}\right)\) should have a convergent subsequence \(\left(x_{n_{k} }\right)\). But the elements of the subsequence must satisfy \(\left|x_{n_{k} }\right|\) \(&gt;\) \(n_{k}\), and consequently \(\left(x_{n_{k} }\right)\) is unbounded. Because convergent sequences are bounded (Theorem 2.3.2), we have a contradiction. Thus, \(K\) must at least be a bounded set.<br>
+
+============================================================
----------------------------

=== Note ID: 1710455798543 (Block 627) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 3.3.4 (Characterization of Compactness in \(\mathbf{R}\) ). \(A\) set \(K \subseteq \mathbf{R}\) is compact if and only if it is closed and bounded.<br><br>Proof:<br>We will show that \(K\) is closed. To see that \(K\) contains its limit points, we let \(x\) = \(\lim x_{n}\), where \(\left(x_{n}\right)\) is contained in \(K\) and argue that \(x\) must be in \(K\) as well. By Definition 3.3.1, the sequence \(\left(x_{n}\right)\) has a convergent&nbsp;subsequence \(\left(x_{n_{k} }\right)\), and by Theorem 2.5.2, we know \(\left(x_{n_{k} }\right)\) converges to the same limit \(x\). Finally, Definition 3.3.1 requires that \(x\) \(\in K\). This proves that \(K\) is closed.
+
+============================================================
----------------------------

=== Note ID: 1710489677812 (Block 628) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       There may be a temptation to consider closed intervals as being a kind of standard archetype for compact sets, but this is misleading. The structure of compact sets can be much more intricate and interesting. For instance, one implication of Theorem 3.3.4 is that the Cantor set is compact. It is more useful to think of compact sets as generalizations of closed intervals. Whenever a fact involving closed intervals&nbsp; is true, it is often the case that the same result holds when we replace "closed interval" with "compact set."
+
+============================================================
----------------------------

=== Note ID: 1710489857600 (Block 629) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 3.3.5 (Nested&nbsp;Compact Set Property). If<br><br><ul><li>\(K_{1}\)&nbsp; \(\supseteq\) \(K_{2}\)&nbsp; \(\supseteq\)&nbsp; \(K_{3}\)&nbsp; \(\supseteq\)&nbsp; \(K_{4}\)&nbsp; \(\supseteq\)&nbsp; \(\cdots\)</li></ul><br>is a nested sequence of nonempty compact sets, then \(\bigcap_{n=1}^{\infty}\) \(K_{n}\) is not empty.<br>
+
+============================================================
----------------------------

=== Note ID: 1710491610341 (Block 630) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Definition</b> 3.3.6. Let \(A \subseteq \mathbf{R}\). An open cover for \(A\) is a (possibly infinite) collection of open sets \(\left\{O_{\lambda}: \lambda \in \Lambda\right\}\) whose union contains the set \(A\); that is, \(A\) \(\subseteq\)\( \bigcup_{\lambda \in \Lambda} O_{\lambda}\). Given an open cover for \(A\), a finite subcover is a finite subcollection of open sets from the original open cover whose union manages to completely contain \(A\).
+
+============================================================
----------------------------

=== Note ID: 1710492034882 (Block 631) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       &nbsp;For \(x \in(0,1)\), the sets \(O_{x}=(x / 2,1)\) do a fine job covering \((0,1)\), but in order to have an open cover of the closed interval \([0,1]\), we must also cover the endpoints. To remedy this, we could fix \(\epsilon&gt;0\), and let \(O_{0}\) = \((-\epsilon, \epsilon)\) and \(O_{1}\) = \((1-\epsilon, 1+\epsilon)\). Then, the collection<br><br>\[<br>\left\{O_{0}, O_{1}, O_{x}: x \in(0,1)\right\}<br>\]<br><br>is an open cover for \([0,1]\).&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1710492418283 (Block 632) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       For \(x \in(0,1)\), the sets \(O_{x}=(x / 2,1)\) do a fine job covering \((0,1)\), but in order to have an open cover of the closed interval \([0,1]\), we must also cover the endpoints. To remedy this, we could fix \(\epsilon&gt;0\), and let \(O_{0}=(-\epsilon, \epsilon)\) and \(O_{1}=(1-\epsilon, 1+\epsilon)\). Then, the collection<br><br>\[<br>\left\{O_{0}, O_{1}, O_{x}: x \in(0,1)\right\}<br>\]<br><br>is an open cover for \([0,1]\). But this time, notice there is a finite subcover. Because of the addition of the set \(O_{0}\), we can choose \(x^{\prime}\) so that \(x^{\prime} /&nbsp;2\)&nbsp;&lt; \(\epsilon\). It follows that \(\left\{O_{0}, O_{x^{\prime} }, O_{1}\right\}\) is a finite subcover for the closed interval \([0,1]\).
+
+============================================================
----------------------------

=== Note ID: 1710492899812 (Block 633) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 3.3.8 (Heine-Borel Theorem). Let \(K\) be a subset of \(\mathbf{R}\). All of the following statements are equivalent in the sense that any one of them implies the two others:<br><ul><li>(i) \(K\) is compact.<br></li><li>(ii) \(K\) is closed and bounded.</li><li>(iii) Every open cover for \(K\) has a finite subcover.</li></ul><div>Proof: Let's first assume (iii), and prove that it implies (ii) (and thus (i) as well).</div><div><br></div><div><ul><li>To show that \(K\) is bounded, we construct an open cover for \(K\) by defining \(O_{x}\) to be an open interval of radius 1 around each point \(x \in K\). In the language of neighborhoods, \(O_{x}\) = \(V_{1}(x)\). The open cover \(\left\{O_{x}: x \in K\right\}\)::math def::math def::math def::math def::math def then must have a finite subcover \(\left\{O_{x_{1} }, O_{x_{2} }, \ldots, O_{x_{n} }\right\}\)::math def::math def::math def. Because \(K\) is contained in a finite union of bounded sets, \(K\) must be bounded.</li></ul></div>
+
+============================================================
----------------------------

=== Note ID: 1710496579061 (Block 634) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 3.3.8. Let \(K\) and \(L\) be nonempty compact sets, and define<br><br>\[<br>d=\inf \{|x-y|: x \in K \text { and } y \in L\} .<br>\]<br><br>This turns out to be a reasonable definition for the distance between \(K\) and \(L\).<br><br><ul><li>(a) If \(K\) and \(L\) are disjoint, show \(d\) \(&gt;0\) and that \(d\) = \(\left|x_{0}-y_{0}\right|\) for some \(x_{0} \in K\) and \(y_{0} \in L\).</li><li>(b) Show that it's possible to have \(d\) = \(0\) if we assume only that the disjoint sets \(K\) and \(L\) are closed.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1710497102985 (Block 635) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       One of the underlying goals of topology is to strip away all of the extraneous information that comes with our intuitive picture of the real numbers and isolate just those properties that are responsible for the phenomenon we are studying. For example, we were quick to observe that any closed interval is a compact set. The content of Theorem 3.3.4, however, is that the compactness of a closed interval has nothing to do with the fact that the set is an interval but is a consequence of the set being bounded and closed. In Chapter 1, we argued that the set of real numbers between 0 and 1 is an uncountable set. This turns out to be the case for any nonempty closed set that does not contain isolated points.
+
+============================================================
----------------------------

=== Note ID: 1710497323991 (Block 636) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Example 3.4.2 (Cantor Set). It is not too hard to see that the Cantor set is perfect. In Section 3.1, we defined the Cantor set as the intersection<br><br><ul><li>\(C=\bigcap_{n=0}^{\infty} C_{n}\)</li></ul><br>where each \(C_{n}\) is a finite union of closed intervals. By Theorem 3.2.14, each \(C_{n}\) is closed, and by the same theorem, \(C\) is closed as well. It remains to show that no point in \(C\) is isolated.<br><br>Let \(x \in C\) be arbitrary. To convince ourselves that \(x\) is not isolated, we must construct a sequence \(\left(x_{n}\right.\) ) of points in \(C\), different from \(x\), that converges to \(x\). From our earlier discussion, we know that \(C\) at least contains the endpoints of the intervals that make up each \(C_{n}\). In Exercise 3.4.3, we sketch the argument that these are all that is needed to construct \(\left(x_{n}\right)\).<br>
+
+============================================================
----------------------------

=== Note ID: 1710579128841 (Block 637) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 3.4.3. A nonempty perfect set is uncountable.<br><br>Proof. If \(P\) is perfect and nonempty, then it must be infinite because otherwise it would consist only of isolated points. Let's assume, for contradiction, that \(P\) is countable. Thus, we can write<br><ul><li>\(P\)= \(\left\{x_{1}, x_{2}, x_{3}, \ldots\right\},\)</li></ul><br>where every element of \(P\) appears on this list. The idea is to construct a sequence of nested compact sets \(K_{n}\), all contained in \(P\), with the property that :<br><ul><li>\(x_{1}\) \(\notin\) \(K_{2}\), \(x_{2}\) \(\notin\) \(K_{3}\), \(x_{3}\) \(\notin\) \(K_{4}\), \(\ldots\).&nbsp;</li></ul><br>
+
+============================================================
----------------------------

=== Note ID: 1710579443659 (Block 638) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 3.4.3. A nonempty perfect set is uncountable.<br><br>Proof. If \(P\) is perfect and nonempty, then it must be infinite because otherwise it would consist only of isolated points. Let's assume, for contradiction, that \(P\) is countable. Thus, we can write<br><ul><li>\(P\)= \(\left\{x_{1}, x_{2}, x_{3}, \ldots\right\},\)</li></ul><br>where every element of \(P\) appears on this list. The idea is to construct a sequence of nested compact sets \(K_{n}\), all contained in \(P\), with the property that&nbsp;<br>\(x_{1} \notin K_{2}, x_{2} \notin K_{3}, x_{3} \notin K_{4}, \ldots\). Some care must be taken to ensure that each \(K_{n}\) is nonempty, for then we can use Theorem 3.3.5 to produce an<br><ul><li>\(x \in \bigcap_{n=1}^{\infty} K_{n} \subseteq P\)</li></ul><br>that cannot be on the list \(\left\{x_{1}, x_{2}, x_{3}, \ldots\right\}\).<br><br>Let \(I_{1}\) be a closed interval that contains \(x_{1}\) in its interior (i.e., \(x_{1}\) is not an endpoint of \(\left.I_{1}\right)\). Now, \(x_{1}\) is not isolated, so there exists some other point \(y_{2} \in P\) that is also in the interior of \(I_{1}\). Construct a closed interval \(I_{2}\), centered on \(y_{2}\), so that \(I_{2} \subseteq I_{1}\) but \(x_{1} \notin I_{2}\). More explicitly, if \(I_{1}=[a, b]\), let<br><br><ul><li>\(\epsilon\) = \(\min \left\{y_{2}-a, b-y_{2},\left|x_{1}-y_{2}\right|\right\} .\)</li></ul><br>Then, the interval \(I_{2}=\left[y_{2}-\epsilon / 2, y_{2}+\epsilon / 2\right]\) has the desired properties as shown in the figure<br><br><img src="paste-106414706d1a272ec1b5324859bc0a9cfaef91ac.jpg"><br><br>This process can be continued. Because \(y_{2} \in P\) is not isolated, there must exist another point \(y_{3} \in P\) in the interior of \(I_{2}\), and we may insist that \(y_{3} \neq x_{2}\). Now, construct \(I_{3}\) centered on \(y_{3}\) and small enough so that \(x_{2} \notin I_{3}\) and \(I_{3} \subseteq I_{2}\). Observe that \(I_{3} \cap P \neq \emptyset\) because this intersection contains at least \(y_{3}\).<br><br>If we carry out this construction inductively, the result is a sequence of closed intervals \(I_{n}\) satisfying<br><br><ul><li>(i) \(I_{n+1}\) \(\subseteq\) \(I_{n}\),</li><li>(ii) \(x_{n}\) \(\notin\) \(I_{n+1}\)</li><li>(iii) \(I_{n}\) \(\cap P\) \(\neq\) \(\emptyset\).</li></ul><br>To finish the proof, we let \(K_{n}\) = \(I_{n} \cap P\). For each \(n \in \mathbf{N}\), we have that \(K_{n}\) is closed because it is the intersection of closed sets, and bounded because it is contained in the bounded set \(I_{n}\). Hence, \(K_{n}\) is compact. By construction, \(K_{n}\) is not empty and \(K_{n+1} \subseteq K_{n}\). Thus, we can employ the Nested Compact Set Property (Theorem 3.3.5) to conclude that the intersection<br><br><ul><li>\(\bigcap_{n=1}^{\infty} K_{n} \) \(\neq\) \(\emptyset\)</li></ul><br>But each \(K_{n}\) is a subset of \(P\), and the fact that \(x_{n} \notin I_{n+1}\) leads to the conclusion that \(\bigcap_{n=1}^{\infty} K_{n}=\emptyset\), which is the sought-after contradiction.<br>
+
+============================================================
----------------------------

=== Note ID: 1710579483743 (Block 639) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 3.4.3. A nonempty perfect set is uncountable.<br><br>Proof. If \(P\) is perfect and nonempty, then it must be infinite because otherwise it would consist only of isolated points. Let's assume, for contradiction, that \(P\) is countable. Thus, we can write<br><ul><li>\(P\)= \(\left\{x_{1}, x_{2}, x_{3}, \ldots\right\},\)</li></ul><br>where every element of \(P\) appears on this list. The idea is to construct a sequence of nested compact sets \(K_{n}\), all contained in \(P\), with the property that&nbsp;<br>\(x_{1} \notin K_{2}, x_{2} \notin K_{3}, x_{3} \notin K_{4}, \ldots\). Some care must be taken to ensure that each \(K_{n}\) is nonempty, for then we can use Theorem 3.3.5 to produce an<br><ul><li>\(x \in \bigcap_{n=1}^{\infty} K_{n} \subseteq P\)</li></ul><br>that cannot be on the list \(\left\{x_{1}, x_{2}, x_{3}, \ldots\right\}\).<br><br>Let \(I_{1}\) be a closed interval that contains \(x_{1}\) in its interior (i.e., \(x_{1}\) is not an endpoint of \(\left.I_{1}\right)\). Now, \(x_{1}\) is not isolated, so there exists some other point \(y_{2} \in P\) that is also in the interior of \(I_{1}\). Construct a closed interval \(I_{2}\), centered on \(y_{2}\), so that \(I_{2} \subseteq I_{1}\) but \(x_{1} \notin I_{2}\). More explicitly, if \(I_{1}=[a, b]\), let<br><br><ul><li>\(\epsilon\) = \(\min \left\{y_{2}-a, b-y_{2},\left|x_{1}-y_{2}\right|\right\} .\)</li></ul><br>Then, the interval \(I_{2}=\left[y_{2}-\epsilon / 2, y_{2}+\epsilon / 2\right]\) has the desired properties as shown in the figure<br><br><img src="paste-106414706d1a272ec1b5324859bc0a9cfaef91ac.jpg"><br><br>This process can be continued. Because \(y_{2} \in P\) is not isolated, there must exist another point \(y_{3} \in P\) in the interior of \(I_{2}\), and we may insist that \(y_{3} \neq x_{2}\). Now, construct \(I_{3}\) centered on \(y_{3}\) and small enough so that \(x_{2} \notin I_{3}\) and \(I_{3} \subseteq I_{2}\). Observe that \(I_{3} \cap P \neq \emptyset\) because this intersection contains at least \(y_{3}\).<br><br>If we carry out this construction inductively, the result is a sequence of closed intervals \(I_{n}\) satisfying<br><br><ul><li>(i) \(I_{n+1}\) \(\subseteq\) \(I_{n}\),</li><li>(ii) \(x_{n}\) \(\notin\) \(I_{n+1}\)</li><li>(iii) \(I_{n}\) \(\cap P\) \(\neq\) \(\emptyset\).</li></ul><br>To finish the proof, we let \(K_{n}\) = \(I_{n} \cap P\). For each \(n \in \mathbf{N}\), we have that \(K_{n}\) is closed because it is the intersection of closed sets, and bounded because it is contained in the bounded set \(I_{n}\). Hence, \(K_{n}\) is compact. By construction, \(K_{n}\) is not empty and \(K_{n+1} \subseteq K_{n}\). Thus, we can employ the Nested Compact Set Property (Theorem 3.3.5) to conclude that the intersection<br><br><ul><li>\(\bigcap_{n=1}^{\infty} K_{n} \) \(\neq\) \(\emptyset\)</li></ul><br>But each \(K_{n}\) is a subset of \(P\), and the fact that \(x_{n} \notin I_{n+1}\) leads to the conclusion that \(\bigcap_{n=1}^{\infty} K_{n}=\emptyset\), which is the sought-after contradiction.<br>
+
+============================================================
----------------------------

=== Note ID: 1710579563603 (Block 640) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 3.4.3. A nonempty perfect set is uncountable.<br><br>Proof. If \(P\) is perfect and nonempty, then it must be infinite because otherwise it would consist only of isolated points. Let's assume, for contradiction, that \(P\) is countable. Thus, we can write<br><ul><li>\(P\)= \(\left\{x_{1}, x_{2}, x_{3}, \ldots\right\},\)</li></ul><br>where every element of \(P\) appears on this list. The idea is to construct a sequence of nested compact sets \(K_{n}\), all contained in \(P\), with the property that&nbsp;<br>\(x_{1} \notin K_{2}, x_{2} \notin K_{3}, x_{3} \notin K_{4}, \ldots\). Some care must be taken to ensure that each \(K_{n}\) is nonempty, for then we can use Theorem 3.3.5 to produce an<br><ul><li>\(x \in \bigcap_{n=1}^{\infty} K_{n} \subseteq P\)</li></ul><br>that cannot be on the list \(\left\{x_{1}, x_{2}, x_{3}, \ldots\right\}\).<br><br>Let \(I_{1}\) be a closed interval that contains \(x_{1}\) in its interior (i.e., \(x_{1}\) is not an endpoint of \(\left.I_{1}\right)\). Now, \(x_{1}\) is not isolated, so there exists some other point \(y_{2} \in P\) that is also in the interior of \(I_{1}\). Construct a closed interval \(I_{2}\), centered on \(y_{2}\), so that \(I_{2} \subseteq I_{1}\) but \(x_{1} \notin I_{2}\). More explicitly, if \(I_{1}=[a, b]\), let<br><br><ul><li>\(\epsilon\) = \(\min \left\{y_{2}-a, b-y_{2},\left|x_{1}-y_{2}\right|\right\} .\)</li></ul><br>Then, the interval \(I_{2}=\left[y_{2}-\epsilon / 2, y_{2}+\epsilon / 2\right]\) has the desired properties as shown in the figure<br><br><img src="paste-106414706d1a272ec1b5324859bc0a9cfaef91ac.jpg"><br><br>This process can be continued. Because \(y_{2} \in P\) is not isolated, there must exist another point \(y_{3} \in P\) in the interior of \(I_{2}\), and we may insist that \(y_{3} \neq x_{2}\). Now, construct \(I_{3}\) centered on \(y_{3}\) and small enough so that \(x_{2} \notin I_{3}\) and \(I_{3} \subseteq I_{2}\). Observe that \(I_{3} \cap P \neq \emptyset\) because this intersection contains at least \(y_{3}\).<br><br>If we carry out this construction inductively, the result is a sequence of closed intervals \(I_{n}\) satisfying<br><br><ul><li>(i) \(I_{n+1}\) \(\subseteq\) \(I_{n}\),</li><li>(ii) \(x_{n}\) \(\notin\) \(I_{n+1}\)</li><li>(iii) \(I_{n}\) \(\cap P\) \(\neq\) \(\emptyset\).</li></ul><br>To finish the proof, we let \(K_{n}\) = \(I_{n} \cap P\). For each \(n \in \mathbf{N}\), we have that \(K_{n}\) is closed because it is the intersection of closed sets, and bounded because it is contained in the bounded set \(I_{n}\). Hence, \(K_{n}\) is compact. <br><br>By construction, \(K_{n}\) is not empty and \(K_{n+1}\) \(\subseteq K_{n}\). Thus, we can employ the Nested Compact Set Property (Theorem 3.3.5) to conclude that<br><br><ul><li>\(\bigcap_{n=1}^{\infty}\) \(K_{n} \) \(\neq\) \(\emptyset\)</li></ul><br>But each \(K_{n}\) is a subset of \(P\), and the fact that \(x_{n}\) \(\notin I_{n+1}\) leads to the conclusion that \(\bigcap_{n=1}^{\infty} K_{n}\) =\(\emptyset\), which is the sought-after contradiction.<br>
+
+============================================================
----------------------------

=== Note ID: 1710580112285 (Block 641) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 3.4.4. Two nonempty sets \(A, B \subseteq \mathbf{R}\) are separated if \(\bar{A}\) \(\cap\) \(B\) and \(A\) \(\cap\) \(\bar{B}\) are both empty.&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1710580193927 (Block 642) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 3.4.4. Two nonempty sets \(A, B \subseteq \mathbf{R}\) are separated if \(\bar{A} \cap B\) and \(A \cap \bar{B}\) are both empty. A set \(E \subseteq \mathbf{R}\) is disconnected if it can be written as \(E\)= \(A \cup B\), where \(A\) and \(B\) are nonempty separated sets.<br><br>A set that is not disconnected is called a connected set.
+
+============================================================
----------------------------

=== Note ID: 1710580325056 (Block 643) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 3.4.4. Two nonempty sets \(A, B \subseteq \mathbf{R}\) are separated if \(\bar{A} \cap B\) and \(A \cap \bar{B}\) are both empty. A set \(E \subseteq \mathbf{R}\) is disconnected if it can be written as \(E=A \cup B\), where \(A\) and \(B\) are nonempty separated sets.<br><br>A set that is not disconnected is called a connected set.<br><br>Example 3.4.5. (i) If we let \(A=(1,2)\) and \(B=(2,5)\), then it is not difficult to verify that \(E\) = \((1,2) \cup(2,5)\) is disconnected. Notice that the sets \(C\) = \((1,2]\) and \(D\) = \((2,5)\) are not separated because \(C \cap \bar{D}\) = \(\{2\}\) is not empty. This should be comforting. The union \(C \cup D\) is equal to the interval \((1,5)\), which better not qualify as a disconnected set.&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1710580345288 (Block 644) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 3.4.4. Two nonempty sets \(A, B \subseteq \mathbf{R}\) are separated if \(\bar{A} \cap B\) and \(A \cap \bar{B}\) are both empty. A set \(E \subseteq \mathbf{R}\) is disconnected if it can be written as \(E=A \cup B\), where \(A\) and \(B\) are nonempty separated sets.<br><br>A set that is not disconnected is called a connected set.<br><br>Example 3.4.5. (i) If we let \(A=(1,2)\) and \(B=(2,5)\), then it is not difficult to verify that \(E=(1,2) \cup(2,5)\) is disconnected. Notice that the sets \(C=(1,2]\) and \(D=(2,5)\) are not separated because \(C \cap \bar{D}=\{2\}\) is not empty. This should be comforting. The union \(C \cup D\) is equal to the interval \((1,5)\), which better not qualify as a disconnected set. We will prove in a moment that every interval is a connected subset of \(\mathbf{R}\) and vice versa.
+
+============================================================
----------------------------

=== Note ID: 1710580866549 (Block 645) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <b>Theorem</b> 3.4.7. A set \(E \subseteq \mathbf{R}\) is connected if and only if whenever \(a\) &lt; \(c\) &lt; \(b\) with \(a, b\) \(\in E\), it follows that \(c\) \(\in E\).
+
+============================================================
----------------------------

=== Note ID: 1710584481078 (Block 646) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 3.4.7. A set \(E \subseteq \mathbf{R}\) is connected if and only if whenever \(a&lt;c&lt;b\) with \(a, b \in E\), it follows that \(c \in E\) as well.<br><br>Proof. Assume \(E\) is connected, and let \(a, b \in E\) and \(a\) &lt; \(c\) &lt; \(b\). Set<br><br><ul><li>\(A\) = \((-\infty, c)\) \(\cap E \)&nbsp;</li><li>\(B\) = \((c, \infty)\) \(\cap E\)&nbsp;</li></ul><br>Because \(a \in A\) and \(b \in B\), neither set is empty and, just as in Example 3.4.5 (ii), neither set contains a limit point of the other. If \(E\) = \(A \cup B\), then we would have that \(E\) is disconnected, which it is not. It must then be that \(A \cup B\) is missing some element of \(E\), and \(c\) is the only possibility. Thus, \(c \in E\).<br><br>
+
+============================================================
----------------------------

=== Note ID: 1710585206798 (Block 647) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 3.4.7. A set \(E\) is totally disconnected if, given any two distinct points \(x, y \in E\), there exist separated sets \(A\) and \(B\) with \(x \in A, y \in B\), and \(E\) = \(A \cup B\).<br><br>(a) Show that \(\mathbf{Q}\) is totally disconnected.
+
+============================================================
----------------------------

=== Note ID: 1710585306015 (Block 648) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 3.4.7. A set \(E\) is totally disconnected if, given any two distinct points \(x, y \in E\), there exist separated sets \(A\) and \(B\) with \(x \in A, y \in B\), and \(E=A \cup B\).<br><br>Exercise 3.4.8. Follow these steps to show that the Cantor set is totally disconnected in the sense described in Exercise 3.4.7.<br><br>Let \(C=\bigcap_{n=0}^{\infty} C_{n}\), as defined in Section 3.1.<br><br><ul><li>(a) Given \(x, y \in C\), with \(x&lt;y\), set \(\epsilon\) = \(y-x\). For each \(n=0,1,2, \ldots\), the set \(C_{n}\) consists of a finite number of closed intervals. Explain why there must exist an \(N\) large enough so that it is impossible for \(x\) and \(y\) both to belong to the same closed interval of \(C_{N}\).</li><li>(b) Show that \(C\) is totally disconnected.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1710587305458 (Block 649) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The structure of open sets is fairly straightforward. Every open set is either a finite or countable union of open intervals. Standing in opposition&nbsp; to this tidy description of all open sets is the Cantor set. The Cantor set is a closed, uncountable set that contains no intervals of any kind.&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1710587523983 (Block 650) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 3.5.1:<br><ul><li>&nbsp;A set \(A \subseteq \mathbf{R}\) is called an \(F_{\sigma}\) set if it can be written as the countable union of closed sets.&nbsp;</li><li>A set \(B \subseteq \mathbf{R}\) is called a \(G_{\delta}\) set if it can be written as the countable intersection of open sets.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1710587960442 (Block 651) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 3.5.3. (This exercise has already appeared as Exercise 3.2.15.)<ul><li>(a) Show that a closed interval \([a, b]\) is a \(G_{\delta}\) set.</li><li>(b) Show that the half-open interval \((a, b]\) is both a \(G_{\delta}\) and an \(F_{\sigma}\) set.</li><li>(c) Show that \(\mathbf{Q}\) is an \(F_{\sigma}\) set, and the set of irrationals \(\mathbf{I}\) forms a \(G_{\delta}\) set.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1710588028268 (Block 652) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 3.5.1. A set \(A \subseteq \mathbf{R}\) is called an \(F_{\sigma}\) set if it can be written as the countable union of closed sets. A set \(B \subseteq \mathbf{R}\) is called a \(G_{\delta}\) set if it can be written as the countable intersection of open sets.<br><br>It is not readily obvious that the class \(F_{\sigma}\) does not include every subset of \(\mathbf{R}\), but we are now ready to argue that \(\mathbf{I}\) is not an \(F_{\sigma}\) set (and consequently \(\mathbf{Q}\) is not a \(G_{\delta}\) set). This will follow from a theorem due to René Louis Baire (1874-1932).
+
+============================================================
----------------------------

=== Note ID: 1710588301067 (Block 653) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 3.5.5. Show that it is impossible to write<br><br><ul><li>\(\mathbf{R}\) = \(\bigcup_{n=1}^{\infty}\) \(F_{n}\)</li></ul><br>where for each \(n \in \mathbf{N}\), \(F_{n}\) is a closed set containing no nonempty open intervals.<br>
+
+============================================================
----------------------------

=== Note ID: 1710588606859 (Block 654) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       We have encountered several equivalent ways to assert that a particular set \(G\) is dense in \(\mathbf{R}\). In Section 3.2, we observed that \(G\) is dense in \(\mathbf{R}\) if and only if every point of \(\mathbf{R}\) is a limit point of \(G\). Because the closure of any set is obtained by taking the union of the set and its limit points, we have that<br><br><ul><li>\(G\) is dense in&nbsp; \(\mathbf{R}\) iff \(\bar{G}\)= \(\mathbf{R} \text {. }\)</li></ul><br>The set \(\mathbf{Q}\) is dense in \(\mathbf{R}\); the set \(\mathbf{Z}\) is clearly not. In fact, in the jargon of analysis, \(\mathbf{Z}\) is nowhere-dense in \(\mathbf{R}\).<br>
+
+============================================================
----------------------------

=== Note ID: 1710588651056 (Block 655) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 3.5.8. Show that a set \(E\) is nowhere-dense in \(\mathbf{R}\) if and only if the complement of \(\bar{E}\) is dense in \(\mathbf{R}\).
+
+============================================================
----------------------------

=== Note ID: 1710588864866 (Block 656) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       We also briefly discussed the concept of "length," or "measure," in Section 3.1. Baire's Theorem offers a third perspective. From this point of view, nowhere-dense sets are considered to be "thin" sets. Any set that is the countable union - i.e., a not very large union - of these small sets is called a "meager" set or a set of "first category." A set that is not of first category is of "second category." Intuitively, sets of the second category are the "fat" subsets. The Baire Category Theorem, as it is often called, states that \(\mathbf{R}\) is of second category.
+
+============================================================
----------------------------

=== Note ID: 1710588977786 (Block 657) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>There is a significance to the Baire Category Theorem that is difficult to appreciate at the moment because we are only seeing a special case of this result.&nbsp;</li><li>The real numbers are an example of a complete metric space.&nbsp;</li><li>Metric spaces are discussed in some detail in Section 8.2, but here is the basic idea.&nbsp;</li><li>Given a set of mathematical objects such as real numbers, points in the plane or continuous functions defined on \([0,1]\), a "metric" is a rule that assigns a "distance" between two elements in the set. In \(\mathbf{R}\), we have been using \(|x-y|\) as the distance between the real numbers \(x\) and \(y\).&nbsp;</li><li>The point is that if we can create a satisfactory notion of "distance" on these other spaces (we will need the triangle inequality to hold, for instance), then the concepts of convergence, Cauchy sequences, and open sets, for example, can be naturally transferred over.&nbsp;</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1710589021618 (Block 658) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       A complete metric space is any set with a suitably defined metric in which Cauchy sequences have limits. We have spent a good deal of time discussing the fact that \(\mathbf{R}\) is a complete metric space whereas \(\mathbf{Q}\) is not.
+
+============================================================
----------------------------

=== Note ID: 1710592121347 (Block 659) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       What is an equivalence relation \(R\) for a set \(S\)?<br><ul><li>\(R\) is a subset of \(S \times S\), formed of pairs \((a,b)\), such that it is:</li></ul><ol><ol><li>Reflexive: \(aRa\)</li><li>Symetric: \(aRb\) =&nbsp; \(bRa\)</li><li>Transitive:&nbsp;\(aRb\)&nbsp;\( \land \) \(bRc\)&nbsp;\(\implies\)&nbsp;\(aRc\)</li></ol></ol>
+
+============================================================
----------------------------

=== Note ID: 1710674057571 (Block 660) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       1. (Abbott 1.2.1(b)) Where does the proof in Theorem 1.1.1 (of the book) breakdown if we try to use it to prove \(\sqrt{4}\) is irrational.<br><br>Solution. We could start by assuming that \((p / q)^{2}=4\) where \(p, q \in \mathbb{Z}\) have no common factor. This implies that \(p^{2}=4 q^{2}\). If we proceed in the same manner of the book we would want to argue that if 4 divides \(p^{2}\) then 4 divides \(p\), but this is not true. So the same proof does not work.<br><br>We could try to deviate from the proof just slightly. Notice that \(p^{2}=4 q^{2}=2 \cdot 2 q^{2}\) implies that \(p\) = \(2 r\) is even. Thus, \((2 r)^{2}\) = \(4 q^{2} \Longrightarrow r^{2}=q^{2} \Longrightarrow r= \pm q\). We have shown that \(r\) divides \(p\) and \(r\) divides \(q\). We conclude \(r\) = \(1\) and therefore \(p\) = \(2\) and \(q\) = \(1\). Of course this is true: \((2 / 1)^{2}=4\).
+
+============================================================
----------------------------

=== Note ID: 1710674750698 (Block 661) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <br>4. (Abbott 1.2.8) Give an example of a function in each of the following cases.<br><br>(a) \(f: \mathbb{N} \rightarrow \mathbb{N}\) that is \(1-1\) but not onto.<br><br>(b) \(f: \mathbb{N} \rightarrow \mathbb{N}\) that is onto but not \(1-1\).<br><br>(c) \(f: \mathbb{N} \rightarrow \mathbb{Z}\) that is \(1-1\) and onto.<br><br><ul><li>For c:</li><ul><li>\[<br>f(n)=\left\{\begin{array}{ll}<br>-n / 2 &amp; \text { if } n \text { is even } \\<br>(n-1) / 2 &amp; \text { if } n \text { is odd }<br>\end{array} .\right.<br>\]</li><li>Notice that \(f(1)=-1, f(2)=0, f(3)=-2, f(4)=1\) and so forth. The idea is that \(f\) is mapping the even natural numbers to negative integers and the odd natural numbers to the positive integers including zero.</li><li>Let \(k \in \mathbb{Z}\). If \(k \geq 0\) then \(f(2 k+1)\) = \(((2 k+1)-1) / 2\) = \(k\).&nbsp;</li><li>If \(k&lt;0\) then \(-2 k \in \mathbb{N}\) and \(f(-2 k)\) = \(-(-2 k) / 2\) = \(k\). Thus, \(f\) is onto.</li><li>Suppose \(f(n)\) = \(f(m)\). Then, \(m\) and \(n\) must be of the same sign, otherwise their images wouldn't match parity. So either \(-m / 2=-n / 2\) or \((n-1) / 2=(m-1) / 2\). In both cases we find with a little algebra that \(m\) = \(n\). Thus \(f\) is one to one.<br></li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1710674940906 (Block 662) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       \section*{7. (Abbott 1.3.3)}<br><br>(a) Let \(A\) be non-empty and bounded below and define \(B=\{b \in \mathbb{R}: b\) is a lower bound for \(A\}\). Show that \(\sup B=\inf A\).<br><br>(a) Notice that by construction \(B\) is non-empty and bounded above by any element of the set \(A\). By completeness, we can let \(s=\sup B\), that is, \(s\) is an upper bound for \(B\) and if \(b\) is an upper bound then \(s \leq b\). (We want to show \(s=\inf A\) ).<br><br>First, we show \(s\) is a lower bound for \(A\). Let \(a \in A\). By definition of \(B\), we have that \(b\) \(\leq a\) for all \(b \in B\). Therefore, \(a\) is an upper bound for \(B\). Since \(s\) is the least upper bound for \(B\) we have that \(s\) \(\leq a\). This shows that \(s\) is a lower bound for \(A\).<br><br>Next, we show that \(s\) is the greatest lower bound for \(A\). Let \(l\) be a lower bound for \(A\). It follows that \(l \in B\). Since \(s\) is an upper bound for \(B\) we have that \(l\) \(\leq s\). Therefore, \(s\) is the greatest lower bound and \(s=\inf A\).
+
+============================================================
----------------------------

=== Note ID: 1710675323868 (Block 663) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       8. (Abbott 1.3.4) Let \(\left\{A_{n}\right\}\) be a collection of non-empty and bounded above sets.<br><br>(a) Find a formula for \(\sup \left(A_{1} \cup A_{2}\right)\) and extend this to \(\sup \left(\bigcup_{n=1}^{N} A_{n}\right)\).<br><ul><li>Fromula is \(\sup \left(\bigcup_{n=1}^{N} A_{n}\right)\) =&nbsp;\(\max(\sup A_1,\ldots, \sup A_n)\)</li><li>Prove base case:</li><ul><li>Let \(a \in A_{1} \cup A_{2}\). Then, \(a \in A_{1}\) or \(a \in A_{2}\) which implies that \(a\) \(\leq \sup A_{1}\) or \(a\) \(\leq \sup A_{2}\). Thus, \(a\) \(\leq \max \left(\sup A_{1}, \sup A_{2}\right)\) and \(\max \left(\sup A_{1}, \sup A_{2}\right)\) is an upper bound for \(A_{1} \cup A_{2}\).</li><li>Let \(b\) be an upper bound for \(A_{1} \cup A_{2}\). Then, \(b\) is an upper bound for \(A_{1}\) and \(A_{2}\). It follows that \(\sup A_{1} \) \(\leq b\) and \(\sup A_{2}\)&nbsp; \(\leq b\). Therefore, \(\max \left(\sup A_{1}, \sup A_{2}\right)\)&nbsp; \(\leq b\) and \(\max \left(\sup A_{1}, \sup A_{2}\right)\) is the least upper bound.<br></li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1710675802439 (Block 664) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       \section*{9. (Abbott 1.3.6)}<br><br>(a) Let \(s=\sup A\) and \(t=\sup B\). Show that \(s+t\) is an upper bound for \(A+B\).<br><br>(b) Let \(u\) be an upper bound for \(A+B\) and fix \(a \in A\). Show that \(t \leq u-a\).<br><br>(c) Show that \(\sup A+B=s+t\).<br><br>(c) From (b) we have that \(t\)&nbsp; \(\leq u-a\) \(\Longrightarrow\) \(a\) \(\leq u-t\). Since \(a\) was arbitrary we have, by the least upper bound definition for \(A\), that \(s\) \(\leq u-t\) \(\Longrightarrow\) \(s+t\)&nbsp; \(\leq u\), where \(u\) was an upper bound. Therefore, \(s+t=\sup A+B\) is the least upper bound.
+
+============================================================
----------------------------

=== Note ID: 1710918095812 (Block 665) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       \subsection*{1.2.1 Classification}<br><br>In classification problems, the output space is a set of \(C\) unordered and mutually exclusive labels known as classes, \(\mathcal{Y}\) = \(\{1,2, \ldots, C\}\). The problem of predicting the class label given an input is also called pattern recognition. (If there are just two classes, often denoted by \(y \in\{0,1\}\) or \(y \in\{-1,+1\}\), it is called binary classification.)
+
+============================================================
----------------------------

=== Note ID: 1710918403299 (Block 666) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The Iris dataset is an example of tabular data. When the inputs are of variable size (e.g., sequences of words, or social networks), rather than fixed-length vectors, the data is usually stored&nbsp;in some other format rather than in a design matrix. However, such data is often converted to a fixed-sized feature representation (a process known as featurization), thus implicitly creating a design matrix for further processing.
+
+============================================================
----------------------------

=== Note ID: 1710918873879 (Block 667) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The goal of supervised learning is to automatically come up with classification models such as the one shown in Figure 1.4a, so as to reliably predict the labels for any given input. A common way to measure performance on this task is in terms of the misclassification rate on the training set:<br><br><ul><li>\(\mathcal{L}(\boldsymbol{\theta})\)&nbsp; \(\triangleq\) \(\frac{1}{N} \sum_{n=1}^{N}\) \( \mathbb{I}\) ( \((y_{n} \neq f\left(\boldsymbol{x}_{n} ; \boldsymbol{\theta}\right) \) )</li></ul><br><br>where \(\mathbb{I}(e)\) is the binary indicator function:<br><br><ul><li>\(\mathbb{I}(e)\) =&nbsp; \(\begin{cases}1 &amp; \text { if } e \text { is true }&nbsp; \tag{1.3}\\ 0 &amp; \text { if } e \text { is false }\end{cases}\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1711006361860 (Block 668) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       A common classification function&nbsp; arises when \(f\) is an affine function of the form<br><br>\[<br>\begin{equation*}<br>f(\boldsymbol{x} ; \boldsymbol{\theta})=b+\boldsymbol{w}^{\top} \boldsymbol{x}=b+w_{1} x_{1}+w_{2} x_{2}+\cdots+w_{D} x_{D} \tag{1.10}<br>\end{equation*}<br>\]<br><br>where \(\boldsymbol{\theta}=(b, \boldsymbol{w})\) are the parameters of the model. This model is called logistic regression, and will be discussed in more detail in Chapter 10.<br><br>In statistics:<br><ul><li>&nbsp;the \(\boldsymbol{w}\) parameters are usually called regression coefficients (and are typically denoted by \(\boldsymbol{\beta}\) )&nbsp;</li><li>and \(b\) is called the intercept.&nbsp;</li><li>In ML, the parameters \(\boldsymbol{w}\) are called the weights and \(b\) is called the bias.&nbsp;</li><li>This terminology arises from electrical engineering, where we view the function \(f\) as a circuit which takes in \(\boldsymbol{x}\) and returns \(f(\boldsymbol{x})\). Each input is fed to the circuit on "wires", which have weights \(\boldsymbol{w}\). The circuit computes the weighted sum of its inputs, and adds a constant bias or offset term \(b\).&nbsp;</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1711006938808 (Block 669) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       When fitting probabilistic models, it is common to use the negative log probability as our loss function:<br><br><ul><li>\(\ell(y, f(\boldsymbol{x} ; \boldsymbol{\theta}))\) = \(-\log p(y \mid f(\boldsymbol{x} ; \boldsymbol{\theta})) \)</li></ul><br>The reasons for this are explained in Section 5.1.6.1, but the intuition is that a good model (with low loss) is one that assigns a high probability to the true output \(y\) for each corresponding input \(\boldsymbol{x}\). The average negative log probability of the training set is given by<br><br><ul><li>\(\mathrm{NLL}(\boldsymbol{\theta})\) = \(-\frac{1}{N}\)&nbsp; \(\sum_{n=1}^{N}\) \(\log p\left(y_{n} \mid f\left(\boldsymbol{x}_{n} ; \boldsymbol{\theta}\right)\right)\))</li></ul><br>This is called the negative log likelihood. If we minimize this, we can compute the maximum likelihood estimate or MLE:<br><br><ul><li>\(\hat{\boldsymbol{\theta} }_{\mathrm{mle} }\) = \(\underset{\boldsymbol{\theta} }{\operatorname{argmin} } \mathrm{NLL}(\boldsymbol{\theta} ) \)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1711031275244 (Block 670) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The linear model in Figure 1.5a is obviously not a very good fit to the data. We can improve the fit by using a polynomial regression model of degree \(D\). This has the form \(f(x ; \boldsymbol{w})=\boldsymbol{w}^{\top} \boldsymbol{\phi}(x)\), where \(\boldsymbol{\phi}(x)\) is a feature vector derived from the input, which has the following form:<br><br>\[<br>\begin{equation*}<br>\phi(x)=\left[1, x, x^{2}, \ldots, x^{D}\right] \tag{1.26}<br>\end{equation*}<br>\]<br><br>This is a simple example of feature preprocessing, also called feature engineering.
+
+============================================================
----------------------------

=== Note ID: 1711317250904 (Block 671) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       We can recursively decompose the feature extractor \(\phi(\boldsymbol{x} ; \mathbf{V})\) into a composition of simpler functions. The resulting model then becomes a stack of \(L\) nested functions:<br><br>\[<br>\begin{equation*}<br>f(\boldsymbol{x} ; \boldsymbol{\theta})=f_{L}\left(f_{L-1}\left(\cdots\left(f_{1}(\boldsymbol{x})\right) \cdots\right)\right) \tag{1.29}<br>\end{equation*}<br>\]<br><br><br><br><br>The final layer is linear and has the form:<br><ul><li>&nbsp;\(f_{L}(\boldsymbol{x})\) = \(\boldsymbol{w}_{L}^{\top} \boldsymbol{x}\),&nbsp;</li><li>so \(f(\boldsymbol{x} ; \boldsymbol{\theta})\) = \(\boldsymbol{w}_{L}^{\top}\) \(f_{1: L-1}(\boldsymbol{x})\)</li><li>where \(f_{1: L-1}(\boldsymbol{x})\) = \(f_{L-1}\left(\cdots\left(f_{1}(\boldsymbol{x})\right) \cdots\right)\) is the learned feature extractor</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1711318901906 (Block 672) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       To detect if a model is overfitting, let us assume (for now) that we have access to the true (but unknown) distribution \(p^{*}(\boldsymbol{x}, \boldsymbol{y})\) used to generate the training set. Then, instead of computing the empirical risk we compute the theoretical expected loss or population risk<br><br>\[<br>\begin{equation*}<br>\mathcal{L}\left(\boldsymbol{\theta} ; p^{*}\right) \triangleq \mathbb{E}_{p^{*}(\boldsymbol{x}, \boldsymbol{y})}[\ell(\boldsymbol{y}, f(\boldsymbol{x} ; \boldsymbol{\theta}))] \tag{1.31}<br>\end{equation*}<br>\]<br><br>The difference \(\mathcal{L}\left(\boldsymbol{\theta} ; p^{*}\right)\) - \(\mathcal{L}\left(\boldsymbol{\theta} ; \mathcal{D}_{\text {train } }\right)\) is called the generalization gap. If a model has a large generalization gap (i.e., low empirical risk but high population risk), it is a sign that it is overfitting.
+
+============================================================
----------------------------

=== Note ID: 1711319351580 (Block 673) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       In practice, we need to partition the data into three sets, namely the training set, the test set and a validation set; the latter is used for model selection, and we just use the test set to estimate future performance (the population risk), i.e., the test set is not used for model fitting or model selection. See Section 4.5.4 for further details.
+
+============================================================
----------------------------

=== Note ID: 1711369559686 (Block 674) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       All models are wrong, but some models are useful. - George Box [BD87, p424]. \({ }^{5}\)<br>
+
+============================================================
----------------------------

=== Note ID: 1711369830154 (Block 675) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       From a probabilistic perspective, we can view the task of unsupervised learning as fitting an unconditional model of the form \(p(\boldsymbol{x})\), which can generate new data \(\boldsymbol{x}\), whereas supervised learning involves fitting a conditional model of the form \(p(\boldsymbol{y} \mid \boldsymbol{x})\), which specifies (a distribution over) outputs given inputs. \({ }^{6}\)
+
+============================================================
----------------------------

=== Note ID: 1711370013631 (Block 676) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Finally, unsupervised learning forces the model to "explain" the high-dimensional inputs, rather than just the low-dimensional outputs. This allows us to learn richer models of "how the world works"
+
+============================================================
----------------------------

=== Note ID: 1711370448715 (Block 677) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       When dealing with high-dimensional data, it is often useful to reduce the dimensionality by projecting it to a lower dimensional subspace which captures the "essence" of the data.
+
+============================================================
----------------------------

=== Note ID: 1711370970204 (Block 678) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Of course, assuming a linear mapping in the style of PCA from \(\boldsymbol{z}_{n}\) to \(\boldsymbol{x}_{n}\) is very restrictive. <br><br>However, we can create nonlinear extensions by defining:<br><ul><li>&nbsp;\(p\left(\boldsymbol{x}_{n} \mid \boldsymbol{z}_{n} ; \boldsymbol{\theta}\right)\) = \(\mathcal{N}\) ( \(\boldsymbol{x}_{n} \mid f\left(\boldsymbol{z}_{n} ; \boldsymbol{\theta}\right), \sigma^{2} \mathbf{I}\))</li><ul><li>&nbsp;where \(f(\boldsymbol{z} ; \boldsymbol{\theta})\) is a nonlinear model, such as a deep neural network.</li></ul><li>It becomes much harder to fit such a model (i.e., to estimate the parameters \(\boldsymbol{\theta}\) ), because the inputs to the neural net have to be inferred, as well as the parameters of the model. However, there are various approximate methods, such as the variational autoencoder which can be applied (see Section 20.3.5).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1711371602079 (Block 679) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       A common method for evaluating unsupervised models is to measure the probability assigned by the model to unseen test examples. We can do this by computing the (unconditional) negative log likelihood of the data:<br><br>\[<br>\begin{equation*}<br>\mathcal{L}(\boldsymbol{\theta} ; \mathcal{D})=-\frac{1}{|\mathcal{D}|} \sum_{\boldsymbol{x} \in \mathcal{D} } \log p(\boldsymbol{x} \mid \boldsymbol{\theta} ) \tag{1.33}<br>\end{equation*}<br>\]<br><br>This treats the problem of unsupervised learning as one of density estimation. The idea is that a good model will not be "surprised" by actual data samples. Furthermore, since probabilities must sum to 1.0, if the model assigns high probability to regions of data space where the data samples come from, it implicitly assigns low probability to the regions where the data does not come from. Thus the model has learned to capture the typical patterns in the data. This can be used inside of a data compression algorithm.
+
+============================================================
----------------------------

=== Note ID: 1711373590124 (Block 680) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       &nbsp;"If intelligence was a cake, unsupervised learning would be the chocolate sponge, supervised learning would be the icing, and reinforcement learning would be the cherry."&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1711388267027 (Block 681) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       \subsection*{1.5.4.1 Bag of words model}<br><br>Let \(x_{n t}\) be the token at location \(t\) in the \(n^{\prime}\) th document. If there are \(D\) unique tokens in the vocabulary, then we can represent the \(n\) 'th document as a \(D\)-dimensional vector \(\tilde{\boldsymbol{x} }_{n}\), where \(\tilde{x}_{n v}\) is the number of times that word \(v\) occurs in document \(n\) :<br><br><ul><li>\(\tilde{x}_{n v}\) = \(\sum_{t=1}^{T}\) \(\mathbb{I}\left(x_{n t}=v\right) \)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1711389122115 (Block 682) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Although the TF-IDF transformation improves on raw count vectors by placing more weight on "informative" words and less on "uninformative" words, it does not solve the fundamental problem with the one-hot encoding (from which count vectors are derived), which is that that semantically similar words may not be any closer (in vector space) than semantically dissimilar words. Thus the assumption that points that are close in input space should have similar outputs, which is implicitly made by most prediction models, is invalid .
+
+============================================================
----------------------------

=== Note ID: 1711391136873 (Block 683) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       What does it mean for input data to be missing:<br>To model this:<br><ul><li>let \(\mathbf{M}\) be an \(N \times D\) matrix of binary variables:</li><ul><li>where \(M_{n d}\) = \(1\) if feature \(d\) in example \(n\) is missing</li><li>and \(M_{n d}\) = \(0\) otherwise.&nbsp;</li></ul><li>Let:</li><ul><li>&nbsp;\(\mathbf{X}_{v}\) be the visible parts of the input feature matrix, corresponding to \(M_{n d}\) = \(0\),&nbsp;</li><li>and \(\mathbf{X}_{h}\) be the missing parts, corresponding to \(M_{n d}\) = \(1\).&nbsp;</li></ul><li>Let \(\mathbf{Y}\) be the output label matrix, which we assume is fully observed.&nbsp;</li><li>If we assume: \(p\left(\mathbf{M} \mid \mathbf{X}_{v}, \mathbf{X}_{h}, \mathbf{Y}\right)\) = \(p(\mathbf{M})\)</li><ul><li>we say the data is missing completely at random or MCAR, since the missingness does not depend on the hidden or observed features.&nbsp;</li></ul><li>If we assume \(p\left(\mathbf{M} \mid \mathbf{X}_{v}, \mathbf{X}_{h}, \mathbf{Y}\right)\) = \(p\left(\mathbf{M} \mid \mathbf{X}_{v}, \mathbf{Y}\right)\)</li><ul><li>we say the data is missing at random or MAR, since the missingness does not depend on the hidden features, but may depend on the visible features. If neither of these assumptions hold, we say the data is not missing at random or NMAR.</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1711392097285 (Block 684) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       We are all comfortable saying that the probability that a (fair) coin will land heads is \(50 \%\). But what does this mean? There are actually two different interpretations of probability. One is called the frequentist interpretation. In this view, probabilities represent long run frequencies of events that can happen multiple times. For example, the above statement means that, if we flip the coin many times, we expect it to land heads about half the time. \({ }^{1}\)&nbsp;<br><br>The other interpretation is called the Bayesian interpretation of probability. In this view, probability is used to quantify our uncertainty or ignorance about something; hence it is fundamentally related to information rather than repeated trials. In the Bayesian view, the above statement means we believe the coin is equally likely to land heads or tails on the next toss.
+
+============================================================
----------------------------

=== Note ID: 1711392154256 (Block 685) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       One big advantage of the Bayesian interpretation is that it can be used to model our uncertainty about one-off events that do not have long term frequencies.&nbsp; We shall therefore adopt the Bayesian interpretation in this book. Fortunately, the basic rules of probability theory are the same, no matter which interpretation is adopted.
+
+============================================================
----------------------------

=== Note ID: 1711392255428 (Block 686) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The uncertainty in our predictions can arise for two fundamentally different reasons. The first is due to our ignorance of the underlying hidden causes or mechanism generating our data. This is&nbsp;called epistemic uncertainty. However, a simpler term for this is model uncertainty. <br><br>The second kind of uncertainty arises from intrinsic variability, which cannot be reduced even if we collect more data. This is sometimes called aleatoric uncertainty [Hac75; KD09], although a simpler term would be data uncertainty.&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1711392466565 (Block 687) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       We define an event, denoted by the binary variable \(A\), as some state of the world that either holds or does not hold.&nbsp; The expression \(\operatorname{Pr}(A)\) denotes the probability with which you believe event \(A\) is true (or the long run fraction of times that \(A\) will occur::frequentist::frequentist).
+
+============================================================
----------------------------

=== Note ID: 1711392608831 (Block 688) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       We require that:<br><ul><li>&nbsp;\(0\)&nbsp; \(\leq\) \(\operatorname{Pr}(A)\) \(\leq\) \(1\),&nbsp;</li><li>where \(\operatorname{Pr}(A)\) = \(0\) means the event definitely will not happen, and \(\operatorname{Pr}(A)\) = \(1\) means the event definitely will happen.&nbsp;</li><li>We write \(\operatorname{Pr}(\bar{A})\) to denote the probability of event \(A\) not happening; this is defined to be \(\operatorname{Pr}(\bar{A})\)&nbsp; =&nbsp; \(1-\operatorname{Pr}(A)\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1711392895585 (Block 689) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The probability of event \(A\) or \(B\) happening is given by<br><br><ul><li>\(\operatorname{Pr}(A \vee B)\) = \(\operatorname{Pr}(A)+\operatorname{Pr}(B)-\operatorname{Pr}(A \wedge B) \)</li></ul><br>If the events are mutually exclusive (so they cannot happen at the same time), we get<br><br><ul><li>\(\operatorname{Pr}(A \vee B)\) = \(\operatorname{Pr}(A)+\operatorname{Pr}(B) \)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1711396221772 (Block 690) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If the sample space \(\mathcal{X}\) is finite or countably infinite, then \(X\) is called a discrete random variable. In this case, we denote the probability of the event that \(X\) has value \(x\) by \(\operatorname{Pr}(X=x)\). We define the probability mass function or pmf as a function which computes the probability of events which correspond to setting the rv to each possible value:<br><br><ul><li>\(p(x)\) \(\triangleq\) \(\operatorname{Pr}(X=x) \)</li><li>The pmf satisfies the properties \(0 \leq p(x) \leq 1\) and \(\sum_{x \in \mathcal{X} } p(x)\) = \(1\).<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1711396476842 (Block 691) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If \(X \in \mathbb{R}\) is a real-valued quantity, it is called a continuous random variable. In this case, we can no longer create a finite (or countable) set of distinct possible values it can take on. However, there are a countable number of intervals which we can partition the real line into. If we associate events with \(X\) being in each one of these intervals, we can use the methods discussed above for discrete random variables.&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1711396653979 (Block 692) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-0622bd44e77dcf18b6e1ca046692817edc392711.jpg"><img src="paste-d39f422d14230b5c93c6df8bdc29f88cfdff179c.jpg"><br>Figure 2.2: (a) Plot of the cdf for the standard normal, \(\mathcal{N}(0,1)\). Generated by gauss_plot.ipynb. (b) Corresponding pdf. The shaded regions each contain \(\alpha / 2\) of the probability mass. Therefore the nonshaded region contains \(1-\alpha\) of the probability mass. The leftmost cutoff point is \(\Phi^{-1}(\alpha / 2)\), where \(\Phi\) is the cdf of the Gaussian. By symmetry, the rightmost cutoff point is \(\Phi^{-1}(1-\alpha / 2)\) = \(-\Phi^{-1}(\alpha / 2)\). Generated by quantile plot.ipynb.
+
+============================================================
----------------------------

=== Note ID: 1711398350656 (Block 693) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       We define the probability density function or pdf as the derivative of the cdf:<br><br><ul><li>\(p(x)\)&nbsp; \(\triangleq\) \(\frac{d}{d x}\) \(P(x) \)</li></ul><br>(Note that this derivative does not always exist, in which case the pdf is not defined.)&nbsp;<br>
+
+============================================================
----------------------------

=== Note ID: 1711398937400 (Block 694) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       For example, let \(\Phi\) be the cdf of the Gaussian distribution \(\mathcal{N}(0,1)\), and \(\Phi^{-1}\) be the inverse cdf. Then points to the left of \(\Phi^{-1}(\alpha / 2)\) contain \(\alpha / 2\) of the probability mass, as illustrated in Figure \(2.2 \mathrm{~b}\). By symmetry, points to the right of \(\Phi^{-1}(1-\alpha / 2)\) also contain \(\alpha / 2\) of the mass. Hence the central interval \(\left(\Phi^{-1}(\alpha / 2), \Phi^{-1}(1-\alpha / 2)\right)\) contains \(1-\alpha\) of the mass. If we set \(\alpha=0.05\), the central \(95 \%\) interval is covered by the range<br><br>\[<br>\begin{equation*}<br>\left(\Phi^{-1}(0.025), \Phi^{-1}(0.975)\right)=(-1.96,1.96) \tag{2.16}<br>\end{equation*}<br>\]<br><br>If the distribution is \(\mathcal{N}\left(\mu, \sigma^{2}\right)\), then the \(95 \%\) interval becomes \((\mu-1.96 \sigma, \mu+1.96 \sigma)\). This is often approximated by writing \(\mu\) \(\pm 2 \sigma\).
+
+============================================================
----------------------------

=== Note ID: 1711400010919 (Block 695) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The most familiar property of a distribution is its mean, or expected value, often denoted by \(\mu\). For continuous rv's, the mean is defined as follows:<br><br><ul><li>\(\mathbb{E}[X]\) \(\triangleq\) \(\int_{\mathcal{X} } x p(x) d x \tag{2.24}\)</li></ul><br>If the integral is not finite, the mean is not defined.<br>
+
+============================================================
----------------------------

=== Note ID: 1711407154623 (Block 696) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Bayes' rule itself is very simple: it is just a formula for computing the probability distribution over possible values of an unknown (or hidden) quantity \(H\) given some observed data \(Y=y\) :<br><br>\[<br>\begin{equation*}<br>p(H=h \mid Y=y)=\frac{p(H=h) p(Y=y \mid H=h)}{p(Y=y)} \tag{2.51}<br>\end{equation*}<br>\]<br><br><ul><li>In Equation (2.51), the term \(p(H)\) represents what we know about possible values of \(H\) before we see any data; this is called the prior distribution.&nbsp;</li><li>(If \(H\) has \(K\) possible values, then \(p(H)\) is a vector of \(K\) probabilities, that sum to 1.)&nbsp;</li><li>The term \(p(Y \mid H=h)\) represents the distribution over the possible outcomes \(Y\) we expect to see if \(H=h\); this is called the observation distribution.&nbsp;</li><li>When we evaluate this at a point corresponding to the actual observations, \(y\), we get the function \(p(Y=y \mid H=h)\), which is called the likelihood. (Note that this is a function of \(h\), since \(y\) is fixed, but it is not a probability distribution, since it does not sum to one.) Multiplying the prior distribution \(p(H=h)\) by the likelihood function \(p(Y=y \mid H=h)\) for each \(h\) gives the unnormalized joint distribution \(p(H=h, Y=y)\)::math::math.&nbsp;</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1711407334277 (Block 697) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Bayes' rule itself is very simple: it is just a formula for computing the probability distribution over possible values of an unknown (or hidden) quantity \(H\) given some observed data \(Y=y\) :<br><br>\[<br>\begin{equation*}<br>p(H=h \mid Y=y)=\frac{p(H=h) p(Y=y \mid H=h)}{p(Y=y)} \tag{2.51}<br>\end{equation*}<br>\]<br><br>Normalizing the joint distribution by computing \(p(H=h, Y=y) / p(Y=y)\) for each \(h\) gives the posterior distribution \(p(H=h \mid Y=y)\); this represents our new belief state about the possible values of \(H\).<br><br>We can summarize Bayes rule in words as follows:<br><br>posterior \(\propto\) prior \(\times\) likelihood
+
+============================================================
----------------------------

=== Note ID: 1711408493260 (Block 698) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>The probability mass function (pmf) of the bernouli distribution is defined as follows:<br></li><li>\(\operatorname{Ber}(y \mid \theta)\) =&nbsp;\(\begin{cases}1-\theta &amp; \text { if } y=0 \\ \theta &amp; \text { if } y=1\end{cases}\)</li></ul>We can write this in a more concise manner as follows:<br><ul><li>\(\operatorname{Ber}(y \mid \theta)\) \(\triangleq\) \(\theta^{y}\) \((1-\theta)^{1-y} \)<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1711408673438 (Block 699) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The Bernoulli distribution is a special case of the binomial distribution. To explain this, suppose we observe a set of \(N\) Bernoulli trials, denoted \(y_{n} \sim \operatorname{Ber}(\cdot \mid \theta)\), for \(n=1: N\). Concretely, think of tossing a coin \(N\) times. Let us define \(s\) to be the total number of heads, \(s \triangleq \sum_{n=1}^{N} \mathbb{I}\left(y_{n}=1\right)\). The distribution of \(s\) is given by the binomial distribution:<br><div><br></div><div><ul><li>\(\operatorname{Bin}(s \mid N, \theta)\) =&nbsp;\(\left(\begin{array}{c}N \\ s\end{array}\right) \theta^s(1-\theta)^{N-s}\)<br></li></ul></div>where<br><br>\[<br>\left(\begin{array}{c}<br>N&nbsp; \tag{2.70}\\<br>k<br>\end{array}\right) \triangleq \frac{N !}{(N-k) ! k !}<br>\]<br><br>is the number of ways to choose \(k\) items from \(N\) (this is known as the binomial coefficient, and is pronounced "N choose k"). See Figure 2.9 for some examples of the binomial distribution. If \(N=1\), the binomial distribution reduces to the Bernoulli distribution.<br>
+
+============================================================
----------------------------

=== Note ID: 1711409169084 (Block 700) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       When we want to predict a binary variable \(y \in\{0,1\}\) given some inputs \(\boldsymbol{x} \in \mathcal{X}\), we need to use a conditional probability distribution of the form<br><br><ul><li>\(p(y \mid \boldsymbol{x}, \boldsymbol{\theta})\) =&nbsp;\(\operatorname{Ber}(y \mid f(\boldsymbol{x} ; \boldsymbol{\theta}))\)<br></li></ul><div>where \(f(\boldsymbol{x} ; \boldsymbol{\theta})\) is some function that predicts the mean parameter of the output distribution. We will consider many different kinds of function \(f\) in Part II-Part IV.<br></div><div><br></div><div>To avoid the requirement that \(0 \leq f(\boldsymbol{x} ; \boldsymbol{\theta}) \leq 1\), we can let \(f\) be an unconstrained function, and use the following model:<br><br><ul><li>\(\operatorname{Ber}(y \mid \sigma(f(\boldsymbol{x} ; \boldsymbol{\theta})))\)<br></li></ul><br>Here \(\sigma()\) is the sigmoid or logistic function, defined as follows:<br><br><ul><li>\(\sigma(a)\)&nbsp; \(\triangleq\)&nbsp; \(\frac{1}{1+e^{-a} } \tag{2.79}\)</li></ul><br>where \(a\) = \(f(\boldsymbol{x} ; \boldsymbol{\theta})\). The term "sigmoid" means S-shaped: see Figure 2.10a for a plot. We see that it<br></div>
+
+============================================================
----------------------------

=== Note ID: 1711409874015 (Block 701) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Plugging the definition of the sigmoid function into Equation (2.78) we get<br><br>\[<br>\begin{align*}<br>&amp; p(y=1 \mid \boldsymbol{x}, \boldsymbol{\theta})=\frac{1}{1+e^{-a}}=\frac{e^{a}}{1+e^{a}}=\sigma(a)&nbsp; \tag{2.81}\\<br>&amp; p(y=0 \mid \boldsymbol{x}, \boldsymbol{\theta})=1-\frac{1}{1+e^{-a}}=\frac{e^{-a}}{1+e^{-a}}=\frac{1}{1+e^{a}}=\sigma(-a) \tag{2.82}<br>\end{align*}<br>\]<br><br>The quantity \(a\) is equal to the \(\log\) odds, \(\log \left(\frac{p}{1-p}\right)\), where \(p\) = \(p(y=1 \mid \boldsymbol{x} ; \boldsymbol{\theta})\). To see this, note that<br><br><ul><li>\(\log \left(\frac{p}{1-p}\right)\) = \(\log \left(\frac{e^a}{1+e^a} \frac{1+e^a}{1}\right)\) = \(\log \left(e^a\right)\) = \(a\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1711410013568 (Block 702) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Plugging the definition of the sigmoid function into Equation (2.78) we get<br><br>\[<br>\begin{align*}<br>&amp; p(y=1 \mid \boldsymbol{x}, \boldsymbol{\theta})=\frac{1}{1+e^{-a}}=\frac{e^{a}}{1+e^{a}}=\sigma(a)&nbsp; \tag{2.81}\\<br>&amp; p(y=0 \mid \boldsymbol{x}, \boldsymbol{\theta})=1-\frac{1}{1+e^{-a}}=\frac{e^{-a}}{1+e^{-a}}=\frac{1}{1+e^{a}}=\sigma(-a) \tag{2.82}<br>\end{align*}<br>\]<br><br>The quantity \(a\) is equal to the \(\log\) odds, \(\log \left(\frac{p}{1-p}\right)\), where \(p=p(y=1 \mid \boldsymbol{x} ; \boldsymbol{\theta})\). To see this, note that<br><br>\[<br>\begin{equation*}<br>\log \left(\frac{p}{1-p}\right)=\log \left(\frac{e^{a}}{1+e^{a}} \frac{1+e^{a}}{1}\right)=\log \left(e^{a}\right)=a \tag{2.83}<br>\end{equation*}<br>\]<br><br>The logistic function or sigmoid function maps the \(\log\)-odds \(a\) to \(p\) :<br><br><ul><li>\(p\) = \(\operatorname{logistic}(a)\) = \(\sigma(a)\)&nbsp; \(\triangleq\) \(\frac{1}{1+e^{-a} }\) = \(\frac{e^a}{1+e^a}\)</li></ul><br>
+
+============================================================
----------------------------

=== Note ID: 1711412515365 (Block 703) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       To avoid the requirement that \(f\) directly predict a probability vector in multilable classification, it is common to pass the output from \(f\) into the softmax function [Bri90], also called the multinomial logit. This is defined as follows:<br><br>\[<br>\begin{equation*}<br>\operatorname{softmax}(\boldsymbol{a}) \triangleq\left[\frac{e^{a_{1}{\sum_{c^{\prime}=1}^{C} e^{a_{c^{\prime}}}}, \ldots, \frac{e^{a_{C}}}{\sum_{c^{\prime}=1}^{C} e^{a_{c^{\prime}}}}\right] \tag{2.94}<br>\end{equation*}<br>\]}}<br><br>This maps \(\mathbb{R}^{C}\) to \([0,1]^{C}\), and satisfies the constraints that \(0 \leq \operatorname{softmax}(\boldsymbol{a})_{c} \leq 1\) and \(\sum_{c=1}^{C} \operatorname{softmax}(\boldsymbol{a})_{c}\)=&nbsp; 1. The inputs to the softmax, \(\boldsymbol{a}=f(\boldsymbol{x} ; \boldsymbol{\theta})\), are called logits, and are a generalization of the log odds.
+
+============================================================
----------------------------

=== Note ID: 1711413651046 (Block 704) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       To avoid numerical problems in softmax, we can use the following identity:<br><br><ul><li>\(\log \sum_{c=1}^{C} \exp \left(a_{c}\right)\) = \(m\) + \(\log \sum_{c=1}^{C} \exp \left(a_{c}-m\right) \)</li></ul><br>This holds for any \(m\). It is common to use \(m\) = \(\max _{c} a_{c}\) which ensures that the largest value you exponentiate will be zero, so you will definitely not overflow, and even if you underflow, the answer will be sensible. This is known as the log-sum-exp trick. We use this trick when implementing the lse function:<br><br>\[<br>\begin{equation*}<br>\operatorname{lse}(\boldsymbol{a}) \triangleq \log \sum_{c=1}^{C} \exp \left(a_{c}\right) \tag{2.101}<br>\end{equation*}<br>\]<br>
+
+============================================================
----------------------------

=== Note ID: 1711415753912 (Block 705) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       So far we have been considering the unconditional Gaussian distribution. In some cases, it is helpful to make the parameters of the Gaussian be functions of some input variables, i.e., we want to create a conditional density model of the form<br><br>\[<br>\begin{equation*}<br>p(y \mid \boldsymbol{x} ; \boldsymbol{\theta})=\mathcal{N}\left(y \mid f_{\mu}(\boldsymbol{x} ; \boldsymbol{\theta}), f_{\sigma}(\boldsymbol{x} ; \boldsymbol{\theta})^{2}\right) \tag{2.120}<br>\end{equation*}<br>\]<br><br>where \(f_{\mu}(\boldsymbol{x} ; \boldsymbol{\theta}) \in \mathbb{R}\) predicts the mean, and \(f_{\sigma}(\boldsymbol{x} ; \boldsymbol{\theta})^{2} \in \mathbb{R}_{+}\)predicts the variance.<br><br>It is common to assume that the variance is fixed, and is independent of the input. This is called homoscedastic regression. Furthermore it is common to assume the mean is a linear function of the input. The resulting model is called linear regression:
+
+============================================================
----------------------------

=== Note ID: 1711415871082 (Block 706) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       to make the parameters of the Gaussian be functions of some input variables, i.e., we want to create a conditional density model of the form<br><br>\[<br>\begin{equation*}<br>p(y \mid \boldsymbol{x} ; \boldsymbol{\theta})=\mathcal{N}\left(y \mid f_{\mu}(\boldsymbol{x} ; \boldsymbol{\theta}), f_{\sigma}(\boldsymbol{x} ; \boldsymbol{\theta})^{2}\right) \tag{2.120}<br>\end{equation*}<br>\]<br><br>where \(f_{\mu}(\boldsymbol{x} ; \boldsymbol{\theta}) \in \mathbb{R}\) predicts the mean, and \(f_{\sigma}(\boldsymbol{x} ; \boldsymbol{\theta})^{2} \in \mathbb{R}_{+}\)predicts the variance.<br><br>It is common to assume that the variance is fixed, and is independent of the input. This is called homoscedastic regression. Furthermore it is common to assume the mean is a linear function of the input. The resulting model is called linear regression:<br><br><ul><li>\(p(y \mid \boldsymbol{x} ; \boldsymbol{\theta})\) = \(\mathcal{N}\) \(\left(y \mid \boldsymbol{w}^{\top} \boldsymbol{x}+b, \sigma^{2}\right) \)</li></ul><br>where \(\boldsymbol{\theta}\) = \(\left(\boldsymbol{w}, b, \sigma^{2}\right)\).&nbsp;<br>
+
+============================================================
----------------------------

=== Note ID: 1711454115339 (Block 707) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       As the variance of a Gaussian goes to 0 , the distribution approaches an infinitely narrow, but infinitely tall, "spike" at the mean. We can write this as follows:<br><br><ul><li>\(\lim _{\sigma \rightarrow 0}\) \(\mathcal{N}\left(y \mid \mu, \sigma^{2}\right)\)&nbsp; \(\rightarrow\)&nbsp; \(\delta(y-\mu) \)</li></ul><div>where \(\delta\) is the Dirac delta function, defined by<br><br><ul><li>\(\delta(x)\) = \(\begin{cases}+\infty &amp; \text { if } x=0&nbsp; \tag{2.125}\\ 0 &amp; \text { if } x \neq 0\end{cases}\)</li></ul><br>where<br><br><ul><li>\(\int_{-\infty}^{\infty}\)&nbsp; \(\delta(x)\) \(d x\) = \(1 \)</li></ul></div>
+
+============================================================
----------------------------

=== Note ID: 1711454289229 (Block 708) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       where \(\delta\) is the Dirac delta function, defined by<br><br>\[<br>\delta(x)= \begin{cases}+\infty &amp; \text { if } x=0&nbsp; \tag{2.125}\\ 0 &amp; \text { if } x \neq 0\end{cases}<br>\]<br><br>where<br><br>\[<br>\begin{equation*}<br>\int_{-\infty}^{\infty} \delta(x) d x=1 \tag{2.126}<br>\end{equation*}<br>\]<br><br>A slight variant of this is to define<br><ul><li>\(\delta_{y}(x)\) = \(\begin{cases}+\infty &amp; \text { if } x=y&nbsp; \tag{2.127}\\ 0 &amp; \text { if } x \neq y\end{cases}\)</li></ul><br>Note that we have<br><br><ul><li>\(\delta_{y}(x)\) = \(\delta(x-y) \tag{2.128}\)</li></ul><div>The delta function distribution satisfies the following sifting property, which we will use later on:<br><br><ul><li>\(\int_{-\infty}^{\infty} f(y) \delta(x-y) d y\) = \(f(x) \tag{2.129}\)</li></ul></div>
+
+============================================================
----------------------------

=== Note ID: 1711455570552 (Block 709) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If \(\nu=1\), the Student distribution is known as the Cauchy or Lorentz distribution. Its pdf is defined by<br><br>\[<br>\begin{equation*}<br>\mathcal{C}(x \mid \mu, \gamma)=\frac{1}{\gamma \pi}\left[1+\left(\frac{x-\mu}{\gamma}\right)^{2}\right]^{-1} \tag{2.132}<br>\end{equation*}<br>\]<br><br>This distribution has very heavy tails compared to a Gaussian. For example, \(95 \%\) of the values from a standard normal are between -1.96 and 1.96 , but for a standard Cauchy they are between -12.7 and 12.7. In fact the tails are so heavy that the integral that defines the mean does not converge.
+
+============================================================
----------------------------

=== Note ID: 1711455985764 (Block 710) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Another distribution with heavy tails is the Laplace distribution \({ }^{10}\), also known as the double sided exponential distribution. This has the following pdf:<br><br>\[<br>\begin{equation*}<br>\operatorname{Laplace}(y \mid \mu, b) \triangleq \frac{1}{2 b} \exp \left(-\frac{|y-\mu|}{b}\right) \tag{2.134}<br>\end{equation*}<br>\]<br><br>See Figure 2.15 for a plot. Here \(\mu\) is a location parameter and \(b&gt;0\) is a scale parameter. This distribution has the following properties:<br><br><ul><li>\(\text { mean }\) = \(\mu\)&nbsp;</li><li>\(\text { mode }\) = \(\mu\)&nbsp;</li><li>\(\text { var }\) = \(2 b^{2} \)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1711656212775 (Block 711) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Many actions have outcomes which are largely unpredictable in advance-tossing a coin and throwing a dart are simple examples. Probability theory is about such actions and their consequences. The mathematical theory starts with the idea of an experiment (or trial), being a course of action whose consequence is not predetermined. This experiment is reformulated as a mathematical object called a probability space. In broad terms, the probability space corresponding to a given experiment comprises three items:<br><ul><li>(i) the set of all possible outcomes of the experiment,<br></li><li>(ii) a list of all the events which may possibly occur as consequences of the experiment,</li><li>(iii) an assessment of the likelihoods of these events.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1711656402045 (Block 712) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The set of all possible outcomes is called the sample space of \(\mathcal{E}\) and we usually denote it by \(\Omega\). The Greek letter \(\omega\) denotes a typical member of \(\Omega\), and we call each member \(\omega\) an elementary event.
+
+============================================================
----------------------------

=== Note ID: 1711656534020 (Block 713) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If, for example, \(\mathcal{E}\) is the experiment of throwing a fair die once, then<br><br>\[<br>\Omega=\{1,2,3,4,5,6\} .<br>\]<br><br>There are many questions which we may wish to ask about the actual outcome of this experiment (questions such as 'is the outcome a prime number?'), and all such questions may be rewritten in terms of subsets of \(\Omega\) (the is the number odd question becomes 'does the outcome lie in the subset \(\{2,3,5\}\) of \(\Omega\) ?').&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1711656668512 (Block 714) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       This relationship between events and subsets is very natural, especially because two or more events combine with each other in just the same way as the corresponding subsets combine.&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1711656905259 (Block 715) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       In a similar way, if \(A_{1}, A_{2}, \ldots\) are events, then the sets \(\bigcup_{i=1}^{\infty} A_{i}\) and \(\bigcap_{i=1}^{\infty} A_{i}\) represent the events ' \(A_{i}\) occurs, for some \(i\) ' and ' \(A_{i}\) occurs, for every \(i\) ', respectively.
+
+============================================================
----------------------------

=== Note ID: 1711657096351 (Block 716) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Thus we write down a collection \(\mathcal{F}\) = \(\left\{A_{i}: i \in I\right\}\) of subsets of \(\Omega\) which are interesting to us; each \(A \in \mathcal{F}\) is called an event. In simple cases, such as the die-throwing example above, we usually take \(\mathcal{F}\) to be the set of all subsets of \(\Omega\) (called the power set of \(\Omega\) ), but for reasons which may be appreciated later there are many circumstances in which we take \(\mathcal{F}\) to be a very much smaller collection than the entire power set. \({ }^{2}\) In all cases we demand a certain consistency of \(\mathcal{F}\), in the following sense. If \(A, B, C, \ldots \in \mathcal{F}\), we may reasonably be interested also in the events ' \(A\) does not occur' and 'at least one of \(A, B, C, \ldots\) occurs'.&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1711657720759 (Block 717) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-c2be4bc8be7fc6778fe427eca426ccc862c96ffc.jpg"><br>(c) The third condition (1.4) is written in terms of unions. An event space is also closed under the operations of taking finite or countable intersections. This follows by the elementary formula \((A \cap B)^{\mathrm{c} }\) = \(A^{\mathrm{c} } \cup B^{\mathrm{c} }\), and its extension to finite and countable families.
+
+============================================================
----------------------------

=== Note ID: 1711657808322 (Block 718) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Here are some examples of pairs \((\Omega, \mathcal{F})\) of sample spaces and event spaces.<br><br>Example 1.5 \(\Omega\) is any non-empty set and \(\mathcal{F}\) is the power set of \(\Omega\).<br><br>Example 1.6 \(\Omega\) is any non-empty set and \(\mathcal{F}\) = \(\{\varnothing, A, \Omega \backslash A, \Omega\}\), where \(A\) is a given nontrivial subset of \(\Omega\).<br>
+
+============================================================
----------------------------

=== Note ID: 1711657928836 (Block 719) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 1.9 The difference \(A \backslash B\) of two subsets \(A\) and \(B\) of \(\Omega\) is the set \(A \cap(\Omega \backslash B)\) of all points of \(\Omega\) which are in \(A\) but not in \(B\). Show that if \(A, B \in \mathcal{F}\), then \(A \backslash B\)&nbsp; \(\in \mathcal{F}\).
+
+============================================================
----------------------------

=== Note ID: 1711658380014 (Block 720) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       From our experiment \(\mathcal{E}\), we have so far constructed a sample space \(\Omega\) and an event space \(\mathcal{F}\) associated with \(\mathcal{E}\), but there has been no mention yet of probabilities. The third thing which we do is to allocate probabilities to each event in \(\mathcal{F}\), writing \(\mathbb{P}(A)\) for the probability of the event \(A\). We shall assume that this can be done in such a way that the probability function \(\mathbb{P}\) satisfies certain intuitively attractive conditions:
+
+============================================================
----------------------------

=== Note ID: 1711658557891 (Block 721) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       We shall assume that this can be done in such a way that the probability function \(\mathbb{P}\) satisfies certain intuitively attractive conditions:<br><br><ul><li>(a) each event \(A\) in the event space has a probability \(\mathbb{P}(A)\) satisfying \(0 \leq \mathbb{P}(A) \leq 1\),</li><li>(b) the event \(\Omega\), that 'something happens', has probability 1 , and the event \(\varnothing\), that 'nothing happens', has probability 0 ,</li><li>(c) if \(A\) and \(B\) are disjoint events (in that \(A \cap B\) = \(\varnothing\) ), then \(\mathbb{P}(A \cup B)\) = \(\mathbb{P}(A)+\mathbb{P}(B)\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1711660993474 (Block 722) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 1.17 Let \(p_{1}, p_{2}, \ldots, p_{N}\) be non-negative numbers such that \(p_{1}+p_{2}+\cdots+p_{N}\) = \(1\), and let \(\Omega\) = \(\left\{\omega_{1}, \omega_{2}, \ldots, \omega_{N}\right\}\), with \(\mathcal{F}\) the power set of \(\Omega\), as in Example 1.16. Show that the function \(\mathbb{Q}\) given by<br><ul><li>\(\mathbb{Q}(A)\) = \(\sum_{i: \omega_{i} \in A}\) \( p_{i}\)&nbsp; \(\quad \text { for } A \in \mathcal{F}\)</li></ul><br>is a probability measure on \((\Omega, \mathcal{F})\). Is \(\mathbb{Q}\) a probability measure on \((\Omega, \mathcal{F})\) if \(\mathcal{F}\) is not the power set of \(\Omega\) but merely some event space of subsets of \(\Omega\) ?<br>
+
+============================================================
----------------------------

=== Note ID: 1711661220425 (Block 723) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Property If \(A, B\) \(\in \mathcal{F}\), then \( A \backslash B\) \(\in \mathcal{F}\).<br><br>Proof The complement of \(A \backslash B\) equals \((\Omega \backslash A) \cup B\), which is the union of events and is therefore an event. Hence \(A \backslash B\) is an event, by (1.3).
+
+============================================================
----------------------------

=== Note ID: 1711661281626 (Block 724) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Property If \(A_{1}, A_{2}, \ldots\) \(\in \mathcal{F}\), then \(\bigcap_{i=1}^{\infty}\) \(A_{i}\) \(\in \mathcal{F}\).
+
+============================================================
----------------------------

=== Note ID: 1711702801642 (Block 725) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Property If \(A, B \in \mathcal{F}\) then \(\mathbb{P}(A \cup B)\) + \(\mathbb{P}(A \cap B)\) = \(\mathbb{P}(A)\) + \(\mathbb{P}(B)\).
+
+============================================================
----------------------------

=== Note ID: 1711703680744 (Block 726) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 1.20 If \(A, B, C \in \mathcal{F}\), show that<br><br><ul><li>\(\mathbb{P}(A \cup B \cup C)\) = \(\mathbb{P}(A)\) + \(\mathbb{P}(B)\) + \(\mathbb{P}(C)\) - \(\mathbb{P}(A \cap B)\) - \(\mathbb{P}(B \cap C)\) - \(\mathbb{P}(A \cap C)\) + \(\mathbb{P}(A \cap B \cap C)\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1711732422038 (Block 727) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Let \(\mathcal{E}\) be an experiment with probability space \((\Omega, \mathcal{F}, \mathbb{P})\). The structure of this space depends greatly on whether \(\Omega\) is a countable set (that is, a finite or countably infinite set) or an uncountable set. If \(\Omega\) is a countable set, we normally take \(\mathcal{F}\) to be the set of all subsets of \(\Omega\).
+
+============================================================
----------------------------

=== Note ID: 1711894749089 (Block 728) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 1.26 We distribute \(r\) distinguishable balls into \(n\) cells at random, multiple occupancy being permitted. Show that<br><br><ul><li>(a) there are \(n^{r}\) possible arrangements,&nbsp;</li><li>(b) there are \(\left(\begin{array}{l}r \\ k\end{array}\right)\)\((n-1)^{r-k}\) arrangements in which the first cell contains exactly \(k\) balls,</li><li>(c) the probability that the first cell contains exactly \(k\) balls is</li><li>\(\left(\begin{array}{l}r \\ k\end{array}\right)\)\((\frac{1}{n})^k\)\(\left(1-\frac{1}{n}\right)^{r-k}\).<br></li></ul><br>
+
+============================================================
----------------------------

=== Note ID: 1711895227384 (Block 729) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 1.31 If \(A, B \in \mathcal{F}\) and \(\mathbb{P}(B)\) &gt; \(0\), the (conditional) probability of \(A\) given \(B\) is denoted by \(\mathbb{P}(A \mid B)\) and defined by<br><br><ul><li>\(\mathbb{P}(A \mid B)\) = \(\frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} \)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1711895287144 (Block 730) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-2e507e919ad4c37c19bca56d57e1804ff58e6ae1.jpg"><br>Note that the constant of proportionality in (1.32) has been chosen so that the probability \(\mathbb{P}(B \mid B)\), that \(B\) occurs given that \(B\) occurs, satisfies \(\mathbb{P}(B \mid B)\) = \(1\).&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1711896071361 (Block 731) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-cd3a30a7fb624abf70ba1aa076f602122ee5e4c8.jpg"><br>Proof We need only check that \(\mathbb{Q}\) is a probability measure on \((\Omega, \mathcal{F})\). Certainly \(\mathbb{Q}(A) \geq 0\) for \(A \in \mathcal{F}\) and<br><br><ul><li>\(\mathbb{Q}(\Omega)\) = \(\mathbb{P}(\Omega \mid B)\) = \(\frac{\mathbb{P}(\Omega \cap B)}{\mathbb{P}(B)}\) = \(1 \)</li></ul><br>and it remains to check that \(\mathbb{Q}\) satisfies (1.14). Suppose that \(A_{1}, A_{2}, \ldots\) are disjoint events in \(\mathcal{F}\). Then<br><ul><li>\(\mathbb{Q}\left(\bigcup_i A_i\right)\)&nbsp;</li><li>= \(\frac{1}{\mathbb{P}(B)}\) \( \mathbb{P}\left(\left(\bigcup_i A_i\right) \cap B\right)\)</li><li>=&nbsp;\(\frac{1}{\mathbb{P}(B)} \) \( \mathbb{P}\left(\bigcup_i\left(A_i \cap B\right)\right)\)</li><li>=&nbsp;\(\frac{1}{\mathbb{P}(B)}\) \(\sum_i\) \(\mathbb{P}\left(A_i \cap B\right)\)</li><li>=&nbsp;\(\sum_i\) \(\mathbb{Q}\left(A_i\right)\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1711896236957 (Block 732) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 1.35 Show that<br><br><ul><li>\(\mathbb{P}(B \mid A)\) = \(\mathbb{P}(A \mid B)\) \(\frac{\mathbb{P}(B)}{\mathbb{P}(A)}\)</li></ul><br>if \(\mathbb{P}(A)\) \(&gt;0\) and \(\mathbb{P}(B)\) \(&gt;0\).<br>
+
+============================================================
----------------------------

=== Note ID: 1711896515431 (Block 733) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 1.38 Events \(A\) and \(B\) of a probability space \((\Omega, \mathcal{F}, \mathbb{P})\) are called independent if<br><br><ul><li>\(\mathbb{P}(A \cap B)\) = \(\mathbb{P}(A)\)\( \mathbb{P}(B) \)</li></ul><br>and dependent otherwise.<br>
+
+============================================================
----------------------------

=== Note ID: 1711896995419 (Block 734) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 1.44 Show that events \(A\) and \(B\) are independent if and only if \(A\) and \(\Omega \backslash B\) are independent.
+
+============================================================
----------------------------

=== Note ID: 1711897130418 (Block 735) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 1.45 Show that events \(A_{1}\), \(A_{2}\), \(\ldots\), \(A_{m}\) are independent if and only if \(\Omega \backslash A_{1}\), \(\Omega \backslash A_{2}\), \(\ldots\), \(\Omega \backslash A_{m}\) are independent.
+
+============================================================
----------------------------

=== Note ID: 1711897657373 (Block 736) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-d2321eec753f0c32ed65d5146a04a6f68102c54a.jpg"><br>Proof We have that:<br><ul><li>\(\mathbb{P}(A)\)&nbsp;</li><li>= \(\mathbb{P}\left(A \cap\left(\bigcup_i B_i\right)\right)\)<br></li><li>=&nbsp;\(\mathbb{P}\left(\bigcup_i\left(A \cap B_i\right)\right)\)</li><li>=&nbsp;\(\sum_i \mathbb{P}\) \(\left(A \cap B_i\right) \quad\)</li><li>=&nbsp;\(\sum_i \mathbb{P}\) \(\left(A \mid B_i\right) \mathbb{P}\left(B_i\right) \quad\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1711898228057 (Block 737) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       There are many practical situations in which you wish to deduce something from a piece of evidence. We write \(A\) for the evidence, and \(B_{1}, B_{2}, \ldots\) for the possible 'states of nature'.
+
+============================================================
----------------------------

=== Note ID: 1711899368082 (Block 738) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Example 1.55 It is intuitively clear that the chance of obtaining no heads in an infinite set of tosses of a fair coin is 0 . A rigorous proof goes as follows. Let \(A_{n}\) be the event that the first \(n\) tosses of the coin yield at least one head. Then<br><br><ul><li>\(A_{n} \subseteq A_{n+1} \quad \text { for } n=1,2, \ldots\)</li></ul><br>so that the \(A_{n}\) form an increasing sequence. The limit set \(A\) is the event that heads occurs sooner or later. By the continuity of \(\mathbb{P}\), Theorem 1.54,<br><br><ul><li>\(\mathbb{P}(A)\) = \(\lim _{n \rightarrow \infty} \mathbb{P}\left(A_{n}\right)\)</li></ul><br>However, \(\mathbb{P}\left(A_{n}\right)\) = \(1-\left(\frac{1}{2}\right)^{n}\), and so \(\mathbb{P}\left(A_{n}\right)\) \(\rightarrow\) \(1\) as \(n \rightarrow\) \(\infty\). Thus \(\mathbb{P}(A)\) = \(1\), giving that the probability \(\mathbb{P}(\Omega \backslash A)\), that no head ever appears, equals 0 .<br>
+
+============================================================
----------------------------

=== Note ID: 1711899990267 (Block 739) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-51059e897772684fbb3a3b3e398a6e9e138a7635.jpg"><br>Proof of Theorem 1.54 Let \(B_{i}\) = \(A_{i} \backslash A_{i-1}\). Then<br><br>\[<br>A=A_{1} \cup B_{2} \cup B_{3} \cup \cdots<br>\]<br><br>is the union of disjoint events in \(\mathcal{F}\) (draw a Venn diagram to make this clear). By (1.14),<br><br><ul><li>\(\mathbb{P}(A)\) = \(\mathbb{P}\left(A_1\right)\) \(+\mathbb{P}\left(B_2\right)+\mathbb{P}\left(B_3\right)+\cdots\)<br></li><li>=&nbsp;\(\mathbb{P}\left(A_1\right)\) + \(\lim _{n \rightarrow \infty}\) \(\sum_{k=2}^n \mathbb{P}\left(B_k\right)\)</li></ul><br>However,<br><br><ul><li>\(\mathbb{P}\left(B_i\right)\) = \(\mathbb{P}\left(A_i\right)-\mathbb{P}\left(A_{i-1}\right)\)<br></li></ul><br>and so<br><br><ul><li>\(\mathbb{P}(A)\) = \(\mathbb{P}\left(A_1\right)\) + \(\lim _{n \rightarrow \infty} \sum_{k=2}^n\left[\mathbb{P}\left(A_k\right)-\mathbb{P}\left(A_{k-1}\right)\right]\)<br></li><li>=&nbsp;\(\lim _{n \rightarrow \infty} \mathbb{P}\left(A_n\right)\)</li></ul><br>as required, since the last sum collapses.<br>
+
+============================================================
----------------------------

=== Note ID: 1711901607069 (Block 740) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       5. Two fair dice are thrown. Let \(A\) be the event that the first shows an odd number, \(B\) be the event that the second shows an even number, and \(C\) be the event that either both are odd or both are even. Show that \(A, B, C\) are pairwise independent but not independent.
+
+============================================================
----------------------------

=== Note ID: 1711901854857 (Block 741) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <br>* 12. Any number \(\omega \in[0,1]\) has a decimal expansion<br><br>\[<br>\omega=0 . x_{1} x_{2} \ldots,<br>\]<br><br>and we write \(f_{k}(\omega, n)\) for the proportion of times that the integer \(k\) appears in the first \(n\) digits in this expansion. We call \(\omega\) a normal number if<br><br>\[<br>f_{k}(\omega, n) \rightarrow \frac{1}{10} \quad \text { as } n \rightarrow \infty<br>\]<br><br>for \(k=0,1,2, \ldots, 9\). On intuitive grounds we may expect that most numbers \(\omega \in[0,1]\) are normal numbers, and Borel proved that this is indeed true. It is quite another matter to exhibit specific normal numbers. Prove the number \section*{\(0.1234567891011121314 \ldots\)}<br><br>is normal. It is an unsolved problem of mathematics to show that \(e-2\) and \(\pi-3\) are normal numbers also.
+
+============================================================
----------------------------

=== Note ID: 1711902045752 (Block 742) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       14. (a) Let \(\mathbb{P}(A)\) denote the probability of the occurrence of an event \(A\). Prove carefully, for events \(A_{1}, A_{2}, \ldots, A_{n}\), that<br><br><ul><li>\(\mathbb{P}\left(\bigcup_{i=1}^n A_i\right)\)<br></li><li>=&nbsp;\(\sum_i \mathbb{P}\left(A_i\right)\) -&nbsp;\(\sum_{i&lt;j} \mathbb{P}\left(A_i \cap A_j\right)\) +&nbsp;\(\sum_{i&lt;j&lt;k}\) \(\mathbb{P}\left(A_i \cap A_j \cap A_k\right)\) -&nbsp;\(\cdots\) +&nbsp;\((-1)^{n+1} \) \(\mathbb{P}\left(\bigcap_i A_i\right)\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1711902291750 (Block 743) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       15. Two identical decks of cards, each containing \(N\) cards, are shuffled randomly. We say that a \(k\)-matching occurs if the two decks agree in exactly \(k\) places. Show that the probability that there is a \(k\)-matching is<br><br><ul><li>\(\pi_{k}\) = \(\frac{1}{k !}\) \(\left(1-\frac{1}{1 !}+\frac{1}{2 !}-\frac{1}{3 !}+\cdots+\frac{(-1)^{N-k} }{(N-k) !}\right)\)</li></ul><br>
+
+============================================================
----------------------------

=== Note ID: 1711907926298 (Block 744) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       1. (Abbott 1.4.2) Let \(A \subset \mathbb{R}\) be non-empty and bounded above, and let \(s \in \mathbb{R}\) have the property that for all \(n \in \mathbb{N}, s+\frac{1}{n}\) is an upper bound for \(A\) and \(s-\frac{1}{n}\) is not an upper bound for \(A\). Show \(s=\sup A\).<br><br>(Now we will show that \(s\) is the l.u.b. by using Lemma 1.3.8.)<br><br>Let \(\epsilon&gt;0\) be given. By the Archimedean Property, there is an \(n \in \mathbb{N}\) such that \(\frac{1}{n}\) &lt; \(\epsilon\). By assumption \(s-\frac{1}{n}\) is not an upper bound. So there is an \(a \in A\) such that \(s-\frac{1}{n}\) &lt; \(a\). Then, \(s-\epsilon\) &lt; \(a\).<br><br>Therefore, by Lemma 1.3.8 \(s=\sup A\).
+
+============================================================
----------------------------

=== Note ID: 1711908061179 (Block 745) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       2. (Abbott 1.4.3) Prove that \(\bigcap_{n=1}^{\infty}(0,1 / n)=\emptyset\).<br><br>Solution. Suppose, for contradiction, that \(\bigcap_{n=1}^{\infty}(0,1 / n) \neq \emptyset\). Let \(x \in\) \(\bigcap_{n=1}^{\infty}(0,1 / n)\). Then, \(x\) \(\in\) \((0,1 / n)\) for all \(n \in \mathbb{N}\). It follows that \(0&lt;x&lt;\frac{1}{n}\) for all \(n \in \mathbb{N}\). This contradicts the Archimedean Property.
+
+============================================================
----------------------------

=== Note ID: 1711908787387 (Block 746) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       3. (Abbott 1.4.4) Let \(a&lt;b\) be real numbers and consider the set \(T=\mathbb{Q} \cap[a, b]\). Show that \(\sup T=b\).<br><br>By definition, if \(t \in T\) then \(t \leq b\). So \(b\) is an upper bound for \(T\).<br><br>Let \(\epsilon&gt;0\) be given. Then, by the density of \(\mathbb{Q}\) in \(\mathbb{R}\) there is a rational \(r \in \mathbb{Q}\) such that<br><br><ul><li>\(b-\epsilon\) &lt; \(r\) &lt; \(b \text {. }\)</li></ul><br>Moreover, this implies that \(r \in T\). Therefore, by Lemma 1.3.8 \(b=\sup T\).<br>
+
+============================================================
----------------------------

=== Note ID: 1711909867811 (Block 747) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       4. (Abbott 1.4.8)<br><br>(a) Give an example of two sets \(A, B \subset \mathbb{R}\) with \(A \cap B=\emptyset\), \(\sup A=\sup B\), \(\sup A \notin A\) and \(\sup B \notin B\).<br><br>(a) Let \(A=\left\{1-\frac{1}{2 j}: j \in \mathbb{N}\right\}\) and \(B=\left\{1-\frac{1}{2 k+1}: k \in \mathbb{N}\right\}\). By construction, \(1 \notin A\) and \(1 \notin B\).<br><br>First, we will show that \(A \cap B=\emptyset\). Suppose \(x \in A \cap B\). Then, \(x \in A\) and \(x \in B\). Thus, there are \(j, k \in \mathbb{N}\) such that \(x=\frac{1}{2 j}\) and \(x=\frac{1}{2 k+1}\). It follows that \(2 j=2 k+1 \Longrightarrow\) \(j-k\)= \(\frac{1}{2}\), but this is impossible since \(j-k\) is an integer. Thus, \(A \cap B=\emptyset\).<br><br>Next, we show that \(\sup A=1=\sup B\).<br><br>Let \(\epsilon&gt;0\) be given. Then, by the Archimedean Property there are natural numbers \(j, k \in \mathbb{N}\) such that \(\frac{1}{\epsilon}\) &lt; \(2 j\) and \(\frac{1}{\epsilon}\) &lt; \(2 k+1\). It follows that<br><br>\[<br>1-\epsilon&lt;1-\frac{1}{2 j}&lt;1 \quad \text { and } \quad 1-\epsilon&lt;1-\frac{1}{2 k+1}&lt;1<br>\]<br><br>By Lemma 1.3.8, \(\sup A=1=\sup B\).
+
+============================================================
----------------------------

=== Note ID: 1711910239834 (Block 748) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       4. (Abbott 1.4.8)<br>(b) Prove or disprove the following: There exists a sequence of closed bounded (not necessarily nested) intervals \(I_{1}, I_{2}, I_{3}, \ldots\) with the property that \(\bigcap_{n=1}^{N} I_{n} \neq \emptyset\) for all \(N \in \mathbb{N}\), but \(\bigcap_{n=1}^{\infty} I_{n}=\emptyset\).<br><br><br>(b) Suppose there is a sequence of closed bounded intervals as in the problem, \(I_{n}=\left[a_{n}, b_{n}\right]\). Define the sequence of sets \(J_{N}\) = \(\bigcap_{n=1}^{N} I_{n}\) for \(N \in \mathbb{N}\). We will show that \(\left\{J_{N}\right\}\) is a nested sequence of intervals. By assumption \(J_{2}\) is non-empty, thus the intervals \(I_{1}\) and \(I_{2}\) must intersect one another. The only possibility is that \(J_{2}\) is a closed bounded interval of the following form, \(I_{1}, I_{2},\left[a_{1}, b_{2}\right]\) or \(\left[a_{2}, b_{1}\right]\)::4 possible options::4 possible options::4 possible options::4 possible options::4 possible options and moreover, \(J_{2}\) = \(I_{1} \cap I_{2}\) = \(J_{1} \cap I_{2} \subset J_{1}\), that is, the intervals are nested. For induction, suppose \(J_{N}\) is a closed bounded set and nested such that \(J_{N+1} \subset J_{N}\). Then,<br><br><ul><li>\(J_{N+1}\) = \(\bigcap_{n=1}^{N+1} I_{n}\) = \(\left(\bigcap_{n=1}^{N} I_{n}\right) \cap I_{N+1}\) = \(J_{N} \cap I_{N+1}\)</li></ul><br>Since \(J_{N}\) and \(I_{N+1}\) are closed bounded and \(J_{N+1}\) is non-empty, it follows from a similar argument to above that \(J_{N+1}\) is a closed bounded interval. Moreover, \(J_{N+1} \subset J_{N}\) are nested. But, we also have that<br><br>\[<br>\bigcap_{n=1}^{\infty} J_{n}=\bigcap_{n=1}^{\infty} I_{n}=\emptyset<br>\]<br><br>This contradicts the nested interval property which states that \(\bigcap_{n=1}^{\infty} J_{n}\) should be non-empty.<br>
+
+============================================================
----------------------------

=== Note ID: 1711910397857 (Block 749) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       (a) Prove if \(A_{1}, \ldots, A_{m}\) are countable sets then \(A_{1} \cup \cdots \cup A_{m}\) is countable.<br><br>(a) Let \(B, C\) be disjoint countable sets. We use the same trick as with the integers and list them as<br><br><ul><li>\(B \cup C\) = \(\left\{b_{1}, c_{1}, b_{2}, c_{2}, \ldots\right\}\)</li></ul><div>Meaning \(B \cup C\) is countable, and \(A_{1} \cup A_{2}\) is also countable since we can let \(B=A_{1}\) and \(C\) = \(A_{2} \backslash A_{1}\).<br><br>Now induction: suppose \(A_{1} \cup \cdots \cup A_{n}\) is countable, \(\left(A_{1} \cup \cdots \cup A_{n}\right) \cup A_{n+1}\) is the union of two countable sets which by above is countable.<br></div>
+
+============================================================
----------------------------

=== Note ID: 1711911135280 (Block 750) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       5. (Abbott 1.5.3(c)) Prove that if \(A_{n}\) is a countable set for each \(n \in \mathbb{N}\) then \(\bigcup_{n=1}^{\infty} A_{n}\) is countable. (See the book for a hint.)<br><br>Solution. From the hint in the book, we can visualize \(\bigcup_{n=1}^{\infty} A_{n}\) as a grid of numbers that is similar to the Cartesian product \(\mathbb{N} \times \mathbb{N}\). Moreover, in class we showed that \(\mathbb{N} \times \mathbb{N} \sim \mathbb{N}\) is countable.<br><br>Let \(f_{n}: A_{n} \rightarrow \mathbb{N}\) be a bijection. Define a function \(f: \bigcup_{n=1}^{\infty} A_{n} \rightarrow \mathbb{N} \times \mathbb{N}\) as follows. Since \(a\) could possibly appear in more than one of the sets \(A_{n}\), we must be careful so our function is well-defined. For each \(a \in \bigcup_{n=1}^{\infty} A_{n}\), let \(n_{a}\) be the smallest number such that \(a \in A_{n_{a}}\) ( if \(n&lt;n_{a}\) then \(a \notin A_{n_{a}}\) ). Then,<br><br>\[<br>f(a)=\left(n_{a}, f_{n_{a}}(a)\right)<br>\]<br><br>Every element of \(a \in \bigcup_{n=1}^{\infty} A_{n}\) has exactly one image in the range and thus, \(f\) is well-defined.<br><br>We claim that \(f\) is an injection. Suppose that \(f(a)=f(b)\). It follows that<br><br>\[<br>\begin{gathered}<br>\left(n_{a}, f_{n_{a} }(a)\right)=\left(n_{b}, f_{n_{b} }(b)\right) \\<br>\Longleftrightarrow \quad n_{a}=n_{b} \quad \text { and } \quad f_{n_{a} }(a)=f_{n_{a} }(b) .<br>\end{gathered}<br>\]<br><br>Since \(f_{n_{a}}\) is injective, it must follow that \(a=b\). We conclude that \(f\) is an injective function.<br><br>The set \(\bigcup_{n=1}^{\infty} A_{n}\) is infinite, since \(A_{1}\) is countably infinite and \(A_{1} \subset \bigcup_{n=1}^{\infty} A_{n}\). Therefore, by a lemma* we proved in class and by using that \(\sim\) is an equivalence relation we have that<br><br><ul><li>\(\bigcup_{n=1}^{\infty} A_{n}\) \(\sim\) \(f\left(\bigcup_{n=1}^{\infty} A_{n}\right)\) \( \sim\) \(\mathbb{N} \times \mathbb{N} \sim \mathbb{N}\)</li></ul><br>
+
+============================================================
----------------------------

=== Note ID: 1711911185465 (Block 751) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Lemma. Let \(A\) be an infinite set and \(B\) be countable. If there is a function \(f: A \rightarrow B\) that is injective, then \(A \sim B\) is countable.
+
+============================================================
----------------------------

=== Note ID: 1711911734132 (Block 752) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       (b) Show that if \(A \sim B\) show that \(B \sim A\).<br><br>(b) Let \(f: A \rightarrow B\) be bijective. Define the inverse function \(f^{-1}: B \rightarrow A\) as follows: Let \(b \in B\). Since \(f\) is onto, there is an \(a \in A\) such that \(f(a)=b\). This leads to the definition \(f^{-1}(f(a))\) = \(a\).<br><br>Suppose \(f^{-1}(b)=f^{-1}\left(b^{\prime}\right)\). Let \(a, a^{\prime} \in A\) be such that \(f(a)=b\) and \(f\left(a^{\prime}\right)=b^{\prime}\). Then, the left hand side of the equation gives \(f^{-1}(b)=f^{-1}(f(a))=a\) and the right hand side gives \(f^{-1}\left(b^{\prime}\right)=f^{-1}\left(f\left(a^{\prime}\right)\right)=a^{\prime}\). Thus, \(a=a^{\prime}\) and \(b=f(a)=f\left(a^{\prime}\right)=b^{\prime}\). Therefore \(f^{-1}\) is \(1-1\).<br><br>Let \(a \in A\). Then, \(f^{-1}(f(a))\) = \(a\). Therefore, \(f^{-1}\) is onto.<br><br>We conclude that \(f^{-1}\) is bijective and \(B \sim A\).
+
+============================================================
----------------------------

=== Note ID: 1711911853304 (Block 753) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       (c) Show that if \(A \sim B\) and \(B \sim C\) show that \(A \sim C\).<br><br>(c) Let \(f: A \rightarrow B\) and \(g: B \rightarrow C\) be bijective functions. Consider the composition \(g \circ f: A \rightarrow C\) defined by \(g \circ f(a)\)=\(g(f(a))\) .<br><br>Suppose \(g \circ f(a)=g \circ f\left(a^{\prime}\right)\). Since \(g\) is injective we have that \(f(a)=f\left(a^{\prime}\right)\). Since \(f\) is injective we have that \(a=a^{\prime}\). Thus, \(g \circ f\) is \(1-1\).<br><br>Let \(c \in C\). Since \(g\) is onto there is a \(b \in B\) such that \(g(b)=c\). Since \(f\) is onto there is a \(a \in A\) such that \(f(a)=b\). Thus, \(g \circ f(a)=g(f(a))=g(b)=c\). Therefore, \(g \circ f\) is onto.<br><br>We have shown that \(A \sim C\).
+
+============================================================
----------------------------

=== Note ID: 1711958155870 (Block 754) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       7. (Abbott 1.5.8) Let \(B\) be a set of positive real numbers with the property that adding together any finite subset of elements from \(B\) always gives a sum of 2 or less. Show that \(B\) must be finite or countable.<br><br><br>Consider the set \(B_{n} \equiv B \backslash\left\{b_{1}, b_{2}, \ldots, b_{n-1}\right\}\) and define inductively \(b_{n}\) = \(\max B_{n}\). We have constructed a subset<br><br>\[<br>\left\{b_{1}, b_{2}, b_{3}, \ldots\right\} \subset B<br>\]<br><br>where \(b_{1}&gt;b_{2}&gt;b_{3}&gt;\ldots\) Finally, we claim that \(B\) = \(\left\{b_{1}, b_{2}, b_{3}, \ldots\right\}\).<br><br>Suppose, for contradiction, that \(b \in B\) and \(b \neq b_{n}\) for all \(n \in \mathbb{N}\). By assumption on the set \(B\) we have that \(0&lt;b\). We claim that \(b&lt;b_{n}\) for all \(n \in \mathbb{N}\). (Base case) since \(b_{1}=\max B\) and \(b \neq b_{1}\) it follows that \(b&lt;b_{1}\).<br><br>Suppose that \(b&lt;b_{n}\). Then, \(b_{n+1}\) = \(\max B \backslash\left\{b_{1}, b_{2}, \ldots, b_{n}\right\}\). By assumption, \(b \neq b_{n+1}\). Then, either \(b&gt;b_{n+1}\) or \(b&lt;b_{n+1}\). If \(b&gt;b_{n+1}\) then \(b\) must be one of the previous maximums, but that cannot be true. Thus, \(b&lt;b_{n}\).<br><br>Choose \(N\) large enough so that<br><br>\[<br>\frac{1}{N}&lt;\frac{b}{2} \quad \text { (by Archimedean Property). }<br>\]<br><br>It follows that<br><br>\[<br>\begin{aligned}<br>2 &amp; &lt;N b=b+b+\cdots+b \quad(N \text { times }) \\<br>&amp; \leq b_{1}+b_{2}+\cdots+b_{N} .<br>\end{aligned}<br>\]<br><br>The right hand side is a finite sum of elements of \(B\) and we reach a contradiction.<br><br>Therefore, \(B=\left\{b_{1}, b_{2}, b_{3}, \ldots\right\}\) and \(B\) is a countable set.
+
+============================================================
----------------------------

=== Note ID: 1711958550348 (Block 755) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       \section*{8. (Abbott 1.6.10(a)(b))}<br><br>(a) Is the set of all functions from \(\{0,1\}\) to \(\mathbb{N}\) countable or uncountable?<br><br>(a) Let \(F(\{0,1\}, \mathbb{N}) \equiv\{f:\{0,1\} \rightarrow \mathbb{N}\}\) denote the set of all functions. Each function \(f \in F(\{0,1\}, \mathbb{N})\) is uniquely determined by the pair \((f(0), f(1))\). Define a function, \(\phi: F(\{0,1\}, \mathbb{N}) \rightarrow \mathbb{N} \times \mathbb{N}\) by \(\phi(f)\) = \((f(0), f(1))\). We claim that \(\phi\) is bijective.<br><br>Suppose \(\phi(f)=\phi(g)\). Then, \((f(0), f(1))\) = \((g(0), g(1))\) \( \Longleftrightarrow\)&nbsp; \(f(0)\) = \(g(0)\) and \(f(1)\) = \(g(1)\) \(\Longleftrightarrow\) \(f=g\). Thus, \(\phi\) is \(1-1\).<br><br>Let \((m, n) \in \mathbb{N} \times \mathbb{N}\). Define the function \(f_{m, n} \in F(\{0,1\}, \mathbb{N})\) by \(f_{m, n}\) \((0)\) = \(m\) and \(f_{m, n}\) \((1)\) = \(n\). It follows that<br><br>\[<br>\phi\left(f_{m, n}=\left(f_{m, n}(0), f_{m, n}(1)\right)=(m, n)\right.<br>\]<br><br>Therefore, \(\phi\) is onto.<br><br>We conclude that \(\phi\) is a bijection and that \(F(\{0,1\}, \mathbb{N}) \sim \mathbb{N} \times \mathbb{N} \sim \mathbb{N}\) is countable.
+
+============================================================
----------------------------

=== Note ID: 1711958845327 (Block 756) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       (b) Is the set of all functions from \(\mathbb{N}\) to \(\{0,1\}\) countable or uncountable?<br><br>High-level argument:<br><ul><li>A function to binary values can be expressed as a mask</li><li>If you claim you have a list of all such masks I can create a new mask which differs from every mask in the list at 1 position via diagonalization</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1711971076144 (Block 757) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Verify, using the definition of convergence of a sequence, that the following sequences converge to the proposed limit.<br><br>(b) \(\lim \frac{2 n^{2}}{n^{3}+3}=0\).<br><br><ul><li>\(\left|\frac{2 n^2}{n^3+3}-0\right|\)</li><li>= \(\frac{2 n^2}{n^3+3}\)</li><li>&lt; \(\frac{2 n^2}{n^3}\)</li><li>=&nbsp;&nbsp;\(\frac{2}{n}\)</li><li>&lt;&nbsp;\(\epsilon\)</li></ul><div>Whih implies&nbsp;</div><div><ul><li>\(n\)&nbsp;&nbsp;\(&gt;\)\(\frac{2}{\epsilon}\)</li></ul></div>
+
+============================================================
----------------------------

=== Note ID: 1711974410716 (Block 758) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       \section*{Exercise 2.2.5}<br><br>Let \([[x]]\) be the greatest integer less than or equal to \(x\). For example, \([[\pi]]=3\) and \([[3]]=3\). For each sequence, find \(\lim a_{n}\) and verify it with the definition of convergence.<br><br>(b) \(a_{n}=[[(12+4 n) / 3 n]]\).<br><br><ul><li>Each individual term is bounded between 1 and 2 becaus the inner equation reduces to</li><ul><li>\(\frac{4}{n} + \frac{4}{3} \)<br></li><li>which means for&nbsp;\(n&gt;6\) this is less than 2 and more than 1</li></ul><li>Thus the whole part of the value that this converges to is 1</li><li>Then for n &gt; 6, epsilon is greater than 0 for all epsilon since the sequence reaches its limit</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1712043143598 (Block 759) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Summary. Discrete random variables are studied via their probability mass functions. This leads to the definition of the 'mean value' or 'expectation' of a random variable. There are discussions of variance, and of functions of random variables. Methods are presented for calculating expectations, including the use of conditional expectation.
+
+============================================================
----------------------------

=== Note ID: 1712043243299 (Block 760) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Given a probability space \((\Omega, \mathcal{F}, \mathbb{P})\), we are often interested in situations involving some realvalued function \(X\) acting on \(\Omega\). For example, let \(\mathcal{E}\) be the experiment of throwing a fair die once, so that \(\Omega=\{1,2,3,4,5,6\}\), and suppose that we gamble on the outcome of \(\mathcal{E}\) in such a way that the profit is<br><br>-1 if the outcome is 1,2 , or 3 ,<br><br>0 if the outcome is 4 ,<br><br>2 if the outcome is 5 or 6 ,<br><br>where negative profits are positive losses. If the outcome is \(\omega\), then our profit is \(X(\omega)\), where \(X\) : \(\Omega\) \(\rightarrow\) \(\mathbb{R}\) is defined by<br><br>\[<br>X(1)=X(2)=X(3)=-1, \quad X(4)=0, \quad X(5)=X(6)=2 .<br>\]<br><br>The mapping \(X\) is an example of a 'discrete random variable'.
+
+============================================================
----------------------------

=== Note ID: 1712044121626 (Block 761) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The most interesting things about a discrete random variable are the values which it may take and the probabilities associated with these values. If \(X\) is a discrete random variable on the probability space \((\Omega, \mathcal{F}, \mathbb{P})\), then its image \(\operatorname{Im} X\)::notation::notation::notation::notation is the image of \(\Omega\) under \(X\), that is, the set of values taken by \(X\).
+
+============================================================
----------------------------

=== Note ID: 1712044791514 (Block 762) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-68536203334620a674137d81785f808412d54d04.jpg"><br>Note that \(\operatorname{Im} X\) is countable for any discrete random variable \(X\), and:<br><ul><li>\(p_X(x)=0\) if&nbsp;\(x \notin \operatorname{Im} X\)<br></li><li>\(\sum_{x \in \operatorname{Im} X}\) \(p_X(x)\)&nbsp; =</li><ul><li>=&nbsp;\(\mathbb{P}\)(\(\bigcup_{x \in \operatorname{Im} X}) \(\omega \in \Omega: X(\omega)=x\}\))</li><li>=&nbsp;\( \mathbb{P}(\Omega \))<br></li><li>= 1</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1712054807713 (Block 763) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 2.7 Let \(S=\left\{s_{i}: i \in I\right\}\) be a countable set of distinct real numbers, and let \(\left\{\pi_{i}: i \in I\right\}\) be a collection of real numbers satisfying<br><br><ul><li>\(\pi_{i}\) \(\geq 0\)&nbsp; \(\text { for }\)&nbsp; \(i \in I\)&nbsp;</li><li>\(\sum_{i \in I}\) \(\pi_{i}\) = \(1\)</li></ul><br>There exists a probability space \((\Omega, \mathcal{F}, \mathbb{P})\) and a discrete random variable \(X\) on \((\Omega, \mathcal{F}, \mathbb{P})\) such that the probability mass function of \(X\) is given by<br><br><ul><li>\(p_X\left(s_i\right)\) = \(\pi_i\)&nbsp;for \(i \in I\),<br></li><li>\(p_X(s)\) = \(0\) if \(s \notin S\).<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1712054858533 (Block 764) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 2.7 Let \(S=\left\{s_{i}: i \in I\right\}\) be a countable set of distinct real numbers, and let \(\left\{\pi_{i}: i \in I\right\}\) be a collection of real numbers satisfying<br><br>\[<br>\pi_{i} \geq 0 \quad \text { for } i \in I, \text { and } \quad \sum_{i \in I} \pi_{i}=1<br>\]<br><br>There exists a probability space \((\Omega, \mathcal{F}, \mathbb{P})\) and a discrete random variable \(X\) on \((\Omega, \mathcal{F}, \mathbb{P})\) such that the probability mass function of \(X\) is given by<br><br>\[<br>\begin{aligned}<br>p_{X}\left(s_{i}\right) &amp; =\pi_{i} &amp; &amp; \text { for } i \in I, \\<br>p_{X}(s) &amp; =0 &amp; &amp; \text { if } s \notin S .<br>\end{aligned}<br>\]
+
+============================================================
----------------------------

=== Note ID: 1712055239482 (Block 765) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 2.8 If \(X\) and \(Y\) are discrete random variables on the probability space \((\Omega, \mathcal{F}, \mathbb{P})\), show that \(U\) and \(V\) are discrete random variables on this space also, where<br><br><ul><li>\(U(\omega)\) = \(X(\omega)\) + \(Y(\omega)\)<br></li><li>\(V(\omega)\) = \(X(\omega)\) \(Y(\omega)\)<br></li><li>for \(\omega \in \Omega\).<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1712057551290 (Block 766) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Sets, as they are usually conceived, have elements or members. An element of a set may be a wolf, a grape, or a pigeon. It is important to know that a set itself may be an element of some other set. Mathematics is full of examples of sets of sets. A line, for instance, is a set of points; the set of all lines in the plane is a natural example of a set of sets (of points). What may be surprising is not so much that sets may occur as elements, but that for mathematical purposes no other elements need ever be considered
+
+============================================================
----------------------------

=== Note ID: 1712057813894 (Block 767) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Whenever possible, however, we shall informally indicate the status of a set in a particular hierarchy under consideration by means of the convention that letters at the beginning of the alphabet denote elements, and letters at the end denote sets containing them; similarly letters of a relatively simple kind denote elements, and letters of the larger and gaudier fonts denote sets containing them. Examples: \(x\) \(\in\) \(A\) , \(A\) \(\in\) \(X\)&nbsp; \(X\) \(\in\) \(\mathbb{C}\)
+
+============================================================
----------------------------

=== Note ID: 1712057853740 (Block 768) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       A possible relation between sets, more elementary than belonging, is equality. The equality of two sets \(A\) and \(B\) is universally denoted by the familiar symbol<br><br><ul><li>\(A\)=\(B\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1712058050277 (Block 769) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       It is valuable to understand that the axiom of extension is not just a logically necessary property of equality but a non-trivial statement about belonging. One way to come to understand the point is to consider a partially analogous situation in which the analogue of the axiom of extension does not hold. Suppose, for instance, that we consider human beings instead of sets, and that, if \(x\) and \(A\) are human beings, we write \(x \in A\) whenever \(x\) is an ancestor of \(A\). (The ancestors of a human being are his parents, his parents' parents, their parents, etc., etc.) The analogue of the axiom of extension would say here that if two human beings are equal, then they have the same ancestors (this is the "only if" part, and it is true), and also that if two human beings have the same ancestors, then they are equal (this is the "if" part, and it is false).
+
+============================================================
----------------------------

=== Note ID: 1712058179732 (Block 770) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <br><br>The wording of the definition implies that each set must be considered to be included in itself \((A \subset A)\); this fact is described by saying that set inclusion is reflexive. (Note that, in the same sense of the word, equality also is reflexive.)&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1712058231154 (Block 771) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If \(A\) and \(B\) are sets such that \(A \subset B\) and \(A \neq B\), the word proper is used (proper subset, proper inclusion).&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1712058409978 (Block 772) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Equality is symmetric, in the sense that if \(A\) = \(B\), then necessarily \(B\) = \(A\).)&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1712058595381 (Block 773) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Observe that belonging \((\epsilon)\)::math::math::math::math and inclusion ( \(\subset\) )::math::math::math::math are conceptually very different things indeed. One important difference has already manifested itself above: inclusion is always reflexive, whereas it is not at all clear that belonging is ever reflexive.
+
+============================================================
----------------------------

=== Note ID: 1712058655572 (Block 774) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       That is: \(A \subset A\) is always true; is \(A \epsilon A\) ever true? It is certainly not true of any reasonable set that anyone has ever seen. Observe, along the same lines, that inclusion is transitive, whereas belonging is not. Everyday examples, involving, for instance, super-organizations whose members are organizations, will readily occur to the interested reader.
+
+============================================================
----------------------------

=== Note ID: 1712062422624 (Block 775) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       All that is lacking for the precise general formulation that underlies the examples above is a definition of sentence. Here is a quick and informal one. There are two basic types of sentences, namely, assertions of belonging,<br><br>\[<br>x \in A,<br>\]<br><br>and assertions of equality,<br><br>\[<br>A=B<br>\]
+
+============================================================
----------------------------

=== Note ID: 1712063102674 (Block 776) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Axiom of extension. Two sets are equal if and only if they have the same elements.<br>Axiom of specification. To every set \(A\) and to every condition \(S(x)\) there corresponds a set \(B\) whose elements are exactly those elements \(x\) of \(A\) for which \(S(x)\) holds.<br><br>It is an immediate consequence of the axiom of extension that the axiom of specification determines the set \(B\) uniquely. To indicate the way \(B\) is obtained from \(A\) and from&nbsp;\(S(x)\) it is customary to write<br><br><ul><li>\(B\) = \(\{x \in A: S(x)\} .\)<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1712063626001 (Block 777) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       To obtain an amusing and instructive application of the axiom of specification, consider, in the role of \(S(x)\), the sentence<br><br>\[<br>\operatorname{not}(x \in x) \text {. }<br>\]<br><br>It will be convenient, here and throughout, to write " \(x \epsilon^{\prime} A\) " (alternatively " \(x \notin A\) ") instead of "not \((x \in A)\) "; in this notation, the role of \(S(x)\) is now played by<br><br>\[<br>x \epsilon^{\prime} x<br>\]<br><br>It follows that, whatever the set \(A\) may be, if \(B=\left\{x \in A: x \epsilon^{\prime} x\right\}\), then, for all \(y\),<br><br>\[<br>\begin{equation*}<br>y \epsilon B \text { if and only if }\left(y \epsilon A \text { and } y \epsilon^{\prime} y\right) \tag{*}<br>\end{equation*}<br>\]<br><br>Can it be that \(B \epsilon A\) ? We proceed to prove that the answer is no. Indeed, if \(B \epsilon A\), then either \(B \epsilon B\) also (unlikely, but not obviously impossible), or else \(B \epsilon^{\prime} B\). If \(B \epsilon B\), then, by (*), the assumption \(B \epsilon A\) yields \(B \epsilon^{\prime} B-a\) contradiction. If \(B \epsilon^{\prime} B\), then, by (*) again, the assumption \(B \in A\) yields \(B \in B\)-a contradiction again. This completes the proof that \(B \epsilon A\) is impossible, so that we must have \(B \epsilon^{\prime} A\). The most interesting part of this conclusion is that there exists something (namely \(B\) ) that does not belong to \(A\)
+
+============================================================
----------------------------

=== Note ID: 1712063671234 (Block 778) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The set \(A\) in this argument was quite arbitrary. We have proved, in other words, that<br><br><ul><li>nothing contains everything,</li></ul><br>or, more spectacularly,<br><br><ul><li>there is no universe.</li></ul><br>"Universe" here is used in the sense of "universe of discourse," meaning, in any particular discussion, a set that contains all the objects that enter into that discussion<br>
+
+============================================================
----------------------------

=== Note ID: 1712075802119 (Block 779) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       To prove that something is true about the empty set, prove that it cannot be false. How, for instance, could it be false that \(\emptyset\) \(\subset A\) ? It could be false only if \(\emptyset\) had an element that did not belong to \(A\). Since \(\emptyset\) has no elements at all, this is absurd. Conclusion: \(\emptyset\) \(\subset A\) is not false, and therefore \(\emptyset\) \(\subset A\)&nbsp;for every \(A\).
+
+============================================================
----------------------------

=== Note ID: 1712076052700 (Block 780) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Indeed, if \(a\) and \(b\) are sets, and if \(A\) is a set such that \(a \epsilon A\) and \(b \epsilon A\), then we can apply the axiom of specification to \(A\) with the sentence " \(x=a\) or \(x=b\)." The result is the set<br><br>\[<br>\{x \in A: x=a \text { or } x=b\},<br>\]<br><br>and that set, clearly, contains just \(a\) and \(b\). The axiom of extension implies that there can be only one set with this property. The usual symbol for that set is<br><br>\[<br>\{a, b\}<br>\]<br><br>the set is called the pair (or, by way of emphatic comparison with a subsequent concept, the unordered pair) formed by \(a\) and \(b\).
+
+============================================================
----------------------------

=== Note ID: 1712127601535 (Block 781) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If, temporarily, we refer to the sentence " \(x=a\) or \(x=b\) " as \(S(x)\), we may express the axiom of pairing by saying that there exists a set \(B\) such that<br><br>\[<br>\begin{equation*}<br>x \in B \text { if and only if } S(x) . \tag{*}<br>\end{equation*}<br>\]<br><br>The axiom of specification, applied to a set \(A\), asserts the existence of a set \(B\) such that<br><br>(**) \(\quad x \in B\) if and only if \((x \in A\) and \(S(x))\).<br><br>The relation between \((*)\) and \((* *)\) typifies something that occurs quite frequently. All the remaining principles of set construction are pseudo-special cases of the axiom of specification in the sense in which \((*)\) is a pseudo-special case of \((* *)\). They all assert the existence of a set specified by a certain condition; if it were known in advance that there exists a set containing all the specified elements, then the existence of a set containing just them would indeed follow as a special case of the axiom of specification.
+
+============================================================
----------------------------

=== Note ID: 1712127904366 (Block 782) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The axiom of pairing ensures that every set is an element of some set and that any two sets are simultaneously elements of some one and the same set. (The corresponding questions for three and four and more sets will be answered later.) Another pertinent comment is that from the assumptions we have made so far we can infer the existence of very many sets indeed. For examples consider the sets \(\emptyset\), \(\{\emptyset\},\{\{\emptyset\}\},\{\{\{\emptyset\}\}\}\), etc.; consider the pairs, such as \(\{\emptyset,\{\emptyset\}\}\), formed by any two of them; consider the pairs formed by any two such pairs and proceed so on ad infinitum.
+
+============================================================
----------------------------

=== Note ID: 1712128238604 (Block 783) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If \(A\) and \(B\) are sets, it is sometimes natural to wish to unite their elements into one comprehensive set. One way of describing such a comprehensive set is to require it to contain all the elements that belong to at least one of the two members of the pair \(\{A, B\}\).&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1712128932185 (Block 784) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       For the time being we restrict our study of the theory of unions to the simplest facts only. The simplest fact of all is that<br><ul><li>\(\bigcup\{X: X \in \emptyset\}\) = \(\emptyset\)</li></ul><br>and the next simplest fact is that<br><ul><li>\(\bigcup\{X: X \epsilon\{A\}\}\) = \(A\)</li></ul><br>In the brutally simple notation mentioned above these facts are expressed by<br><br><ul><li>\(\bigcup \emptyset\) = \(\emptyset\)</li></ul><br>and<br><br><ul><li>\(\bigcup\{A\}\) = \(A\)</li></ul><br>The proofs are immediate from the definitions.<br>
+
+============================================================
----------------------------

=== Note ID: 1712130691022 (Block 785) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       An equally simple but quite suggestive fact is that<br><br><ul><li>\(\{a\} \cup\{b\}\) = \(\{a, b\}\)</li></ul><br>What this suggests is the way to generalize pairs. Specifically, we write<br><br><ul><li>\(\{a, b, c\}\) = \(\{a\} \cup\{b\} \cup\{c\} .\)</li></ul><br>The equation defines its left side. The right side should by rights have at least one pair of parentheses in it, but, in view of the associative law, their omission can lead to no misunderstanding. Since it is easy to prove that<br><br><ul><li>\(\{a, b, c\}\) = \(\{x: x=a \text { or } x=b \text { or } x=c\}\)</li></ul><br>we know now that for every three sets there exists a set that contains them and nothing else; it is natural to call that uniquely determined set the (unordered) triple formed by them. The extension of the notation and terminology thus introduced to more terms (quadruples, etc.) is obvious.<br>
+
+============================================================
----------------------------

=== Note ID: 1712130844778 (Block 786) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The formation of unions has many points of similarity with another set-theoretic operation. If \(A\) and \(B\) are sets, the intersection of \(A\) and \(B\) is the set<br><br>\[<br>A \cap B<br>\]<br><br>defined by<br><br><ul><li>\(A \cap B\) = \(\{x \in A: x \in B\} .\)</li></ul><br>The definition is symmetric in \(A\) and \(B\) even if it looks otherwise; we have<br><br><ul><li>\(A \cap B\) = \(\{x \in B: x \in A\}\)</li></ul><br>and, in fact, since \(x \in\) \(A \cap B\) if and only if \(x\) belongs to both \(A\) and \(B\), it follows that<br><br><ul><li>\(A \cap B\) = \(\{x: x \in A \text { and } x \in B\} .\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1712131066919 (Block 787) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       \(A\) \(\subset\) \(B\) if and only if \(A \cap B\)= \(A\).
+
+============================================================
----------------------------

=== Note ID: 1712131127283 (Block 788) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Pairs of sets with an empty&nbsp;intersection occur frequently enough to justify the use of a special word: if \(A \cap B\) = \(\emptyset\), the sets \(A\) and \(B\) are called disjoint. The same word is sometimes applied to a collection of sets to indicate that any two distinct sets of the collection are disjoint; alternatively we may speak in such a situation of a pairwise disjoint collection.
+
+============================================================
----------------------------

=== Note ID: 1712131342284 (Block 789) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Two useful facts about unions and intersections involve both the operations at the same time:<br><br>\[<br>\begin{aligned}<br>&amp; A \cap(B \cup C)=(A \cap B) \cup(A \cap C), \\<br>&amp; A \cup(B \cap C)=(A \cup B) \cap(A \cup C) .<br>\end{aligned}<br>\]<br><br>These identities are called the distributive laws.By way of a sample of a settheoretic proof, we prove the second one:<br><ul><li>&nbsp;If \(x\) belongs to the left side, then \(x\) belongs either to \(A\) or to both \(B\) and \(C\);&nbsp;</li><li>if \(x\) is in \(A\), then \(x\) is in both \(A \cup B\) and \(A \cup C\),&nbsp;</li><li>and if \(x\) is in both \(B\) and \(C\), then, again, \(x\) is in both \(A \cup B\) and \(A \cup C\); it follows that, in any case, \(x\) belongs to the right side.&nbsp;</li><li>This proves that the right side includes the left. To prove the reverse inclusion, just observe that if \(x\) belongs to both \(A \cup B\) and \(A \cup C\), then \(x\) belongs either to \(A\) or to both \(B\) and \(C\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1712131735267 (Block 790) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Let \(A\) be any particular set in a collection \(\mathfrak{C}\) (this step is justified by the fact that \(\mathfrak{C}\) \(\neq \emptyset\) ) and write<br><br><ul><li>\(V\) = \(\{x \in A: x \in X \text { for every } X \text { in } \mathfrak{C}\} .\)</li></ul><br>(The condition means "for all \(X\) (if \(X \in(\mathbb{\complement}\), then \(x \epsilon X\) ).") The dependence of \(V\) on the arbitrary choice of \(A\) is illusory; in fact<br><br><ul><li>\(V\) = \(\{x: x \in X \text { for every } X \text { in } \mathfrak{C}\} \)</li></ul><br>The set \(V\) is called the intersection of the collection \(\mathfrak{C}\) of sets; the axiom of extension guarantees its uniqueness.&nbsp;<br>
+
+============================================================
----------------------------

=== Note ID: 1712137350228 (Block 791) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 1.17 Let \(p_{1}, p_{2}, \ldots, p_{N}\) be non-negative numbers such that \(p_{1}+p_{2}+\cdots+p_{N}=1\), and let \(\Omega=\left\{\omega_{1}, \omega_{2}, \ldots, \omega_{N}\right\}\), with \(\mathcal{F}\) the power set of \(\Omega\), as in Example 1.16. Show that the function \(\mathbb{Q}\) given by<br><br>\[<br>\mathbb{Q}(A)=\sum_{i: \omega_{i} \in A} p_{i} \quad \text { for } A \in \mathcal{F}<br>\]<br><br>is a probability measure on \((\Omega, \mathcal{F})\). Is \(\mathbb{Q}\) a probability measure on \((\Omega, \mathcal{F})\) if \(\mathcal{F}\) is not the power set of \(\Omega\) but merely some event space of subsets of \(\Omega\) ?<br><br>Condition (a): \(\mathbb{Q}(A) \geq 0\) for \(A \in \mathcal{F}\) because<br>\[<br>\mathbb{Q}(A)=\sum_{i: \omega_i \in A} p_i \geq 0 \quad \text { for } A \in \mathcal{F}<br>\]<br>since \(p_i \geq 0\) for all \(i\). <br><br>Condition \((\mathrm{b}): \mathbb{Q}(\Omega)=1\) because<br>\[<br>\mathbb{Q}(\Omega)=\sum_{i: \omega_i \in \Omega} p_i=\sum_{i=1}^N p_i=1 .<br>\]<br>\(\mathbb{Q}(\varnothing)=0\) by its defintion. <br><br>Condition (c):<br><ul><li>Induction gives the right answer, may not hold in infinite case</li><li>However, the union of countable sets is countable and everything here is countable so I am pretty sure about it</li><li>The answer to the question is yes&nbsp;</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1712140293009 (Block 792) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 1.21 Let \(A, B, C\) be three events such that<br><br>\[<br>\begin{array}{rlrlrl}<br>\mathbb{P}(A) &amp; =\frac{5}{10}, &amp; \mathbb{P}(B) &amp; =\frac{7}{10}, &amp; \mathbb{P}(C) &amp; =\frac{6}{10}, \\<br>\mathbb{P}(A \cap B) &amp; =\frac{3}{10}, &amp; \mathbb{P}(B \cap C) &amp; =\frac{4}{10}, &amp; \mathbb{P}(A \cap C) &amp; =\frac{2}{10}, \\<br>\mathbb{P}(A \cap B \cap C) &amp; =\frac{1}{10} . &amp; &amp;<br>\end{array}<br>\]<br><br>By drawing a Venn diagram or otherwise, find the probability that exactly two of the events \(A, B, C\) occur.<br><br>Solution:<br><ul><li>Venn diagram<img src="paste-f2ca2b5eb3daf5f0878be3a074e64b7d371b437e.jpg"><br></li><li>Thus we need to add the sum of the pairwise intersections minus the shared bit</li><li>The trick is that the interesection of A,B,C is repeated in each pairwise intersection</li><li>Final equaiton:</li><ul><li>\(\mathcal{P}(A\cap B)\) + \(\mathcal{P}(A\cap C)\)&nbsp; + \(\mathcal{P}(B\cap C)\) - 3*\(\mathcal{P}(A\cap B \cap C)\)&nbsp;<br></li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1712215325154 (Block 793) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Here are some easy exercises on complementation:<br><ul><li>\(A-B\)&nbsp;= \(A \cap B^{\prime} . \)<br></li><li>\(A \subset B\) \(\text { if and only if }\) \(A-B\) = \(\emptyset . \)<br></li><li>\(A-(A-B)\) = \(A \cap B . \)<br></li><li>\(A \cap(B-C)\) = \((A \cap B)-(A \cap C) \)<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1712215615063 (Block 794) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If \(A\) and \(B\) are sets, the symmetric difference (or Boolean sum) of \(A\) and \(B\) is the set \(A+B\) defined by<br><br><ul><li>\(A+B\) = \((A-B)\) \(\cup\) \((B-A) \text {. }\)</li></ul><br>This operation is commutative and associative, and is such that \(A+\emptyset\) = \(A\) and \(A+A\) = \(\emptyset\).<br>
+
+============================================================
----------------------------

=== Note ID: 1712215818089 (Block 795) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       This may be the right time to straighten out a trivial but occasionally puzzling part of the theory of intersections. Recall, to begin with, that intersections were defined for non-empty collections only. The reason is that the same approach to the empty collection does not define a set. Which \(x\) 's are specified by the sentence<br><br>\[<br>x \in X \text { for every } X \text { in } \emptyset ?<br>\]<br><br>As usual for questions about \(\emptyset\) the answer is easier to see for the corresponding negative question. Which \(x\) 's do not satisfy the stated condition? If it is not true that \(x \in X\) for every \(X\) in \(\emptyset\), then there must exist an \(X\) in \(\emptyset\) such that \(x \epsilon^{\prime} X\); since, however, there do not exist any \(X\) 's in \(\emptyset\) at all, this is absurd. Conclusion: no \(x\) fails to satisfy the stated condition, or, equivalently, every \(x\) does satisfy it. In other words, the \(x\) 's that the condition specifies exhaust the (nonexistent) universe.&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1712216109298 (Block 796) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If we restrict our attention to subsets of a particular set \(E\), as we have temporarily agreed to do, then the unpleasantness described in the preceding paragraph appears to go away. The point is that in that case we can define the intersection of a collection ( \(\mathfrak{C}\) (of subsets of \(E\) ) to be the set<br><br>\[<br>\{x \in E: x \in X \text { for every } X \text { in } \mathfrak{C}\} .<br>\]<br><br>This is nothing revolutionary; for each non-empty collection, the new definition agrees with the old one. The difference is in the way the old and the new definitions treat the empty collection; according to the new definition \(\bigcap_{X \in \emptyset} X\) is equal to \(E\). The difference is just a matter of language. A little reflection reveals that the "new" definition offered for the intersection of a collection \(\mathfrak{C}\) of subsets of \(E\) is really the same as the old definition of the intersection of the collection \(\mathfrak{C} \cup\{E\}\), and the latter is never empty.
+
+============================================================
----------------------------

=== Note ID: 1712217023844 (Block 797) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If \(\mathfrak{C}\) is a collection of subsets of a set \(E\) (that is, \(\mathfrak{C}\) is a subcollection of \(\mathfrak{P}(E)\) ), then write<br><br>\[<br>\mathfrak{D}=\left\{X \in \mathfrak{P}(E): X^{\prime} \epsilon \mathfrak{C}\right\} .<br>\]&nbsp;<br><br>It is customary to denote the union and the intersection of the collection \(\mathfrak{D}\) of subsets by the symbols:<br><ul><li>\(\bigcup_{X \in \mathbb{C} } X^{\prime}\)<br></li><li>\(\bigcap_{X \in \mathbb{E} } X^{\prime}\).<br></li></ul><div>In this notation the general forms of the De Morgan laws become<br></div><div><ul><li>\(\left(\bigcup_{X \in \mathbb{C} } X\right)^{\prime}\)= \(\bigcap_{X \in \mathbb{C} } X^{\prime}\)<br></li><li>\(\left(\bigcap_{X \in \mathbb{S} } X\right)^{\prime}\) = \(\bigcup_{X \in \mathbb{C} } X^{\prime}\)<br></li></ul></div>
+
+============================================================
----------------------------

=== Note ID: 1712332118170 (Block 798) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The ordered pair of \(a\) and \(b\), with first coordinate \(a\) and second coordinate \(b\), is the set \((a, b)\) defined by<br><br><ul><li>\((a, b)\) = \(\{\{a\},\{a, b\}\}\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1712332592994 (Block 799) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The ordered pair of \(a\) and \(b\), with first coordinate \(a\) and second coordinate \(b\), is the set \((a, b)\) defined by<br><br>\[<br>(a, b)=\{\{a\},\{a, b\}\}<br>\]<br><br>However convincing the motivation of this definition may be, we must still prove that the result has the main property that an ordered pair must have to deserve its name:<br><ul><li>We must show that if \((a, b)\) and \((x, y)\) are ordered pairs and if \((a, b)=(x, y)\), then \(a=x\) and \(b=y\).&nbsp;</li><li>Suppose now that \((a, b)=(x, y)\). If \(a=b\), then both \((a, b)\) and \((x, y)\) are singletons, so that \(x=y\); since \(\{x\} \epsilon(a, b)\) and \(\{a\} \epsilon(x, y)\), it follows that \(a, b, x\), and \(y\) are all equal.&nbsp;</li><li>If \(a \neq b\), then both \((a, b)\) and \((x, y)\) contain exactly one singleton, namely \(\{a\}\) and \(\{x\}\) respectively, so that \(a\) = \(x\). Since in this case it is also true that both \((a, b)\) and \((x, y)\) contain exactly one unordered pair that is not a singleton, namely \(\{a, b\}\) and \(\{x, y\}\) respectively, it follows that \(\{a, b\}\) = \(\{x, y\}\), and therefore, in particular, \(b \in\) \(\{x, y\}\). Since \(b\) cannot be \(x\) (for then we should have \(a\) = \(x\) and \(b\) = \(x\), and, therefore, \(a\) = \(b\) ), we must have \(b\) = \(y\), and the proof is complete.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1712564391736 (Block 800) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>If \(A\) and \(B\) are sets, does there exist a set that contains all the ordered pairs \((a, b)\) with \(a\) in \(A\) and \(b\) in \(B\) ?&nbsp;</li><li>It is quite easy to see that the answer is yes. Indeed, if \(a \in A\) and \(b \in B\), then \(\{a\}\) \(\subset A\) and \(\{b\} \)\(\subset B\), and therefore \(\{a, b\}\) \(\subset A \cup B\).&nbsp;</li><li>Since also \(\{a\}\) \(\subset A \cup B\), it follows that both \(\{a\}\) and \(\{a, b\}\) are elements of \(\mathfrak{P}(A \cup B)\).&nbsp;</li><li>This implies that \(\{\{a\},\{a, b\}\}\) is a subset of \(\mathfrak{P}(A \cup B)\), and hence that it is an element of \(\mathfrak{P}(\mathfrak{P}(A \cup B))\);&nbsp;</li><li>in other words \((a, b)\) \(\in\) \(\mathfrak{P}(\mathfrak{P}(A \cup B))\) whenever \(a\) \(\epsilon A\) and \(b\) \(\in B\).</li><li>&nbsp;Once this is known, it is a routine matter to apply the axiom of specification and the axiom of extension to produce the unique set \(A \times B\) that consists exactly of the ordered pairs \((a, b)\) with \(a\) in \(A\) and \(b\) in \(B\). This set is called the Cartesian product</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1712566325095 (Block 801) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       sets. In other words: if \(R\) is a set such that every element of \(R\) is an ordered pair, then there exist two sets \(A\) and \(B\) such that \(R \subset A \times B\). The proof is elementary. Suppose indeed that \(x \in R\), so that \(x=\{\{a\},\{a, b\}\}\) for some \(a\) and for some \(b\). The problem is to dig out \(a\) and \(b\) from under the braces:<br><ul><li>&nbsp;Since the elements of \(R\) are sets, we can form the union of the sets in \(R\); since \(x\) is one of the sets in \(R\), the elements of \(x\) belong to that union.&nbsp;</li><li>Since \(\{a, b\}\) is one of the elements of \(x\), we may write \(\{a, b\} \) \(\epsilon\)&nbsp; \(\cup R\).&nbsp;</li><li>Form the union of the sets in \(\bigcup R\). Since \(\{a, b\}\) is one of those sets, it&nbsp; follows that the elements of \(\{a, b\}\) belong to that union, and hence both \(a\) and \(b\) belong to \(\bigcup \bigcup R\).&nbsp;</li><li>This fulfills the promise made above; to exhibit \(R\) as a subset of some \(A \times B\), we may take both \(A\) and \(B\) to be \(\bigcup \bigcup R\).&nbsp;</li><li>Then we can make them small by applying the axiom of specification</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1712566878606 (Block 802) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If either \(A\) = \(\emptyset\) or \(B\) = \(\emptyset\), then \(A \times B\) = \(\emptyset\), and conversely. If \(A\) \(\subset\) \(X\) and \(B \) \(\subset\) \(Y\), then \(A \times B\) \(\subset\) \(X \times Y\), and (provided \(A \times B \neq\) \(\emptyset\) ) conversely.
+
+============================================================
----------------------------

=== Note ID: 1712596772452 (Block 803) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 1.44 Show that events \(A\) and \(B\) are independent if and only if \(A\) and \(\Omega \backslash B\) are independent.<br><br>Exercise 1.45 Show that events \(A_{1}, A_{2}, \ldots, A_{m}\) are independent if and only if \(\Omega \backslash A_{1}, \Omega \backslash A_{2}, \ldots\), \(\Omega \backslash A_{m}\) are independent.<br><br>Proof:<br><ul><li>Assume that previous excerise has been shown to be true</li><li>For n sets, you can prove that the intersection of the first n-1 and the n'th being independent means that the intersection of the first n-1 is independent w.r.t the complement of the n'th</li><li>Repeat down or induction up</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1712697077671 (Block 804) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Example 1.4.4 (Chessboard). How many squares are there in an \(8 \times 8\) chessboard, as in Figure 1.3? Even the name " \(8 \times 8\) chessboard" makes this easy: there are \(8 \cdot 8=64\) squares on the board. The grid structure makes this clear, but we can also think of this as an example of the multiplication rule: to specify a square, we can specify which row and which column it is in. There are 8 choices of row, for each of which there are 8 choices of column.
+
+============================================================
----------------------------

=== Note ID: 1712727777931 (Block 805) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 1.4.8 (Sampling without replacement). Consider \(n\) objects and making \(k\) choices from them, one at a time without replacement (i.e., choosing a certain object precludes it from being chosen again). Then there are \(n(n-1) \cdots(n-k+1)\) possible outcomes for \(1 \leq k \leq n\), and 0 possibilities for \(k&gt;n\) (where order matters). By convention, \(n(n-1) \cdots(n-k+1)\) = \(n\) for \(k=1\).
+
+============================================================
----------------------------

=== Note ID: 1712729506288 (Block 806) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 1.4.15 (Binomial coefficient formula). For \(k \leq n\), we have<br><ul><li>\(\left(\begin{array}{l}n \\ k\end{array}\right)\) =&nbsp;\(\frac{n(n-1) \cdots(n-k+1)}{k !}\) =&nbsp;\(\frac{n !}{(n-k) ! k !}\)<br></li></ul>For&nbsp;\(k &gt; n\), \(\left(\begin{array}{l}n \\ k\end{array}\right)\) = \(0\).<br>
+
+============================================================
----------------------------

=== Note ID: 1712731432556 (Block 807) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Example 1.5.1 (Choosing the complement). For any nonnegative integers \(n\) and \(k\) with \(k \leq n\), we have<br><br>\[<br>\left(\begin{array}{l}<br>n \\<br>k<br>\end{array}\right)=\left(\begin{array}{c}<br>n \\<br>n-k<br>\end{array}\right)<br>\]<br><br>This is easy to check algebraically (by writing the binomial coefficients in terms of factorials), but a story proof makes the result easier to understand intuitively.<br><br>Story proof: Consider choosing a committee of size \(k\) in a group of \(n\) people. We know that there are \(\left(\begin{array}{l}n \\ k\end{array}\right)\) possibilities. But another way to choose the committee is to specify which \(n-k\) people are not on the committee; specifying who is on the committee determines who is not on the committee, and vice versa.
+
+============================================================
----------------------------

=== Note ID: 1712731729754 (Block 808) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Example 1.5.3 (Vandermonde's identity). A famous relationship between binomial coefficients, called Vandermonde's identity, \({ }^{2}\) says that:<br><ul><li>\(\left(\begin{array}{c}m+n \\ k\end{array}\right)\) =&nbsp;\(\sum_{j=0}^k\) \(\left(\begin{array}{c}m \\ j\end{array}\right)\) \(\left(\begin{array}{c}n \\ k-j\end{array}\right)\).<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1712733984212 (Block 809) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 1.6.3 (Inclusion-exclusion). For any events \(A_{1}, \ldots, A_{n}\),<br><ul><li>\(P\left(\bigcup_{i=1}^n A_i\right)\) =&nbsp;\(\sum_i P\left(A_i\right)\) -&nbsp;\(\sum_{i&lt;j} P\left(A_i \cap A_j\right)\) +&nbsp;\(\sum_{i&lt;j&lt;k} P\left(A_i \cap A_j \cap A_k\right)\) -&nbsp;\(\ldots\) +&nbsp;\((-1)^{n+1}\) \(P\left(A_1 \cap \cdots \cap A_n\right)\).<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1712739447054 (Block 810) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Example 1.6.4 (de Montmort's matching problem). Consider a well-shuffled deck of \(n\) cards, labeled 1 through \(n\). You flip over the cards one by one, saying the numbers 1 through \(n\) as you do so. You win the game if, at some point, the number you say aloud is the same as the number on the card being flipped over (for example, if the 7th card in the deck has the label 7). What is the probability of winning?<br><br>In the inclusion-exclusion formula, there are \(n\) terms involving one event, \(\left(\begin{array}{l}n \\ 2\end{array}\right)\) terms involving two events, \(\left(\begin{array}{l}n \\ 3\end{array}\right)\) terms involving three events, and so forth. By the symmetry of the problem, all \(n\) terms of the form \(P\left(A_{i}\right)\) are equal, all \(\left(\begin{array}{l}n \\ 2\end{array}\right)\) terms of the form \(P\left(A_{i} \cap A_{j}\right)\) are equal, and the whole expression simplifies considerably:<br><br>\[<br>\begin{aligned}<br>P\left(\bigcup_{i=1}^{n} A_{i}\right) &amp; =\frac{n}{n}-\frac{\left(\begin{array}{l}<br>n \\<br>2<br>\end{array}\right)}{n(n-1)}+\frac{\left(\begin{array}{l}<br>n \\<br>3<br>\end{array}\right)}{n(n-1)(n-2)}-\cdots+(-1)^{n+1} \cdot \frac{1}{n !} \\<br>&amp; =1-\frac{1}{2 !}+\frac{1}{3 !}-\cdots+(-1)^{n+1} \cdot \frac{1}{n !} .<br>\end{aligned}<br>\]<br><br>Comparing this to the Taylor series for 1/e (see Section A. 8 of the math appendix),<br><br>\[<br>e^{-1}=1-\frac{1}{1 !}+\frac{1}{2 !}-\frac{1}{3 !}+\ldots<br>\]<br><br>we see that for large \(n\), the probability of winning the game is extremely close to \(1-1 / e\), or about 0.63 . Interestingly, as \(n\) grows, the probability of winning approaches \(1-1 / e\) instead of going to 0 or 1 . With a lot of cards in the deck, the number of possible locations for matching cards increases while the probability of any particular match decreases, and these two forces offset each other and balance to give a probability of about \(1-1 / e\).
+
+============================================================
----------------------------

=== Note ID: 1712741063107 (Block 811) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1,2 +1,4 @@
       How can one define a logfactorial function in scipy? (log(n!))<br><br><pre><span style="color: var(--jp-mirror-editor-keyword-color); font-weight: bold;">def</span> logfactorial<span style="color: var(--jp-mirror-editor-punctuation-color);">(</span>n<span style="color: var(--jp-mirror-editor-punctuation-color);">):</span>
     <span style="color: var(--jp-mirror-editor-keyword-color); font-weight: bold;">return</span> gammaln<span style="color: var(--jp-mirror-editor-punctuation-color);">(</span>n<span style="color: var(--jp-mirror-editor-operator-color); font-weight: bold;">+</span><span style="color: var(--jp-mirror-editor-number-color);">1</span><span style="color: var(--jp-mirror-editor-punctuation-color);">)</span></pre>
+
+============================================================
----------------------------

=== Note ID: 1712741158688 (Block 812) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -5,3 +5,5 @@
     num <span style="color: var(--jp-mirror-editor-operator-color); font-weight: bold;">=</span> logfactorial<span style="color: var(--jp-mirror-editor-punctuation-color);">(</span>n<span style="color: var(--jp-mirror-editor-punctuation-color);">)</span>
     denom <span style="color: var(--jp-mirror-editor-operator-color); font-weight: bold;">=</span> logfactorial<span style="color: var(--jp-mirror-editor-punctuation-color);">(</span>n<span style="color: var(--jp-mirror-editor-operator-color); font-weight: bold;">-</span>k<span style="color: var(--jp-mirror-editor-punctuation-color);">)</span> <span style="color: var(--jp-mirror-editor-operator-color); font-weight: bold;">+</span> logfactorial<span style="color: var(--jp-mirror-editor-punctuation-color);">(</span>k<span style="color: var(--jp-mirror-editor-punctuation-color);">)</span>
     <span style="color: var(--jp-mirror-editor-keyword-color); font-weight: bold;">return</span> num <span style="color: var(--jp-mirror-editor-operator-color); font-weight: bold;">-</span> denom</pre>
+
+============================================================
----------------------------

=== Note ID: 1712741674888 (Block 813) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <pre><table class="highlighttable"><tbody><tr><td><div class="highlight"><pre><code><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">13</span><span class="p">)</span><br><br><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">365</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="mi">23</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><br><span class="n">u</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><br></code></pre></div></td></tr></tbody></table>We can use the above code to see how many unique birthdays, abd their counts exist in a population of 23 people</pre>
+
+============================================================
----------------------------

=== Note ID: 1712779912466 (Block 814) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Explicitly: a set \(R\) is a relation if each element of \(R\) is an ordered pair; this means, of course, that if \(z \in R\), then there exist \(x\) and \(y\) so that \(z\) = \((x, y)\). If \(R\) is a relation, it is sometimes convenient to express the fact that \((x, y) \in R\) by writing<br><br><ul><li>\(x R y\)</li></ul><br>and saying, as in everyday language, that \(x\) stands in the relation \(R\) to \(y\).<br>
+
+============================================================
----------------------------

=== Note ID: 1712780104523 (Block 815) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Here is a slightly more interesting example of a relation: let \(X\) be any set, and let \(R\) be the set of all those pairs \((x, y)\) in \(X \times X\) for which \(x\) = \(y\). The relation \(R\) is just the relation of equality between elements of \(X\); if \(x\) and \(y\) are in \(X\), then \(x R y\) means the same as \(x\) = \(y\).&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1712781293918 (Block 816) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>If \(R\) is an equivalence relation in \(X\), and if \(x\) is in \(X\), the equivalence class of \(x\) with respect to \(R\) is the set of all those elements \(y\) in \(X\) for which \(x R y\).&nbsp;<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1712781431964 (Block 817) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       There is no standard notation for the equivalence class of \(x\) with respect to \(R\); we shall usually denote it by \(x / R\), and we shall write \(X / R\) for the set of all equivalence classes. (Pronounce \(X / R\) as " \(X\) modulo \(R\),"
+
+============================================================
----------------------------

=== Note ID: 1712781639785 (Block 818) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Now forget \(R\) for a moment and begin anew with a partition \(\mathfrak{C}\) of \(X\). A relation, which we shall call \(X / \mathfrak{C}\), is defined in \(X\) by writing<br><br><ul><li>\(x\) \(\quad X / \mathfrak{C} \quad\) \(y\)</li></ul><br>just in case \(x\) and \(y\) belong to the same set of the collection \(\mathfrak{C}\). We shall call \(X / \mathfrak{C}\) the relation induced by the partition \(\mathfrak{C}\).<br>
+
+============================================================
----------------------------

=== Note ID: 1712782054936 (Block 819) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       More explicitly: if \(R\) is an equivalence relation in \(X\), then the set of equivalence classes is a partition of \(X\) that induces the relation \(R\), and if \(\mathfrak{C}\) is a partition of \(X\), then the induced relation is an equivalence relation whose set of equivalence classes is exactly \(\mathfrak{C}\).
+
+============================================================
----------------------------

=== Note ID: 1712782355500 (Block 820) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The second half is easier. Start with a partition \(\mathfrak{C}\) and consider the induced relation. Since every element of \(X\) belongs to some set of \(\mathfrak{C}\), reflexivity just says that \(x\) and \(x\) are in the same set of \(\mathbb{C}\). Symmetry says that if \(x\) and \(y\) are in the same set of \(\mathfrak{C}\), then \(y\) and \(x\) are in the same set of \(\mathfrak{C}\), and this is obviously true. Transitivity says that if \(x\) and \(y\) are in the same set of \(\mathfrak{C}\) and if \(y\) and \(z\) are in the same set of \(\mathfrak{C}\), then \(x\) and \(z\) are in the same set of \(\mathfrak{C}\), and this too is obvious. The equivalence class of each \(x\) in \(X\) is just the set of \(\mathfrak{C}\) to which \(x\) belongs.&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1712864220546 (Block 821) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The domain of a function \(f\) from \(X\) into \(Y\) is, by definition, equal to \(X\), but its range need not be equal to \(Y\); the range consists of those elements \(y\) of \(Y\) for which there exists an \(x\) in \(X\) such that \(f(x)\) = \(y\). If
+
+============================================================
----------------------------

=== Note ID: 1712865145590 (Block 822) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The definition of restriction can be expressed by writing \((f \mid X)\) (x)=\(f(x)\) for each \(x\) in \(X\); observe also that \(\operatorname{range}\) \((f \mid X)\) = \(f(X)\).
+
+============================================================
----------------------------

=== Note ID: 1712865308316 (Block 823) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Here is a simple but useful example of a function. Consider any two sets \(X\) and \(Y\), and define a function \(f\) from \(X \times Y\) onto \(X\) by writing \(f(x, y)\) = \(x\).&nbsp; The function \(f\) is called the projection from \(X \times Y\) onto \(X\); if, similarly, \(g(x, y)\) = \(y\), then \(g\) is the projection from \(X \times Y\) onto \(Y\)
+
+============================================================
----------------------------

=== Note ID: 1712865758395 (Block 824) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>For each element \(y\) of \(Y\), let \(g(y)\) be the set of all those elements \(x\) in \(X\) for which \(f(x)=y\).&nbsp;</li><li>The function \(g\) has the following special property: if \(u\) and \(v\) are distinct elements of \(Y\), then \(g(u)\) and \(g(v)\) are distinct elements of \(X / R\).&nbsp;</li><li>A function that always maps distinct elements onto distinct elements is called one-to-one (usually a one-to-one correspondence).<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1712869662643 (Block 825) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       &nbsp;(ii) if \(X\) is not empty, then \(\emptyset^{X}\)::set of functions to X is empty.
+
+============================================================
----------------------------

=== Note ID: 1712902797388 (Block 826) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Example 2.2.7 (A girl born in winter). A family has two children. Find the probability that both children are girls, given that at least one of the two is a girl who was born in winter. In addition to the assumptions from Example 2.2.5, assume that the four seasons are equally likely and that gender is independent of season. (This means that knowing the gender gives no information about the probabilities of the seasons, and vice versa; see Section 2.5 for much more about independence.)<br><br>Solution:<br><ul><li>\(P(\) both girls|at least one winter girl \()\) = \(P(\) both girls, at least one winter girl \()\)<br>\(P(\) at least one winter girl \()\)</li></ul><div>Since the probability that a specific child is a winter-born girl is \(1 / 8\), the denominator equals:<br></div><div><ul><li>\(P(\) at least one winter girl \()\) =&nbsp;&nbsp;\(1-(7 / 8)^2\)<br></li></ul><div>To compute the numerator, use the fact that "both girls, at least one winter girl" is the same event as "both girls, at least one winter child"; then use the assumption that gender and season are independent:</div></div><div><ul><li>\(P(\) both girls, at least one winter girl \()\)&nbsp;<br></li><li>&nbsp;=&nbsp;\(P\) (both girls, at least one winter child)</li><li>=&nbsp;\((1 / 4)(1-P(\) both are non-winter \())\)</li><li>=&nbsp; \((1 / 4)\left(1-(3 / 4)^2\right)\).<br></li></ul><div>Thus:<br><ul><li>\(P(\) both girls|at least one winter girl) =&nbsp;\(\frac{(1 / 4)\left(1-(3 / 4)^2\right)}{1-(7 / 8)^2}\) =&nbsp;\(7 / 15\).<br></li></ul><div>The point is that information about the birth season brings "at least one is a girl" closer to "a specific one is a girl". Conditioning on more and more specific information brings the probability closer and closer to \(1 / 2\).<br></div></div></div>
+
+============================================================
----------------------------

=== Note ID: 1712903078924 (Block 827) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 2.3.1 (Probability of the intersection of two events). For any events \(A\) and \(B\) with positive probabilities,<br><br><ul><li>\(P(A \cap B)\) =<br></li><li>=&nbsp;\(P(A \cap B)\)</li><li>=&nbsp;\(P(A) P(B \mid A)\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1712904373749 (Block 828) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       2.3.8 (Prior vs. posterior). It would not be correct in the calculation in the above example to say after the first step, " \(P(A)=1\) because we know \(A\) happened." It is true that \(P(A \mid A)=1\), but \(P(A)\) is the prior probability of \(A\) and \(P(F)\) is the prior probability of \(F\)-both are the probabilities before we observe any data in the&nbsp; experiment. These must not be confused with posterior probabilities conditional on the evidence \(A\).
+
+============================================================
----------------------------

=== Note ID: 1712905272571 (Block 829) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       2.4.1. When we write \(P(A \mid E)\), it does not mean that \(A \mid E\) is an event and we're taking its probability; \(A \mid E\) is not an event. Rather, \(P(\cdot \mid E)\) is a probability function which assigns probabilities in accordance with the knowledge that \(E\) has occurred, and \(P(\cdot)\) is a different probability function which assigns probabilities without regard for whether \(E\) has occurred or not.
+
+============================================================
----------------------------

=== Note ID: 1712990484369 (Block 830) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Conditional probabilities are probabilities, and all probabilities are conditional.
+
+============================================================
----------------------------

=== Note ID: 1712991271668 (Block 831) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-4dee3e4cec81c49d1de42bd1923a7e4a668c825d.jpg"><br>Example 2.4.4 (Random coin, continued). Continuing with the scenario from Example 2.3.7, suppose that we have now seen our chosen coin land Heads three times. If we toss the coin a fourth time, what is the probability that it will land Heads once more?<br><br><br>As before, let \(A\) be the event that the chosen coin lands Heads three times, and define a new event \(H\) for the chosen coin landing Heads on the fourth toss. We are interested in \(P(H \mid A)\). It would be very helpful to know whether we have the fair coin. LOTP with extra conditioning gives us \(P(H \mid A)\) as a weighted average of \(P(H \mid F, A)\) and \(P\left(H \mid F^{c}, A\right)\), and within these two conditional probabilities we \(d o\) know whether we have the fair coin:<br><ul><li>\(P(H \mid A)\) =&nbsp; \(P(H \mid F, A) P(F \mid A)\) + \(P\left(H \mid F^c, A\right) P\left(F^c \mid A\right)\)<br></li></ul><div>The posterior probabilities \(P(F \mid A)\) and \(P\left(F^{c} \mid A\right)\) are from our answer to Example 2.3.7.<br></div>
+
+============================================================
----------------------------

=== Note ID: 1712992952570 (Block 832) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       We often want to condition on more than one piece of information, and we now have several ways of doing that. For example, here are some approaches for finding \(P(A \mid B, C)\) :<br><br><ul><li>1. We can think of \(B, C\) as the single event \(B \cap C\) and use the definition of conditional probability to get<br></li><ul><li>\(P(A \mid B, C)\) =&nbsp;\(\frac{P(A, B, C)}{P(B, C)}\).</li><li>This is a natural approach if it's easiest to think about \(B\) and \(C\) in tandem.&nbsp;</li></ul><li>2. We can use Bayes' rule with extra conditioning on \(C\) to get</li><ul><li>\(P(A \mid B, C)\) = \(\frac{P(B \mid A, C) P(A \mid C)}{P(B \mid C)}\).<br></li><li>This is a natural approach if we want to think of everything in our problem as being conditioned on \(C\).<br></li></ul><li>3. We can use Bayes' rule with extra conditioning on \(B\) to get<br></li><ul><li>\(P(A \mid B, C)\) = \(\frac{P(C \mid A, B) P(A \mid B)}{P(C \mid B)}\).<br></li></ul></ul><br>
+
+============================================================
----------------------------

=== Note ID: 1713006250073 (Block 833) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       2.5.2. Independence is completely different from disjointness. If \(A\) and \(B\) are disjoint, then \(P(A \cap B)\) = \(0\), so disjoint events can be independent only if \(P(A)\) = \(0\) or \(P(B)\) = \(0\). Knowing that \(A\) occurs tells us that \(B\) definitely did not occur, so \(A\) clearly conveys information about \(B\), meaning the two events are not independent (except if \(A\) or \(B\) already has zero probability).
+
+============================================================
----------------------------

=== Note ID: 1713006349294 (Block 834) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Intuitively, it makes sense that if \(A\) provides no information about whether or not \(B\) occurred, then it also provides no information about whether or not \(B^{c}\) occurred.
+
+============================================================
----------------------------

=== Note ID: 1713006417449 (Block 835) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Proposition 2.5.3. If \(A\) and \(B\) are independent, then \(A\) and \(B^{c}\) are independent, \(A^{c}\) and \(B\) are independent, and \(A^{c}\) and \(B^{c}\) are independent.
+
+============================================================
----------------------------

=== Note ID: 1713006693902 (Block 836) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Example 2.5.5 (Pairwise independence doesn't imply independence). Consider two fair, independent coin tosses, and let \(A\) be the event that the first is Heads, \(B\) the event that the second is Heads, and \(C\) the event that both tosses have the same result. Then \(A, B\), and \(C\) are pairwise independent but not independent, since \(P(A \cap B \cap C)\) = \(1 / 4\) while \(P(A) P(B) P(C)\) = \(1 / 8\). The point is that just knowing about \(A\) or just knowing about \(B\) tells us nothing about \(C\), but knowing what happened with both \(A\) and \(B\) gives us perfect information about \(C\)
+
+============================================================
----------------------------

=== Note ID: 1713006760747 (Block 837) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       On the other hand, \(P(A \cap B \cap C)\) = \(P(A) P(B) P(C)\) does not imply pairwise independence; this can be seen quickly by looking at the extreme case \(P(A)\) = \(0\), when the equation becomes \(0\) = \(0\), which tells us nothing about \(B\) and \(C\).
+
+============================================================
----------------------------

=== Note ID: 1713006870945 (Block 838) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       For infinitely many events, we say that they are independent if every finite subset of the events is independent.
+
+============================================================
----------------------------

=== Note ID: 1713007002074 (Block 839) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       2.5.8. It is easy to make terrible blunders stemming from confusing independence and conditional independence. Two events can be conditionally independent given \(E\), but not independent given \(E^{c}\). Two events can be conditionally independent given \(E\), but not independent. Two events can be independent, but not conditionally independent given \(E\).
+
+============================================================
----------------------------

=== Note ID: 1713007151993 (Block 840) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       In particular, \(P(A, B)\) = \(P(A) P(B)\) does not imply \(P(A, B \mid E)\) = \(P(A \mid E) P(B \mid E)\); we can't just insert "given \(E\) " everywhere, as we did in going from LOTP to LOTP with extra conditioning. This is because LOTP always holds (it is a consequence of the axioms of probability), whereas \(P(A, B)\) may or may not equal \(P(A) P(B)\), depending on what \(A\) and \(B\) are.
+
+============================================================
----------------------------

=== Note ID: 1713007223972 (Block 841) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Example 2.5.9 (Conditional independence given \(E\) vs. given \(E^{c}\) ). Suppose there are two types of classes: good classes and bad classes. In good classes, if you work hard, you are very likely to get an A. In bad classes, the professor randomly assigns grades to students regardless of their effort. Let \(G\) be the event that a class is good, \(W\) be the event that you work hard, and \(A\) be the event that you receive an A. Then \(W\) and \(A\) are conditionally independent given \(G^{c}\), but they are not conditionally independent given \(G\).
+
+============================================================
----------------------------

=== Note ID: 1713008205426 (Block 842) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Example 2.5.10 (Conditional independence doesn't imply independence). Returning once more to the scenario from Example 2.3.7, suppose we have chosen either a fair coin or a biased coin with probability \(3 / 4\) of Heads, but we do not know which one we have chosen. We flip the coin a number of times. Conditional on choosing the fair coin, the coin tosses are independent,. Similarly, conditional on choosing the biased coin, the tosses are independent
+
+============================================================
----------------------------

=== Note ID: 1713011597403 (Block 843) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Exercise 2.8 If \(X\) and \(Y\) are discrete random variables on the probability space \((\Omega, \mathcal{F}, \mathbb{P})\), show that \(U\) and \(V\) are discrete random variables on this space also, where<br><br><ul><li>\(U(\omega)\) = \(X(\omega)\)+\(Y(\omega)\)<br></li><li>\(V(\omega)\) = \(X(\omega)\)\(\times\)\(Y(\omega)\)<br></li><li>for&nbsp;\(\omega \in \Omega\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1713011761989 (Block 844) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Bernoulli distribution. We say that the discrete random variable \(X\) has the Bernoulli distribution with parameter \(p\) if the image of \(X\) is \(\{0,1\}\), so that \(X\) takes the values 0 and 1 only.
+
+============================================================
----------------------------

=== Note ID: 1713012321233 (Block 845) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Poisson distribution. We say that \(X\) has the Poisson distribution with parameter \(\lambda(&gt;0)\) if \(X\) takes values in \(\{0,1,2, \ldots\}\) and<br><ul><li>\(\mathbb{P}(X=k)\) = \(\frac{1}{k !}\) \(\lambda^k\) \(e^{-\lambda}\)<br></li><li>for k = 0,1.2..</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1713012519880 (Block 846) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Poisson distribution. We say that \(X\) has the Poisson distribution with parameter \(\lambda(&gt;0)\) if \(X\) takes values in \(\{0,1,2, \ldots\}\) and<br><br>\[<br>\begin{equation*}<br>\mathbb{P}(X=k)=\frac{1}{k !} \lambda^{k} e^{-\lambda} \quad \text { for } k=0,1,2, \ldots \tag{2.15}<br>\end{equation*}<br>\]<br><br><ul><li>\(\sum_{k=0}^{\infty}\) \(\frac{1}{k !} \lambda^{k} e^{-\lambda}\)&nbsp;</li><li>= \(e^{-\lambda}\) \(\sum_{k=0}^{\infty}\) \(\frac{1}{k !} \lambda^{k}\)&nbsp;</li><li>= \(e^{-\lambda} e^{\lambda}\)&nbsp;</li><li>= \(1\)</li></ul><br>
+
+============================================================
----------------------------

=== Note ID: 1713012952825 (Block 847) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Negative binomial distribution. We say that \(X\) has the negative binomial distribution with parameters \(n\) and \(p \in(0,1)\) if \(X\) takes values in \(\{n, n+1, n+2, \ldots\}\) and<br><br>\[<br>\mathbb{P}(X=k)=\left(\begin{array}{l}<br>k-1&nbsp; \tag{2.17}\\<br>n-1<br>\end{array}\right) p^{n} q^{k-n} \quad \text { for } k=n, n+1, n+2, \ldots<br>\]<br><br><ul><li>\(\sum_{k=n}^{\infty}\) \(\left(\begin{array}{l}k-1 \\ n-1\end{array}\right) p^n q^{k-n}\)&nbsp;<br></li><li>=&nbsp;\(p^n\) \(\sum_{l=0}^{\infty}\) \(\left(\begin{array}{c}n+l-1 \\ l\end{array}\right)\) \(q^l\)</li><li>=&nbsp;\(p^n\) \(\sum_{l=0}^{\infty}\) \(\left(\begin{array}{c}-n \\ l\end{array}\right)(-q)^l\)</li><li>= \(p^n\) \((1-q)^{-n}\)&nbsp;</li><li>= 1<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1713014020209 (Block 848) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       In the binomial distribution&nbsp;If \(n\) is very large and \(p\) is very small but \(n p\) is a 'reasonable size' \((n p\)= \(\lambda\), say) then the distribution of \(S_{n}\) may be approximated by the Poisson distribution with parameter \(\lambda\), as follows. For fixed \(k \geq 0\), write \(p\)= \(\lambda / n\) and suppose that \(n\) is large to find that<br><ul><li>\(\mathbb{P}\left(S_n=k\right)\)&nbsp;</li><li>= \(\left(\begin{array}{l}n \\ k\end{array}\right) p^k(1-p)^{n-k}\)<br></li><li>\(\approx\)&nbsp;\(\frac{n^k}{k !}\left(\frac{\lambda}{n}\right)^k\)&nbsp;\(\left(1-\frac{\lambda}{n}\right)^n\)&nbsp;\(\left(1-\frac{\lambda}{n}\right)^{-k}\)<br></li><li>\(\approx\)&nbsp;\(\frac{1}{k !} \lambda^k e^{-\lambda}\)<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1713015151229 (Block 849) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 2.27 If \(X\) is a discrete random variable, the expectation of \(X\) is denoted by \(\mathbb{E}(X)\) and defined by<br><ul><li>\(\mathbb{E}(X)\) = \(\sum_{x \in \operatorname{Im} X}\) \(x\) \(\mathbb{P}(X=x)\)<br></li></ul><div>whenever this sum converges absolutely, in that \(\sum_{x}|x \mathbb{P}(X=x)|\) &lt; \(\infty\).<br></div>
+
+============================================================
----------------------------

=== Note ID: 1713015236752 (Block 850) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       \[<br>\begin{equation*}<br>\mathbb{E}(X)=\sum_{x \in \operatorname{Im} X} x \mathbb{P}(X=x) \tag{2.28}<br>\end{equation*}<br>\]<br>This equation is often written<br><ul><li>\(\mathbb{E}(X)\) = \(\sum_{x}\) \(x \mathbb{P}(X=x)\) = \(\sum_{x}\) \(x p_{X}(x)\)<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1713016355856 (Block 851) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 2.32 The variance \(\operatorname{var}(X)\) of a discrete random variable \(X\) is defined by<br><br><ul><li>\(\operatorname{var}(X)\)=\(\mathbb{E}\)\(\left([X-\mathbb{E}(X)]^2\right)\)<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1713016523881 (Block 852) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       A rough motivation for this definition is as follows. If the dispersion of \(X\) about its expectation is very small, then \(|X-\mu|\) tends to be small, giving that \(\operatorname{var}(X)=\) \(\mathbb{E}\left(|X-\mu|^{2}\right)\) is small also; on the other hand, if there is often a considerable difference between \(X\) and its mean, then \(|X-\mu|\) may be large, giving that \(\operatorname{var}(X)\) is large also.
+
+============================================================
----------------------------

=== Note ID: 1713017254742 (Block 853) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 2.40 If \(X\) is a discrete random variable and \(\mathbb{P}(B)\) \(&gt;0\), the conditional expectation of \(X\) given \(B\) is denoted by \(\mathbb{E}(X \mid B)\) and defined by<br><br><ul><li>\(\mathbb{E}(X \mid B)\)=\(\sum_{x \in \operatorname{Im} X}\) \(x \)\(\mathbb{P}(X=x \mid B)\),<br></li></ul><br>whenever this sum converges absolutely.<br>
+
+============================================================
----------------------------

=== Note ID: 1713017969782 (Block 854) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Example 2.7.3 (Gambler's ruin). Two gamblers, A and B, make a sequence of \(\$ 1\) bets. In each bet, gambler \(\mathrm{A}\) has probability \(p\) of winning, and gambler \(\mathrm{B}\) has probability \(q=1-p\) of winning. Gambler A starts with \(i\) dollars and gambler B starts with \(N-i\) dollars; the total wealth between the two remains constant since every time A loses a dollar, the dollar goes to B, and vice versa.<br><br>We can visualize this game as a random walk on the integers between 0 and \(N\), where \(p\) is the probability of going to the right in a given step. The game ends when the random walk reaches 0 or \(N\). What is the probability that A wins the game (walking away with all the money)?<br>Visualisation:<br><img src="paste-c5586c58fe232178adbb6396ee4c9d883d10055e.jpg">
+
+============================================================
----------------------------

=== Note ID: 1713018728987 (Block 855) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       2.8.1 (Prosecutor's fallacy). In 1998, Sally Clark was tried for murder after two of her sons died shortly after birth. During the trial, an expert witness for the prosecution testified that the probability of a newborn dying of sudden infant death syndrome (SIDS) was \(1 / 8500\), so the probability of two deaths due to SIDS in one family was \((1 / 8500)^{2}\), or about one in 73 million. Therefore, he continued, the probability of Clark's innocence was one in 73 million.<br><br>Second, the so-called expert has confused two different conditional probabilities: \(P\) (innocence|evidence) is different from \(P\) (evidence|innocence). The witness claims that the probability of observing two newborn deaths if the defendant were innocent is extremely low; that is, \(P\) (evidence|innocence) is small. What we are interested in<br>however, is \(P\) (innocence|evidence), the probability that the defendant is innocent given all the evidence.&nbsp; So to calculate the conditional probability of innocence given the evidence, we must take into account \(P\) (innocence), the prior probability of innocence.&nbsp;<br><br>The posterior probability of innocence given the evidence depends strongly on both \(P\) (evidence|innocence), which is very low, and \(P\) (innocence), which is very high. The expert's probability of \((1 / 8500)^{2}\), questionable in and of itself, is only part of the equation.
+
+============================================================
----------------------------

=== Note ID: 1713019438793 (Block 856) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       2.8.2 (Defense attorney's fallacy). A woman has been murdered, and her husband is put on trial for this crime. Evidence comes to light that the defendant had a history of abusing his wife. The defense attorney argues that the evidence of abuse should be excluded on grounds of irrelevance, since only 1 in 10,000 men with wives they abuse subsequently murder their wives. Should the judge grant the defense attorney's motion to bar this evidence from trial?<br><br>Let \(A\) be the event that the husband commits abuse against his wife, and let \(G\) be the event that the husband is guilty. The defense's argument is that \(P(G \mid A)\) = \(1 / 10,000\), so guilt is still extremely unlikely conditional on a previous history of abuse.<br><br>However, the defense attorney fails to condition on a crucial fact: in this case, we know that the wife was murdered. Therefore, the relevant probability is not \(P(G \mid A)\), but \(P(G \mid A, M)\), where \(M\) is the event that the wife was murdered.<br><ul><li>\(P(G \mid A, M)\) = \(\frac{P(A \mid G, M) P(G \mid M)}{P(A \mid G, M) P(G \mid M)+P\left(A \mid G^c, M\right) P\left(G^c \mid M\right)}\)<br></li></ul><div>In the above calculation of \(P(G \mid A, M)\), we did not use the defense attorney's \(P(G \mid A)\) number anywhere; it is irrelevant to our calculation because it does not account for the fact that the wife was murdered. We must condition on all the evidence.<br></div>
+
+============================================================
----------------------------

=== Note ID: 1713019661604 (Block 857) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Let's use event notation to make this precise. For events \(A, B\), and \(C\), we say that we have a Simpson's paradox if<br><ul><li>\(P(A \mid B, C)\) &lt;&nbsp;\(P\left(A \mid B^c, C\right)\)</li><li>\(P\left(A \mid B, C^c\right)\) &lt;&nbsp;\(P\left(A \mid B^c, C^c\right)\),<br></li></ul><div>but</div><div><ul><li>\(P(A \mid B)\) &gt; \(P\left(A \mid B^c\right)\).<br></li></ul></div>
+
+============================================================
----------------------------

=== Note ID: 1713019850414 (Block 858) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Let's use event notation to make this precise. For events \(A, B\), and \(C\), we say that we have a Simpson's paradox if<br><br>\[<br>\begin{gathered}<br>P(A \mid B, C)&lt;P\left(A \mid B^{c}, C\right) \\<br>P\left(A \mid B, C^{c}\right)&lt;P\left(A \mid B^{c}, C^{c}\right),<br>\end{gathered}<br>\]<br><br>but<br><br>\[<br>P(A \mid B)&gt;P\left(A \mid B^{c}\right)<br>\]<br><br>In this case, let \(A\) be the event of a successful surgery, \(B\) be the event that Dr. Nick is the surgeon, and \(C\) be the event that the surgery is a heart surgery. The conditions for Simpson's paradox are fulfilled because the probability of a successful surgery is lower under Dr. Nick than under Dr. Hibbert whether we condition on heart surgery or on Band-Aid removal, but the overall probability of success is higher for Dr. Nick.<br><br>The law of total probability tells us mathematically why this can happen:<br><ul><li>\(P(A \mid B)\) = \(P(A \mid C, B) P(C \mid B)\)+\(P\left(A \mid C^c, B\right) P\left(C^c \mid B\right)\)<br></li><li>\(P\left(A \mid B^c\right)\) =&nbsp;\(P\left(A \mid C, B^c\right) P\left(C \mid B^c\right)\)+\(P\left(A \mid C^c, B^c\right) P\left(C^c \mid B^c\right)\).<br></li></ul><div>The above equations express \(P(A \mid B)\) as a weighted average of \(P(A \mid C, B)\) and \(P\left(A \mid C^{c}, B\right)\), and \(P\left(A \mid B^{c}\right)\) as a weighted average of \(P\left(A \mid C, B^{c}\right)\) and \(P\left(A \mid C^{c}, B^{c}\right)\). If the corresponding weights were the same in both of these weighted averages, then Simpson's paradox could not occur. But the weights here are different:<br><br>\[<br>P(C \mid B)&lt;P\left(C \mid B^{c}\right) \text { and } P\left(C^{c} \mid B\right)&gt;P\left(C^{c} \mid B^{c}\right) \text {, }<br>\]<br><br>since Dr. Nick is much less likely than Dr. Hibbert to be performing a heart surgery.<br></div>
+
+============================================================
----------------------------

=== Note ID: 1713019961058 (Block 859) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Simpson's paradox arises in many real-world contexts. In the following examples, you should try to identify the events \(A, B\), and \(C\) that create the paradox.<br><br>- Gender discrimination in college admissions: In the 1970s, men were significantly more likely than women to be admitted for graduate study at the University of California, Berkeley, leading to charges of gender discrimination. Yet within most individual departments, women were admitted at a higher rate than men. It was found that women tended to apply to the departments with more competitive admissions, while men tended to apply to less competitive departments.
+
+============================================================
----------------------------

=== Note ID: 1713020238517 (Block 860) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Figure 2.7 illustrates how probabilities can be updated as new evidence comes in sequentially. Imagine that there is some event \(A\) that we are interested in. On Monday morning, for example, our prior probability for \(A\) is \(P(A)\). If we observe on Monday afternoon that \(B\) occurred, then we can use Bayes' rule (or the definition of conditional probability) to compute the posterior probability \(P(A \mid B)\).<br><br>We use this posterior probability for \(A\) as the new prior on Tuesday morning, and then we continue to collect evidence. Repeat as often as necessary
+
+============================================================
----------------------------

=== Note ID: 1713021659071 (Block 861) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>Suppose, for instance, that \(x\) is a function from a set \(I\) to a set \(X\).</li><li>&nbsp;An element of the domain \(I\) is called an index, \(I\) is called the index set, the range of the function is called an indexed set, the function itself is called a family, and the value of the function \(x\) at an index \(i\), called a term of the family, is denoted by \(x_{i}\).&nbsp;</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1713021778778 (Block 862) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       An unacceptable but generally accepted way of communicating the notation and indicating the emphasis is to speak of a family \(\left\{x_{i}\right\}\) in \(X\), or of a family \(\left\{x_{i}\right\}\) of whatever the elements of \(X\) may be; when necessary, the index set \(I\) is indicated by some such parenthetical expression as \((i \epsilon I)\). Thus, for instance, the phrase "a family \(\left\{A_{i}\right\}\) of subsets of \(X\) " is usually understood to refer to a function \(A\), from some set \(I\) of indices, into \(\mathfrak{P}(X)\).
+
+============================================================
----------------------------

=== Note ID: 1713021897694 (Block 863) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If \(\left\{A_{i}\right\}\) is a family of subsets of \(X\), the union of the range of the family is called the union of the family \(\left\{A_{i}\right\}\), or the union of the sets \(A_{i}\); the standard notation for it is<br><ul><li>\(\bigcup_{i \in I}\) \(A_{i}\)</li><li>&nbsp;or \(\bigcup_{i}\) \(A_{i}\)</li></ul><br>
+
+============================================================
----------------------------

=== Note ID: 1713091074604 (Block 864) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       An empty union makes sense (and is empty), but an empty intersection does not make sense.&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1713091157079 (Block 865) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Thus, for instance, if \(\left\{A_{i}\right\}\) is a non-empty family of sets, the intersection of the range of the family is called the intersection of the family \(\left\{A_{i}\right\}\), or the intersection of the sets \(A_{i}\); the standard notation for it is<br><br><ul><li>\(\bigcap_{i \in I} \) \(A_{i}\)&nbsp;</li><li>\(\bigcap_{i}\) \(A_{i}\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1713091193451 (Block 866) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       (By a "non-empty family" we mean a family whose domain \(I\) is not empty.
+
+============================================================
----------------------------

=== Note ID: 1713091252517 (Block 867) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       It follows immediately from the definition of intersections that if \(I\) \(\neq \emptyset\), then a necessary and sufficient condition that \(x\) belong to \(\bigcap_{i} A_{i}\) is that \(x\) belong to \(A_{i}\) for all \(i\).
+
+============================================================
----------------------------

=== Note ID: 1713091654935 (Block 868) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The notation of families is the one normally used in generalizing the concept of Cartesian product. The Cartesian product of two sets \(X\) and \(Y\) was defined as the set of all ordered pairs \((x, y)\) with \(x\) in \(X\) and \(y\) in \(Y\). There is a natural one-to-one correspondence between this set and a certain set of families.
+
+============================================================
----------------------------

=== Note ID: 1713091832564 (Block 869) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The notation of families is the one normally used in generalizing the concept of Cartesian product.&nbsp;<br><br>Consider, indeed, any particular unordered pair \(\{a, b\}\), with \(a \neq b\), and consider the set \(Z\) of all families \(z\), indexed by \(\{a, b\}\), such that \(z_{a}\) \(\in X\) and \(z_{b}\) \(\in Y\). If the function \(f\) from \(Z\) to \(X \times Y\) is defined by \(f(z)\)=\(\left(z_{a}, z_{b}\right)\), then \(f\) is the promised one-to-one correspondence. The difference between \(Z\) and \(X \times Y\) is merely a matter of notation. The generalization of Cartesian products generalizes \(Z\) rather than \(X \times Y\) itself.
+
+============================================================
----------------------------

=== Note ID: 1713092844067 (Block 870) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       It is clear that if every \(X_{i}\) is equal to one and the same set \(X\), then \( \bigtimes_i X_{i}\) =&nbsp; \(X^{I}\). If \(I\) is a pair \(\{a, b\}\), with \(a \neq b\), then it is customary to identify \(\bigtimes_{i \epsilon I} X_{i}\) with the Cartesian product \(X_{a} \times X_{b}\) as defined earlier, and if \(I\) is a singleton \(\{a\}\), then, similarly, we identify \(\bigtimes_{i \epsilon I} X_{i}\) with \(X_{a}\) itself.
+
+============================================================
----------------------------

=== Note ID: 1713093521556 (Block 871) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose that \(\left\{X_{i}\right\}\) is a family of sets \((i \in I)\) and let \(X\) be its Cartesian product. If \(J\) is a subset of \(I\), then to each element of \(X\) there corresponds in a natural way an element of the partial Cartesian product \(\bigtimes_{i \epsilon J} X_{i}\)::math::math::math::math.&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1713094238945 (Block 872) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Prove that:<br><ul><li>&nbsp;\(\left(\bigcup_{i} A_{i}\right)\)\(\times\)\(\left(\bigcup_{j} B_{j}\right)\) = \(\bigcup_{i, j}\left(A_{i} \times B_{j}\right)\)</li><li>&nbsp;\(\left(\bigcap{i} A_{i}\right)\)\(\times\)\(\left(\bigcap{j} B_{j}\right)\) = \(\bigcap{i, j}\left(A_{i} \times B_{j}\right)\) if the domains of the families involved are {{c4<b>va</b>::non-empty}}</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1713094355456 (Block 873) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Prove also (with appropriate provisos about empty families) that \(\bigcap_{i} X_{i}\) \(\subset\) \(X_{j}\) \(\subset\) \(\bigcup_{i} X_{i}\) for each index \(j\) and that intersection and union can in fact be characterized as the extreme solutions of these inclusions.&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1713094420365 (Block 874) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Prove also (with appropriate provisos about empty families) that \(\bigcap_{i} X_{i} \subset X_{j} \subset \bigcup_{i} X_{i}\) for each index \(j\) and that intersection and union can in fact be characterized as the extreme solutions of these inclusions. This means that if \(X_{j}\) \(\subset Y\) for each index \(j\), then \(\bigcup_{i} X_{i}\) \(\subset Y\), and that \(\bigcup_{i} X_{i}\) is the only set satisfying this minimality condition; the formulation for intersections is similar.
+
+============================================================
----------------------------

=== Note ID: 1713096389948 (Block 875) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Given a function \(f\) from \(X\) to \(Y\), let \(f^{-1}\), the inverse of \(f\), be the function from \(\mathfrak{P}(Y)\) to \(\mathfrak{P}(X)\) such that if \(B\) \(\subset Y\)&nbsp;then<br><ul><li>\(f^{-1}\)\((B)\) = \(\{x \in X: f(x) \in B\} .\)<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1713096421042 (Block 876) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       In words: \(f^{-1}(B)\) consists of exactly those elements of \(X\) that \(f\) maps into \(B\); the set \(f^{-1}(B)\) is called the inverse image of \(B\) under \(f\).&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1713096634055 (Block 877) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>&nbsp;A necessary and sufficient condition that \(f\) be one-to-one is that the inverse image under \(f\) of each singleton in the range of \(f\) be a singleton in \(X\).</li><li>If the last condition is satisfied, then the symbol \(f^{-1}\) is frequently assigned a second interpretation, namely as the function whose domain is the range of \(f\), and whose value for each \(y\) in the range of \(f\) is the unique \(x\) in \(X\) for which \(f(x)\) = \(y\).<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1713096809733 (Block 878) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If \(B\) \(\subset\) \(Y\), then<br><br><ul><li>\(f\left(f^{-1}(B)\right)\) \(\subset\) \(B .\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1713096920433 (Block 879) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If \(B \subset Y\), then<br><br>\[<br>f\left(f^{-1}(B)\right) \subset B .<br>\]<br><br>Proof. If \(y \epsilon\) \(f\left(f^{-1}(B)\right)\), then \(y\)=\(f(x)\) for some \(x\) in \(f^{-1}(B)\); this means that \(y\)=\(f(x)\) and \(f(x)\) \(\epsilon B\), and therefore \(y\) \(\in\) \(B\).
+
+============================================================
----------------------------

=== Note ID: 1713098228999 (Block 880) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If \(f\) maps \(X\) onto \(Y\), then<br><br>\[<br>f\left(f^{-1}(B)\right)=B .<br>\]<br><br>Proof. If \(y\) \(\in B\), then \(y\)=\(f(x)\) for some \(x\) in \(X\), and therefore for some \(x\) in \(f^{-1}(B)\); this means that \(y \in\) \(f\left(f^{-1}(B)\right)\).
+
+============================================================
----------------------------

=== Note ID: 1713098281983 (Block 881) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If \(A\) \(\subset\) \(X\), then<br><br><ul><li>\(A\) \(\subset\) \(f^{-1}(f(A))\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1713098358444 (Block 882) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If \(A \subset X\), then<br><br>\[<br>A \subset f^{-1}(f(A))<br>\]<br>Proof. If \(x \in A\), then \(f(x)\) \(\in\) \(f(A)\); this means that \(x\) \(\in\) \(f^{-1}(f(A))\).
+
+============================================================
----------------------------

=== Note ID: 1713098635472 (Block 883) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If \(f\) is one-to-one, then<br><br>\[<br>A=f^{-1}(f(A)) \text {. }<br>\]<br><br>Proof. If \(x\) \(\in\) \(f^{-1}(f(A))\), then \(f(x)\) \(\epsilon\) \(f(A)\), and therefore \(f(x)\)=\(f(u)\) for some \(u\) in \(A\); this implies that \(x\)=\(u\) and hence that \(x\) \(\in\) \(A\).
+
+============================================================
----------------------------

=== Note ID: 1713098724012 (Block 884) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The algebraic behavior of \(f^{-1}\) is unexceptionable. If \(\left\{B_{i}\right\}\) is a family of subsets of \(Y\), then<br><ul><li>\(f^{-1}\) \(\left(\bigcup_i B_i\right)\)=\(\bigcup_i f^{-1}\left(B_i\right)\)<br></li><li>\(f^{-1}\) \(\left(\bigcap_i B_i\right)\)=\(\bigcap_i f^{-1}\left(B_i\right)\)<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1713098845792 (Block 885) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       &nbsp;If, for instance, \(x\) \(\in\) \(f^{-1}\left(\bigcap_{i} B_{i}\right)\), then \(f(x)\) \(\in\) \(B_{i}\) for all \(i\), so that \(x\) \(\in\) \(f^{-1}\left(B_{i}\right)\) for all \(i\), and therefore \(x\)&nbsp;\(\in\) \(\bigcap_{i} f^{-1}\left(B_{i}\right)\); all the steps in this argument are reversible.&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1713099110202 (Block 886) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The formation of inverse images commutes with complementation also; i.e.,<br><br>\[<br>f^{-1}(Y-B)=X-f^{-1}(B)<br>\]<br><br>(Observe that the last equation is indeed a kind of commutative law: it says that complementation followed by inversion is the same as inversion followed by complementation.)
+
+============================================================
----------------------------

=== Note ID: 1713099194694 (Block 887) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If, to be explicit, \(f\) is a function from \(X\) to \(Y\) and \(g\) is a function from \(Y\) to \(Z\), then every element in the range of \(f\) belongs to the domain of \(g\), and, consequently, \(g(f(x))\) makes sense for each \(x\) in \(X\).
+
+============================================================
----------------------------

=== Note ID: 1713099935978 (Block 888) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Observe that the order of events is important in the theory of functional composition. In order that \(g f\) be defined, the range of \(f\) must be included in the domain of \(g\), and this can happen without it necessarily happening in the other direction at the same time. Even if both \(f g\) and \(g f\) are defined, which happens if, for instance, \(f\) maps \(X\) into \(Y\) and \(g\) maps \(Y\) into \(X\), the functions \(f g\) and \(g f\) need not be the same; in other words, functional composition is not necessarily commutative.
+
+============================================================
----------------------------

=== Note ID: 1713100037337 (Block 889) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Functional composition may not be commutative, but it is always associative. If \(f\) maps \(X\) into \(Y\), if \(g\) maps \(Y\) into \(Z\), and if \(h\) maps \(Z\) into \(U\), then we can form the composite of \(h\) with \(g f\) and the composite of \(h g\) with \(f\);&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1713100353899 (Block 890) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If \(f\) maps \(X\) into \(Y\) and \(g\) maps \(Y\) into \(Z\), then \(f^{-1}\) maps \(\mathfrak{P}(Y)\) into \(\mathfrak{P}(X)\) and \(g^{-1}\) maps \(\mathfrak{P}(Z)\) into \(\mathfrak{P}(Y)\). In this situation, the composites that are formable are \(g f\) and \(f^{-1} g^{-1}\); the assertion is that the latter is the inverse of the former. <br><br>Proof: if \(x \epsilon\) \((g f)^{-1}(C)\), where \(x \epsilon\) \(X\) and \(C\) \(\subset\) \(Z\), then \(g(f(x))\) \(\epsilon\) \(C\), so that \(f(x)\) \(\epsilon\) \(g^{-1}(C)\), and therefore \(x\) \(\epsilon\) \(f^{-1}\left(g^{-1}(C)\right)\); the steps of the argument are reversible.
+
+============================================================
----------------------------

=== Note ID: 1713100533003 (Block 891) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       By definition \(y\) \(R^{-1}\) \(x\) means that \(x\) \(R\) \(y\).
+
+============================================================
----------------------------

=== Note ID: 1713100563284 (Block 892) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       &nbsp;Example: if \(R\) is the relation of belonging, from \(X\) to \(\mathfrak{P}(X)\), then \(R^{-1}\) is the relation of containing, from \(\mathfrak{P}(X)\) to \(X\).&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1713101001272 (Block 893) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Thus, in particular, composition is commutative&nbsp;&nbsp;by accident only, but it is always associative, and it is always connected with inversion via the equation \((S R)^{-1}\)=\(R^{-1} S^{-1}\)
+
+============================================================
----------------------------

=== Note ID: 1713101267156 (Block 894) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The three defining properties of an equivalence relation can be formulated in algebraic terms as follows:<br><ul><li>&nbsp;reflexivity means \(I\) \(\subset\) \(R\),&nbsp;</li><li>symmetry means \(R\) \(\subset\) \(R^{-1}\)</li><li>and transitivity means \(R R\) \(\subset\) \(R\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1713102088992 (Block 895) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>&nbsp;If \(X\) is a discrete random variable and \(\mathbb{E}\)\(\left(X^{2}\right)\) =\(0\)</li><li>Show that \(\mathbb{P}\)\((X=0)\) = \(1\).&nbsp;</li><li>Deduce that, if \(\operatorname{var}\)\((X)\)=\(0\), then \(\mathbb{P}\)\((X=\mu)\)=\(1\), whenever \(\mu=\mathbb{E}(X)\) is finite.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1713110817514 (Block 896) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       (b) \(\left(\right.\) probability that a random 2 letter word is a palindrome \(\left.{ }^1\right)=(\) probability that a random 3 letter word is a palindrome)<br><br>Solution:<br><ul><li>For two letters you have 26 options and then the second is determined, out of&nbsp; 26^2 total</li><li>For three letters you have 26 options for the first, then the last is determined and then another 26 for the middle. Out of a total of 26^3</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1713111629562 (Block 897) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       4. A norepeatword is a sequence of at least one (and possibly all) of the usual 26 letters a,b,c,..,z, with repetitions not allowed. For example, "course" is a norepeatword, but "statistics" is not. Order matters, e.g., "course" is not the same as "source".<br><ul><li>Since nonrepeat words out of a n letter alphabet can contain all arangements of the alphabet into k letters the probability of a word of length k is:</li><ul><li>\(\frac{n!}{(n-k)!}\)<br></li></ul><li>The probability of a word of length n is n!</li><li>thus we have:</li><ul><li>\(n! / (\sum \frac{n!}{(n-k)!}) \)<br></li><li>Where&nbsp;&nbsp;\((\sum \frac{n!}{(n-k)!})\) = \((n ! \sum \frac{1}{(n-k)!})\) =&nbsp;&nbsp;\((n ! \sum \frac{1}{(k)!})\) by symmetry</li></ul><li>Thus:&nbsp;</li><ul><li>\(n! / (\sum \frac{n!}{(n-k)!}) \)</li><li>=&nbsp;&nbsp;</li><li>= \(1 / (\sum \frac{1}{(k)!}) \)</li><li>=&nbsp;\(1/e\)</li></ul></ul><br><br>
+
+============================================================
----------------------------

=== Note ID: 1713118465970 (Block 898) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       1. A certain family has 6 children, consisting of 3 boys and 3 girls. Assuming that all birth orders are equally likely, what is the probability that the 3 eldest children are the 3 girls?<br><br><ul><li>Ways to permute 1,2,3 = 3!,&nbsp;</li><li>Ways to permute 4,5,6 = 3!</li><li>P =&nbsp;\(\frac{(3 !)^2}{6 !}\)</li><li>Alternatively, we can use the fact that there are \(\left(\begin{array}{l}6 \\ 3\end{array}\right)\) options for where the girls appear in the birth order ignoring their odering, out of which only 1 has them as consecutive<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1713119109666 (Block 899) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       3. A college has 10 (non-overlapping) time slots for its courses, and blithely assigns<br>courses to time slots randomly and independently. A student randomly chooses 3 of<br>the courses to enroll in (for the PTP, to avoid getting fined). What is the probability<br>that there is a conflict in the student's schedule?<br><br>The probability of no conflict is&nbsp;\(\frac{10 \cdot 9 \cdot 8}{10^3}\) since we need to pick different positions. Thus the probability of conflict is 1-that
+
+============================================================
----------------------------

=== Note ID: 1713851716409 (Block 900) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       A typical task in analysis is to decipher whether a property possessed by every term in a convergent sequence is necessarily inherited by the limit. Assume \(\left(a_{n}\right) \rightarrow a\), and determine the validity of each claim. Try to produce a counterexample for any that are false.<br><br>(a) If every \(a_{n}\) is an upper bound for a set \(B\), then \(a\) is also an upper bound for \(B\).<br><br><ul><li>Assume that&nbsp;\(a_n &gt; M &gt; a\)&nbsp;</li><li>This implies:</li><li>\(a_n - a\)&nbsp;\(&gt;\)&nbsp;\(M - a\) &gt;&nbsp;\(0\)<br></li><li>However, by the definition of convergence:</li><li>\(a_n - a\)&nbsp;\(&lt;\)&nbsp;\(\epsilon\) for all&nbsp;\(\epsilon\) after a given n<br></li><li>So pick \(\epsilon\) =&nbsp;\(M-a -1\) leading to a contradiction</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1713852144462 (Block 901) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       A typical task in analysis is to decipher whether a property possessed by every term in a convergent sequence is necessarily inherited by the limit. Assume \(\left(a_{n}\right) \rightarrow a\), and determine the validity of each claim. Try to produce a counterexample for any that are false.<br><br>(b) If every \(a_{n}\) is in the complement of the interval \((0,1)\), then \(a\) is also in the complement of \((0,1)\).<br><br><ul><li>(b) True, since if \(a \in(0,1)\) then there would exist an \(\epsilon\)-neighborhood inside \((0,1)\) that \(a_{n}\) would have to fall in, contradicting the fact that \(a_{n}\) \(\notin(0,1)\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1713852585931 (Block 902) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       (a) Prove that the sequence defined by \(x_{1}=3\) and<br><br>\[<br>x_{n+1}=\frac{1}{4-x_{n}}<br>\]<br><br>converges.<br><br>(b) Now that we know \(\lim x_{n}\) exists, explain why \(\lim x_{n+1}\) must also exist and equal the same value.<br><br>(c) Take the limit of each side of the recursive equation in part (a) to explicitly compute \(\lim x_{n}\).<br><br><ul><li>\(x\)=\(\frac{1}{4-x}\)<br></li><li>\(\Longleftrightarrow\) \(x^2-4 x+1\)=\(0\)<br></li><li>\((x-2)^2\) = \(3\)<br></li><li>\(x\) = \(2 \pm \sqrt{3}\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1713853220596 (Block 903) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div>Let \(\left(a_{n}\right)\) be a bounded sequence.<br><br>(a) Prove that the sequence defined by \(y_{n}=\sup \left\{a_{k}: k \geq n\right\}\) converges.<br></div><div><br></div><div><ul><li>\(y_n\) is monotone since the supremum of&nbsp;\(y_m\) must also be an upper bound of&nbsp;\(y_{m+1}\)<br></li><li>\(y_n\) is bounded since the original sequence is bounded and the supremum is the least upper bound<br></li></ul></div>
+
+============================================================
----------------------------

=== Note ID: 1713853388632 (Block 904) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       (c) Prove that \(\lim \inf a_{n} \leq \lim \sup a_{n}\) for every bounded sequence, and give an example of a sequence for which the inequality is strict.<br><br>(d) If \(\liminf a_{n}=\lim \sup a_{n}\) then the squeeze theorem (Exercise 2.3.3) implies \(a_{n}\) converges to the same value, since \(\inf \left\{a_{k \geq n}\right\}\) \(\leq\) \(a_{n}\) \(\leq\) \(\sup \left\{a_{k \geq n}\right\}\)
+
+============================================================
----------------------------

=== Note ID: 1714200370008 (Block 905) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       1. A certain family has 6 children, consisting of 3 boys and 3 girls. Assuming that all birth orders are equally likely, what is the probability that the 3 eldest children are the 3 girls?<br><br>Solution:<br><ul><li>How many ways can you permute 4,5,6 and 1,2,3 out of the ways to permute 1,2,3,4,5,6</li><li>P =&nbsp;\(\frac{(3 !)^2}{6 !}\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1714201137874 (Block 906) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       4. A city with 6 districts has 6 robberies in a particular week. Assume the robberies<br>are located randomly, with all possibilities for which robbery occurred where equally<br>likely. What is the probability that some district had more than 1 robbery?<br><br>There are \(6^6\) possible configurations for which robbery occurred where. There are 6 ! configurations where each district had exactly 1 of the 6 , so the probability of the complement of the desired event is \(6 ! / 6^6\). So the probability of some district having more than 1 robbery is<br>\[<br>1-6 ! / 6^6 \approx 0.9846 \text {. }<br>\]<br><br>Note that this also says that if a fair die is rolled 6 times, there's over a \(98 \%\) chance that some value is repeated!
+
+============================================================
----------------------------

=== Note ID: 1714204892893 (Block 907) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       6. A jar contains r red balls and g green balls, where r and g are fixed positive<br>integers. A ball is drawn from the jar randomly (with all possibilities equally likely),<br>and then a second ball is drawn randomly.<br><br><br>(a) Explain intuitively why the probability of the second ball being green is the same<br>as the probability of the first ball being green.<br><br>This is true by symmetry. The first ball is equally likely to be any of the \(g+r\) balls, so the probability of it being green is \(g /(g+r)\). But the second ball is also equally likely to be any of the \(g+r\) balls (there aren't certain balls that enjoy being chosen second and others that have an aversion to being chosen second); once we know whether the first ball is green we have information that affects our uncertainty about the second ball, but before we have this information, the second ball is equally likely to be any of the balls.<br><br>Alternatively, intuitively it shouldn't matter if we pick one ball at a time, or take one ball with the left hand and one with the right hand at the same time. By symmetry, the probabilities for the ball drawn with the left hand should be the same as those for the ball drawn with the right hand.
+
+============================================================
----------------------------

=== Note ID: 1714212831822 (Block 908) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       7. (a) Show using a story proof that<br>\[<br>\left(\begin{array}{l}<br>k \\<br>k<br>\end{array}\right)+\left(\begin{array}{c}<br>k+1 \\<br>k<br>\end{array}\right)+\left(\begin{array}{c}<br>k+2 \\<br>k<br>\end{array}\right)+\cdots+\left(\begin{array}{l}<br>n \\<br>k<br>\end{array}\right)=\left(\begin{array}{l}<br>n+1 \\<br>k+1<br>\end{array}\right),<br>\]<br>where \(n\) and \(k\) are positive integers with \(n \geq k\).<br>Hint: imagine arranging a group of people by age, and then think about the oldest person in a chosen subgroup.<br><br><br>Consider choosing \(k+1\) people out of a group of \(n+1\) people. Call the oldest person in the subgroup "Aemon." If Aemon is also the oldest person in the full group, then there are \(\left(\begin{array}{l}n \\ k\end{array}\right)\) choices for the rest of the subgroup. If Aemon is the second oldest in the full group, then there are \(\left(\begin{array}{c}n-1 \\ k\end{array}\right)\) choices since the oldest person in the full group can't be chosen. In general, if there are \(j\) people in the full group who are younger than Aemon, then there are \(\left(\begin{array}{l}j \\ k\end{array}\right)\) possible choices for the rest of the subgroup. Thus,<br>\[<br>\sum_{j=k}^n\left(\begin{array}{l}<br>j \\<br>k<br>\end{array}\right)=\left(\begin{array}{l}<br>n+1 \\<br>k+1<br>\end{array}\right)<br>\]
+
+============================================================
----------------------------

=== Note ID: 1716570936910 (Block 909) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The preceding paragraph motivates a set-theoretic construction that makes sense for every set, but that is of interest in the construction of numbers only. For every set \(x\) we define the successor \(x^{+}\) of \(x\) to be the set obtained by adjoining \(x\) to the elements of \(x\); in other words,<br><br><ul><li>\(x^{+}\) = \(x \cup\{x\}\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1716663406132 (Block 910) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       (Abbott 2.5.5) Assume that \(\left(a_n\right)\) is a bounded sequence with the property that every convergent subsequence of \(\left(a_n\right)\) converges to the same limit \(a \in \mathbb{R}\). Show that \(\left(a_n\right)\) must converge to \(a\).<br><br>Solution if it diverges<ul><li>A convergent subsqeuence must exist by Bolzano<br></li><li>\(\left(a_n\right)\) does not converge to \(a\) if there exists \(\epsilon&gt;0\) such that for every \(k \in \mathbb{N}\) there exists a number \(m\) such that \(m \geq k\) with \(\left|a-a_m\right| \geq \epsilon\).<br></li><li>Construct a new subsequence \(\left(a_{m_k}\right)\) as follows. Let \(m_1 \geq 1\) be such that \(\left|a-a_{m_1}\right|\)&nbsp; \(\geq \epsilon\).&nbsp;</li><li>And define \(m_k\) inductively by, \(m_k\) \(\geq m_{k-1}+1\) be such that \(\left|a-a_{m_k}\right|\) \(\geq \epsilon\).&nbsp;</li><li>We apply Bolzano-Weierstrass to the subsequence \(\left(a_{m_k}\right)\) to get that there exists a subsequence of \(\left(a_{m_k}\right)\) that converges.&nbsp;</li><li>Every subsequence of \(\left(a_{m_k}\right)\) is also a subsequence of \(\left(a_n\right)\).&nbsp;</li><li>Abusing notation, we call this convergent subsequence \(\left(a_{m_k}\right)\) and let \(a^{\prime}\) be the limit.<br></li><li>Now we show that \(a \neq a^{\prime}\). Since \(\left|a-a_{m_k}\right|\) \(\geq \epsilon\) for all \(k \in \mathbb{N}\) we have that either \(a-a_{m_k}\) \(&gt;\epsilon\) or \(a_{m_k}-a\) \(&gt;\epsilon\) for all \(k \in \mathbb{N}\). By monotonicity of order, we have that \(a-a^{\prime}\) \(\geq \epsilon\) or \(a^{\prime}-a\) \(\geq \epsilon\). In either case, \(a \neq a^{\prime}\). Therefore we have constructed two convergent subsequences \(a_{n_k}\) and \(a_{m_k}\) that converge to distinct limits.<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1716730844719 (Block 911) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Let \(\left(a_n\right)\) be a bounded sequence. Recall from HW4, we defined a sequence \(y_n=\sup \left\{a_k: k \geq n\right\}\) and proved that \(\left(y_n\right)\) is bounded and monotone. Thus, by the Monotone Convergence Theorem \(\left(y_n\right)\) converges. Let \(y=\lim _{n \rightarrow \infty} y_n\).<br>Prove that there exists a subsequence \(\left(a_{n_k}\right)\) of \(\left(a_n\right)\) that converges to \(y\).<br><br>Proof:<br><ul><li>Since&nbsp;\(y_{n_i}\) is always a supremum of a set</li><li>There always exists</li><li>\(y_{n_{k-1}+1}-\frac{1}{k}\)&nbsp;\(\leq\)&nbsp;\(a_{n_{k}}\)&nbsp;\(\leq\)&nbsp;\(y_{n_{k-1}+1}\)<br></li><li>Since \(y_{n_{k-1}+1}\)=\(\sup A_{n_{k-1}+1}\) \(a_{n_k}\) there exists such an&nbsp;\(a_{n_k}\) from&nbsp; \(A_n=\left\{a_k: k \geq n\right\}\),&nbsp;</li><li>Since:</li><ul><li>\(\left(y_{n_{k-1}+1}\right)\) must converge to y&nbsp;</li><li>And &nbsp;\(\lim \frac{1}{k}\) = \(0\)&nbsp;</li></ul><li>By the sequeze theorem&nbsp;\(a_{n_k}\) must also converge to y</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1716849947919 (Block 912) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If \(\left(x_n\right)\) and \(\left(y_n\right)\) are Cauchy sequences prove directly that \(\left(x_n y_n\right)\) is a Cauchy sequence.<br><br>Solution:<br><ul><li>Both are bounded by some bound M = \(\max(M_x,M_y)\)</li><li>Let:</li><ul><li>\(\left|x_n-x_m\right|\)&nbsp;\(\leq\)&nbsp;\(\frac{\epsilon}{2 M}\)<br></li><li>\(\left|y_n-y_m\right|\)&nbsp;\(\leq\)&nbsp;\(\frac{\epsilon}{2 M}\)</li></ul><li>\(\left|x_n y_n-x_m y_m\right| \) \(\leq\) \(\left|x_n y_n-x_m y_n\right|+\left|x_m y_n-x_m y_m\right|\)<br>(by triangle inequality)<br></li><li>=&nbsp;\(\left|x_n-x_m\right|\left|y_n\right|+\left|x_m\right|\left|y_n-y_m\right|\)</li><li>=&nbsp;\(\frac{\epsilon}{2 M} M+M \frac{\epsilon}{2 M}\)&nbsp;</li><li>=&nbsp;\(\epsilon\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1716920968601 (Block 913) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Alice attends a small college in which each class meets only once a week. She is deciding between 30 non-overlapping classes. There are 6 classes to choose from for each day of the week, Monday through Friday. Trusting in the benevolence of randomness, Alice decides to register for 7 randomly selected classes out of the 30 , with all choices equally likely. What is the probability that she will have classes every day, Monday through Friday? (This problem can be done either directly using the naive definition of probability, or using inclusion-exclusion.)<br><br>Solution:<br><ol><li>\(P(A_i)\) is the probability of having at least one class on a given day while \(P(A_i^c)\) is the probability of no classes on a given day&nbsp;</li><li>\(P(\cap A_i)\) = 1 -&nbsp;\(P(\cup A_i^c)\)<br></li><li>\(P(\cup A_i^c)\) = inclusion exclusion, all probabilities are symmetric so we just get binomial coefficients</li><li>\(P(A_i^c)\) =&nbsp;\(\binom{24}{7}\) \(/\) \({\binom{30}{7} }\) because:<br></li><ol><li>There are \({\binom{30}{7} }\)&nbsp;possible selections of classes</li><li>Out of which \(\binom{24}{7}\) do not include the 6 classes allocated to the exluded day</li></ol><li>\(P(A_i^c \cap A_j^c)\)&nbsp; = \(\binom{18}{7}\) \(/\) \({\binom{30}{7} }\)&nbsp;because we subtract the other 6 classes of day j&nbsp; from the allocation</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1718714664152 (Block 914) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       1. Arby has a belief system assigning a number \(P_{\text {Arby }}(A)\) between 0 and 1 to every event \(A\) (for some sample space). This represents Arby's subjective degree of belief about how likely \(A\) is to occur. For any event \(A\), Arby is willing to pay a price of \(1000 \cdot P_{\text {Arby }}(A)\) dollars to buy a certificate.<br><br>Likewise, Arby is willing to sell such a certificate at the same price. Indeed, Arby is willing to buy or sell any number of certificates at this price, as Arby considers it the "fair" price.<br><br>Arby, not having taken Stat 110, stubbornly refuses to accept the axioms of probability. In particular, suppose that there are two disjoint events \(A\) and \(B\) with<br>\[<br>P_{\text {Arby }}(A \cup B) \neq P_{\text {Arby }}(A)+P_{\text {Arby }}(B) .<br>\]<br><br>Show how to make Arby go bankrupt, by giving a list of transactions Arby is willing to make that will guarantee that Arby will lose money (you can assume it will be known whether \(A\) occurred and whether \(B\) occurred the day after any certificates are bought/sold).<br><br>Solution:<br><ul><li>Suppose that</li><ul><li>\(P_{\text {Arby }}(A \cup B)\) \(&lt;\) \(P_{\text {Arby }}(A)+P_{\text {Arby } }(B)\).</li><ul><li>Since the probability of the&nbsp; union is lower, Arby pays\(P_{\text {Arby } }(A)+P_{\text {Arby } }(B)\) and buys certificates for &nbsp;\(P_{\text {Arby } }(A \cup B)\)</li><li>Thus every time she loses the difference&nbsp;\(P_{\text {Arby } }(A)+P_{\text {Arby } }(B)-P_{\text {Arby } }(A \cup B)\)</li></ul><li>\(P_{\text {Arby } }(A \cup B)\) \(&lt;\) \(P_{\text {Arby } }(A)+P_{\text {Arby }}(B)\)</li><ul><li>Then arby loses \(P_{\text {Arby }&nbsp; }(A \cup B)-\left(P_{\text {Arby } }(A)+P_{\text {Arby } }(B)\right) \)  per transatction&nbsp;</li><li>Even if A and B are not disjoint in this case</li></ul></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1718898318777 (Block 915) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       6. A family has two children. Assume that birth month is independent of gender, with boys and girls equally likely and all months equally likely, and assume that the elder child's characteristics are independent of the younger child's characteristics).<br>(a) Find the probability that both are girls, given that the elder child is a girl who was born in March.<br><br>Solution:<br><ul><li>Let \(G_j\) be the event that the \(j\) th born child is a girl and \(M_j\) be the event that the \(j\) th born child was born in March, for \(j \in\{1,2\}\). Then \(P\left(G_1 \cap G_2 \mid G_1 \cap M_1\right)\) =&nbsp; \(P\left(G_2 \mid G_1 \cap M_1\right)\), since if we know that \(G_1\) occurs, then \(G_1 \cap G_2\) occurring is the same thing as \(G_2\) occurring. By independence of the characteristics of the children, \(P\left(G_2 \mid G_1 \cap M_1\right)\) = \(P\left(G_2\right)\) = \(1 / 2\).<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1718898638536 (Block 916) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Where does the result come from?<br><br><ul><li>\(\frac{P(\text { both girls, at least one born in March })}{P(\text { at least one March-born girl })}\)</li><li>=&nbsp;\(\frac{(1 / 4)\left(1-(11 / 12)^2\right)}{1-(23 / 24)^2}\)</li></ul><div>Solution:</div><div><ul><li>Top: probability that they are both girls times probability at least one is born in march</li><li>Botttom: Probability that at least one is born in march when we do not know that they are both girls&nbsp;</li></ul></div>
+
+============================================================
----------------------------

=== Note ID: 1718913074130 (Block 917) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Problem 1. (25 pts) For each statement, circle \(T\) if it is true and \(F\) if it is false. If true, give a brief explanation (a complete proof is not required), and if false, give a counterexample.<br><br>c. \((T / F)\) If the sequence \(\left(a_n\right)\) converges and the sequence \(\left(a_n+b_n\right)\) converges then the sequence \(\left(b_n\right)\) converges.<br><br>Solution:<br><ul><li>\(|a_n + b_n - c|\)&nbsp;\(&lt;\)&nbsp;\(\epsilon/2\)</li><li>\(|a_n - a|\)&nbsp;\(&lt;\)&nbsp;\(\epsilon/2\)</li><li>\(b_n\) =&nbsp;\(a_n + b_n - a_n\)</li><li>= \(|a_n + b_n - a_n - c + a|\) by taking b=c-a</li><li>= \(|a_n + b_n - c + - (a_n - a)|\)&nbsp;&nbsp;</li><li>\(\leq\) \(|a_n + b_n - c|\)&nbsp; +&nbsp; \(|a_n - a|\)&nbsp; by triangle inequality<br></li><li>\(\leq\)&nbsp;\(\epsilon\)<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1718928568231 (Block 918) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Problem 1. (25 pts) For each statement, circle \(T\) if it is true and \(F\) if it is false. If true, give a brief explanation (a complete proof is not required), and if false, give a counterexample.<br><br>e. \((T / F)\) If \(\left(a_n\right)\) converges to \(a\) and \(\left(b_n\right)\) converges to \(b\) then the sequence \(\left(a_{2 n} b_{2 n+1}\right)\) converges to \(a b\).<br><br>Solution:<br><ul><li>\(|a_{2n} - a|\)&nbsp;\( &lt;\)&nbsp;\(\frac{\epsilon} {M+b}\)<br></li><li>\(|b_{2n+1} - b|\) \( &lt;\)&nbsp; \(\frac{\epsilon} {M+b}\)<br></li><li>\(|a_{2n}b_{2n+1} - ab| \)</li><li>= \(|a_{2n}b_{2n+1} - ab| \)</li><li>= | \( a_{2n}b_{2n+1} - a_{2n} b\) + \(a_{2n}b -&nbsp; ab \) |</li><li>\(\leq\) \(|a_{2n}b_{2n+1} - a_{2n} b|\) + \(|a_{2n}b -&nbsp; ab| \)<br></li><li>\(\leq\) \(|a_{2n}(b_{2n+1} - b) |\)&nbsp; + \(|b (a_{2n} -&nbsp; a)| \)</li><li>\(\leq\) |\(a_{2n}\) \( \frac{\epsilon} {M+b} \)&nbsp;| + | \(b\) \(\frac{\epsilon} {M+b}\)&nbsp;|&nbsp;</li><li>\(\leq\)&nbsp;\(\frac{\epsilon} {M+b}\)&nbsp;\(|a_{2n} +b|\)<br></li><li>\(\leq\)&nbsp;\(\frac{\epsilon} {M+b}\)&nbsp;\(|M +b|\)</li><li>\(\leq\)&nbsp;\(\epsilon\)<br></li></ul><div><br></div>
+
+============================================================
----------------------------

=== Note ID: 1718929968727 (Block 919) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Problem 4. Define a sequence recursively by \(x_1=1\) and<br>\[<br>x_{n+1}=\sqrt{6+x_n} \quad \text { for } \quad n \in \mathbb{N} \text {. }<br>\]<br>a. (10 pts) Show that the sequence is increasing.<br><br>Solution:<br><ul><li>Assume induction</li><li>\(x_n\) &gt; \(x_{n-1}\)</li><li>\(6+ x_n\) &gt; \(6+x_{n-1} \)<br></li><li>\(\sqrt{6+x_n}\) &gt; \(\sqrt{6+x_{n-1} }\)<br></li><li>\(\sqrt{6+x_n}\) &gt; \(x_n\)</li><li>\(x_{n+1}\) &gt;&nbsp;\(x_n\)<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1719257618509 (Block 920) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       e. \((T / F)\) If \(0 \leq a_n \leq 7\) for all \(n \geq 10\), then \(\left(a_n\right)\) has a convergent subsequence.<br><br>Solution:<br><ul><li>Let \(M\) = \(\max\) ( \(\left|a_1\right|,\left|a_2\right|, \ldots,\left|a_9\right|, 8\) )<br></li><li>It follows that&nbsp;\(a_n\) is bounded and that, by Bolzano-Weierstrass, there exists a convergent subsequence</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1719271143290 (Block 921) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Problem 3. Let \(a \in \mathbb{R}\) and define the set \(S=\{x \in \mathbb{Q}: x&lt;a\} \subset \mathbb{R}\).<br>a. (5 pts) Prove that \(S\) has no lower bound.<br><br>Solution:<br><ul><li>Suppose S is bounded bellow by q &lt; a for all a</li><li>By archimedean property: there exists M such that -q &lt; M thus -M &lt; q &lt; a&nbsp;</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1719271288398 (Block 922) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Problem 3. Let \(a \in \mathbb{R}\) and define the set \(S=\{x \in \mathbb{Q}: x&lt;a\} \subset \mathbb{R}\).<br><ul><li>b. (10 pts) Prove that \(\sup S=a\).<br></li></ul><div>Solution:</div><div><ul><li>By definition, a is an upper bound</li><li>If a lower upper bound existed, s &lt; a, then by density of rationals in R there would exist s &lt; q &lt;a with q in S, meaning s was not an upper bound</li></ul></div>
+
+============================================================
----------------------------

=== Note ID: 1719271522415 (Block 923) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       c. (10 pts) Find a sequence \(\left(a_n\right)\) such that \(a_n \in S\) where S contains all rational for all \(n \in \mathbb{N}\) and \(\left(a_n\right)\) converges to \(a\).<br><br>Solution:<br><ul><li>Notice that \(a-\frac{1}{n}\) &lt; \(a\) for all \(n \in \mathbb{N}\).<br></li><li>By densitory of rationals:</li><ul><li>\(a-\frac{1}{n}\)&nbsp; &lt; \(q_n\) &lt; \(a\).<br></li></ul><li>Since&nbsp;\(\lim _{n \rightarrow \infty}\) \(a-\frac{1}{n}\) = \(a\) = \(\lim _{n \rightarrow \infty}\) \(a\). by the squeeze theorem&nbsp;\(q_n\) must also converge to &nbsp;\(a\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1719332205081 (Block 924) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Previous guidelines recommended picking the largest vocabulary such that over 95% of tokens appear more than 100 times. However, the optimal baseline does not have this property as only ~80% of tokens appear more than 100 times
+
+============================================================
----------------------------

=== Note ID: 1719352105793 (Block 925) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 3.1.1 (Random variable). Given an experiment with sample space \(S\), a random variable (r.v.) is a function from the sample space \(S\) to the real numbers \(\mathbb{R}\). It is common, but not required, to denote random variables by capital letters.
+
+============================================================
----------------------------

=== Note ID: 1719352509617 (Block 926) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       As we've mentioned earlier, the source of the randomness in a random variable is the experiment itself, in which a sample outcome \(s \in S\) is chosen according to a probability function \(P\). Before we perform the experiment, the outcome \(s\) has not yet been realized, so we don't know the value of \(X\), though we could calculate the probability that \(X\) will take on a given value or range of values. After we perform the experiment and the outcome \(s\) has been realized, the random variable crystallizes into the numerical value \(X(s)\).
+
+============================================================
----------------------------

=== Note ID: 1719475687388 (Block 927) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 3.2.1 (Discrete random variable). A random variable \(X\) is said to be discrete if there is a finite list of values \(a_{1}, a_{2}, \ldots, a_{n}\) or an infinite list of values \(a_{1}, a_{2}, \ldots\) such that \(P\left(X=a_{j}\right.\) for some \(\left.j\right)\) = \(1\). <br><br>If \(X\) is a discrete r.v., then the&nbsp; finite or countably infinite set of values \(x\) such that \(P(X=x)\) &gt;\(0\) is called the support of \(X\).
+
+============================================================
----------------------------

=== Note ID: 1719476271531 (Block 928) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       3.2.3. In writing \(P(X=x)\), we are using \(X=x\) to denote an event, consisting of all outcomes \(s\) to which \(X\) assigns the number \(x\). This event is also written as \(\{X=x\}\); formally, \(\{X=x\}\) is defined as \(\{s \in S: X(s)=x\}\), but writing \(\{X=x\}\) is shorter and more intuitive.
+
+============================================================
----------------------------

=== Note ID: 1719476708184 (Block 929) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="FH90eCriWUT2kprWenSQSUhyHsojoq8VZMDsPK2jY1E.original.fullsize.png"><img src="FH90eCriWUT2kprWenSQSUhyHsojoq8VZMDsPK2jY1E.original.fullsize.png"><img src="FH90eCriWUT2kprWenSQSUhyHsojoq8VZMDsPK2jY1E.original.fullsize.png"><br><br>FIGURE 3.3<br><br>Left to right: PMFs of \(X, Y\), and \(I\), with \(X\) the number of Heads in two fair coin tosses, \(Y\) the number of Tails, and \(I\) the indicator of Heads on the first toss.
+
+============================================================
----------------------------

=== Note ID: 1719481475568 (Block 930) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 3.2.7 (Valid PMFs). Let \(X\) be a discrete r.v. with support \(x_{1}, x_{2}, \ldots\) (assume these values are distinct and, for notational simplicity, that the support is countably infinite; the analogous results hold if the support is finite). The PMF \(p_{X}\) of \(X\) must satisfy the following two criteria:<br><br><ul><li>- Nonnegative: \(p_{X}(x)&gt;0\) if \(x=x_{j}\) for some \(j\), and \(p_{X}(x)=0\) otherwise;</li><li>- Sums to 1: \(\sum_{j=1}^{\infty} p_{X}\left(x_{j}\right)=1\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1719482669530 (Block 931) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 3.3.1 (Bernoulli distribution). An r.v. \(X\) is said to have the Bernoulli distribution with parameter \(p\) if P\((X=1)\) = \(p\) and P\((X=0)\) = \(1-p\), where \(0\) &lt; \(p\) &lt; \(1\). We write this as \(X \sim\)&nbsp; \(\operatorname{Bern}(p)\). The symbol \(\sim\) is read "is distributed as".
+
+============================================================
----------------------------

=== Note ID: 1719482722444 (Block 932) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Any r.v. whose possible values are 0 and 1 has a \(\operatorname{Bern}(p)\) distribution, with \(p\) the probability of the r.v. equaling 1 . This number \(p\) in \(\operatorname{Bern}(p)\) is called the parameter of the distribution; it determines which specific Bernoulli distribution we have.
+
+============================================================
----------------------------

=== Note ID: 1719487769397 (Block 933) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 3.3.2 (Indicator random variable). The indicator random variable of an event \(A\) is the r.v. which equals 1 if \(A\) occurs and 0 otherwise. We will denote the indicator r.v. of \(A\) by \(I_{A}\) or \(I(A)\). Note that \(I_{A}\) \(\sim \) \(\operatorname{Bern}(p)\) with \(p\)=\(P(A)\).
+
+============================================================
----------------------------

=== Note ID: 1719487871529 (Block 934) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Story 3.3.3 (Bernoulli trial). An experiment that can result in either a "success" or a "failure" (but not both) is called a Bernoulli trial. A Bernoulli random variable can be thought of as the indicator of success in a Bernoulli trial: it equals 1 if success occurs and 0 if failure occurs in the trial.
+
+============================================================
----------------------------

=== Note ID: 1719488129019 (Block 935) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Story 3.3.4 (Binomial distribution). Suppose that \(n\) independent Bernoulli trials are performed, each with the same success probability \(p\). Let \(X\) be the number of successes. The distribution of \(X\) is called the Binomial distribution with parameters \(n\) and \(p\). We write \(X\) \(\sim\) \(\operatorname{Bin}(n, p)\) to mean that \(X\) has the Binomial distribution with parameters \(n\) and \(p\), where \(n\) is a positive integer and \(0\)&lt;\(p\)&lt;\(1\).
+
+============================================================
----------------------------

=== Note ID: 1719488358863 (Block 936) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       3.3.6. To save writing, it is often left implicit that a PMF is zero wherever it is not specified to be nonzero, but in any case it is important to understand what the support of a random variable is, and good practice to check that PMFs are valid. If two discrete r.v.s have the same PMF, then they also must have the same support. So we sometimes refer to the support of a discrete distribution; this is the support of any r.v. with that distribution.
+
+============================================================
----------------------------

=== Note ID: 1719489492663 (Block 937) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       We've used Story 3.3.4 to find the \(\operatorname{Bin}(n, p)\) PMF. The story also gives us a straightforward proof of the fact that if \(X\) is Binomial, then \(n-X\) is also Binomial.
+
+============================================================
----------------------------

=== Note ID: 1719489861449 (Block 938) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 3.3.7. Let \(X \sim \operatorname{Bin}(n, p)\), and \(q=1-p\) (we often use \(q\) to denote the failure probability of a Bernoulli trial). Then \(n-X \sim \operatorname{Bin}(n, q)\).<br><br>Proof. Using the story of the Binomial, interpret \(X\) as the number of successes in \(n\) independent Bernoulli trials. Then \(n-X\) is the number of failures in those trials. Interchanging the roles of success and failure, we have \(n-X \sim\) \(\operatorname{Bin}(n, q)\). Alternatively, we can check that \(n-X\) has the \(\operatorname{Bin}(n, q)\) PMF. Let \(Y=n-X\). The PMF of \(Y\) is<br><br><ul><li>\(P(Y=k)\)&nbsp;<br></li><li>= \(P(X=n-k)\)</li><li>=&nbsp;\(\binom{n}{n-k}\)&nbsp;\(p^{n-k}\)&nbsp;\(q^k\)</li><li>=&nbsp;\(\binom{n}{k}\)&nbsp;\(q^k\)&nbsp;\(p^{n-k}\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1719669734121 (Block 939) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Which ordered fileds have the least upper bound property?<br><ul><li>R and fields isomporphic to R</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1719679351974 (Block 940) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       \(A\) \(\subset\) \(B\)&nbsp;\(\implies\)&nbsp;\(\sup\) \(A \) \(&lt;\) \(\sup\) \(B\)
+
+============================================================
----------------------------

=== Note ID: 1719679578911 (Block 941) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       To show that&nbsp;\(\sup\) \(A \) &nbsp;\(\leq\)&nbsp;\(\sup\) \(B\), you need to show that&nbsp;\(\forall\)::quant::quant::quant \(a \in A\)&nbsp; \(\exists\)::quant::quant::quant \(b \in B, s.t\)&nbsp;\(a\)&nbsp;\(\leq\)&nbsp;\(b\)
+
+============================================================
----------------------------

=== Note ID: 1719687156322 (Block 942) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div>How can we extend addition to the extended reals?</div><div><ul><li>\(x\)&nbsp;\(+\)&nbsp;\(\infty\) =&nbsp;\(\infty\)<br></li><li>\(x\)&nbsp;\(+\)&nbsp;\(-\infty\) =&nbsp;\(-\infty\)</li></ul></div>
+
+============================================================
----------------------------

=== Note ID: 1719687225682 (Block 943) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div>How can we extend multiplication to the extended reals?</div><div><ul><li>\(\forall x &gt; 0\)::quant+condition::quant+condition::quant+condition::quant+condition\(x\)&nbsp;\(\times\)&nbsp;\(\infty\) =&nbsp;\(\infty\)<br></li><li>\(\forall x &lt; 0\)::quant+condition::quant+condition::quant+condition::quant+condition \(x\)&nbsp;\(\times\)&nbsp;\(\infty\) =&nbsp;\(\infty\)</li></ul></div>
+
+============================================================
----------------------------

=== Note ID: 1719687316283 (Block 944) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       What are the undefined operations of the extended reals?<br><ul><li>0 times infinity</li><li>infinity minus infinity&nbsp;</li><li>e.t.c</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1719871829023 (Block 945) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If we have an urn filled with \(w\) white and \(b\) black balls, then drawing \(n\) balls out of the urn with replacement yields a \(\operatorname{Bin}(n, w /(w+b))\) distribution. If we instead sample without replacement, as illustrated in Figure 3.7, then the number of white balls follows a Hypergeometric distribution.
+
+============================================================
----------------------------

=== Note ID: 1719872135660 (Block 946) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-51727ddd92009de6b344b6e8ad4a4c89c511a354.jpg"><br>\section*{FIGURE 3.7}<br>Hypergeometric story. An urn contains \(w=6\) white balls and \(b=4\) black balls. We sample \(n=5\) without replacement. The number \(X\) of white balls in the sample is Hypergeometric; here we observe \(X\) = \(3\).
+
+============================================================
----------------------------

=== Note ID: 1719872787940 (Block 947) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 3.4.2 (Hypergeometric PMF). If \(X \sim\) \( \operatorname{HGeom}(w, b, n)\), then the PMF of \(X\) is<br><br><ul><li>\(P(X=k)\) =&nbsp;\(\binom{w}{k}\)&nbsp;\(\binom{b}{n-k}\)&nbsp;\(/\)&nbsp;\(\binom{w+b}{n}\)<br></li></ul><br>for integers \(k\) satisfying \(0\) \(\leq\) k \(\leq\) \(w\) and \(0\) \(\leq\) n-k \(\leq\) \(b\), and \(P(X=k)\) = \(0\) otherwise.<br>
+
+============================================================
----------------------------

=== Note ID: 1719873099165 (Block 948) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The Hypergeometric distribution comes up in many scenarios which, on the surface, have little in common with white and black balls in an urn. The essential structure of the Hypergeometric story is that items in a population are classified using two sets of tags: in the urn story, each ball is either white or black (this is the first set of tags), and each ball is either sampled or not sampled (this is the second set of tags). Furthermore, at least one of these sets of tags is assigned completely at random (in the urn story, the balls are sampled randomly, with all sets of the correct size equally likely). Then \(X \sim \operatorname{HGeom}(w, b, n)\) represents the number of twice-tagged items: in the urn story, balls that are both white and sampled.
+
+============================================================
----------------------------

=== Note ID: 1719904659048 (Block 949) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 3.4.5. The \(\operatorname{HGeom}(w, b, n)\) and \(\operatorname{HGeom}(n, w+b-n, w)\) distributions are identical. That is, if \(X \sim\) \(\operatorname{HGeom}(w, b, n)\) and \(Y \sim\) \(\operatorname{HGeom}(n, w+b-n, w)\), then \(X\) and \(Y\) have the same distribution.
+
+============================================================
----------------------------

=== Note ID: 1719904802313 (Block 950) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 3.4.5. The \(\operatorname{HGeom}(w, b, n)\) and \(\operatorname{HGeom}(n, w+b-n, w)\) distributions are identical. That is, if \(X \sim \operatorname{HGeom}(w, b, n)\) and \(Y \sim \operatorname{HGeom}(n, w+b-n, w)\), then \(X\) and \(Y\) have the same distribution.<br><br>Proof. Using the story of the Hypergeometric, imagine an urn with \(w\) white balls, \(b\) black balls, and a sample of size \(n\) made without replacement. Let \(X \sim\) \(\operatorname{HGeom}(w, b, n)\) be the number of white balls in the sample, thinking of white/black as the first set of tags and sampled/not sampled as the second set of tags. Let \(Y \sim \operatorname{HGeom}(n, w+b-n, w)\) be the number of sampled balls among the white balls, thinking of sampled/not sampled as the first set of tags and white/black as&nbsp;<br>the second set of tags. Both \(X\) and \(Y\) count the number of white sampled balls, so they have the same distribution.
+
+============================================================
----------------------------

=== Note ID: 1719904891984 (Block 951) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       3.4.6 (Binomial vs. Hypergeometric). The Binomial and Hypergeometric distributions are often confused. Both are discrete distributions taking on integer values between 0 and \(n\) for some \(n\), and both can be interpreted as the number of successes in \(n\) Bernoulli trials (for the Hypergeometric, each tagged elk in the recaptured sample can be considered a success and each untagged elk a failure). However, a crucial part of the Binomial story is that the Bernoulli trials involved are independent. The Bernoulli trials in the Hypergeometric story are dependent, since the sampling is done without replacement: knowing that one elk in our sample is tagged decreases the probability that the second elk will also be tagged.
+
+============================================================
----------------------------

=== Note ID: 1720892622031 (Block 952) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Every infinite subset of a countable set is countable
+
+============================================================
----------------------------

=== Note ID: 1720892948837 (Block 953) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Proof that every infinite subset E of a countable set&nbsp; A is countable:<br><ul><li>A can be written as a sequence&nbsp;\(\{x_1,x_2, \cdots,&nbsp; \}\)</li><li>Let&nbsp;\(n_1\) =&nbsp;\(\inf\) \(\{ i: x_i \in E\}\), which exists due to the well-ordering principle applied over the indicies</li><li>Let&nbsp;\(n_2\) = \(\inf\) \(\{ i: x_i \in E, i&gt;n_1\} \)</li><li>Let&nbsp;&nbsp;\(n_k\) = \(\inf\) \(\{ i: x_i \in E, i&gt;n_{k-1}\} \)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1720893343279 (Block 954) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If A is countable then&nbsp;\(A\)&nbsp;\(\times\)&nbsp;\(A\) is countable
+
+============================================================
----------------------------

=== Note ID: 1720894970515 (Block 955) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       A natural question to ask is whether the theorems we have proved about sequences, series, and functions in \(\mathbf{R}\) have analogues in the plane \(\mathbf{R}^{2}\) or in even higher dimensions. Looking back over the proofs, one crucial observation is that most of the arguments depend on just a few basic properties of the absolute value function. Interpreting the statement " \(|x-y|\) " to mean the "distance from \(x\) to \(y\) in \(\mathbf{R}\)," our aim is to experiment with other ways of measuring distance on other sets such as \(\mathbf{R}^{2}\) and \(C[0,1]\), the space of continuous functions on \([0,1]\).
+
+============================================================
----------------------------

=== Note ID: 1720895112796 (Block 956) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 8.2.1. Given a set \(X\), a function \(d: X \times X \rightarrow \mathbf{R}\) is a metric on \(X\) if for all \(x, y \in X\) :<br><br><ul><li>(i) \(d(x, y)\)&nbsp; \(\geq\) \(0\) with \(d(x, y)\) = \(0\) if and only if \(x\) = \(y\),</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1720895163043 (Block 957) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 8.2.1. Given a set \(X\), a function \(d: X \times X \rightarrow \mathbf{R}\) is a metric on \(X\) if for all \(x, y \in X\) :<br><br><ul><li>(i) \(d(x, y)\)&nbsp; \(\geq\) \(0\) with \(d(x, y)\) = \(0\) if and only if \(x\) = \(y\),</li><li>(ii) \(d(x, y)\) = \(d(y, x)\), and</li><li>(iii) for all \(z \in X\), \(d(x, y)\) \(\leq\) \(d(x, z)\)+\(d(z, y)\).</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1720895443934 (Block 958) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 8.2.2. Let \((X, d)\) be a metric space. A sequence \(\left(x_{n}\right) \subseteq X\) converges to an element \(x \in X\) if for all \(\epsilon\) \(&gt;0\) there exists an \(N \in \mathbf{N}\) such that \(d\left(x_{n}, x\right)\) \(&lt;\) \(\epsilon\) whenever \(n\) \(\geq N\).
+
+============================================================
----------------------------

=== Note ID: 1720895593420 (Block 959) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 8.2.3. A sequence \(\left(x_{n}\right)\) in a metric space \((X, d)\) is a Cauchy sequence if for all \(\epsilon\) \(&gt;0\) there exists an \(N \in \mathbf{N}\) such that \(d\) \(\left(x_{m}, x_{n}\right)\) \(&lt;\) \(\epsilon\) whenever \(m, n\) \(\geq N\).
+
+============================================================
----------------------------

=== Note ID: 1720896938838 (Block 960) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The Cauchy Criterion, as it is called in \(\mathbf{R}\), was an "if and only if" statement. In the general metric space setting, however, the converse statement does not always hold. Recall that, in \(\mathbf{R}\), the assertion that "Cauchy sequences converge" was shown to be equivalent to the Axiom of Completeness. In order to transport the Axiom of Completeness into a metric space, we would need to have an ordering on our space so that we could discuss such things as upper bounds. It is an interesting observation that not every set can be ordered in a satisfying way (the points in \(\mathbf{R}^{2}\) for example). Even without an ordering, we are still going to want completeness. For metric spaces, the convergence of Cauchy sequences is taken to be the definition of completeness.
+
+============================================================
----------------------------

=== Note ID: 1720897334381 (Block 961) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 8.2.5. Let \(\left(X, d_{1}\right)\) and \(\left(Y, d_{2}\right)\) be metric spaces. A function \(f\) : \(X \rightarrow Y\) is continuous at \(x \in X\) if for all \(\epsilon\) \(&gt;0\) there exists a \(\delta\) \(&gt;0\) such that \(d_{2}(f(x), f(y))\) \(&lt;\) \(\epsilon\) whenever \(d_{1}(x, y)\) \(&lt;\) \(\delta\).
+
+============================================================
----------------------------

=== Note ID: 1720897439006 (Block 962) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 8.2.6. Given \(\epsilon\) \(&gt;0\) and an element \(x\) in the metric space \((X, d)\), the \(\epsilon\)-neighborhood of \(x\) is the set \(V_{\epsilon}(x)\) = \(\{y \in X: d(x, y)&lt;\epsilon\}\).
+
+============================================================
----------------------------

=== Note ID: 1720897532706 (Block 963) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       With the definition of an \(\epsilon\)-neighborhood, we can now define open sets, limit points, and closed sets exactly as we did before. A set \(O \subseteq X\) is open if for every \(x \in O\) we can find a neighborhood \(V_{\epsilon}(x)\) \(\subseteq O\). A point \(x\) is a limit point of a set \(A\) if every \(V_{\epsilon}(x)\) intersects \(A\) in some point other than \(x\). A set \(C\) is closed if it contains its limit points.
+
+============================================================
----------------------------

=== Note ID: 1720897603031 (Block 964) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 8.2.7. A subset \(K\) of a metric space \((X, d)\) is compact if every sequence in \(K\) has a convergent subsequence that converges to a limit in \(K\).
+
+============================================================
----------------------------

=== Note ID: 1720897903222 (Block 965) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 8.2.9. A set \(A \subseteq X\) is dense in the metric space \((X, d)\) if \(\bar{A}\) = \(X\). A subset \(E\) of a metric space \((X, d)\) is nowhere-dense in \(X\) if \(\bar{E}^{\circ}\) is empty.
+
+============================================================
----------------------------

=== Note ID: 1720899087484 (Block 966) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 8.2.8. Given a subset \(E\) of a metric space \((X, d)\):<br><ul><li>The closure \(\bar{E}\) is the union of \(E\) together with its limit points.&nbsp;</li><li>The interior of \(E\) is denoted by \(E^{\circ}\) and is defined as</li><ul><li>\(E^{\circ}\) = \(\left\{x \in E: \text { there exists } V_{\epsilon}(x) \subseteq E\right\}\)</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1720901474524 (Block 967) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 8.2.10. Let \((X, d)\) be a complete metric space, and let \(\left\{O_{n}\right\}\) be a countable collection of dense, open subsets of \(X\). Then, \(\bigcap_{n=1}^{\infty} O_{n}\) is not empty.<br><br>Proof. When we proved this theorem on \(\mathbf{R}\), completeness manifested itself in the form of the Nested Interval Property. We could derive something akin to NIP in the metric space setting, but instead let's take an approach that uses the convergence of Cauchy sequences (because this is how we have defined completeness).<br><br>Pick \(x_{1} \in O_{1}\). Because \(O_{1}\) is open, there exists an \(\epsilon_{1}&gt;0\) such that \(V_{\epsilon_{1}}\left(x_{1}\right) \subseteq O_{1}\).<br><br>Exercise 8.2.14. <br><br><ul><li>(a) Give the details for why we know there exists a point \(x_{2} \in\) \(V_{\epsilon_{1} }\left(x_{1}\right)\) \(\cap\) \(O_{2}\) and an \(\epsilon_{2}&gt;0\) satisfying \(\epsilon_{2}&lt;\epsilon_{1} / 2\) with \(V_{\epsilon_{2} }\left(x_{2}\right)\) contained in \(\mathrm{O}_{2}\) and</li><ul><li>\(\overline{V_{\epsilon_{2} }\left(x_{2}\right)}\) \(\subseteq\) \(V_{\epsilon_{1} }\left(x_{1}\right)\)</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1720901640560 (Block 968) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       This result is called the Baire Category Theorem because it creates two categories of size for subsets in a metric space. A set of "first category" is one that can be written as a countable union of nowhere-dense sets. These are the small, intuitively thin subsets of a metric space. We now see that if our metric space is complete, then it is necessarily of "second category," meaning it cannot be written as a countable union of nowhere-dense sets.
+
+============================================================
----------------------------

=== Note ID: 1720901668536 (Block 969) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Given a subset \(A\) of a complete metric space \(X\), showing that \(A\) is of first category is a mathematically precise way of demonstrating that \(A\) constitutes a very minor portion of the set \(X\). The term "meager" is often used to mean a set of first category.
+
+============================================================
----------------------------

=== Note ID: 1720904100955 (Block 970) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 8.2.12. The set<br><br>\[<br>D=\left\{f \in C[0,1]: f^{\prime}(x) \text { exists for some } x \in[0,1]\right\}<br>\]<br><br>is a set of first category in \(C[0,1]\).<br><br>Proof. For each pair of natural numbers \(m, n\), define<br><br>\[<br>\begin{aligned}<br>&amp; A_{m, n}=\{f \in C[0,1]: \text { there exists } x \in[0,1] \text { where } \\<br>&amp; \left.\qquad\left|\frac{f(x)-f(t)}{x-t}\right| \leq n \text { whenever } 0&lt;|x-t|&lt;\frac{1}{m}\right\} .<br>\end{aligned}<br>\]<br><br>This definition takes some time to digest. Think of \(1 / m\) as defining a \(\delta\) neighborhood around the point \(x\), and view \(n\) as an upper bound on the magnitude of the slopes of lines through the two points \((x, f(x))\) and \((t, f(t))\). The set \(A_{m, n}\) contains any function in \(C[0,1]\) for which it is possible to find at least one point \(x\) where the slopes through \((x, f(x))\) and points on the function nearby — within \(1 / m\) to be precise - are bounded by \(n\).<br><br>Exercise 8.2.16. Show that if \(f \in C[0,1]\) is differentiable at a point \(x \in[0,1]\), then \(f \in A_{m, n}\) for some pair \(m, n \in \mathbf{N}\).<br><br>The collection of subsets \(\left\{A_{m, n}: m, n \in \mathbf{N}\right\}\) is countable, and we have just seen that the union of these sets contains our set \(D\). Because it is not difficult to see that a subset of a set of first category is first category, the final hurdle in the argument is to prove that each \(A_{m, n}\) is nowhere-dense in \(C[0,1]\).
+
+============================================================
----------------------------

=== Note ID: 1720910278124 (Block 971) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The countable union of countable sets is countable
+
+============================================================
----------------------------

=== Note ID: 1720944236465 (Block 972) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The finite union of finite sets is finite
+
+============================================================
----------------------------

=== Note ID: 1720953770873 (Block 973) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The cardinality of the reals R is often called the cardinality of the continuum C
+
+============================================================
----------------------------

=== Note ID: 1720956663643 (Block 974) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       A point&nbsp;\(p \in X\), with X a metric space, is a limit point of a subset&nbsp;\(E\), if every neigbourhoud of p contains a point \(q\) \(\neq p\) such that&nbsp;\(q\) \(\in E\)
+
+============================================================
----------------------------

=== Note ID: 1720973783470 (Block 975) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Which points are limit points?<br><img src="paste-4f295a4aa9d76d0f2289e551c2b12ede6654b8b0.jpg"><br><ul><li>All points on the solid contour</li><li>All points on the dotted contour since a point does not have to be in the subset to be a limit point</li><li>All points on the inside</li><li>The point in the middle of the circle</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1720979465104 (Block 976) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div>Taking out a finite number of points from an open set keeps it open</div>
+
+============================================================
----------------------------

=== Note ID: 1720980020762 (Block 977) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>A set that is both open and closed&nbsp; is called clopen</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1721167256126 (Block 978) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If&nbsp;\(E\)&nbsp;\(\subset\) \(F\), with F closed, then&nbsp;\(\bar{E}\)&nbsp;\(\subset\)&nbsp;\(F\)
+
+============================================================
----------------------------

=== Note ID: 1721167346975 (Block 979) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If a set E is contained in a closed set, its closure must also be contained in the closed set. THus the closure is the smallest closed set containing E
+
+============================================================
----------------------------

=== Note ID: 1721199822714 (Block 980) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The closure of a set&nbsp;\(\bar{E}\) is the smallest closed set containing a subset E of a closed set F:<br>Proof<br><ul><li>Since F is a closed set, if p is a limit point of E then it is a limit point of F</li><li>Since F contains all its limit points, it contains all its limit points including the limit points of E</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1721200813022 (Block 981) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Proof that a set is closed iff its complement is open:<br>Proof:<br><ul><li>Assume E is open</li><li>Which means any point is an interior point</li><li>\(\forall\)&nbsp;\(x\)&nbsp;\(\in\)&nbsp;\(E\),&nbsp;\(\exists\)&nbsp;\(N(x)\) s.t&nbsp;\(N(x)\)&nbsp;\(\subset\)&nbsp;\(E\)<br></li><li>\(N(x)\)&nbsp;\(\subset\)&nbsp;\(E\)&nbsp;\(\implies \) \(N(x)\) is disjoint from \(E^c\)&nbsp;<br></li><li>Pictographically:</li><ul><li><img src="paste-ed4668c47721b35d7232d909a780fffed0004256.jpg"><br></li></ul><li>Thus&nbsp;\(\forall\)&nbsp;\(x\)&nbsp;\(\in\)&nbsp;\(E\),&nbsp;\(x\) is not a limit point of&nbsp;\(E^c\)</li><li>Which means&nbsp;\(E^c\) contains all of its limit points</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1721200897938 (Block 982) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If a set is clopen its complement is also clopen, e.g,&nbsp;\(\emptyset\) and&nbsp;\( \emptyset^c&nbsp; = R\)
+
+============================================================
----------------------------

=== Note ID: 1721201087099 (Block 983) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If a set is neither closed nor open then its complement must be neither closed nor open
+
+============================================================
----------------------------

=== Note ID: 1721201282274 (Block 984) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Every point of the union of two open sets was interior to one of the sets, meaning that they will be interior to the union
+
+============================================================
----------------------------

=== Note ID: 1721225633165 (Block 985) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The union of a finite number of closed sets is closed
+
+============================================================
----------------------------

=== Note ID: 1721227531413 (Block 986) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>The arbitrary union of open sets is open&nbsp;</li><ul><li>The arbitray intersection is not: \(\cap^\infty(-1/n,1/n)\) =&nbsp;\([0]\)</li></ul><li>The arbitrary intersection of closed sets is closed</li><ul><li>The arbitrary union is not:&nbsp; &nbsp;\(\cup^\infty(0,n)\) =&nbsp;\([0, \infty )\)</li></ul><li>The finite intersection of open sets is open:&nbsp;</li><li>The finite union of closed sets is closed</li></ul><br>
+
+============================================================
----------------------------

=== Note ID: 1721227794972 (Block 987) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The arbitrary union of open sets is open:<br><ul><li>\(x \in \cup_{\alpha} O_\alpha\)<br></li><li>This means&nbsp;\(x\)&nbsp;\(\in\) some&nbsp;\(O_{\alpha}\) which is open<br></li><li>Thus&nbsp;\(\exists\) a neighbourhood \(N(x)\) s.t&nbsp;\(N(x)\)&nbsp;\(\subset\)&nbsp;\(O_{\alpha}\)</li><li>However, \(O_{\alpha}\)&nbsp;\(\in\)&nbsp;\(\) \(\cup_{\alpha} O\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1721380404724 (Block 988) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The word "cover" always refers to an open cover
+
+============================================================
----------------------------

=== Note ID: 1721382336587 (Block 989) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       What is the open cover of&nbsp;\([1/2, 1 )\)?<br><ul><li>\(\{\)&nbsp;\(W_{x}\) \(\}_{x \in [1/2, 1]}\) such that&nbsp;\(W_x\) =&nbsp;\((x - \frac{1}{10}\),&nbsp; \(x+ \frac{1}{10})\)<br></li><li>Pictographically: <img src="paste-469125cd5f89a49e0a3abc6f4773dfec42999f10.jpg"></li><li>There are in-fact uncountably many elements in the open cover as there are uncountably many balls of a finite radius on the interval</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1721382634282 (Block 990) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Subcovers may be the original covers themselves
+
+============================================================
----------------------------

=== Note ID: 1721395410736 (Block 991) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition: A set&nbsp;\(K\) in a metric space&nbsp;\(X,d\) is bounded if&nbsp;\(\exists\) a&nbsp; point \(p\) and radius&nbsp;\(r\) such that&nbsp;\(K\)&nbsp;\(\subset\)&nbsp;\(N_r(p) \),&nbsp; where \(N_r(p) \) is the r-radius ball around p<br>Pictographically:&nbsp; <img src="paste-083edbdd71173d007966dbefa31a246e3060de01.jpg">
+
+============================================================
----------------------------

=== Note ID: 1721400835774 (Block 992) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       An open set in R is no longer necessarily open once you go to higher dimensions<br><br>Pictographically: <img src="paste-4e88b9a510f63335ce3379ba8becf0460f1249de.jpg">
+
+============================================================
----------------------------

=== Note ID: 1721400954308 (Block 993) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Being an open set depends on the metric space in which you are embedded (think of an open set of R being translated to R^2), whilst being a compact set does not
+
+============================================================
----------------------------

=== Note ID: 1721401985716 (Block 994) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem:&nbsp; If \(E\)&nbsp;\(\subset\)&nbsp;\(Y\)\( \subset\)&nbsp;\(X\), then&nbsp;\(E\) is open in&nbsp;\(Y\) iff&nbsp;\(E\) =&nbsp;\(Y\)&nbsp;\(\cap\)&nbsp;\(G \) for some&nbsp;\(G\) open in X
+
+============================================================
----------------------------

=== Note ID: 1721404254798 (Block 995) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Story 3.5.1 (Discrete Uniform distribution). Let \(C\) be a finite, nonempty set of numbers. Choose one of these numbers uniformly at random (i.e., all values in \(C\) are equally likely). Call the chosen number \(X\). Then \(X\) is said to have the Discrete Uniform distribution with parameter \(C\); we denote this by \(X \sim\) \(\operatorname{DUnif}(C)\).
+
+============================================================
----------------------------

=== Note ID: 1721406583574 (Block 996) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Another function that describes the distribution of an r.v. is the cumulative distribution function (CDF). Unlike the PMF, which only discrete r.v.s possess, the CDF is defined for all r.v.s.
+
+============================================================
----------------------------

=== Note ID: 1721406744674 (Block 997) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 3.6.1. The cumulative distribution function (CDF) of an r.v. \(X\) is the function \(F_{X}\) given by \(F_{X}\) \((x)\) = \(P\) \((X \leq x)\). When there is no risk of ambiguity, we sometimes drop the subscript and just write \(F\) (or some other letter) for a CDF.
+
+============================================================
----------------------------

=== Note ID: 1721406892743 (Block 998) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-c4ab8dec94ca180185c41f13f9e7d808a7d467c4.jpg"><br>FIGURE 3.8<br><br>\(\operatorname{Bin}(4,1 / 2)\) PMF and CDF. The height of the vertical bar \(P(X=2)\) in the PMF is also the height of the jump in the CDF at 2.
+
+============================================================
----------------------------

=== Note ID: 1721407320646 (Block 999) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <br>Theorem 3.6.3 (Valid CDFs). Any CDF \(F\) has the following properties.<br><br><ul><li>- Increasing: If \(x_{1}\) \(\leq\) \(x_{2}\), then \(F\left(x_{1}\right)\) \(\leq\) \(F\left(x_{2}\right)\).</li></ul><br>
+
+============================================================
----------------------------

=== Note ID: 1721407462089 (Block 1000) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-17a41104e00bfbadfaa5044fd41281a66ca4b1af.jpg"><br>Theorem 3.6.3 (Valid CDFs). Any CDF \(F\) has the following properties.<br><br><ul><li>Right-continuous: As in Figure 3.8, the CDF is continuous except possibly for having some jumps. Wherever there is a jump, the CDF is continuous from the right. That is, for any \(a\), we have</li><li>\(F(a)\) = \(\lim _{x \rightarrow a^{+} } \) \(F(x)\)<br></li></ul><br>
+
+============================================================
----------------------------

=== Note ID: 1721407548626 (Block 1001) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-17a41104e00bfbadfaa5044fd41281a66ca4b1af.jpg"><br>Theorem 3.6.3 (Valid CDFs). Any CDF \(F\) has the following properties.<br><br><ul><li>Convergence to 0 and 1 in the limits:</li><li>\(<br>\lim _{x \rightarrow-\infty}\) \(F(x)\) = \(0\)</li><li>\(\lim _{x \rightarrow \infty}\) \(F(x)\)= \(1<br>\)<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1721408006301 (Block 1002) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div>Theorem 3.6.3 (Valid CDFs). Any CDF \(F\) has the following properties.<br><br><ul><li>- Increasing:</li><li>- Right-continuous:&nbsp;</li><li>- Convergence to 0 and 1 in the limits:</li></ul></div><div>Proof for discrete RVs with values 0,1,...</div><div><ul><li>For the second criterion, note that:<br></li><ul><li>\(<br>P\) \((X\) \(\leq x\) ) = \(P\) \((X\) \(\leq\) \(\lfloor x\rfloor\) )<br></li><li>where \(\lfloor x\rfloor\) is the greatest integer less than or equal to \(x\). For example, \(P(X \leq 4.9)=\) \(P(X \leq 4)\) since \(X\) is integer-valued. So \(F(a+b)\) = \(F(a)\) for any \(b&gt;0\) that is small enough so that \(a+b\) &lt; \(\lfloor a\rfloor+1\), e.g., for \(a=4.9\), this holds for \(0&lt;b&lt;0.1\). This implies \(F(a)\) = \(\lim _{x \rightarrow a^{+}} F(x)\) (in fact, it's much stronger since it says \(F(x)\) equals \(F(a)\) when \(x\) is close enough to \(a\) and on the right).</li></ul></ul></div>
+
+============================================================
----------------------------

=== Note ID: 1721408153579 (Block 1003) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div>Theorem 3.6.3 (Valid CDFs). Any CDF \(F\) has the following properties.<br><br><ul><li>- Increasing:</li><li>- Right-continuous:&nbsp;</li><li>- Convergence to 0 and 1 in the limits:</li></ul></div><div>Proof for discrete RVs with values 0,1,...</div><div><ul><li>For the third criterion, we have \(F(x)=0\) for \(x&lt;0\), and<br></li><li>\(<br>\lim _{x \rightarrow \infty} F(x)\)&nbsp;</li><li>= \(\lim _{x \rightarrow \infty}\) \(P\) ( \(X \) \(\leq\) \(\lfloor x\rfloor\) )&nbsp;</li><li>= \(\lim _{x \rightarrow \infty}\) \( \sum_{n=0}^{\lfloor x\rfloor}\) \(P(X=n)\)</li><li>= \(\sum_{n=0}^{\infty}\) \(P(X=n)\)</li><li>=\(1\)<br></li></ul></div>
+
+============================================================
----------------------------

=== Note ID: 1721421937225 (Block 1004) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Picking K times from a set of n objects when order does not matter, with replacement: \(\binom{n+k-1}{k}\)<br><ul><li>K indistinguishable objects in n distinguishable boxes</li><li>Pictoral Proof:</li><li><img src="paste-cf88e4a31f506953882613f2a6c7f5ea36f2ce5e.jpg"><br></li><li>E.g n=4, k = 6</li><li>We can represetns this as dots and bars</li><li>****||**|*</li><li>In this configuration there must be K dots and must be n-1 separators</li><li>The total number of positions is thus n-1+k, then the configuration is fully determined by the positions of the k dots as the remaining positions are those of the separators</li><li>Alternatively you can choose where the separators are and then the dots are determined</li><li>Pictographically:<img src="paste-50ff37b741728f3b8842c6eaa2cc22cb4a5156de.jpg"></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1721483548651 (Block 1005) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>A probability space is a sample space S together with&nbsp;</li><li>A probability measure&nbsp;\(P:\)&nbsp;\(2^S\)&nbsp;\(\to\)&nbsp;\((0,1)\)</li><li>P must obey the probability axioms :</li><ul><li>\(P\) (\(\emptyset\)) = 0 and \(P\) (\(S\)) = 0</li><li>\(P\) ( \(\cup_{i=1}^\infty\) \(A_i\)&nbsp; ) =&nbsp;\(\sum_{i=1}^\infty\)&nbsp;\(P\) (\(A_i\)) if&nbsp;\(A_i\) are disjoint</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1721483737109 (Block 1006) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div>To recap, we have now seen three equivalent ways of expressing the distribution of a random variable. Two of these are the PMF and the CDF: we know these two functions contain the same information, since we can always figure out the CDF from the PMF and vice versa. Generally the PMF is easier to work with for discrete r.v.s, since evaluating the CDF requires a summation.<br></div>
+
+============================================================
----------------------------

=== Note ID: 1721483843382 (Block 1007) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div>In this section we will discuss what it means to take a function of a random variable, and we will build understanding for why a function of a random variable is a random variable. That is, if \(X\) is a random variable, then \(X^{2}, e^{X}\), and \(\sin (X)\) are also random variables, as is \(g(X)\) for any function \(g: \mathbb{R} \rightarrow \mathbb{R}\).<br></div>
+
+============================================================
----------------------------

=== Note ID: 1721484094149 (Block 1008) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 3.7.1 (Function of an r.v.). For an experiment with sample space \(S\), an r.v. \(X\), and a function \(g:\) \(\mathbb{R}\) \(\rightarrow\) \(\mathbb{R}\)&nbsp; \(g(X)\) is the r.v. that maps \(s\) to \(g(X(s))\) for all \(s \in S\).
+
+============================================================
----------------------------

=== Note ID: 1721484159250 (Block 1009) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Functions of an r.v are obtained via simple functional composition with the r.v
+
+============================================================
----------------------------

=== Note ID: 1721484262138 (Block 1010) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <img src="paste-75a9a707ccc9a3df7603646b4abdc87e110152f3.jpg"><br>FIGURE 3.9<br><br>The r.v. \(X\) is defined on a sample space with 6 elements, and has possible values 0 , 1 , and 4 . The function \(g\) is the square root function. Composing \(X\) and \(g\) gives the random variable \(g(X)\) = \(\sqrt{X}\), which has possible values 0,1 , and 2 .
+
+============================================================
----------------------------

=== Note ID: 1721484954947 (Block 1011) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Given a discrete r.v. \(X\) with a known PMF, how can we find the PMF of \(Y=g(X)\) ? In the case where \(g\) is a one-to-one function, the answer is straightforward: the support of \(Y\) is the set of all \(g(x)\) with \(x\) in the support of \(X\), and<br><br><ul><li>P ( \(Y\)&nbsp;= \(g(x) \) )&nbsp;</li><li>= P\( (g(X)=g(x)) \)&nbsp;</li><li>= P\((X=x)\)<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1721485073150 (Block 1012) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div>The case where \(Y=g(X)\) with \(g\) one-to-one is illustrated in the following tables; the idea is that if the distinct possible values of \(X\) are \(x_{1}, x_{2}, \ldots\) with probabilities \(p_{1}, p_{2}, \ldots\) (respectively), then the distinct possible values of \(Y\) are \(g\left(x_{1}\right), g\left(x_{2}\right), \ldots\), with the list \(p_{1}, p_{2}, \ldots\) of probabilities.<br></div><div><ul><li><img src="paste-6164b1862aa069256a4938f9007db245ddee2ec9.jpg"><br></li><ul><li>PMF of \(X\), in table form<br></li></ul><li><img src="paste-f3c99cabe13b7b13cde996bcb4082b7a530413a4.jpg"><br></li><ul><li>PMF of \(Y\), in table form<br></li></ul></ul></div>
+
+============================================================
----------------------------

=== Note ID: 1721486386073 (Block 1013) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 3.7.3 (PMF of \(g(X)\) ). Let \(X\) be a discrete r.v. and \(g:\) \(\mathbb{R}\) \(\rightarrow\) \(\mathbb{R}\). Then the support of \(g(X)\) is the set of all \(y\) such that \(g(x)=y\) for at least one \(x\) in the support of \(X\), and the PMF of \(g(X)\) is<br><br><ul><li>\(P\) \((g(X)=y)\) = \(\sum_{x: g(x)=y}\)&nbsp; \(P\) \((X=x)\)</li></ul><br><br>for all \(y\) in the support of \(g(X)\).<br>
+
+============================================================
----------------------------

=== Note ID: 1721490846042 (Block 1014) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       3.7.7 (Category errors and sympathetic magic). Many common mistakes in probability can be traced to confusing two of the following fundamental objects with each other: distributions, random variables, events, and numbers. Such mistakes are examples of category errors. In general, a category error is a mistake that doesn't just happen to be wrong, but in fact is necessarily wrong since it is based on the wrong category of object. For example, answering the question "How many people live in Boston?" with " -42 " or " \(\pi\) " or "pink elephants" would be a category error-we may not know the population size of a city, but we do know that it is a nonnegative integer at any point in time. To help avoid being categorically wrong, always think about what category an answer should have.<br><br>An especially common category error is to confuse a random variable with its distribution. We call this error sympathetic magic; this term comes from anthropology, where it is used for the belief that one can influence an object by manipulating a representation of that object.&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1721490911650 (Block 1015) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       We can think of the distribution of a random variable as a map or blueprint describing the r.v. Just as different houses can share the same blueprint, different r.v.s can have the same distribution, even if the experiments they summarize, and the sample spaces they map from, are not the same.
+
+============================================================
----------------------------

=== Note ID: 1721494290406 (Block 1016) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       It does not make sense to multiply a PMF by 2 to get the PMF of 2X , since the probabilities would no longer sum to 1 . As we saw above, if \(X\) takes on values \(x_{j}\) with probabilities \(p_{j}\), then \(2 X\) takes on values \(2 x_{j}\) with probabilities \(p_{j}\).&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1721494472438 (Block 1017) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Example of sympathetic magic:<br><ul><li>Claiming that because \(X\) and \(Y\) have the same distribution, \(X\) must always equal \(Y\), i.e., \(P(X=Y)\) = \(1\). Just because two r.v.s have the same distribution does not mean they are always equal, or ever equal. We saw this in Example 3.2.5. As another example, consider flipping a fair coin once. Let \(X\) be the indicator of Heads and \(Y=1-X\) be the indicator of Tails. Both \(X\) and \(Y\) have the \(\operatorname{Bern}(1 / 2)\) distribution, but the event \(X=Y\) is impossible. The PMFs of \(X\) and \(Y\) are the same function, but \(X\) and \(Y\) are different mappings from the sample space to the real numbers.<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1721574571405 (Block 1018) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 3.8.2 (Independence of many r.v.s). Random variables \(X_{1}, \ldots, X_{n}\) are independent if<br><br><ul><li>\(P\)\(\left(X_{1} \leq x_{1}, \ldots, X_{n} \leq x_{n}\right)\) = \(P\left(X_{1} \leq x_{1}\right)\) \(\ldots\) \(P\left(X_{n} \leq x_{n}\right)\)</li></ul><br>for all \(x_{1}, \ldots, x_{n} \in \mathbb{R}\). For infinitely many r.v.s, we say that they are independent if every finite subset of the r.v.s is independent.<br>
+
+============================================================
----------------------------

=== Note ID: 1721575127735 (Block 1019) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>3.8.3. If \(X_{1}, \ldots, X_{n}\) are independent, then they are pairwise independent,&nbsp;</li><li>The idea behind proving that \(X_{i}\) and \(X_{j}\) are independent is to let all the \(x_{k}\) other than \(x_{i}, x_{j}\) go to \(\infty\) in the definition of independence, since we already know \(X_{k}&lt;\infty\) is true (though it takes some work to give a complete justification for the limit). But pairwise independence does not imply independence in general, as we saw in Chapter 2 for events.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1721575345836 (Block 1020) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div>If \(X\) and \(Y\) are independent then it is also true, e.g., that \(X^{2}\) is independent of \(Y^{4}\), since if \(X^{2}\) provided information about \(Y^{4}\), then \(X\) would give information about \(Y\) (using \(X^{2}\) and \(Y^{4}\) as intermediaries: \(X\) determines \(X^{2}\), which would give information about \(Y^{4}\), which in turn would give information about \(Y\) ).&nbsp;<br></div>
+
+============================================================
----------------------------

=== Note ID: 1721575399297 (Block 1021) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 3.8.5 (Functions of independent r.v.s). If \(X\) and \(Y\) are independent r.v.s, then any function of \(X\) is independent of any function of \(Y\).
+
+============================================================
----------------------------

=== Note ID: 1721575443663 (Block 1022) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 3.8.6 (i.i.d.). We will often work with random variables that are independent and have the same distribution. We call such r.v.s independent and identically distributed, or i.i.d. for short.
+
+============================================================
----------------------------

=== Note ID: 1721575523156 (Block 1023) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       3.8.7 (i. vs. i.d.). "Independent" and "identically distributed" are two oftenconfused but completely different concepts. Random variables are independent if they provide no information about each other; they are identically distributed if they have the same PMF (or equivalently, the same CDF).&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1721576607143 (Block 1024) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 3.8.9. If \(X \sim \operatorname{Bin}(n, p), Y \sim \operatorname{Bin}(m, p)\), and \(X\) is independent of \(Y\), then \(X+Y \sim \operatorname{Bin}(n+m, p)\).<br><br>1. LOTP: We can directly find the PMF of \(X+Y\) by conditioning on \(X\) (or \(Y\), whichever we prefer) and using the law of total probability:<br><br><ul><li>\(P(X+Y=k) \)&nbsp;</li><li>= \(\sum_{j=0}^{k}\) \(P\) \((X+Y=k \mid X=j)\) \( P\) \((X=j) \)</li><li>= \(\sum_{j=0}^{k\)}\)&nbsp; \(P\) \((Y=k-j)\) \(P\) \((X=j) \)</li><li>= \(\sum_{j=0}^{k}\) \(\left(\begin{array}{c}m \\k-j\end{array}\right)\) \(p^{k-j} q^{mk+j}\) \(\left(\begin{array}{c}n \\j\end{array}\right) \) \(p^{j} q^{n-j} \)</li><li>= \(p^{k} q^{n+m-k}\) \( \sum_{j=0}^{k}\) \(\left(\begin{array}{c}m \\k-j\end{array}\right)\left(\begin{array}{c}n \\j\end{array}\right) \)</li><li>= \(\left(\begin{array}{c}n+m \\k\end{array}\right)\)&nbsp; \(p^{k} q^{n+m-k} \)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1721576860934 (Block 1025) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 3.8.9. If \(X \sim \operatorname{Bin}(n, p), Y \sim \operatorname{Bin}(m, p)\), and \(X\) is independent of \(Y\), then \(X+Y \sim \operatorname{Bin}(n+m, p)\).<br><br>2. Representation: A much simpler proof is to represent both \(X\) and \(Y\) as the sum of i.i.d. \(\operatorname{Bern}(p)\) r.v.s: \(X=X_{1}+\cdots+X_{n}\) and \(Y=Y_{1}+\cdots+Y_{m}\), where the \(X_{i}\) and \(Y_{j}\) are all i.i.d. \(\operatorname{Bern}(p)\). Then \(X+Y\) is the sum of \(n+m\) i.i.d. \(\operatorname{Bern}(p)\) r.v.s, so its distribution, by the previous theorem, is \(\operatorname{Bin}(n+m, p)\).
+
+============================================================
----------------------------

=== Note ID: 1721576923869 (Block 1026) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 3.8.9. If \(X \sim \operatorname{Bin}(n, p), Y \sim \operatorname{Bin}(m, p)\), and \(X\) is independent of \(Y\), then \(X+Y \sim \operatorname{Bin}(n+m, p)\).<br><br>3. Story: By the Binomial story, \(X\) is the number of successes in \(n\) independent trials and \(Y\) is the number of successes in \(m\) additional independent trials, all with the same success probability, so \(X+Y\) is the total number of successes in the \(n+m\) trials, which is the story of the \(\operatorname{Bin}(n+m, p)\) distribution.
+
+============================================================
----------------------------

=== Note ID: 1721607518975 (Block 1027) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Example 3.8.13 (Two friends). Consider again the "I have only two friends who ever call me" scenario from Example 2.5.11, except now with r.v. notation. Let \(X\) be the indicator of Alice calling me next Friday, \(Y\) be the indicator of Bob calling me next Friday, and \(Z\) be the indicator of exactly one of them calling me next Friday. Then \(X\) and \(Y\) are independent (by assumption). But given \(Z\) = \(1\), we have that \(X\) and \(Y\) are completely dependent: given that \(Z\) = \(1\), we have \(Y\) = \(1-X\).
+
+============================================================
----------------------------

=== Note ID: 1721607629570 (Block 1028) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Example 3.8.14 (Mystery opponent). Suppose that you are going to play two games of tennis against one of two identical twins. Against one of the twins, you are evenly matched, and against the other you have a 3/4 chance of winning. Suppose that you can't tell which twin you are playing against until after the two games. Let \(Z\) be the indicator of playing against the twin with whom you're evenly matched, and let \(X\) and \(Y\) be the indicators of victory in the first and second games, respectively.<br><br>Conditional on \(Z=1, X\) and \(Y\) are i.i.d. Bern(1/2), and conditional on \(Z=0\), \(X\) and \(Y\) are i.i.d. Bern(3/4). So \(X\) and \(Y\) are conditionally independent&nbsp; given \(Z\). Unconditionally, \(X\) and \(Y\) are dependent because observing \(X=1\) makes it more likely that we are playing the twin who is worse. That is,<br><br>\[<br>P(Y=1 \mid X=1)&gt;P(Y=1)<br>\]<br><br>Past games give us information which helps us infer who our opponent is, which in turn helps us predict future games! Note that this example is isomorphic to the "random coin" scenario from Example 2.3.7.
+
+============================================================
----------------------------

=== Note ID: 1721607677391 (Block 1029) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The Binomial and Hypergeometric distributions are connected in two important ways. As we will see in this section, we can get from the Binomial to the Hypergeometric by conditioning, and we can get from the Hypergeometric to the Binomial by taking a limit.
+
+============================================================
----------------------------

=== Note ID: 1721607912323 (Block 1030) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>Example 3.9.1 (Fisher exact test). A scientist wishes to study whether women or men are more likely to have a certain disease, or whether they are equally likely.&nbsp;</li><li>A random sample of \(n\) women and \(m\) men is gathered, and each person is tested for the disease (assume for this problem that the test is completely accurate).&nbsp;</li><li>The numbers of women and men in the sample who have the disease are \(X\) and \(Y\) respectively, with \(X \sim \operatorname{Bin}\left(n, p_{1}\right)\) and \(Y \sim \operatorname{Bin}\left(m, p_{2}\right)\), independently. Here \(p_{1}\) and \(p_{2}\) are unknown, and we are interested in testing whether \(p_{1}=p_{2}\) (this is known as a null hypothesis in statistics).</li><li>Consider a \(2 \times 2\) table with rows corresponding to disease status and columns corresponding to gender. Each entry is the count of how many people have that disease status and gender, so \(n+m\) is the sum of all 4 entries. Suppose that it is observed that \(X+Y=r\).<br></li><li>The Fisher exact test is based on conditioning on both the row and column sums, so \(n, m, r\) are all treated as fixed, and then seeing if the observed value of \(X\) is "extreme" compared to this conditional distribution.&nbsp;<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1721613536513 (Block 1031) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 3.9.3. If \(X \sim \operatorname{HGeom}(w, b, n)\) and \(N=w+b \rightarrow \infty\) such that \(p=\) \(w /(w+b)\) remains fixed, then the PMF of \(X\) converges to the \(\operatorname{Bin}(n, p) \mathrm{PMF}\).<br><br>Proof. We take the stated limit of the \(\operatorname{HGeom}(w, b, n)\) PMF:<br><br><ul><li>\(P(X=k)\)&nbsp;</li><li>=\( \frac{\left(\begin{array}{l}w \\k\end{array}\right)\left(\begin{array}{c}b \\n-k\end{array}\right)}{\left(\begin{array}{c}w+b \\n\end{array}\right)} \)</li><li>= \(\left(\begin{array}{l}n \\k\end{array}\right)\frac{\left(\begin{array}{c}w+b-n \\w-k\end{array}\right)}{\left(\begin{array}{c}w+b \\w\end{array}\right)} \quad \text { by Theorem 3.4.5 } \)&nbsp;</li><li>= \(\left(\begin{array}{l}n \\k\end{array}\right) \frac{w !}{(w-k) !} \frac{b !}{(b-n+k) !} \frac{(w+b-n) !}{(w+b) !} \)</li><li>= \(\left(\begin{array}{l}n \\k\end{array}\right) \frac{w(w-1) \ldots(w-k+1) b(b-1) \ldots(b-n+k+1)}{(w+b)(w+b-1) \ldots(w+b-n+1)} \)</li><li>= \(\left(\begin{array}{l}n \\k\end{array}\right) \frac{p\left(p-\frac{1}{N}\right) \ldots\left(p-\frac{k-1}{N}\right) q\left(q-\frac{1}{N}\right) \ldots\left(q-\frac{n-k-1}{N}\right)}{\left(1-\frac{1}{N}\right)\left(1-\frac{2}{N}\right) \ldots\left(1-\frac{n-1}{N}\right)} \)</li></ul><div>As \(N \rightarrow \infty\), the denominator goes to 1 , and the numerator goes to \(p^{k}\) \(q^{n-k}\). Thus<br></div><div><ul><li>\(P\) \((X=k)\) \(\rightarrow\) \(\left(\begin{array}{l}n \\k\end{array}\right)\)&nbsp;\(p^{k}\) \(q^{n-k}\)<br></li></ul></div>
+
+============================================================
----------------------------

=== Note ID: 1721613851724 (Block 1032) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The distribution of a discrete r.v. can be defined using a PMF, a CDF, or a story. The PMF of \(X\) is the function \(P(X=x)\) for \(x \in \mathbb{R}\). The CDF of \(X\) is the function \(P(X \leq x)\) for \(x \in \mathbb{R}\).
+
+============================================================
----------------------------

=== Note ID: 1721613906394 (Block 1033) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       For a PMF to be valid, it must be nonnegative and sum to 1 . For a CDF to be valid, it must be increasing, right-continuous, converge to 0 as \(x \rightarrow\) \(-\infty\), and converge to 1 as \(x \rightarrow\) \(\infty\).
+
+============================================================
----------------------------

=== Note ID: 1721675007517 (Block 1034) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       For a PMF to be valid, it must be nonnegative and sum to 1 . For a CDF to be valid, it must be increasing, right-continuous, converge to 0 as \(x \rightarrow-\infty\), and converge to 1 as \(x \rightarrow \infty\).
+
+============================================================
----------------------------

=== Note ID: 1721675053148 (Block 1035) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       It is important to distinguish between a random variable and its distribution: the distribution is a blueprint for building the r.v., but different r.v.s can have the same distribution, just as different houses can be built from the same blueprint.
+
+============================================================
----------------------------

=== Note ID: 1721675258420 (Block 1036) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       A function of a random variable is still a random variable. If we know the PMF of \(X\), we can find \(P\) \((g(X)=k)\), the PMF of \(g(X)\), by translating the event \(\{g(X)\) = \(k\}\) into an equivalent event involving \(X\), then using the PMF of \(X\).
+
+============================================================
----------------------------

=== Note ID: 1721688583895 (Block 1037) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <br>\(P(A \cup B) \)&nbsp;=&nbsp;\(P(A)\)&nbsp;+&nbsp;\(P(B) - P(A \cap B)\)<br>Proof:<br><ul><li>\(P(A \cup B)\) = \(P(A \cup (B \cap A^C))\) =&nbsp;\(P(A)\) +&nbsp;\(P(B \cap A^c)\)<br></li><li>Which means</li><li>\(P(A \cap B)\) +&nbsp;\(P(B \cap A^c)\) =&nbsp;\(P(B)\)<br></li><li>Which is true because the two are disjoint by complementation&nbsp;</li><li>Their union is B by LOTP</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1721689816524 (Block 1038) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The matchin problem?<br>Shuffle the cards, flip cards and you count what is the likelihood you match<br>Solution<br><ul><li>\(A_j\) = jth card matches<br></li><li>\(P( \cup A_j)\) =&nbsp;\(\sum P(A_j)\) - \(\sum P(A_j \cap A_i)\) + ....<br></li><li>\(P(A_j)\) =&nbsp;\(P(A_i)\) =&nbsp;\(\frac{1}{n}\) since all positions are equally likely for the card labelled j<br></li><li>&nbsp;\(\sum P(A_j \cap A_i)\)&nbsp;=&nbsp;&nbsp;\(\frac{(n-2)!}{n!}\) since all other cards can be in any position</li><li>\(P( \cup A_j)\) =&nbsp;\(\binom{n}{1}\) \(\frac{1}{n}\)&nbsp;-&nbsp; \(\binom{n}{2}\)&nbsp;&nbsp;&nbsp;\(\frac{(n-2)!}{n!}\)<br></li><li>e.t.c</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1722090276759 (Block 1039) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Monty Hall problem, except that Monty enjoys opening Door 2 enjoys opening Door 3 , and if he has a choice between opening rs, he opens Door 2 with probability \(p\), where \(\frac{1}{2} \leq p \leq 1\).<br>re are three doors, behind one of which there is a car (which you ehind the other two of which there are goats (which you don't lly, all possibilities are equally likely for where the car is. You ;, which for concreteness we assume is Door 1. Monty Hall then to reveal a goat, and offers you the option of switching. Assume Iall knows which door has the car, will always open a goat door and offer the option of switching, and as above assume that if Monty Hall has a choice between opening Door 2 and Door 3, he chooses Door 2 with probability \(p\left(\right.\) with \(\left.\frac{1}{2} \leq p \leq 1\right)\)<br><br><ul><li>(a) Find the unconditional probability that the strategy of always switching succeeds (unconditional in the sense that we do not condition on which of Doors 2,3 Monty opens).<br></li><ul><li>Let \(C_j\) be the event that the car is hidden behind door \(j\) and let \(W\) be the event that we win using the switching strategy. Using the law of total probability, we can find the unconditional probability of winning in the same way as in class:</li><li>\(P(W)&nbsp;\) = \( P\left(W \mid C_1\right) P\left(C_1\right)\) + \(P\left(W \mid C_2\right) P\left(C_2\right)\) + \(P\left(W \mid C_3\right) P\left(C_3\right) \)<br></li><li>= \( 0 \cdot 1 / 3+1 \cdot 1 / 3+1 \cdot 1 / 3\)&nbsp; because we cannot win by switching if the car is behind door one</li><li>=&nbsp;\(2/3\)</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1722092310960 (Block 1040) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       (a) In the World Series of baseball, two teams (call them \(A\) and \(B\) ) play a sequence of games against each other, and the first team to win four games wins the series. Let \(p\) be the probability that \(A\) wins an individual game, and assume that the games are independent. What is the probability that team \(A\) wins the series?<br><br><ul><li>\(P(\text { A wins })\) =\(P \text { (A winning in } 4 \text { games })\) +&nbsp;\(P \text { (A winning in } 5 \text { games) }\) +&nbsp;\(P \text { (A wins in } 6 \text { games })\) +&nbsp;\(P \text { (A wins in } 6 \text { games })\)<br></li><ul><li>=&nbsp;\(p^4\)+\(\binom{4}{3}\) \(p^4\) \(q\)+\(\binom{5}{3}\) \(p^4\) \(q^2\)+\(\binom{6}{3}\) \(p^4\) \(q^3\)</li></ul><li>Because, for example:</li><li>\(P(\mathrm{~A} \text { wins in } 5)\) =&nbsp;\(P \text { (A wins } 3 \text { out of first } 4 \text { ) }\) * \(P(\text { A wins } 5 \text { th game } \mid \mathrm{A} \text { wins } 3 \text { out of first } 4)\)<br></li><li>=\(\binom{4}{3}\)\(p^3 q p\)</li><li>(This value can also be found from the PMF of a distribution known as the<br>Negative Binomial, which we will see later in the course.)<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1722093693433 (Block 1041) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       2. A sequence of \(n\) independent experiments is performed. Each experiment is a success with probability \(p\) and a failure with probability \(q=1-p\). Show that conditional on the number of successes, all possibilities for the list of outcomes of the experiment are equally likely (of course, we only consider lists of outcomes where the number of successes is consistent with the information being conditioned on).<br><br><ul><li>Let \(X_j\) be 1 if the \(j\) th experiment is a success and 0 otherwise, and let \(X=\) \(X_1+\cdots+X_n\) be the total number of successes. Then for any \(k\) and any&nbsp;\(a_1, \ldots, a_n \in\{0,1\} \text { with } a_1+\cdots+a_n=k\)<br></li><li>\(P\left(X_1=a_1, \ldots, X_n=a_n \mid X=k\right)\) =&nbsp;<br></li><ul><li>= \(P\left(X_1=a_1, \ldots, X_n=a_n, X=k\right)\)&nbsp;\(/\)&nbsp;\(P(X=k)\)<br></li><li>=&nbsp;\(P\left(X_1=a_1, \ldots, X_n=a_n\right)\)&nbsp;\(/\) \(P(X=k)\)</li><li>=&nbsp;\(p^k q^{n-k}\)&nbsp;\(/\)&nbsp;\(\binom{n}{k} p^k q^{n-k}\)&nbsp;</li><li>= \(\frac{1}{\binom{n}{k} }\)</li></ul><li>This does not depend on \(a_1, \ldots, a_n\). Thus, for \(n\) independent Bernoulli trials, given that there are exactly \(k\) successes, the \(\binom{n}{k}\) possible sequences consisting of \(k\) successes and \(n-k\) failures are equally likely. Interestingly, the conditional probability above also does not depend on \(p\) (this leads to the notion of a sufficient statistic, which is studied in Stat 111).<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1722093766179 (Block 1042) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       &nbsp;Let \(X \sim \operatorname{Bin}(n, p)\) and \(Y \sim \operatorname{Bin}(m, p)\), independent of \(X\).<br><ul><li>(a) Show that \(X+Y \sim \operatorname{Bin}(n+m, p)\), using a story proof.<br></li><ul><li>Interpret \(X\) as the number of successes in \(n\) independent Bernoulli trials and \(Y\) as the number of successes in \(m\) more independent Bernoulli trials, where each trial has probability \(p\) of success. Then \(X+Y\) is the number of successes in the \(n+m\) trials, so \(X+Y \sim \) \(\operatorname{Bin}(n+m, p)\).<br></li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1722093805278 (Block 1043) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       &nbsp;Let \(X \sim \operatorname{Bin}(n, p)\) and \(Y \sim \operatorname{Bin}(m, p)\), independent of \(X\).<br><ul><li>\(\text { (b) Show that } X-Y \text { is not Binomial. }\)<br></li><ul><li>A Binomial can't be negative, but X - Y is negative with positive probability.<br></li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1722094222975 (Block 1044) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       &nbsp;Let \(X \sim \operatorname{Bin}(n, p)\) and \(Y \sim \operatorname{Bin}(m, p)\), independent of \(X\).<br><ul><li>(c) Find \(P(X=k \mid X+Y=j)\). How does this relate to the elk problem from HW 1?<br></li><ul><li>\(P(X=k \mid X+Y=j)&nbsp;\) =&nbsp;<br></li><li>=\( P(X=k, X+Y=j) \) \(/\)&nbsp;\(P(X+Y=j)\)</li><li>=&nbsp;\(P(X=k) P(Y=j-k)\) \(/\)&nbsp;\(P(X+Y=j)\)</li><li>since X=k is independent of Y= j-k this becomes</li><li>=\(&nbsp;\binom{n}{k}\) \(p^k(1-p)^{n-k}\) \(\binom{m}{j-k}\) \(p^{j-k}(1-p)^{m-(j-k)}\)&nbsp;\(/\) \(&nbsp;\binom{m+n}{j}\) \(p^j(1-p)^{m+n-j}\)</li><li>=\(\binom{n}{k}\)\(\binom{m}{j-k}\)&nbsp;\(/\)&nbsp;\(\binom{n+m}{j} .\)</li></ul><li>Note that the p disappeared! This is exactly the same distribution as in the<br>elk problem (it is called the Hypergeometric distribution). To see why, imagine that there are n male elk and m female elk, each of which is tagged with the word "success" with probability p (independently). Suppose we then want to know how many of the male elk are tagged, given that a total of j elk have been tagged. For this, p is no longer relevant, and we can "capture" the male elk and count how many are tagged, analogously to the original elk problem.<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1722094930357 (Block 1045) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem: For a subset&nbsp;\(E\) \(\subset\)\(Y\)&nbsp;&nbsp;\(\subset\)&nbsp;\(X\), then&nbsp;\(E\) is open in \(Y\) iff&nbsp;\(E\) =&nbsp;\(Y\)&nbsp;\(\cap \)&nbsp;\(G\) for some&nbsp;\(G\) open&nbsp;in&nbsp;\(X\)
+
+============================================================
----------------------------

=== Note ID: 1722095142971 (Block 1046) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem: If \(Y\)&nbsp;\(\subset\)&nbsp;\(X\), then&nbsp;\(K\) compact in&nbsp;\(Y\) is equvalent to&nbsp;\(K\) being compact in X
+
+============================================================
----------------------------

=== Note ID: 1722096782970 (Block 1047) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem: If \(Y\)&nbsp;\(\subset\)&nbsp;\(X\), then&nbsp;\(K\) compact in&nbsp;\(Y\) is equvalent to&nbsp;\(K\) being compact in X<br><br><b>Forward</b> Proof that \(K\) compact in&nbsp;\(Y\) is equvalent to&nbsp;\(K\) being compact in X<br><ul><li>Assume K is compact in Y</li><li>Consider an open cover of K in X, \(O_{\alpha}\), we need to find a finite subcover</li><li>Since K is also&nbsp; subset of Y, we can consider \(O_{\alpha} \cap Y\)</li><li>Since every set \(O_{\alpha} \cap Y\) is in&nbsp;\(Y\), then the open cover admits a finite sub-cover \(O_{i} \cap Y\)<br></li><li>Then the \(O_{i}\) original sets cover&nbsp;\(K\) in X</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1722097136665 (Block 1048) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem: If \(Y\)&nbsp;\(\subset\)&nbsp;\(X\), then&nbsp;\(K\) compact in&nbsp;\(Y\) is equvalent to&nbsp;\(K\) being compact in X<br><br><b>Backward</b> Proof that \(K\) compact in&nbsp;\(Y\) is equvalent to&nbsp;\(K\) being compact in X<br><ul><li>Assume K is compact in X</li><li>Assume an open cover in Y exists&nbsp;\(O_{\alpha}\)</li><li>Every element&nbsp;&nbsp;\(O_{\alpha}\) =&nbsp;\(Y\)&nbsp;\(\cap\)&nbsp;\(G_{\alpha}\) for some open \(G_{\alpha}\) in X</li><li>Then \(G_{\alpha}\) is an open cover in X and thus admits a finite subcover \(G_{i}\)</li><li>Then \(O_{i}\) is a finite subcover for K in Y</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1722097253903 (Block 1049) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Compactness is an intrinsic property of a set, since compactness in a subset always implies compactness in the enclosing set and vice-versa (as long as the same metric is used in both)
+
+============================================================
----------------------------

=== Note ID: 1722121042185 (Block 1050) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       (Observe that the last equation is indeed a kind of commutative law: it says that complementation followed by inversion is the same as inversion followed by complementation.)
+
+============================================================
----------------------------

=== Note ID: 1722172310370 (Block 1051) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Proof that compact sets are closed:<br><ul><li>Pictographically: <img src="paste-864c826b4377fd119e0a7488b59bc23b51c3a0fb.jpg"><br></li><li>Let K be compact and consider&nbsp;\(p\)&nbsp;\(\not \in\)&nbsp;\(K\)</li><li>We want to show that&nbsp;\(p\) has a neighbourhood that does not intersect K which is the same as showing it is not a limit point of K i.e it is interior to&nbsp;\(K^c\)</li><li>Or it is the same as showing that the complement of K is open</li><li>\(\forall\)&nbsp;\(q\)&nbsp;\(in\)&nbsp;\(K\), let&nbsp;\(V_q\) =&nbsp;\(N_{r/2} (q)\) be a ball around q and \(U_p\) =&nbsp;\(N_{r/2} (p)\)</li><ul><li>&nbsp;Where&nbsp;\(r\) =&nbsp;\(d\)&nbsp;\( (p,q)\)</li></ul><li>Notice that the collection \({V_q}\) is an open cover of K</li><li>By compactness of K there exists a finite subcover&nbsp;\(V_{q_{n} }\)</li><li>Which allows us to pick the equivalent finite set from the complement of K</li><li>Define&nbsp;\(W\) =&nbsp;\(\cap_n\) \(U_{q_{n} }\) which is open because it is a finite intersection of open sets</li><li>Specifically it is a ball whose radius is the minimum distance between&nbsp;\(p_i\) and&nbsp;\(q_i\)</li><li>Pictographically: <img src="paste-40a5b413ea121ae49567ef8eefa6c56396c2bf5e.jpg"></li><li>\(W\) \(\cap\) \(V_{q_i}\) =&nbsp;\(\emptyset\) because&nbsp;\(W\)&nbsp;\(\in\)&nbsp;\(U_{q_i}\) and&nbsp; \(U_{q_i}\) &nbsp;\(\cap\)\(V_{q_i}\) =&nbsp;\(\emptyset\)</li><li>Thus W is the desired neighbourhood showing that any arbitrary point p is interrior&nbsp;to&nbsp;\(K^c\) meaning that&nbsp; \(K^c\) is open, making&nbsp;\(K\) closed</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1722174512947 (Block 1052) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       A closed set subset B of a compact set K is also compact<br>Proof:<br><ul><li>Pictographically: <img src="paste-79cebeddeb9dc2c92d2259a1776cd7cc45f6318f.jpg"></li><li>Consider an open cover of B,&nbsp;\( {O_{\alpha} }\)</li><li>Pictographically:&nbsp; <img src="paste-93cca5e41f2d3dbccb9633e19f1422c9d6253674.jpg"></li><li>Since B is embedded in K,&nbsp;&nbsp;&nbsp;\( {O_{\alpha} }\) \(\in\) \(K\)</li><li>Since B is closed,&nbsp;\(B^c\) is open&nbsp;</li><li>Pictographically: <img src="paste-eea0fe6c837389cef626738870dd46c4af1264bf.jpg"></li><li>This means that&nbsp;\(O_{\alpha}\)&nbsp;\(\cup\)&nbsp;\(B^c\) is an open cover of K</li><li>By compactness, there must be a finite subcover {\(O_{\alpha_{i} }\) \(B^c\)}</li><li>Since&nbsp;\(B\) \(\cap\) \(B^c\) = \(\emptyset\), the finite subcover&nbsp; \(O_{\alpha_{i} }\) covers B&nbsp;&nbsp;</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1722174705007 (Block 1053) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If you have a closed set F and a compact set K in a metric space X, then their intersection is going to be compact<br>Pictographically:&nbsp;<img src="paste-49a5ebdc62c0cf09e18d7c2c5fecd8865482da91.jpg">
+
+============================================================
----------------------------

=== Note ID: 1722175027156 (Block 1054) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem:The intersection of nested closed intervals in R is not empty, which means the intersection closed nested \(K\)-cells in&nbsp; \(R^k\)  is not empty<br>Pictographically:&nbsp; <img src="paste-790e5ba446bf632b069fa2f8363543fc1d621e51.jpg">
+
+============================================================
----------------------------

=== Note ID: 1722175442148 (Block 1055) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Proof for the nested interval property:<br><ul><li>Pictographically:&nbsp; <img src="paste-7d1661b37ebc51d59c54e68c0e319a11b2c3fdc0.jpg"></li><li>\(I_n\) =&nbsp;\([a_n, b_n]\)<br></li><li>\(I_{n+1}\)&nbsp;\(\subset\)&nbsp;\(I_n\)<br></li><li>Let x =&nbsp;\(\sup\)&nbsp;\({a_i}\) which exists because all intervals are bounded by&nbsp;\(b_1\)</li><li>\(x\)&nbsp;\(\geq\)&nbsp;\(a_i\) because it is the supremum<br></li><li>\(x\)&nbsp;\(\ &lt; \)&nbsp;\(b_n\) for all&nbsp;\(n\) because&nbsp;\(b_n\)&nbsp;\(\geq\)&nbsp;\(a_i\) for all i<br></li><li>Thus&nbsp;\(x\)&nbsp;\(\in\)&nbsp;\(I_n\) for all n and thus&nbsp;\(x\)&nbsp;\(\in\)&nbsp;\(\cap I_n\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1722176082869 (Block 1056) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Proof that R is uncountable:<br><ul><li>Pictographically: <img src="paste-475b17f8a3bfbb503aaa63e15f3725581f074712.jpg"></li><li>Suppose it were countable and we have a list of all elements&nbsp;\(\{x_i \}\)</li><li>We can choose&nbsp;\(I_1\) such that&nbsp;\(x_i\)&nbsp;\(\not \in\)&nbsp;\(I_1\)</li><li>Then choose&nbsp;\(I_n\)&nbsp;\(\subset\)&nbsp;\(I_{n-1}\) such that&nbsp;\(x_n\)&nbsp;\(\not \in\)&nbsp;\(I_n\)</li><li>By the nested interval property there exists an element in&nbsp;\(\cap_n\)&nbsp;\(I_n\) which is not on the original list</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1722182014869 (Block 1057) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Proof that any closed interval [a,b] in R is compact:<br><ul><li>Assume the interval is not compact</li><li>Then there exists an open cover \(O_{\alpha}\) without a finite subcover&nbsp;</li><li>Pictographically: <img src="paste-f454a7e35dc3ea3bcc0fdc1e9066cf9636efb0ae.jpg"></li><li>Divide the interval into&nbsp;\([a, c_1]\) and&nbsp;&nbsp;\([c_1, b]\)</li><li>Thus&nbsp;\(O_{\alpha}\) covers&nbsp;&nbsp;&nbsp;\([a, c_1]\)&nbsp; and \([c_1, b]\)</li><li>At least one of these intervals must also not have a finite subcover otherwise you coult just union two finite subcovers<br></li><li>Pick the one without a finite subcover&nbsp;\(I_1\) (WLOG \([a,c_1]\)</li><li>Then repeat the process splitting&nbsp;\(I_1\) into two halves by&nbsp;\(c_2\) to get&nbsp;\(I_2\)</li><li>Generally&nbsp;\(I_{n+1}\)&nbsp;\(\subset\)&nbsp;\(I_n\) created by difviding&nbsp;\(I_{n}\) by&nbsp;\(c_n\) such that we choose the </li><li>half without a finite subcover</li><li></li><li>By the nested interval property there exists \(x\) such that \(\cap_n I_n\) =&nbsp;\(x\)&nbsp;<br></li><li>Pictographically: <img src="paste-dd9c9e556ab016c8ec5ac318d70901050a90a0c3.jpg"></li><li>\(x\)&nbsp;\(\in\)&nbsp;\(O_{\alpha^\prime}\) for some&nbsp;\(\alpha^\prime\)<br></li><li>So there exists&nbsp;\(r\)&nbsp;\(&gt;\)&nbsp;\(0\) such that&nbsp;\(\exists\)&nbsp;\(N_{r}(x)\)&nbsp;\(\subset\)&nbsp;\(\)&nbsp;&nbsp;\(O_{\alpha^\prime}\)</li><li>Since the intervals get arbitrarily small, there exists&nbsp;\(I_m\)&nbsp;\(\subset \)&nbsp;&nbsp;\(N_{r}(x)\)&nbsp; meaning that&nbsp; \(O_{\alpha^\prime}\) covers \(I_m\)</li><li>Which contradicts \(I_m\) not having a finite subcover</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1722183224988 (Block 1058) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Proof for the Heinel-Borel theorem in R: A set is K compact iff it is closed and bounded<br><ul><li>Backward direction, assume a set is closed and bounded</li><li>Since K is bounded&nbsp;\(K\)&nbsp;\(\subset\)&nbsp;\([-r, r]\) or of an n-cell if operating in higher dim spaces</li><ul><li>Since \([-r, r]\)&nbsp; is compact then K must be compact&nbsp;</li></ul></ul>
+
+============================================================
----------------------------

=== Note ID: 1722188431839 (Block 1059) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Proof for Bolzano-Weirstrass: every infinite bounded subset of&nbsp;\(R^n\) has a limit point:<br><ul><li>&nbsp;If a subset&nbsp;\(E\) is bounded then&nbsp;\(E\) is in some compact&nbsp;\(K\)-cell&nbsp;</li><li>Since it is a bounded subset of a compact set, it has a limit point in the&nbsp;\(K\)-cell by the Heinel-Borel theorem</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1722189318493 (Block 1060) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       What is Cantor's Finite Intersection property?<br><ul><li>A collection of compact subsets&nbsp;\(K_{\alpha}\) of some arbitrary metric space&nbsp;\(X\)</li><li>If any finite sub-collection has a non-empty intersection,&nbsp;\(\cap_{\alpha} K_{\alpha}\)&nbsp;\(\neq\)&nbsp;\(\emptyset\)</li><li>This is a generalization of the nested interval property to arbitrary metric spaces</li></ul><div>Proof:</div><div><ul><li>Assume that it is possible for a finite sub-collection to have a non-empty intersection but the intersection of all Ks to be empty</li><li>Let&nbsp;\(U_{\alpha}\) =&nbsp;\(K_{\alpha}^c\), which is open</li><li>Pick an arbitrary K in&nbsp;\(K_{\alpha}\)</li><li>If&nbsp;\(\cap_{\alpha}\)&nbsp;\(K_{\alpha}\) =&nbsp;\(\emptyset\) then&nbsp;\( \{ U_{\alpha} \}\) would cover&nbsp;\(K\) which we know to be compact</li><ul><li>This is because if no point is in the intersection than all points must be in the complement of one K</li></ul><li>Which implies there exists&nbsp;\(\{U_{\alpha_n} \}\) covering&nbsp;\(K\)</li><li>Implying that&nbsp; \( \cap_{i=1}^{n}\) \(K_{\alpha_i}\) \(\cap\) \(K\) =&nbsp;\(\emptyset\)</li></ul></div>
+
+============================================================
----------------------------

=== Note ID: 1723310652604 (Block 1061) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem: For \(\{K_{\alpha}\}\) compact subsets of a metric space X, if for any finite subcollection&nbsp;&nbsp;\(\cap_i\) \(K_{i}\)&nbsp;\(\neq\)&nbsp;\(\emptyset\) then&nbsp;&nbsp;\(\cap_\alpha\) \(K_{\alpha}\)&nbsp;\(\neq\)&nbsp;\(\emptyset\)&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1723386798926 (Block 1062) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       &nbsp;A space X is compact iff any collection of closed sets satisfied the finite intersection property<br><br>Proof:<br><ul><li>Assume X is compact and that we have a collection of closed sets&nbsp;\(D_{\alpha}\) \(\subset X\)</li><li>Since these are closed subsets of a compact space, they are also compact</li><li>Since they are compact, it is necessarily true that any finite subcollection having non-empty intersection implies&nbsp;&nbsp;\(\cap_{\alpha}\) \(D_{\alpha} \)&nbsp;\(\neq\)&nbsp;\(\emptyset\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1723387865343 (Block 1063) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       &nbsp;A space X is compact iff any collection of closed sets satisfied the finite intersection property (FIP)<br><br>Proof by contradiction:<br><ul><li>Assume that any collection of closed sets satisfied the FIP</li><li>Then, for any open cover&nbsp;\(O_{\alpha}\) its complement&nbsp;&nbsp;\(O_{\alpha}^c\) is closed</li><li>Thus \(\cap_{i} \) \(O_{i}^c\) \(\neq\)&nbsp;\(\emptyset\) implies \(\cap_{\alpha}\) \(O_{\alpha}^c\)&nbsp;\(\neq\)&nbsp;\(\emptyset\)</li><li>Negate the entire expression via de-morgan</li><li>If \(\cup_{\alpha}\) \(O_{\alpha}\)&nbsp;\(=\)&nbsp;\(X\) then \(\cup_{i}\) \(O_{i}\)&nbsp;\(=\)&nbsp;\(X\)</li><li>Thus every open cover has a finite subcover</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1723391775615 (Block 1064) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       What is a basis for the topology of a metric space?<br><ul><li>A basis is a collection&nbsp;\(\{V_{\alpha}\}\) such that<br></li><li>\(\forall\)&nbsp;\(x\)&nbsp;\(\in\)&nbsp;\(U\), where&nbsp;\(U\) is any open set<br></li><li>\(\exists\)&nbsp;\(V_{\beta}\) such that&nbsp;\(x\)&nbsp;\(\in\)&nbsp;\(\) \(V_{\beta}\) and \(V_{\beta}\)&nbsp;\(\subset\)&nbsp;\(U\)<br></li><li>Meaning that every open set is the union of base elements</li><li>Thus any set that that has a countable basis, must be small</li><li>And a compact metric space has a countable basis (countable dense subset)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1723392186211 (Block 1065) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       What is a separated set?<br><br><ul><li>Picographically connected vs separated: <img src="paste-2f5fb451489476cc9349ed525cfa095a422d333d.jpg"></li><li>Two sets&nbsp;\(A\) and&nbsp;\(B \) in a metric space&nbsp;\(X\) are separated if</li><li>\(A \cap \bar{B}\) =&nbsp;\(\emptyset\)<br></li><li>\(\bar{A}&nbsp;\cap B\) =&nbsp;\(\emptyset\)</li><li>And thus no limit point of one is a limit point of the other</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1723392239938 (Block 1066) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       When is a set E connected?<br><ul><li>A set E is connected if it is not the union of two separated sets</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1723393619709 (Block 1067) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Is the set of all pairs&nbsp; (x,y) of rational x,y connected? Why?<br><br><ul><li>Pictographically : <img src="paste-4a299da253efb63ed253fbc7e6d1895f3c9fb557.jpg"></li><li>Since we are only talking about rationals, dividing A and B by&nbsp;\(\pi\) means that neither contains &nbsp;\(\pi\) on the x-axis</li><li>Thus, while&nbsp;\(\pi\) is a limit point of both, the two sets are separated&nbsp;</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1723394324795 (Block 1068) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Proof that the any interval [a,b] is connected<br><ul><li>Assume there exists a separation A,B</li><li>with&nbsp;\(a\)&nbsp;\(\in A\) and&nbsp;\(b\)&nbsp;\(\in\)&nbsp;\(B\)</li><li>Then&nbsp;\(\bar{A} \cap B\)&nbsp;\(\neq\)&nbsp;\(\emptyset\) and vice-versa</li><li>Lets consider&nbsp;\(s\) \(=\) \(\sup\)&nbsp;\(A\)</li><li>Then \(s\)&nbsp;\(\in\)&nbsp;\(\bar{A}\) because it is either in the set or a limit point</li><li>Thus&nbsp;&nbsp;\(s\) \(\not \in\)&nbsp;\(B\)</li><li>Since \([a,b]\) =&nbsp;\(A \cup B\), then&nbsp;\(s\) must be in&nbsp;\(A\)</li><li>If&nbsp;\(s\)&nbsp;\(\in\)&nbsp;\(A\) then&nbsp;&nbsp;\(s\)&nbsp;\(\not \in\)&nbsp;\(\bar{B}\)</li><li>Then \(\exists \epsilon\) such that&nbsp;\(V_{\epsilon}(s)\)&nbsp;\(\not \in \)&nbsp;\(B\)&nbsp;</li><li>Since&nbsp;&nbsp;\([a,b]\) =&nbsp;\(A \cup B\),&nbsp; \(V_{\epsilon}(s)\)&nbsp;\(\in\)&nbsp;\(A\)</li><li>Which contradicts&nbsp;\(s\) being the supremum as a larger number exists in the nighbourhood.</li><li>If&nbsp;\(s\) is one of the endpoints we can use the infimum of the other set</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1723398212295 (Block 1069) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       The matchin problem continued:<br><ul><li>A deck of N cards</li><li>The event \(A_i\) is the event that the i-th card matches, i.e the i-th card in the deck has the number i on it</li><li>To compute \(P\)&nbsp;\(\cup_i A_i\) we need the probabilities</li><li>\(P\) ( \(\cap^k_{i=1} A_i\) ) for all k, by symmetry<br></li><li>\(P\) ( \(\cap^k_{i=1} A_i\) ) =&nbsp;\((n-k)!\)&nbsp;\(/\)&nbsp;\(n!\) by naive definition because we are assuming all orders of the n-k cards are equally likely while the first k are constrained<br></li><li>Importantly, there are&nbsp;\(\binom{n}{k}\) terms representing intersections of k events, all of which are the same by symmetry</li><li>By inclusion-exclusion</li><li>\(P\)&nbsp;\(\cup_i A_i\) =&nbsp;\(\sum_{k=1}\)&nbsp;\((-1)^{k-+1}\)\(\frac{n!}{(n-k)! k!}\) \(\frac{(n-k)!}{n!} \) = \(\sum_{k=1}\)&nbsp;\((-1)^{k+1}\)&nbsp;&nbsp;\(\frac{1}{k!}\)<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1723416117017 (Block 1070) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       When are three events independent?<br><ul><li>\(P(A,B)\)&nbsp; =&nbsp;\(P(A)\)&nbsp;\(P(B)\)<br></li><li>\(P(A,C)\)&nbsp; =&nbsp;\(P(A)\)&nbsp;\(P(C)\)</li><li>\(P(B,C)\)&nbsp; =&nbsp;\(P(B)\)&nbsp;\(P(C)\)</li><li>\(P(A,B, C)\)&nbsp; =&nbsp;\(P(A)\)&nbsp;\(P(B)\) \(P(C)\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1723416492358 (Block 1071) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       When are N events independent?<br><ul><li>Definition 2.5.6 (Independence of many events). For \(n\) events \(A_{1}, A_{2}, \ldots, A_{n}\) to be independent, we require any pair to satisfy \(P\) \(\left(A_{i} \cap A_{j}\right)\) = \(P\left(A_{i}\right) P\left(A_{j}\right)\) (for \(i \neq j\) ), any triplet to satisfy P\(\left(A_{i} \cap A_{j} \cap A_{k}\right)\)=\(P\left(A_{i}\right) P\left(A_{j}\right) P\left(A_{k}\right)\) (for \(i, j, k\) distinct), and similarly for all quadruplets, quintuplets, and so on.</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1723452922911 (Block 1072) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Newton Pepys problem: which is more likely<br><ol><li>(A) At least one 6 with 6 dice</li><li>(B) At least two 6s with 12 dice</li><li>(C) At least three 6s with 18 dice</li></ol><div>Solution:</div><div><ul><li><br></li><li>P( \(\cup C_i\) ) = 1 - P(&nbsp;\(\cap C_i\) &nbsp;) = 1 -&nbsp;\(\sum_{k=0}^{3}\)&nbsp;\(\binom{18}{k}\) \(\frac{1}{6^k}\) \((\frac{5}{6})^{18-k}\)<br></li><ul><li>\(\binom{18}{k}\) means the number of ways to choose k dice<br></li><li>\(&nbsp;\frac{1}{6^k}\) is the probability of getting that many sixes<br></li><li>&nbsp;\( (\frac{5}{6})^{18-k} \) is the probability that the other dice are non-sixes<br></li></ul><li>Pictographically: <img src="paste-26d144180ab020f962d9426d425c27fa1fc8973f.jpg"></li><li>This is called a binomial probability, replace 18 with n for the general case of n dice and change the 3 to i for any number of sixes</li></ul></div>
+
+============================================================
----------------------------

=== Note ID: 1723454519101 (Block 1073) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       What is the probability&nbsp;\(P(A \cap B)\), using conditional probability?<br><br><ul><li>\(P(A \cap B)\) = \(P(A | B)\)&nbsp;\(P(B)\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1723504460349 (Block 1074) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>A collection of sets has the Finite Intersection Condition if every finite subcollection has a non-empty intersection</li><li>A set is compact iff any collection of closed sets&nbsp;\(K_{\alpha}\) that has the finite intersection condition has a non-empty intersection</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1723505887015 (Block 1075) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>A sequence&nbsp;\(p_n\) in a metric space \(X\) converges if&nbsp;\(\exists\)&nbsp;\(p\)&nbsp;\(\in\)&nbsp;\(X\) such that&nbsp;\(\forall\)&nbsp;\(\epsilon\)&nbsp;\(&gt;\)&nbsp;\(0\),&nbsp;\(\exists\)&nbsp;\(N\) such that&nbsp;\(\forall\)&nbsp;\(n\)&nbsp;\(&gt;\)&nbsp;\(N\) implies \(d\)\((p_n,p)\)&nbsp;\(&lt;\)&nbsp;\(\epsilon\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1723533259765 (Block 1076) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div><br></div><div><ul><li>\(p_n\)&nbsp;\(\to\)&nbsp;\(p\)<br></li><li>\(\lim_{n \to \infty}\)&nbsp;\(p_n\)&nbsp;\(=\)&nbsp;\(p\)</li><li>\(p_n\) converges to p<br></li><li>\(p\) is the limit of&nbsp;\(p_n\)<br></li></ul></div>
+
+============================================================
----------------------------

=== Note ID: 1723534538215 (Block 1077) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div><div>What does&nbsp;\(p_n = \frac{n+1}{n}\) converge to in R?</div></div><div><ul><li><br></li><li>\(\forall\)&nbsp;\( \epsilon \)&nbsp;\(&gt;\)&nbsp;\(0\)<br></li><li>\(| \frac{n+1}{n} - 1|\)&nbsp;\(&lt;\)&nbsp;\(\epsilon\)<br></li><li>\(\forall\)&nbsp;\(n\)&nbsp;\(&gt;\) \(N\)<br></li><li>Because we can pick</li><li>\(N\)&nbsp;\(&gt;\)&nbsp;\(\frac{1}{\epsilon}\)<br></li><li>For&nbsp; example&nbsp;</li><li>\(N\)&nbsp;\(=\)&nbsp;\(\lceil \frac{1}{\epsilon} \rceil \)&nbsp; + \(1\)</li><li>If&nbsp;\(n\)&nbsp;\(\geq\)&nbsp;\(N\) then&nbsp;\(n\)&nbsp;\(&gt;\)&nbsp;\(\frac{1}{\epsilon}\) hence&nbsp;</li><li>\( |\frac{1}{n}| \)&nbsp;\(&lt;\)&nbsp;\(\frac{1}{\epsilon}\)<br></li></ul></div>
+
+============================================================
----------------------------

=== Note ID: 1723536739841 (Block 1078) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div>Why a sequence of a metric space&nbsp;\(X,d\) must have a unique limit</div><div><ul><li>Pictographically: <img src="paste-f588701d705dd4d9983372cdecf8b04e18d959f4.jpg"></li><li>Assume&nbsp;\(p_n\) converges to&nbsp;\(p\) and&nbsp;\(q\)</li><li>Let&nbsp;\(\epsilon\) =&nbsp;\(d(p,q)\)</li><li>\(\exists\)&nbsp;\(N_p\) such that&nbsp;\(n\) &gt;&nbsp;\(N_p\) implies&nbsp;\(d(p_n,p)\)&nbsp;\(&lt;\)&nbsp;\(\frac{\epsilon}{2}\)<br></li><li>Also</li><li>\(\exists\)&nbsp;\(N_q\) such that&nbsp;\(n\) &gt;&nbsp;\(N_q\) implies&nbsp;\(d(p_n,q)\)&nbsp;\(&lt;\)&nbsp;\(\frac{\epsilon}{2}\)</li><li>Finally,</li><li>Let&nbsp;\(N\) =&nbsp;\(\max\) ( \(N_p, N_q\) )</li><li>Then, \(\forall\)&nbsp;\(n\)&nbsp;\(&gt;\)&nbsp;\(N\)&nbsp;<br></li><ul><li>\(\epsilon\) = \(d(p,q)\)&nbsp;\(&lt;\) \(d(p,p_n)\) + \(d(p_n,q)\)&nbsp;\(&lt;\) \( \frac{\epsilon}{2}\) 7 \( \frac{\epsilon}{2}\) =&nbsp;\(\epsilon\)<br></li></ul><li>This proof could also have been done by showing that the distance between p,q must always be less than every epsilon, thus 0</li></ul></div>
+
+============================================================
----------------------------

=== Note ID: 1723536977905 (Block 1079) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       A convergent sequence must be bounded because:<br><ul><li>&nbsp;For&nbsp;\(\epsilon\) =&nbsp;\(1\)</li><li>\(\exists\)&nbsp;\(N\) sucht that&nbsp;\(d\)&nbsp;\((p_n,p)\)&nbsp;\(&lt;\)&nbsp;\(1\)<br></li><li>Let&nbsp;\(R\) =&nbsp;\(\max\)&nbsp;\((*d(p_n,p)\forall n\))</li><li>So all&nbsp;\(p_n\) are in&nbsp;\(\mathcal{N}_R(p)\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1723765778857 (Block 1080) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose we have a sequence \(s_n\in C\) if&nbsp;\(s_n \to s\) then&nbsp;\(c\)\(s_n\)&nbsp;\(\to\)&nbsp;\(c\)\(s\)<br><ul><li>For all&nbsp;\(\epsilon &gt;0\),&nbsp;\(\exists\)&nbsp;\(N\) such that&nbsp;\(\forall n &gt; N\),&nbsp; \(|\)&nbsp;\(s_n - s\) \(|\)&nbsp;\(&lt;\)&nbsp;\(\frac{\epsilon}{|c|}\)</li><li>Then</li><li>&nbsp;\(|\)&nbsp;\(c s_n - c s\) \(|\)<br></li><li>= \(|\)&nbsp;\(c (s_n - s)\) \(|\)</li><li>=&nbsp;&nbsp;\(|c|\)\(|\)&nbsp;\(s_n - s\) \(|\)</li><li>\(&lt;\)&nbsp;\(|c|\)\(\frac{\epsilon}{|c|}\)<br></li><li>\(&lt;\)&nbsp;\(\epsilon\)<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1723766593590 (Block 1081) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose we have twos sequencees \(s_n, t_n \in C\) if&nbsp;\(s_n \to s\) and \(t_n \to t\), then&nbsp;\((s_n t_n) \to (st)\)<br><ul><li>Both sequences must have a maximum, pick &nbsp;\(|M|\) =&nbsp;\(\max\) \((|M_s|,|M_t|)\)</li><li>\(\forall\)&nbsp;\(\epsilon &gt;0 \),&nbsp;\(\exists\)&nbsp;\(N_s\) such that&nbsp;\(|s_n -s| \)\(&lt;\)&nbsp;\(\frac{\epsilon}{|M|}\)<br></li><li>Analagous for&nbsp;\(t_n\), pick&nbsp;\(N\) =&nbsp;\(\max\) \((N_s,N_t)\)</li><li>\(| s_n t_n - st| \)<br></li><li>= \(| s_n t_n + s_n t - s_n t - st| \)</li><li>= \(| s_n (t_n - t)&nbsp; + t(s_n - s)| \)<br></li><li>\(\leq\)&nbsp;\(| s_n (t_n - t) |\) +&nbsp;\(|t(s_n - s)|\)<br></li><li>\(\leq\) \(| M (t_n - t) |\) +&nbsp;\(| M(s_n - s)|\)<br></li><li>\(\leq \)&nbsp;\(\epsilon\)<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1723767194714 (Block 1082) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose we have twos sequencees \(s_n, t_n \in C\) if&nbsp;\(s_n \to s\) and \(t_n \to t\), then&nbsp;\((s_n t_n) \to (st)\)<br><ul><li>Let&nbsp;\(K\) =&nbsp;\(\max\) &nbsp;\((s,t,1)\), the 1 is necesasry for later expansion</li><li>\(\forall \epsilon &gt; 0\),\(\exists\)&nbsp;\(N_1, N_2\) such that&nbsp;\(n\)&nbsp;\(&gt;\)&nbsp;\(N-1\) implies that&nbsp;\(s_n - s\)&nbsp;\(&lt;\)&nbsp;\(\frac{\epsilon}{3K}\), same for&nbsp;\(t_n\) and&nbsp;\(N_2\)<br></li><li>Pick&nbsp;\(N\) =&nbsp;\(\max\)&nbsp;\((N_1,N_2)\)</li><li>\(| s_n t_n - st|\)&nbsp;</li><li>\(\leq\)&nbsp;\(|\)\((s_n-s)\) \((t_n-t)\) + \(s(t_n-t)\) +&nbsp;\(t(s_n-s)\)&nbsp;\(|\)<br></li><li>\(&lt;\)&nbsp;\(\frac{\epsilon^2}{9K^2}\) + \(\frac{\epsilon}{3K}\) + \(\frac{\epsilon}{3K}\)<br></li><li>\(&lt;\)&nbsp;&nbsp;\(\frac{\epsilon}{9K}\) + \(\frac{\epsilon}{3K}\) + \(\frac{\epsilon}{3K}\)<br></li></ul><div><br></div>
+
+============================================================
----------------------------

=== Note ID: 1723810957675 (Block 1083) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Suppose&nbsp;\(\{p_n\}\) is a sequence, let&nbsp;\(n_1 \)&nbsp;\(&lt;\)&nbsp;\(n_2\)&nbsp;\(&lt;\)&nbsp;\(n_3\)&nbsp;\(\cdots\) in&nbsp;\(N\) be an increasing sequence, then&nbsp;\(\{p_{n_i}\}\) is a subsequence
+
+============================================================
----------------------------

=== Note ID: 1723822690735 (Block 1084) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       A sequence which does not converge may have a convergent subsequence.<br><br>For example:<br><ul><li>\(1,\pi, \frac{1}{2}, \pi, \frac{1}{4}, \cdots\)<br></li><li>Does not converge but has at least two convergent subsequences</li><li>\(\pi \cdots\)<br></li><li>And</li><li>\(\frac{1}{n}\)<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1723823462935 (Block 1085) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Every compact set is also sequentially compact
+
+============================================================
----------------------------

=== Note ID: 1723823689013 (Block 1086) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       In a compact metric space every sequence has a convergent subsequence with a limit in X, which means that if&nbsp;\(X\) is compact then&nbsp;\(X\) is sequentially compact
+
+============================================================
----------------------------

=== Note ID: 1723823764728 (Block 1087) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Converging to a point inside a metric space is crucial for understanding sequential compactness, this is why a series approaching&nbsp;\(\pi\) in&nbsp;\(Q\) does not converge to&nbsp;\(\pi\) since it is not in&nbsp;\(Q\)
+
+============================================================
----------------------------

=== Note ID: 1723824100900 (Block 1088) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Every bounded sequence in&nbsp;\(R^K\) contains a convergent subsequence <br>Proof:<br><ul><li>Despite&nbsp;\(R^K\) not being compact, since the bounded sequence lives in a compact subset of \(R^K\) and thus a compact subset of the metric space meaning it has a convergent subsequence in the compact subset and thus in \(R^K\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1723831147136 (Block 1089) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>A sequence&nbsp;\(\{p_n\}\) is Cacuchy in an abstract metric space&nbsp;\(X,d\) iff&nbsp;\(\forall\)&nbsp;\(\epsilon\)&nbsp;\(&gt;\)&nbsp;\(0\),&nbsp;\(\exists\)&nbsp;\(N\) such that&nbsp;\(\forall\)&nbsp;\(m,n\)&nbsp;\(&gt;\)&nbsp;\(N\),&nbsp;\(d\) \((p_m,p_n)\)&nbsp;\(&lt;\)&nbsp;\(\epsilon\)</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1723831323297 (Block 1090) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div><ul><li>If&nbsp;\(\{p_n\}\) converges then&nbsp;\(\{p_n\}\) is also Cauchy</li></ul><div>Proof Idea:</div></div><div><ul><li>Bound&nbsp;\(d\)\((p_n,p_m)\) based on the distances&nbsp;\(d\)\((p_n,p)\) and \(d\)\((p_m,p)\) using the triangle inequality</li></ul></div>
+
+============================================================
----------------------------

=== Note ID: 1723831623945 (Block 1091) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div><div><br></div><div>Proof that convegent sequences are Cauchy:</div></div><div><ul><li>Given&nbsp;\(\epsilon\)&nbsp;\(&gt;\)&nbsp;\(0\),&nbsp;\(\exists\)&nbsp;\(N\) such that&nbsp;\(d(p_n, p) \)&nbsp;\(&lt;\)&nbsp;\(\frac{\epsilon}{2}\) for all&nbsp;\(n\)&nbsp;\(&gt;\)&nbsp;\(N\)</li><li>For&nbsp;\(m,n\)&nbsp;\(&gt;\)&nbsp;\(N\):</li><ul><li>\(d\)\((p_n,p_m)\)&nbsp;</li><li>\(\leq\) \(d\)\((p_n,p)\) + \(d\)\((p_m,p)\) by triangle inequality<br></li><li>\(&lt;\)&nbsp;\(\frac{\epsilon}{2}\) +&nbsp;\(\frac{\epsilon}{2}\)<br></li><li>\(&lt;\)&nbsp;\(\epsilon\)<br></li></ul></ul></div>
+
+============================================================
----------------------------

=== Note ID: 1723831803863 (Block 1092) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div><div>Cauchy sequences are not always convergent on all metric spaces, for example a series approximating&nbsp;\(\pi\) in&nbsp;\(Q\) is Cauchy but not convergent because while the numbers get close to each other the limit cannot be&nbsp;\(\pi\) since&nbsp;\(\pi\)&nbsp;\(\not \in\) \(Q\)</div></div>
+
+============================================================
----------------------------

=== Note ID: 1723835029967 (Block 1093) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Proof the compact metric spaces are complete:<br><ul><li>Let&nbsp;\(\{x_n\}\) be a Cauchy sequence in the space&nbsp;\(X\)</li><li>&nbsp;Since&nbsp;\(X\) is compact it is also sequentially compact, meaning that every sequence has a convergent subsequence&nbsp;</li><li>As such there exists \(x_{n_k}\)&nbsp;\(\to\)&nbsp;\(x\)&nbsp;\(\in\)&nbsp;\(X\)</li><li>Pictographically: <img src="paste-22571f69de45ed76447262771338cf6e10c6c260.jpg"></li><li>Fix&nbsp;\(\epsilon\)&nbsp;\(&gt;\)&nbsp;\(0\)</li><li>\(x_n\) being Cauchy implies&nbsp;\(\exists\)&nbsp;\(N_1\) such that&nbsp;\(\forall i,j\)&nbsp;\(&gt;\)&nbsp;\(N\) then&nbsp;\(d\)\((x_i,x_j)\)&nbsp;\(&lt;\)&nbsp;\(\frac{\epsilon}{2}\)<br></li><li>\(\{x_{n_k}\}\)&nbsp;\(\to\)&nbsp;\(x\) implies that&nbsp;\(\forall\)&nbsp;\(n_k\)&nbsp;\(&gt;\)&nbsp;\(N_2\),&nbsp;\(d\)&nbsp;\((x_{n_k},x)\)&nbsp;\(&lt;\)&nbsp;\(\frac{\epsilon}{2}\)<br></li><li>Let \(N\) =&nbsp;\(\max\)&nbsp;\((N_1,N_2)\)</li><li>\(\forall\)&nbsp;\(n\)&nbsp;\(&gt;N\):<br></li><ul><li>\(d\)\((x_n,x)\)&nbsp;</li><li>\(\leq\)&nbsp;\(d\)&nbsp;\((x_n,x_{n_k})\) +&nbsp;&nbsp;\(d\)&nbsp;\((x_{n_k},x)\) for any&nbsp;\(n_k\)&nbsp;\(&gt;\)&nbsp;\(N\) , fix one</li><li>\(&lt;\)&nbsp;\(\frac{\epsilon}{2}\) <span style="font-size: 16.6667px;">+</span><span style="font-size: 16.6667px;">&nbsp;</span>\(\frac{\epsilon}{2}\)</li><li>\(&lt;\)&nbsp;\(\epsilon\)<br></li></ul><li>Since&nbsp;\(\epsilon\) was arbitrary, \(X\) is complete</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1723835099362 (Block 1094) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       [0,1] is a complete metric space since it is compact, similarly k-cells in&nbsp;\(R^n\) are complete
+
+============================================================
----------------------------

=== Note ID: 1723835277501 (Block 1095) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Since closed subsets of compact metric spaces are also compact, they are in fact complete. This surprisingly implies that the cantor set is complete
+
+============================================================
----------------------------

=== Note ID: 1723835639530 (Block 1096) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       \(R^n\) is complete proof:<br><ul><li>If&nbsp;\(x_n\) is Cauchy it is also bounded because</li><ul><li>&nbsp;for&nbsp;\(\epsilon\) =&nbsp;\(1\) there exists&nbsp;\(N\) such that&nbsp;\(d\)&nbsp;\((x_n,x_m)\)&nbsp;&nbsp;\(&lt;\)&nbsp;\(1\)&nbsp;\(\forall n,m &gt; N\).&nbsp;</li><li>Let&nbsp;\(R\) =&nbsp;\(\max\)&nbsp;\((d(x_n,x_m),1)\)&nbsp;\(\forall\)&nbsp;\(n,m\)</li><li>Then the sequence is bounded by &nbsp;\(N_{R}(x_n)\)</li></ul><li>Due to boundedness, \(N_{R}(x_n)\) is inside some k-cell in&nbsp;\(R^n\), thus the ball is complete</li><li>Thus&nbsp;\(x_n\) converges because the k-cell is complete</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1723836207695 (Block 1097) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Proof that the harmonic series&nbsp;\(x_n\)&nbsp;\(=\)&nbsp;\(\sum_{k=1}^n\)&nbsp;\(\frac{1}{k}\) does not converge using cauchyness:<br><ul><li>Consider, for&nbsp;\(n&gt;m\), \(| x_n - x_m|\)&nbsp;</li><li>=&nbsp;\(\sum_{k=n+1}^m\)\(\frac{1}{k}\)&nbsp;</li><li>\(&gt;\) \(\sum_{k=n+1}^m\)\(\frac{1}{n}\)&nbsp;</li><li>\(=\)&nbsp;\(\frac{n-m}{n}\)</li><li>\(=\)&nbsp;\(1 - \frac{n}{m}\)<br></li><li>Let&nbsp;\(n\) =&nbsp;\(2m\), then&nbsp;\(|x_{2n} - x_{m}|\)&nbsp;\(&gt;\)&nbsp;\(\frac{1}{2}\)</li><li>Thus, considering&nbsp;\(\epsilon\)&nbsp;\(&lt;\)&nbsp;\(\frac{1}{2}\) the sequence can never be cauchy so it does not converge</li><li>This show that we do not need to propose a limit to determine if a sequence converges in a complete metric space</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1723836908421 (Block 1098) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem: Every metric space&nbsp;\((X,d)\) has a completion, denoted ( \(X^*\),\(d^*\) ) such that&nbsp;\(X\)&nbsp;\(\subset\)\(X^*\) and&nbsp;\(d^*_{|X}\) =&nbsp;\(d\)
+
+============================================================
----------------------------

=== Note ID: 1734022442790 (Block 1099) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Theorem 3.4.5. The \(\operatorname{HGeom}(w, b, n)\) and \(\operatorname{HGeom}\) (n, w+b-n, w) distributions are identical. That is, if \(X \sim \operatorname{HGeom}(w, b, n)\) and \(Y \sim \operatorname{HGeom}\) (n, w+b-n, w), then \(X\) and \(Y\) have the same distribution.
+
+============================================================
----------------------------

=== Note ID: 1734406959248 (Block 1100) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       A closed subset of a compact set must be complete since closed subsets of compact sets are in fact compact, e.g, perhaps surprisingly the cantor set is complete
+
+============================================================
----------------------------

=== Note ID: 1734515001623 (Block 1101) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Proof that every metric space has a completion:<br><ol><li>Given&nbsp;\(X\), let&nbsp;\(X^*\) be the set of all cauchy sequences in&nbsp;\(X\) under an equivalence relation&nbsp;\(\approx\) where&nbsp;\(\{p_n\}\)&nbsp;\(\approx\)&nbsp;\(\{q_n\}\) iff&nbsp;\(\lim_{n \infty}\) \(d(p_n,q_n)\) =&nbsp;\(0\)</li><li>Denote arbitray equivalence classes \(E\) and \(R\) with&nbsp;\(E,R\)&nbsp;\(\in\)&nbsp;\(X^*\)</li><li>For example, we can pick&nbsp;\(\{e_n\}\) and&nbsp;\(\{r_n\}\) as representatives of the equivalence classes&nbsp;\(E,R\)</li><li>Define a distance between them:&nbsp;\(\Delta(E,R)\) =&nbsp;\(\lim_{n \to \infty}\)&nbsp;\(d(e_n,r_n)\) for two representatives where this limit is meaningful since the metric is a real number and thus we are taking the limit in R which is complete</li><li>The limit of the distances is guaranteed to exist because the sequences are cauchy</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1734517645048 (Block 1102) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Proof that bounded monotonically increasing sequences in R converge to their supremum:<br><ul><li>Since the sequnce is bounded,&nbsp;\(s_n\)&nbsp;\(\leq\)&nbsp;\(M\)</li><li>The elements of the sequence form a bounded nonempty subset of R</li><li>By the axiom of completeness, a supremum exists&nbsp;\(s\) =&nbsp;\(\sup(\{\mathrm{range}(s_n)\})\)</li><li>For any&nbsp;\(s_n\), \(\exists\)&nbsp;\(\epsilon^\prime &gt;0\) such that&nbsp;\(s - \epsilon^\prime \) &lt; \(s_n\) which implies that we&nbsp;\(|s - s_n|\)&nbsp;\(&lt;\)&nbsp;\(\epsilon^\prime\)</li><li>Now, given that&nbsp;\(\epsilon^\prime\) exists, we need to prove convergence for any&nbsp;\(\epsilon\) unconditional on picking an&nbsp;\(s_n\)</li><li>For any&nbsp;\(\epsilon\)&nbsp;\(&gt;\)&nbsp;\(0\), choose&nbsp;\(N\) such that&nbsp;\(|s_N - s|\)&nbsp;\(&lt;\)&nbsp;\(\epsilon\)</li><li>Since the sequence is monotonically increasing,&nbsp;\(\forall\)&nbsp;\(n \geq N\),&nbsp;\(|s_n - s|\)&nbsp;\(&lt;\)&nbsp;\(\epsilon\)&nbsp;</li></ul>
+
+============================================================
----------------------------

=== Note ID: 1734623101096 (Block 1103) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       What are the two comparison tests for series?<br><ol><li>If&nbsp;\(|a_n|\)&nbsp;\(\leq\)&nbsp;\(c_n\),&nbsp;\(\forall\)&nbsp;\(n\)&nbsp;\(\geq\)&nbsp;\(N\)-constant::condition for n::condition for n::condition for n::condition for n</li><ol><li>And&nbsp;\(\sum c_n\) converges, then&nbsp;\(\sum a_n\) converges</li></ol><li>If&nbsp;\(a_n\)&nbsp;\(&gt;\)&nbsp;&nbsp;\(d_n\)&nbsp;\(\geq\)&nbsp;\(0\),&nbsp;\(\forall\)&nbsp;\(n\)&nbsp;\(\geq\)&nbsp;\(N\)-constant::condition for n::condition for n::condition for n::condition for n</li><ol><li>If&nbsp;\(\sum d_n\) diverges, then&nbsp;\(\sum a_n\) diverges</li></ol></ol>
+
+============================================================
----------------------------

=== Note ID: 1734624964293 (Block 1104) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div><ol><li>If&nbsp;\(a_n\)&nbsp;\(&gt;\)&nbsp;&nbsp;\(d_n\)&nbsp;\(\geq\)&nbsp;\(0\),&nbsp;\(\forall\)&nbsp;\(n\)&nbsp;\(\geq\)&nbsp;\(N\)-constant</li><ol><li>If&nbsp;\(\sum d_n\) diverges, then&nbsp;\(\sum a_n\) diverges</li></ol></ol><div>Proof:</div></div><div><ol><li>We can prove the contrapositive by showing that&nbsp;\(\sum a_n\) convergent implies&nbsp;\(\sum\)&nbsp;\(d_n\) convergent, using the standard comparison test&nbsp;</li></ol></div><div><ol></ol></div>
+
+============================================================
----------------------------

=== Note ID: 1734694653923 (Block 1105) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div><ol><li>For&nbsp;\(a_1\)&nbsp;\(\geq\)&nbsp;\(a_2\)\(\ldots\)\(\geq\)\(a_n\)\(\geq\)\(0\), so monotone decreasing to 0</li><li>Then&nbsp;\(\sum a_k\) converges iff&nbsp;\(\sum 2^k a_{2^k}\)</li></ol><div>Proof idea:</div></div><div><ul><li>Write&nbsp;\(s_n\) =&nbsp;\(\sum_{k=1}^n a_k\) and&nbsp;\(t_n\) = \(\sum_{k=1}^n 2^k a_{2^k}\)<br></li><li>\(2\) \(s_n\) =&nbsp;\(2\)&nbsp;\(a_1\) <span style="font-size: 16.6667px;">+&nbsp;</span>\(2\)&nbsp;\(a_2\)+&nbsp;\(2\) (\(a_3+a_4\)) +&nbsp;\(2\) (\(a_5+a_6+a_7+a_8\))&nbsp;+&nbsp;\(\ldots\)<br></li><li>\(t_n\) =&nbsp;\(a_1\) + (\(a_2+a_2\)) +&nbsp;\(4 a_4\) +&nbsp;\(8 a_8\)+&nbsp;\(\ldots\)<br></li><li>Thus for&nbsp;\(n\)&nbsp;\(&gt;\)&nbsp;\(2^k\), then&nbsp;\(t_n\)&nbsp;\(&lt;\)&nbsp;\(2\)&nbsp;\(s_n\), since&nbsp;\(a_1\)&nbsp;\(&lt;\)&nbsp;\(2 a_1\),&nbsp;\(4 a_4\)&nbsp;\(&lt;\)\(2\) (\(a_3+a_4\)) since&nbsp;\(a_4\)&nbsp;\(&lt;\)&nbsp;\(a_3\) and so on</li><ul><li>If&nbsp;\(t_n\)&nbsp;\(&lt;\)&nbsp;\(2 s_n\) and&nbsp;\(t_n\) converges so does&nbsp;\(2\)&nbsp;\(s_n\) by comparison test</li><li>If&nbsp;\(2 s_n\) converges so does&nbsp;\(s_n\)</li></ul></ul></div>
+
+============================================================
----------------------------

=== Note ID: 1735161071198 (Block 1106) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ol><ol><li>Given a series&nbsp;\(\sum a_n\), then let&nbsp;\(\alpha\) =&nbsp;\(\lim\)&nbsp;\(\sup\)&nbsp;\(\sqrt[n]{a_n}\), which can be calculated in the extended reals</li><li>Then:</li><ol><li>\(\alpha\)&nbsp;\(\leq\)&nbsp;\(1\)&nbsp;\(\implies\)&nbsp;\(\sum a_n\) converges<br></li><li>\(\alpha\)&nbsp;\(&gt;\)&nbsp;\(1\)&nbsp;\(\implies\)&nbsp;\(\sum a_n\) diverges</li><li>\(\alpha\)&nbsp;\(=\)&nbsp;\(1\)&nbsp;\(\implies\) test inconclusive</li></ol></ol><div>Proof:</div><div><ol><li>If&nbsp;\(\alpha\)&nbsp;\(&gt;\)&nbsp;\(1\)</li><ol><li>There exists a subsequence \(\sqrt[n_k]{a_{n_k} }\)&nbsp; which converges to&nbsp;\(\alpha\)&nbsp;\(&gt;\)&nbsp;\(1\) which implies&nbsp;\(|a_{n_k}|\)&nbsp;\(&gt;\)&nbsp;\(1\) for infinitely many terms so \(a_n\) \(\not \to\)&nbsp;\(0\) meaning the series does not converge<br></li></ol></ol></div></ol>
+
+============================================================
----------------------------

=== Note ID: 1735173298900 (Block 1107) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If a series&nbsp;\(\sum\)&nbsp;\(a_n\) converges absolutely then it converges
+
+============================================================
----------------------------

=== Note ID: 1735248827610 (Block 1108) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Which of the following are continous/not and why?<br><img src="paste-1e90975d470adf571e57eb17cb93685201d5a13d.jpg"><br><ol><li>Continous, no gaps</li><li>Not continous, f(p) very disimilar to the limits from left and right</li><li>Not continous, discontinuity at p</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1735250443222 (Block 1109) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Sums and products of continous functions are continous when the domain is R
+
+============================================================
----------------------------

=== Note ID: 1735250492617 (Block 1110) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Quotients of continous functions with domain R \(\frac{f}{g}\) are also continous&nbsp; when&nbsp;\(g\)&nbsp;\(\neq\)&nbsp;\(0\)
+
+============================================================
----------------------------

=== Note ID: 1735253553382 (Block 1111) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       For&nbsp;\(f,g\)&nbsp;\(\in\)&nbsp;\(X\)&nbsp;\(\to\)&nbsp;\(R^k\) , which can be represented as&nbsp;\(f\) =&nbsp;\((f_1,\ldots,f_n)\) we can check if&nbsp;\(f\) is continous by checking if each indepdendent&nbsp;\(f_j\) is continous
+
+============================================================
----------------------------

=== Note ID: 1735303671022 (Block 1112) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       &nbsp;\(f\)&nbsp;\(:\)&nbsp;\(X\)&nbsp;\(\to\)&nbsp;\(Y\) is continous iff&nbsp;\(\forall\) open sets&nbsp;\(U\) in&nbsp;\(Y\),&nbsp;\(f^{-1}\)\((U)\) is open in&nbsp;\(X\)<br>Proof<br><ol><li><img src="paste-c034d751b0d8adaa299738bc65c8aa07760b1800.jpg"><br></li><li>For any \(U\)&nbsp;\(\in Y\) open, pick any point&nbsp;\(x\)&nbsp;\(\in\)&nbsp;\(f^{-1}(U)\), we want to show&nbsp;\(x\) is interior to \(f^{-1}(U)\)</li><li>Since U is open,&nbsp;\(f(x)\) is interior so there exists a neighbourhood \(N_{\epsilon}(f(x))\) =&nbsp;\(\{y \in Y, d(f(x),y) &lt; \epsilon\}\) with \(N_{\epsilon}(f(x))\)&nbsp;&nbsp;\(\subset\)&nbsp;\(U\)</li><li>By continuity of&nbsp;\(f\),&nbsp;\(\exists\) a \(\delta\)-ball&nbsp;\(N_{\delta}(x)\)&nbsp; such that \(N_{\delta}(x)\) is mapped into \(N_{\epsilon}(f(x))\)</li><li>Because \(N_{\epsilon}(f(x))\)&nbsp;\(\subset\)&nbsp;\(U\), this means \(N_{\delta}(x)\)&nbsp;\(\subset\)&nbsp;\(f^{-1}(U)\)</li><li>Which means&nbsp;\(x\) is an interior point of \(f^{-1}(U)\)&nbsp;</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1735306866611 (Block 1113) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       For&nbsp;\(f,g\) continous,&nbsp;\(f:\) \(X \to&nbsp; Y\) and&nbsp;\(g:\) \(Y \to Z\),&nbsp;\(g \circ f\) is continous
+
+============================================================
----------------------------

=== Note ID: 1735307615435 (Block 1114) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ol><li>\(f:\)\(X\)&nbsp;\(\to\)\(Y\) is continous iff&nbsp;\(\forall\)&nbsp;\(K\) closed in&nbsp;\(Y\),&nbsp;\(f^{-1}\)\((K)\) is closed in&nbsp;\(X\)</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1735316266082 (Block 1115) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If \(f\)\(:\)&nbsp;\(X \to Y\) is continous and&nbsp;\(X\) is compact, then \(f\)\((X)\) is compact
+
+============================================================
----------------------------

=== Note ID: 1735332107444 (Block 1116) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       For&nbsp;\(f\)\(:\)\(X\)\(\to\)\(Y\) continous, if&nbsp;\(E\) is a connected subset of&nbsp;\(X\), then&nbsp;\(f\)\((E)\) is connected
+
+============================================================
----------------------------

=== Note ID: 1735338731021 (Block 1117) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       For a function&nbsp;\(f:\)\((a,b)\)&nbsp;\(\to\)&nbsp;\(R\)<br><ol><li>If&nbsp;\(\forall\) sequences \(\{t_n\}\) \(\in\) \((x,b)\) with&nbsp;\(t_n\)&nbsp;\(\to\)&nbsp;\(x\),&nbsp;\(f\)\((t_n)\)&nbsp;\(\to\)&nbsp;\(q\), then&nbsp;\(f(x^+)\)&nbsp;\(=\)&nbsp;\(q\) =&nbsp;\(\lim_{t \to x^+}\) \(f(t) \) is the right-hand limit of&nbsp;\(f\)</li><li>Mutatis mulandis for left-hand limit&nbsp;&nbsp;\(f(x^-)\)&nbsp;\(=\)&nbsp;\(p\) =&nbsp;\(\lim_{t \to x^-}\) \(f(t) \)</li><li>If&nbsp;\(\lim_{t \to x}\) exists then \(\lim_{t \to x}\) = \(\lim_{t \to x^-}\) \(f(t) \) = \(\lim_{t \to x^+}\) \(f(t) \)&nbsp;</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1736250075071 (Block 1118) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If \(a\) \(&lt;\) \(b\) and \(c\) \(&lt;\) \(0\) then \(a c\) \(&gt;\) \(b c\) and \(\frac{a}{c}\) \(&gt;\) \(\frac{b}{c}\)
+
+============================================================
----------------------------

=== Note ID: 1736256944851 (Block 1119) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If \(b\) is a positive number<br><ol><li>\(|p|\) =\(b\) \(\Rightarrow\)&nbsp; \(p\) = \(-b\) or \(p\) =\(b\)<br></li></ol>
+
+============================================================
----------------------------

=== Note ID: 1736257041156 (Block 1120) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div>If \(b\) is a positive number<br></div><div><ol><li>\(|p|\) \(&lt;\) \(b\) \(\Rightarrow\) \(-b\) \(&lt;\) \(p\) \(&lt;\) \(b\)<br></li></ol></div>
+
+============================================================
----------------------------

=== Note ID: 1736257142252 (Block 1121) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       If \(b\) is a positive number<br><ol><li>\(|p|\)\(&gt;\)\(b\) \(\Rightarrow\) \(p\)\(&lt;\)\(-b\) or \(p\)\(&gt;\)\(b\)<br></li></ol>
+
+============================================================
----------------------------

=== Note ID: 1736354566042 (Block 1122) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ol><li>If \(r\) \(&gt;\) \(0\) then \(\lim _{x \rightarrow \infty}\) \(\frac{b}{x^{r}}\) = \(0\)<br></li></ol>
+
+============================================================
----------------------------

=== Note ID: 1736354611016 (Block 1123) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ol><li>If \(r\) \(&gt;\) \(0\) and \(x^{r}\) is real for negative \(x\) then \(\lim _{x \rightarrow-\infty}\) \(\frac{b}{x^{r}}\) = \(0\)<br></li></ol>
+
+============================================================
----------------------------

=== Note ID: 1736355594097 (Block 1124) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div>Note: \(\operatorname{sgn}\) (\(a\)) = \(1\) if \(a\) \(&gt;\) \(0\) and \(\operatorname{sgn}\)(\(a\)) = \(-1\) if \(a\) \(&lt;\) \(0\).</div>
+
+============================================================
----------------------------

=== Note ID: 1736356837024 (Block 1125) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ol><li>\(f(x)\) is continuous at \(b\) and \(\lim _{x \rightarrow a} g(x)\) = \(b\) then</li><ol><li>\(\lim _{x \rightarrow a} f(g(x))\)&nbsp;</li><li>= \(f\left(\lim _{x \rightarrow a} g(x)\right)\)</li><li>= \(f(b)\)</li></ol></ol>
+
+============================================================
----------------------------

=== Note ID: 1736424637639 (Block 1126) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ol><li>\(\frac{d}{d x}\)(\(\sec\) (\(x\)))=\(\sec\) (\(x\)) \(\tan\) (\(x\))</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1736425611697 (Block 1127) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ul><li>\(\frac{d}{d x}\)\(a^x\)=\(a^x\) \(\ln (a)\)<br></li></ul>
+
+============================================================
----------------------------

=== Note ID: 1736694561774 (Block 1128) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ol><li>\(\frac{d}{d x}\)\(\mathbf{e}^{f(x)}\)=\(\mathbf{e}^{f(x)}\)\(f^{\prime}(x)\)<br></li></ol>
+
+============================================================
----------------------------

=== Note ID: 1736694967335 (Block 1129) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ol><li>\(\frac{d}{d x}\) \(\sec\) [\(f(x)\)] = \(f^{\prime}(x)\) \(\sec\) [\(f(x)\)] \(\tan\) [\(f(x)\)]<br></li></ol>
+
+============================================================
----------------------------

=== Note ID: 1736696468981 (Block 1130) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Implicit diferentiation of&nbsp;\(\mathbf{e}^{2 x-9 y}+x^{3} y^{2}=\sin (y)+11 x\)<br><ol><li>\(\mathbf{e}^{2x - 9y}\)\(\left(2 - 9\frac{dy}{dx}\right)\) + \(3x^2y^2\) + \(2x^3y\frac{dy}{dx}&nbsp;\) = \(\cos(y)\)\(\frac{dy}{dx}\) + \(11\)&nbsp;</li><li>From this, put all terms with&nbsp;\(\frac{dy}{dx}\) on one side and then resolve&nbsp;\(\frac{dy}{dx}\), the final fraction may include both x and&nbsp;\(y\)</li><li>\(&nbsp;y^{\prime}=\frac{11-2 \mathbf{e}^{2 x-9 y}-3 x^2 y^2}{2 x^3 y-9 \mathbf{e}^{2 x-9 y}-\cos (y)}\)</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1736696740023 (Block 1131) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       \section*{Increasing/Decreasing}<br><ol><li>If \(f^{\prime}(x)\) &gt; \(0\) for all \(x\) in an interval \(I\) then \(f(x)\) is increasing on the interval \(I\).</li><li>If \(f^{\prime}(x)\) &lt; \(0\) for all \(x\) in an interval \(I\) then \(f(x)\) is decreasing on the interval \(I\).</li><li>If \(f^{\prime}(x)\) = \(0\) for all \(x\) in an interval \(I\) then \(f(x)\) is constant on the interval \(I\).<br></li></ol>
+
+============================================================
----------------------------

=== Note ID: 1736697014223 (Block 1132) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <br>1. If \(f^{\prime \prime}(x)\) \(&gt;\) \(0\) for all \(x\) in an interval \(I\) then \(f(x)\) is convex on the interval \(I\).<br>2. If \(f^{\prime \prime}(x)\) \(&lt;\) \(0\) for all \(x\) in an interval \(I\) then \(f(x)\) is concave on the interval \(I\).
+
+============================================================
----------------------------

=== Note ID: 1736706792454 (Block 1133) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       \section*{\(1^{\text {st }}\) Derivative Test}<br><br>If \(x=c\) is a critical point of \(f(x)\) then \(x=c\) is<br><ol><li>a relative maximum of \(f(x)\) if \(f^{\prime}(x)\) \(&gt;\) \(0\) to the left of \(x\) = \(c\) and \(f^{\prime}(x)\) \(&lt;\) \(0\) to the right of \(x=c\).</li><li>a relative minimum of \(f(x)\) if \(f^{\prime}(x)\) \(&lt;\) \(0\) to the left of \(x=c\) and \(f^{\prime}(x)\) \(&gt;\) \(0\) to the right of \(x=c\).</li><li>not a relative extrema of \(f(x)\) if \(f^{\prime}(x\) is the same sign on both sides of \(x=c\).</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1736707031062 (Block 1134) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       \(2^{\text {nd \)}} Derivative Test<br><br>If \(x=c\) is a critical point of \(f(x)\) such that \(f^{\prime}(c)\) = \(0\) then \(x=c\)<br><ol><li>is a relative maximum of \(f(x)\) if \(f^{\prime \prime}(c)\) \(&lt;\) \(0\).</li><li>is a relative minimum of \(f(x)\) if \(f^{\prime \prime}(c)\) \(&gt;\) \(0\).</li><li>may be a relative maximum, relative minimum, or neither if \(f^{\prime \prime}(c)\) = \(0\).</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1736707590657 (Block 1135) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ol><li>ab</li><li>ba</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1736709697269 (Block 1136) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ol><li>\(\int_a^b\) \(c\) \(d\) \(x\)=\(c\)\((b-a)\), if \(c \text { is a constant }\)<br></li></ol>
+
+============================================================
----------------------------

=== Note ID: 1736709985962 (Block 1137) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div><ol><li>If \(f(x)\) \(\geq\) \(g(x)\) on \(a \leq x \leq b\) then \(\int_{a}^{b}\) f(x) \(d x\) \(\geq\) \(\int_{a}^{b}\) \(g(x)\) \(d x\)</li></ol></div>
+
+============================================================
----------------------------

=== Note ID: 1736710043639 (Block 1138) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ol><li>If \(f(x)\) \(\geq\) \(0\) on \(a \leq x \leq b\) then \(\int_{a}^{b}\) \(f(x)\) \(d x\) \(\geq\) \(0\)<br></li></ol>
+
+============================================================
----------------------------

=== Note ID: 1736711022244 (Block 1139) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ol><li>\(\int\) \(k\) \(d x\)=\(k\) \(x\)\(+\)\(c\)<br></li></ol>
+
+============================================================
----------------------------

=== Note ID: 1736711066783 (Block 1140) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ol><li>\(\int\) \(\mathbf{e}^u\) \(d u\)=\(\mathbf{e}^u\)\(+c\)<br></li></ol>
+
+============================================================
----------------------------

=== Note ID: 1736711629984 (Block 1141) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ol><li>\(\int\) \(\csc\)(\(u\))\(\cot\)(\(u\)) \(d u\)=\(-\)\(\csc\)(\(u\))\(+c\)<br></li></ol>
+
+============================================================
----------------------------

=== Note ID: 1736711902332 (Block 1142) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div><ol><li>\(\int\) \(\ln\)(\(u\)) \(d u\)=\(u\)\(\ln\)(\(u\))\(-\)\(u\)\(+c\)</li></ol></div>
+
+============================================================
----------------------------

=== Note ID: 1736722322172 (Block 1143) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <br>3. \(\int_{-\infty}^{\infty}\) \(f(x) d x\)=\(\int_{-\infty}^{c}\) \(f(x)\) \(d x\)\(+\)\(\int_{c}^{\infty}\) \(f(x)\) \(d x\) provided both integrals are convergent.
+
+============================================================
----------------------------

=== Note ID: 1736722836249 (Block 1144) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Comparison Test for Improper Integrals: If \(f(x)\) \(\geq\) \(g(x)\) \(\geq\) \(0\) on \([a, \infty)\) then,<br><ol><li>If \(\int_{a}^{\infty} f(x) d x\) is convergent then \(\int_{a}^{\infty} g(x) d x\) is convergent (if larger converges so does the smaller).<br></li></ol>
+
+============================================================
----------------------------

=== Note ID: 1736722901856 (Block 1145) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div>Comparison Test for Improper Integrals: If \(f(x)\) \(\geq\) \(g(x)\) \(\geq\) \(0\) on \([a, \infty)\) then,<br></div><div><ol><li>If \(\int_{a}^{\infty} g(x) d x\) is divergent then \(\int_{a}^{\infty} f(x) d x\) is divergent (if smaller diverges so does the larger).<br></li></ol></div>
+
+============================================================
----------------------------

=== Note ID: 1736724263358 (Block 1146) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <div><div>Useful fact: If \(a\) \(&gt;\) \(0\) then \(\int_{a}^{\infty}\) \(\frac{1}{x^{p} }\) \(d x\) converges if \(p\)\(&gt;\) \(1\) and diverges for \(p\) \(\leq\) \(1\).<br></div></div>
+
+============================================================
----------------------------

=== Note ID: 1736804982531 (Block 1147) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 2.1.3 We define \(^3 1\) to be the number \(0++\), \(2\) to be the number \((0++)++\), 3 to be the number \(((0++)++)++\), etc. (In other words, \(1:=\)\(0++\) , \(2:\)=\(1++\), \(3\):= \(2++\), etc.) In this text I use " \(x\)\(:=\)\(y\) " to denote the statement that \(x\) is defined to equal \(y\).)
+
+============================================================
----------------------------

=== Note ID: 1736806205434 (Block 1148) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Proposition Template 2.1.11 A certain property \(P(n)\) is true for every natural number \(n\).<br><br>Proof Template:<br><ol><li>&nbsp;We use induction. We first verify the base case \(n=0\),</li><li>&nbsp;Now suppose inductively that \(n\) is a natural number, and \(P(n)\) has already been proven.&nbsp;</li><li>We now prove \(P(n++)\). , assuming that \(P(n)\) is true</li><li>&nbsp;This closes the induction, and thus \(P(n)\) is true for all numbers \(n\).</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1736808209528 (Block 1149) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Proposition 2.2.6 (Cancellation law). Let \(a, b, c\) be natural numbers such that \(a+\) \(b\)=\(a+c\). Then we have \(b\)=\(c\).
+
+============================================================
----------------------------

=== Note ID: 1736808328877 (Block 1150) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Proposition 2.2.8 If a is a positive natural number, and \(b\) is a natural number, then \(a+b\) is positive&nbsp;
+
+============================================================
----------------------------

=== Note ID: 1736808388988 (Block 1151) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Corollary 2.2.9 If \(a\) and \(b\) are natural numbers such that \(a+b\)=\(0\), then \(a\)=\(0\) and \(b\)=\(0\).
+
+============================================================
----------------------------

=== Note ID: 1736811028855 (Block 1152) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <li><br class="Apple-interchange-newline">Proposition 2.2.12 (Basic properties of order for natural numbers). Let \(a, b, c\) be natural numbers. Then</li><li>(a) (Order is reflexive) \(a \geq a\).</li><li>(b) (Order is transitive) If \(a \geq b\) and \(b \geq c\), then \(a \geq c\).</li><li>(c) (Order is antisymmetric) If \(a \geq b\) and \(b \geq a\), then \(a=b\).</li><li>(d) (Addition preserves order) \(a \geq b\) if and only if \(a+c \geq b+c\).</li><li>(e) \(a&lt;b\) if and only if \(a+1 \leq b\).</li><li>(f) \(a&lt;b\) if and only if \(b=a+d\) for some positive number \(d\).</li>
+
+============================================================
----------------------------

=== Note ID: 1736811886483 (Block 1153) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Definition 2.3.11 (Exponentiation for natural numbers). Let \(m\) be a natural number. To raise \(m\) to the power 0 , we define \(m^0\)\(:=\)\(1\); in particular, we define \(0^0\)\(:=\)\(1\). Now suppose recursively that \(m^n\) has been defined for some natural number \(n\), then we define \(m^{n++}\)\(:=\)\(m^n \times m\).
+
+============================================================
----------------------------

=== Note ID: 1736854921606 (Block 1154) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Axiom 3.3 (Empty set). There exists a set \(\emptyset\), known as the empty set, which contains no elements, i.e., for every object \(x\) we have \(x\) \(\notin\) \(\varnothing\).
+
+============================================================
----------------------------

=== Note ID: 1736855115749 (Block 1155) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Lemma 3.1.5 (Single choice). Let A be a non-empty set. Then there exists an object \(x\) such that \(x\) \(\in\) \(A\).
+
+============================================================
----------------------------

=== Note ID: 1736855947662 (Block 1156) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Axiom 3.4 (Singleton sets). <br><ol><li>If \(a\) is an object, then there exists a set \(\{a\}\) whose only element is \(a\), i.e., for every object \(y\), we have \(y\) \(\in\{a\}\) if and only if \(y\)=\(a\); we refer to \(\{a\}\) as the singleton set whose element is \(a\).&nbsp;</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1736856056526 (Block 1157) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       <ol><li>Furthermore, if \(a\) and \(b\) are objects, then there exists a set \(\{a, b\}\) whose only elements are \(a\) and \(b\); i.e., for every object \(y\), we have \(y\) \(\in\)\(\{a, b\}\) if and only if \(y\)=\(a\) or \(y\)=\(b\); we refer to this set as the pair set formed by \(a\) and \(b\).</li></ol>
+
+============================================================
----------------------------

=== Note ID: 1736857955428 (Block 1158) ===
  => Content differs beyond just cloze markup!
----- Diff (Normalized) -----
--- before (normalized)
+++ after (normalized)
@@ -1 +1,3 @@
       Lemma 3.1.12 If \(a\) and \(b\) are objects, then \(\{a, b\}\)=\(\{a\} \cup\{b\}\). If \(A, B, C\) are sets, then the union operation is commutative and associative. Also, we have \(A\) \(\cup\) \(A\)=\(A\) \(\cup\) \(\emptyset\)=\(\emptyset\) \(\cup\) \(A\)=\(A\).
+
+============================================================
----------------------------

